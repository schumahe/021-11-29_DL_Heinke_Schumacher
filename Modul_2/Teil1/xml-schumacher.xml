<?xml version="1.0" encoding="UTF-8"?>
<publications>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/248ca0b845a2f67f5923eafd9426a20ed/aucelum</id>
		<tags>music</tags>
		<tags>mir</tags>
		<tags>mem</tags>
		<description></description>
		<date>2010-02-02 17:03:01</date>
		<count>1</count>
		<journal>Journal of Research in Music Education</journal>
		<publisher>MENC: The National Association for Music Education</publisher>
		<year>1974</year>
		<url></url>
		<author>William S. Lane</author>
		<authors>
			<first>William S.</first>
		</authors>
		<authors>
			<last>Lane</last>
		</authors>
		<volume>22</volume>
		<number>4</number>
		<pages>251--257</pages>
		<abstract>Techniques and procedures were developed for cataloging, indexing, and storing information concerning diverse music education materials in a computer-assisted information retrieval system. Indexing procedures and bibliographic file formats were developed, and a data base of 830 documents was created. One subject area was indexed for content and a thesaurus of descriptors developed for it. The subfile so indexed consisted of 107 documents. Searches were made of the data base, with completely accurate results demonstrating that many different types of music education materials can be efficiently manipulated in a computer-assisted information retrieval system.</abstract>
		<issn>00224294</issn>
		<copyright>Copyright © 1974 MENC: The National Association for Music Education</copyright>
		<title>The Application of Information Science Technology to Music Education Materials</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/220bd2b6e2398f3e89b1a0d1e9693935d/moustaki</id>
		<tags>music</tags>
		<tags>cognition</tags>
		<tags>analysis</tags>
		<description>My bibtex file</description>
		<date>2007-03-28 16:44:39</date>
		<count>1</count>
		<journal>Music Analysis</journal>
		<year>1998</year>
		<url>/home/moustaki/work/phd/papers/ai/Music analysis.html</url>
		<author>Ian Cross</author>
		<authors>
			<first>Ian</first>
		</authors>
		<authors>
			<last>Cross</last>
		</authors>
		<volume>17</volume>
		<pdf>/home/moustaki/work/phd/papers/ai/Music analysis.html</pdf>
		<title>Music Analysis and Music Perception</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20707beab54c8a6d6327db84b684b6ebe/tfalk</id>
		<tags>music</tags>
		<tags>preferences</tags>
		<tags>lifestyle</tags>
		<description>Lifestyle correlates of musical preference: 2. Media, leisure time
	and music -- North and Hargreaves 35 (2): 179 -- Psychology of Music</description>
		<date>2009-02-28 21:01:39</date>
		<count>2</count>
		<journal>Psychology of Music</journal>
		<year>2007</year>
		<url>http://pom.sagepub.com/cgi/content/abstract/35/2/179</url>
		<author>Adrian C. North</author>
		<author>David J. Hargreaves</author>
		<authors>
			<first>Adrian C.</first>
		</authors>
		<authors>
			<last>North</last>
		</authors>
		<authors>
			<first>David J.</first>
		</authors>
		<authors>
			<last>Hargreaves</last>
		</authors>
		<volume>35</volume>
		<number>2</number>
		<pages>179-200</pages>
		<abstract>Several studies indicate that musical preferences provide a means
	of discriminating between social groups, and suggest indirectly that
	musical preferences should correlate with a variety of different
	lifestyle choices. In this study, 2532 participants responded to
	a questionnaire asking them to state their musical preference and
	also to provide data on various aspects of their lifestyle (namely
	media preferences, leisure interests and music usage). Numerous associations
	existed between musical preference and these aspects of participants'
	lifestyle. The nature of these associations was consistent in part
	with previous research on taste publics concerning the high-culture-low-culture
	divide, such that fans of high-art' and low-art' musical styles demonstrated
	a preference for other high-art' and low-art' media objects respectively,
	as reflected in reading, TV and radio preferences, and leisure activities.</abstract>
		<doi>10.1177/0305735607070302</doi>
		<eprint>http://pom.sagepub.com/cgi/reprint/35/2/179.pdf</eprint>
		<title>Lifestyle correlates of musical preference: 2. Media, leisure time
	and music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29b923cd9c353ec76e96006299f929073/tfalk</id>
		<tags>emotion</tags>
		<tags>music</tags>
		<description>Using music to induce emotions: Influences of musical preference and
	absorption -- Kreutz et al. 36 (1): 101 -- Psychology of Music</description>
		<date>2009-02-28 21:01:39</date>
		<count>2</count>
		<journal>Psychology of Music</journal>
		<year>2008</year>
		<url>http://pom.sagepub.com/cgi/content/abstract/36/1/101</url>
		<author>Gunter Kreutz</author>
		<author>Ulrich Ott</author>
		<author>Daniel Teichmann</author>
		<author>Patrick Osawa</author>
		<author>Dieter Vaitl</author>
		<authors>
			<first>Gunter</first>
		</authors>
		<authors>
			<last>Kreutz</last>
		</authors>
		<authors>
			<first>Ulrich</first>
		</authors>
		<authors>
			<last>Ott</last>
		</authors>
		<authors>
			<first>Daniel</first>
		</authors>
		<authors>
			<last>Teichmann</last>
		</authors>
		<authors>
			<first>Patrick</first>
		</authors>
		<authors>
			<last>Osawa</last>
		</authors>
		<authors>
			<first>Dieter</first>
		</authors>
		<authors>
			<last>Vaitl</last>
		</authors>
		<volume>36</volume>
		<number>1</number>
		<pages>101-126</pages>
		<abstract>The present research addresses the induction of emotion during music
	listening in adults using categorical and dimensional theories of
	emotion as background. It further explores the influences of musical
	preference and absorption trait on induced emotion. Twenty-five excerpts
	of classical music representing `happiness', `sadness', `fear', `anger'
	and `peace' were presented individually to 99 adult participants.
	Participants rated the intensity of felt emotions as well as the
	pleasantness and arousal induced by each excerpt. Mean intensity
	ratings of target emotions were highest for 20 out of 25 excerpts.
	Pleasantness and arousal ratings led to three main clusters within
	the two-dimensional circumplex space. Preference for classical music
	significantly influenced specificity and intensity ratings across
	categories. Absorption trait significantly correlated with arousal
	ratings only. In sum, instrumental music appears effective for the
	induction of basic emotions in adult listeners. However, careful
	screening of participants in terms of their musical preferences should
	be mandatory.</abstract>
		<doi>10.1177/0305735607082623</doi>
		<eprint>http://pom.sagepub.com/cgi/reprint/36/1/101.pdf</eprint>
		<title>Using music to induce emotions: Influences of musical preference
	and absorption</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22a219bd790ad85a6bbbd271c93390239/syslogd</id>
		<tags>filtering,</tags>
		<tags>collaborative</tags>
		<tags>genre</tags>
		<tags>classification,</tags>
		<tags>Classification,</tags>
		<tags>taxonomy</tags>
		<description></description>
		<date>2010-01-07 13:57:09</date>
		<count>9</count>
		<journal>Journal of New Music Research</journal>
		<publisher>Routledge</publisher>
		<year>2003</year>
		<url>http://www.idiap.ch/~paiement/references/to_read/music/genre_classification/pachet-02c.pdf</url>
		<author>Jean-Julien Aucouturier</author>
		<author>Francois Pachet</author>
		<authors>
			<first>Jean-Julien</first>
		</authors>
		<authors>
			<last>Aucouturier</last>
		</authors>
		<authors>
			<first>Francois</first>
		</authors>
		<authors>
			<last>Pachet</last>
		</authors>
		<volume>32</volume>
		<number>1</number>
		<pages>83--93</pages>
		<abstract>Musical genre is probably the most popular music descriptor. In the context of large musical databases and Electronic Music Distribution, genre is therefore a crucial metadata for the description of music content. However, genre is intrinsically ill-defined and attempts at defining genre precisely have a strong tendency to end up in circular, ungrounded projections of fantasies. Is genre an intrinsic attribute of music titles, as, say, tempo? Or is genre a extrinsic description of the whole piece? In this article, we discuss the various approaches in representing musical genre, and propose to classify these approaches in three main categories: manual, prescriptive and emergent approaches. We discuss the pros and cons of each approach, and illustrate our study with results of the Cuidado IST project.</abstract>
		<title>Representing Musical Genre: A State of the Art</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2395508181dedc0e22f3f3e533984a80f/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-11-29 00:00:00</date>
		<count>1</count>
		<booktitle>Ubiquitous Music</booktitle>
		<series>Computational Music Science</series>
		<publisher>Springer</publisher>
		<year>2014</year>
		<url>http://dblp.uni-trier.de/db/series/music/um2014.html#PimentaKFLL14</url>
		<author>Marcelo Soares Pimenta</author>
		<author>Damián Keller</author>
		<author>Luciano Vargas Flores</author>
		<author>Maria Helena de Lima</author>
		<author>Victor Lazzarini</author>
		<authors>
			<first>Marcelo Soares</first>
		</authors>
		<authors>
			<last>Pimenta</last>
		</authors>
		<authors>
			<first>Damián</first>
		</authors>
		<authors>
			<last>Keller</last>
		</authors>
		<authors>
			<first>Luciano Vargas</first>
		</authors>
		<authors>
			<last>Flores</last>
		</authors>
		<authors>
			<first>Maria Helena</first>
		</authors>
		<authors>
			<last>de Lima</last>
		</authors>
		<authors>
			<first>Victor</first>
		</authors>
		<authors>
			<last>Lazzarini</last>
		</authors>
		<editor>Damián Keller</editor>
		<editor>Victor Lazzarini</editor>
		<editor>Marcelo Soares Pimenta</editor>
		<editors>
			<first>Victor</first>
		</editors>
		<editors>
			<last>Lazzarini</last>
		</editors>
		<editors>
			<first>Victor</first>
		</editors>
		<editors>
			<last>Lazzarini</last>
		</editors>
		<editors>
			<first>Victor</first>
		</editors>
		<editors>
			<last>Lazzarini</last>
		</editors>
		<pages>25-48</pages>
		<isbn>978-3-319-11151-3</isbn>
		<title>Methods in Creativity-Centred Design for Ubiquitous Musical Activities.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/262d4aa904f16830f8048b2d64c8055da/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-11-29 00:00:00</date>
		<count>1</count>
		<booktitle>Ubiquitous Music</booktitle>
		<series>Computational Music Science</series>
		<publisher>Springer</publisher>
		<year>2014</year>
		<url>http://dblp.uni-trier.de/db/series/music/um2014.html#LazzariniKPT14</url>
		<author>Victor Lazzarini</author>
		<author>Damián Keller</author>
		<author>Marcelo Soares Pimenta</author>
		<author>Joseph Timoney</author>
		<authors>
			<first>Victor</first>
		</authors>
		<authors>
			<last>Lazzarini</last>
		</authors>
		<authors>
			<first>Damián</first>
		</authors>
		<authors>
			<last>Keller</last>
		</authors>
		<authors>
			<first>Marcelo Soares</first>
		</authors>
		<authors>
			<last>Pimenta</last>
		</authors>
		<authors>
			<first>Joseph</first>
		</authors>
		<authors>
			<last>Timoney</last>
		</authors>
		<editor>Damián Keller</editor>
		<editor>Victor Lazzarini</editor>
		<editor>Marcelo Soares Pimenta</editor>
		<editors>
			<first>Joseph</first>
		</editors>
		<editors>
			<last>Timoney</last>
		</editors>
		<editors>
			<first>Joseph</first>
		</editors>
		<editors>
			<last>Timoney</last>
		</editors>
		<editors>
			<first>Joseph</first>
		</editors>
		<editors>
			<last>Timoney</last>
		</editors>
		<pages>129-150</pages>
		<isbn>978-3-319-11151-3</isbn>
		<title>Ubiquitous Music Ecosystems: Faust Programs in Csound.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/244dace058984bb1fb9328da434f61dc4/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-11-29 00:00:00</date>
		<count>1</count>
		<booktitle>Ubiquitous Music</booktitle>
		<series>Computational Music Science</series>
		<publisher>Springer</publisher>
		<year>2014</year>
		<url>http://dblp.uni-trier.de/db/series/music/um2014.html#BrownSHS14</url>
		<author>Andrew R. Brown</author>
		<author>Donald Stewart</author>
		<author>Amber Hansen</author>
		<author>Alanna Stewart</author>
		<authors>
			<first>Andrew R.</first>
		</authors>
		<authors>
			<last>Brown</last>
		</authors>
		<authors>
			<first>Donald</first>
		</authors>
		<authors>
			<last>Stewart</last>
		</authors>
		<authors>
			<first>Amber</first>
		</authors>
		<authors>
			<last>Hansen</last>
		</authors>
		<authors>
			<first>Alanna</first>
		</authors>
		<authors>
			<last>Stewart</last>
		</authors>
		<editor>Damián Keller</editor>
		<editor>Victor Lazzarini</editor>
		<editor>Marcelo Soares Pimenta</editor>
		<editors>
			<first>Alanna</first>
		</editors>
		<editors>
			<last>Stewart</last>
		</editors>
		<editors>
			<first>Alanna</first>
		</editors>
		<editors>
			<last>Stewart</last>
		</editors>
		<editors>
			<first>Alanna</first>
		</editors>
		<editors>
			<last>Stewart</last>
		</editors>
		<pages>65-81</pages>
		<isbn>978-3-319-11151-3</isbn>
		<title>Making Meaningful Musical Experiences Accessible Using the iPad.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2284223d78cd9cb37849740657732133b/yevb0</id>
		<tags>music\_cognition,perception,syntax,temp</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Music Perception</journal>
		<year>2007</year>
		<url>http://ucp.literatumonline.com/doi/abs/10.1525/mp.2007.24.5.485</url>
		<author>Elizabeth H. Margulis</author>
		<authors>
			<first>Elizabeth H.</first>
		</authors>
		<authors>
			<last>Margulis</last>
		</authors>
		<volume>24</volume>
		<number>5</number>
		<pages>485--506</pages>
		<abstract>SILENCES IN MUSIC ARE DISTINGUISHED acoustically along only one dimension:
	the length of time they occupy. However, like pauses in speech, they
	are distinguished syntactically along many dimensions, depending
	on the context in which they occur. In two experiments, one using
	musical excerpts from commercially available recordings, and the
	other using simpler constructed excerpts, participants' reactions
	to silences were assessed. Participants pressed a button when they
	heard a period of silence begin and end, moved a slider to indicate
	perceived changes in musical tension across the course of each excerpt,
	and answered a series of questions about each silence, including
	questions about its duration, placement, salience, and metric qualities.
	Musical context, especially tonal context, affected the response
	to silence as measured by all three tasks. Specifically, silences
	following tonal closure were identified more quickly and perceived
	as less tense than silences following music lacking such closure.</abstract>
		<doi>10.1525/mp.2007.24.5.485</doi>
		<title>Silences in Music are Musical Not Silent: \A\n Exploratory Study
	of Context Effects on the Experience of Musical Pauses</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e35cb5547e626cd8863a069478fca835/knaevelboerrar</id>
		<tags>music</tags>
		<tags>mathematics</tags>
		<description>Musimathics: The Mathematical Foundations of Music, Volume 1: Amazon.de: Gareth Loy: Fremdsprachige Bücher</description>
		<date>2015-12-22 16:06:54</date>
		<count>1</count>
		<publisher>MIT Press</publisher>
		<address>Cambridge, Mass.; London</address>
		<year>2006</year>
		<url>http://www.amazon.de/Musimathics-Mathematical-Foundations-Music-Volume/dp/0262122820/ref=pd_sim_14_2?ie=UTF8&dpID=41G7IKtZIjL&dpSrc=sims&preST=_AC_UL160_SR113%2C160_&refRID=0FBZ3EKEMPCWGBFGSM5T</url>
		<author> na</author>
		<authors>
			<first></first>
		</authors>
		<authors>
			<last>na</last>
		</authors>
		<abstract>"In this volume, Gareth Loy presents the materials of music (notes, intervals, and scales); the physical properties of music (frequency, amplitude, duration, and timbre); the perception of music and sound (how we hear); and music composition. Musimathics is carefully structured so that new topics depend strictly on topics already presented, carrying the reader progressively from basic subjects to more advanced ones. Cross-references point to related topics and an extensive glossary defines commonly-used terms. The book explains the mathematics and physics of music for the reader whose mathematics may not have gone beyond the early undergraduate level. Calling himself ä composer seduced into mathematics," Loy provides answers to foundational questions about the mathematics of music accessibly yet rigorously. The topics are all subjects that contemporary composers, musicians, and musical engineers have found to be important. The examples given are all practical problems in music and audio. The level of scholarship and the pedagogical approach also make Musimathics ideal for classroom use. Additional material can be found at http://www.musimathics.com Publisher description of vol. 1. Volume 2 of Musimathics continues the story of music engineering begun in volume 1, focusing on the digital and computational domain. Loy goes deeper into the mathematics of music and sound, beginning with digital audio, sampling, and binary numbers, as well as complex numbers and how they simplify representation of musical signals. Chapters cover the Fourier transform, convolution, filtering, resonance, the wave equation, acoustical systems, sound synthesis, the short-time Fourier transform, and the wavelet transform. These subjects provide the theoretical underpinnings of today's music technology. The examples given are all practical problems in music and audio Publisher description.</abstract>
		<isbn>9780262122825 0262122820 9780262122856 0262122855 9780262516563 026251656X</isbn>
		<title>Musimathics : the mathematical foundations of music</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a2429cb8932298245ba887ad497d88b2/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>Proceedings of the 12th International Conference on Music Perception and Cognition (ICMPC) and 8th Triennial Conference of the European Society for the Cognitive Sciences of Music (ESCOM)</booktitle>
		<series>Proceedings of the 12th International Conference on Music Perception and Cognition and the 8th Triennial Conference of the European Society for the Cognitive Sciences of Music</series>
		<publisher>School of Music Studies, Aristotle University of Thessaloniki</publisher>
		<year>2012</year>
		<url></url>
		<author>Anja Volk</author>
		<author>W. Bas De Haas</author>
		<author>Peter van Kranenburg</author>
		<authors>
			<first>Anja</first>
		</authors>
		<authors>
			<last>Volk</last>
		</authors>
		<authors>
			<first>W. Bas De</first>
		</authors>
		<authors>
			<last>Haas</last>
		</authors>
		<authors>
			<first>Peter</first>
		</authors>
		<authors>
			<last>van Kranenburg</last>
		</authors>
		<pages>1085--1094</pages>
		<abstract>This paper investigates the concept of variation in music from the
	perspective of music similarity. Music similarity is a central concept
	in Music Information Retrieval (MIR), however there exists no comprehensive
	approach to music similarity yet. As a consequence, MIR faces the
	challenge on how to relate musical features to the experience of
	similarity by listeners. Musicologists and studies in music cognition
	have argued that variation in music leads to the experience of similarity.
	In this paper we review the concept of variation from three different
	research strands: MIR, Musicology, and cognitive Science. We show
	that all of these disciplines have contributed insights to the study
	of variation that are important for modelling variation as a foundation
	for similarity. We introduce research steps that need to be taken
	to model variation as a base for music similarity estimation within
	a computational approach.</abstract>
		<title>Towards Modelling Variation in Music as Foundation for Similarity</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>http://hdl.handle.net/10230/45719</id>
		<tags>coverdetection</tags>
		<tags>cover</tags>
		<tags>retrieval</tags>
		<tags>music</tags>
		<tags>detection</tags>
		<tags>mir</tags>
		<tags>feature</tags>
		<tags>information</tags>
		<description>Combining musical features for cover detection</description>
		<date>2021-06-03 14:20:35</date>
		<count>1</count>
		<publisher>International Society for Music Information Retrieval (ISMIR)</publisher>
		<year>2020</year>
		<url>https://repositori.upf.edu/handle/10230/45719</url>
		<author>Guillaume Doras</author>
		<author>Furkan Yesiler</author>
		<author>Joan Serrà Julià</author>
		<author>1975 Gómez Gutiérrez, Emilia</author>
		<author>Geoffroy Peeters</author>
		<authors>
			<first>Guillaume</first>
		</authors>
		<authors>
			<last>Doras</last>
		</authors>
		<authors>
			<first>Furkan</first>
		</authors>
		<authors>
			<last>Yesiler</last>
		</authors>
		<authors>
			<first>Joan</first>
		</authors>
		<authors>
			<last>Serrà Julià</last>
		</authors>
		<authors>
			<first>1975</first>
		</authors>
		<authors>
			<last>Gómez Gutiérrez, Emilia</last>
		</authors>
		<authors>
			<first>Geoffroy</first>
		</authors>
		<authors>
			<last>Peeters</last>
		</authors>
		<abstract>Comunicació presentada a: International Society for Music Information Retrieval Conference celebrat de l'11 al 16 d'octubre de 2020 de manera virtual., Recent works have addressed the automatic cover detection problem from a metric learning perspective. They employ different input representations, aiming to exploit melodic or harmonic characteristics of songs and yield promising performances. In this work, we propose a comparative study of these different representations and show that systems combining melodic and harmonic features drastically outperform those relying on a single input representation. We illustrate how these features complement each other with both quantitative and qualitative analyses. We finally investigate various fusion schemes and propose methods yielding state-of-the-art performances on two publicly-available large datasets., FY is supported by the MIP-Frontiers project, the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No. 765068, and EG by TROMPA, the Horizon 2020 project 770376-2.</abstract>
		<title>Combining musical features for cover detection</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20087b4c134112d0660c03ee282556492/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>COLLEGE MUSIC SYMPOSIUM, Journal of the College Music Society</journal>
		<series>Voice leading and harmony as expressive devices in the early music of the Beatles: 'She Loves You.'</series>
		<year>1992</year>
		<url>http://ezproxy.library.yorku.ca/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=mah&AN=MAH0000609971&site=ehost-live</url>
		<author>WALTER TRIPP EVERETT</author>
		<author>WALTER TRIPP EVERETT</author>
		<authors>
			<first>WALTER TRIPP</first>
		</authors>
		<authors>
			<last>EVERETT</last>
		</authors>
		<authors>
			<first>WALTER TRIPP</first>
		</authors>
		<authors>
			<last>EVERETT</last>
		</authors>
		<volume>32</volume>
		<pages>19--37</pages>
		<issn>0069-5696</issn>
		<shorttitle>Voice leading and harmony as expressive devices in the early music of the Beatles</shorttitle>
		<doi>Article</doi>
		<title>Voice leading and harmony as expressive devices in the early music of the Beatles: 'She Loves You.'</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22028f153bfbd163b6de597728f9e35f2/ks-plugin-devel</id>
		<tags>DLfrage</tags>
		<tags>MaMu</tags>
		<tags>Musik</tags>
		<description>JSTOR: Journal of Music Theory, Vol. 42, No. 2 (Autumn, 1998), pp. 167-180</description>
		<date>2013-02-02 14:42:53</date>
		<count>2</count>
		<journal>Journal of Music Theory</journal>
		<publisher>Duke University Press on behalf of the Yale University Department of Music</publisher>
		<year>1998</year>
		<url>http://www.jstor.org/stable/843871</url>
		<author>Richard Cohn</author>
		<authors>
			<first>Richard</first>
		</authors>
		<authors>
			<last>Cohn</last>
		</authors>
		<volume>42</volume>
		<number>2</number>
		<pages>pp. 167-180</pages>
		<abstract>The papers collected in this issue represent an emerging species of transformational theory drawn together under the "Neo-Riemannian" designation. This introductory essay sketches the origins and recent development of neo-Riemannian theory, and positions it with respect to several other genera of music theory, as well as to an evolving post-structuralist critical practice. Along the way, it seeks to situate Hugo Riemann amidst the flurry of activity occuring beneath the banner bearing his name, and to provide comfort to the perplexed observer who, innocently prying apart these covers and peering within, spies the theoretical tradition of A. B. Marx and Oettingen romping merrily with that of Babbitt, Forte, and Morris. Strange fellows indeed!... or perhaps, upon reflection, not so strange....</abstract>
		<issn>00222909</issn>
		<language>English</language>
		<copyright>Copyright © 1998 Yale University Department of Music</copyright>
		<title>Introduction to Neo-Riemannian Theory: A Survey and a Historical Perspective</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24d80686d7677a9e063d5f40062e52392/knaevelboerrar</id>
		<tags>music</tags>
		<tags>mathematics</tags>
		<description>Mathematics and Music (Mathematical World): Amazon.de: David Wright: Bücher</description>
		<date>2015-12-22 16:03:21</date>
		<count>1</count>
		<publisher>American Mathematical Society</publisher>
		<address>Providence, R.I.</address>
		<year>2009</year>
		<url>http://www.amazon.de/Mathematics-Music-Mathematical-World-Wright/dp/0821848739/ref=sr_1_1?s=books&ie=UTF8&qid=1450796436&sr=8-1&keywords=9780821848739</url>
		<author>David Wright</author>
		<authors>
			<first>David</first>
		</authors>
		<authors>
			<last>Wright</last>
		</authors>
		<abstract>Many people intuitively sense that there is a connection between mathematics and music. If nothing else, both involve counting. There is, of course, much more to the association. David Wright's book is an investigation of the interrelationships between mathematics and music, reviewing the needed background concepts in each subject as they are encountered. Along the way, readers will augment their understanding of both mathematics and music. The text explores the common foundations of the two subjects, which are developed side by side. Musical and mathematical notions are brought together, such as scales and modular arithmetic, intervals and logarithms, tone and trigonometry, and timbre and harmonic analysis. When possible, discussions of musical and mathematical notions are directly interwoven. Occasionally the discourse dwells for a while on one subject and not the other, but eventually the connection is established, making this an integrative treatment of the two subjects. The book is a text for a freshman level college course suitable for musically inclined or mathematically inclined students, with the intent of breaking down any apprehension that either group might have for the other subject. Exercises are given at the end of each chapter. The mathematical prerequisites are a high-school level familiarity with algebra, trigonometry, functions, and graphs. Musically, the student should have had some exposure to musical staffs, standard clefs, and key signatures, though all of these are explained in the text Publisher description.</abstract>
		<isbn>9780821848739 0821848739</isbn>
		<title>Mathematics and music</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2476dc161a46edb04f76d3a7310b3c111/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<series>Essays in American music; 2107</series>
		<publisher>General Music Publishing Co.</publisher>
		<address>New York, NY</address>
		<year>2000</year>
		<url></url>
		<editor>Michael Saffle</editor>
		<editors>
			<first>David</first>
		</editors>
		<editors>
			<last>Wright</last>
		</editors>
		<abstract>Examines musical life in the United States during the first half of the 20th-c., with an emphasis on the rise and triumph of popular forms such as jazz, swing, and blues, and the importance of Ives, Cage, and Copland, among others, to art music. American contributions to music technology and dissemination, and the role of these forms in extending the audience for music are also discussed. The following contributions are cited separately in RILM: Jean A. BOYD, "Western swing: Working-class southwestern jazz of the 1930s and 1940s" (RILM 2000-32868); Alfred W. COCHRAN, "Cinema music of distinction: Virgil Thomson, Aaron Copland, and Gail Kubik" (RILM 2000-32873); Raymond DESSY, "Mapping the blues genes: Technological, economic, and social strands--A spectral analysis" (RILM 2000-32813); Mark DEVOTO, "Melville Smith: Organist, educator, early music pioneer, and American composer" (RILM 2000-32871); Kent A. HOLLIDAY, "Some American firms and their contributions to the development of the reproducing piano" (RILM 2000-32814); Timothy Michael KALIL, "Thomas A. Dorsey and the development and diffusion of traditional Black gospel" (RILM 2000-32816); Ellen KNIGHT, "Boston's "French Connection" at the turn of the twentieth century" (RILM 2000-32811); Donald C. MEYER, "Toscanini and the NBC Symphony Orchestra: High, middle, and low culture" (RILM 2000-32872); Leta E. MILLER, "The art of noise: John Cage, Lou Harrison, and the West coast percussion ensemble" (RILM 2000-32869); Karen REGE, "Ticklers' secrets: Ragtime performance practices, 1900-1920--A bibliographic essay" (RILM 2000-32812); Marc RICE, "Dances, frolics, and orchestra wars: The territory bands and ballrooms of Kansas City, Missouri, 1925-1935" (RILM 2000-32815); John C. TIBBETTS, "The new Tin Pan Alley: 1940s Hollywood looks at American popular songwriters" (RILM 2000-32874).</abstract>
		<title>Perspectives on American music 1900-1950</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20707beab54c8a6d6327db84b684b6ebe/snarc</id>
		<tags>music</tags>
		<tags>preferences</tags>
		<tags>lifestyle</tags>
		<description>Lifestyle correlates of musical preference: 2. Media, leisure time and music -- North and Hargreaves 35 (2): 179 -- Psychology of Music</description>
		<date>2009-02-23 16:43:09</date>
		<count>2</count>
		<journal>Psychology of Music</journal>
		<year>2007</year>
		<url>http://pom.sagepub.com/cgi/content/abstract/35/2/179</url>
		<author>Adrian C. North</author>
		<author>David J. Hargreaves</author>
		<authors>
			<first>Adrian C.</first>
		</authors>
		<authors>
			<last>North</last>
		</authors>
		<authors>
			<first>David J.</first>
		</authors>
		<authors>
			<last>Hargreaves</last>
		</authors>
		<volume>35</volume>
		<number>2</number>
		<pages>179-200</pages>
		<abstract>Several studies indicate that musical preferences provide a means of discriminating between social groups, and suggest indirectly that musical preferences should correlate with a variety of different lifestyle choices. In this study, 2532 participants responded to a questionnaire asking them to state their musical preference and also to provide data on various aspects of their lifestyle (namely media preferences, leisure interests and music usage). Numerous associations existed between musical preference and these aspects of participants' lifestyle. The nature of these associations was consistent in part with previous research on taste publics concerning the high-culture-low-culture divide, such that fans of  high-art' and  low-art' musical styles demonstrated a preference for other  high-art' and  low-art' media objects respectively, as reflected in reading, TV and radio preferences, and leisure activities.</abstract>
		<doi>10.1177/0305735607070302</doi>
		<eprint>http://pom.sagepub.com/cgi/reprint/35/2/179.pdf</eprint>
		<title>Lifestyle correlates of musical preference: 2. Media, leisure time and music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d78865f08228c03a02476865f07bf6bc/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-11-29 00:00:00</date>
		<count>1</count>
		<booktitle>Ubiquitous Music</booktitle>
		<series>Computational Music Science</series>
		<publisher>Springer</publisher>
		<year>2014</year>
		<url>http://dblp.uni-trier.de/db/series/music/um2014.html#LazzariniCYf14</url>
		<author>Victor Lazzarini</author>
		<author>Edward Costello</author>
		<author>Steven Yi</author>
		<author>John ffitch</author>
		<authors>
			<first>Victor</first>
		</authors>
		<authors>
			<last>Lazzarini</last>
		</authors>
		<authors>
			<first>Edward</first>
		</authors>
		<authors>
			<last>Costello</last>
		</authors>
		<authors>
			<first>Steven</first>
		</authors>
		<authors>
			<last>Yi</last>
		</authors>
		<authors>
			<first>John</first>
		</authors>
		<authors>
			<last>ffitch</last>
		</authors>
		<editor>Damián Keller</editor>
		<editor>Victor Lazzarini</editor>
		<editor>Marcelo Soares Pimenta</editor>
		<editors>
			<first>John</first>
		</editors>
		<editors>
			<last>ffitch</last>
		</editors>
		<editors>
			<first>John</first>
		</editors>
		<editors>
			<last>ffitch</last>
		</editors>
		<editors>
			<first>John</first>
		</editors>
		<editors>
			<last>ffitch</last>
		</editors>
		<pages>111-128</pages>
		<isbn>978-3-319-11151-3</isbn>
		<title>Development Tools for Ubiquitous Music on the World Wide Web.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29bfa9b1cfb998705793e281dccf1432b/bauerc</id>
		<tags>imported</tags>
		<tags>myown</tags>
		<tags>music</tags>
		<tags>socialties</tags>
		<description></description>
		<date>2019-04-27 18:22:14</date>
		<count>2</count>
		<booktitle>19th International Society for Music Information Retrieval Conference</booktitle>
		<series>ISMIR 2018</series>
		<publisher>ISMIR</publisher>
		<year>2018</year>
		<url></url>
		<author>Christine Bauer</author>
		<author>Markus Schedl</author>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<pages>678-686</pages>
		<abstract>We investigate the complex relationship between the fac- tors (i) preference for music mainstream, (ii) social ties in an online music platform, and (iii) demographics. We define (i) on a global and a country level, (ii) by several network centrality measures such as Jaccard index among users’ connections, closeness centrality, and betweenness centrality, and (iii) by country and age information. Using the LFM-1b dataset of listening events of Last.fm users, we are able to uncover country-dependent differences in consumption of mainstream music as well as in user behavior with respect to social ties and users’ centrality. We could identify that users inclined to mainstream music tend to have stronger connections than the group of less mainstreamy users. Furthermore, our analysis revealed that users typically have less connections within a country than cross-country ones, with the first being stronger social ties, though. Results will help building better user models of listeners and in turn improve personalized music retrieval and recommendation algorithms.</abstract>
		<language>English</language>
		<title>Investigating cross-country relationship between users' social ties and music mainstreaminess</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2683bfea8bc740d816e5c171bf08fbefe/svrist</id>
		<tags>MUSIC</tags>
		<tags>music,</tags>
		<tags>recognition,</tags>
		<tags>OPTICAL</tags>
		<tags>COMPUTERS</tags>
		<tags>Optical</tags>
		<tags>recognition</tags>
		<tags>equipment,</tags>
		<tags>Performance,</tags>
		<tags>devices,</tags>
		<tags>music</tags>
		<tags>character</tags>
		<tags>ELECTRONIC</tags>
		<tags>COMPUTER</tags>
		<tags>pattern</tags>
		<tags>--</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>2</count>
		<journal>Computer Music Journal</journal>
		<year>2007</year>
		<url>http://search.ebscohost.com/login.aspx?direct=true&db=aph&AN=24949976&site=ehost-live</url>
		<author>Pierfrancesco Bellini</author>
		<author>Ivan Bruno</author>
		<author>Paolo Nesi</author>
		<authors>
			<first>Pierfrancesco</first>
		</authors>
		<authors>
			<last>Bellini</last>
		</authors>
		<authors>
			<first>Ivan</first>
		</authors>
		<authors>
			<last>Bruno</last>
		</authors>
		<authors>
			<first>Paolo</first>
		</authors>
		<authors>
			<last>Nesi</last>
		</authors>
		<volume>31</volume>
		<number>1</number>
		<pages>68 - 93</pages>
		<abstract>This article addresses the problem of Optical Music Recognition
    (ORM)-tool assessment. ORM is commonly known as Optical Character
      Recognition (OCR) for Music. Studies and approaches for the definition of
      OMR performance evaluation models were described. Two kinds of metrics for
      performance evaluation were proposed. These are metrics based on basic
      symbols, and metrics based on composite music symbols and their
      relationships. A rigorous model for the assessment of OMR systems is
      presented. The proposed metrics can produce evaluations close to those
      provided by experts considering a restricted number of terms.</abstract>
		<issn>01489267</issn>
		<title>Assessing Optical Music Recognition Tools.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29b923cd9c353ec76e96006299f929073/snarc</id>
		<tags>emotion</tags>
		<tags>music</tags>
		<description>Using music to induce emotions: Influences of musical preference and absorption -- Kreutz et al. 36 (1): 101 -- Psychology of Music</description>
		<date>2009-02-23 16:43:09</date>
		<count>2</count>
		<journal>Psychology of Music</journal>
		<year>2008</year>
		<url>http://pom.sagepub.com/cgi/content/abstract/36/1/101</url>
		<author>Gunter Kreutz</author>
		<author>Ulrich Ott</author>
		<author>Daniel Teichmann</author>
		<author>Patrick Osawa</author>
		<author>Dieter Vaitl</author>
		<authors>
			<first>Gunter</first>
		</authors>
		<authors>
			<last>Kreutz</last>
		</authors>
		<authors>
			<first>Ulrich</first>
		</authors>
		<authors>
			<last>Ott</last>
		</authors>
		<authors>
			<first>Daniel</first>
		</authors>
		<authors>
			<last>Teichmann</last>
		</authors>
		<authors>
			<first>Patrick</first>
		</authors>
		<authors>
			<last>Osawa</last>
		</authors>
		<authors>
			<first>Dieter</first>
		</authors>
		<authors>
			<last>Vaitl</last>
		</authors>
		<volume>36</volume>
		<number>1</number>
		<pages>101-126</pages>
		<abstract>The present research addresses the induction of emotion during music listening in adults using categorical and dimensional theories of emotion as background. It   further explores the influences of musical preference and absorption trait on induced emotion. Twenty-five excerpts of classical music representing `happiness',   `sadness', `fear', `anger' and `peace' were presented individually to 99 adult participants. Participants rated the intensity of felt emotions as well as the   pleasantness and arousal induced by each excerpt. Mean intensity ratings of target emotions were highest for 20 out of 25 excerpts. Pleasantness and arousal ratings  led to three main clusters within the two-dimensional circumplex space. Preference for classical music significantly influenced specificity and intensity ratings   across categories. Absorption trait significantly correlated with arousal ratings only. In sum, instrumental music appears effective for the induction of basic  emotions in adult listeners. However, careful screening of participants in terms of their musical preferences should be mandatory.</abstract>
		<doi>10.1177/0305735607082623</doi>
		<eprint>http://pom.sagepub.com/cgi/reprint/36/1/101.pdf</eprint>
		<title>Using music to induce emotions: Influences of musical preference and absorption</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c6b48317042af62970d7beb2202482a1/keinstein</id>
		<tags>DLfrage</tags>
		<tags>MaMu</tags>
		<tags>Musik</tags>
		<description>JSTOR: Perspectives of New Music, Vol. 26, No. 1 (Winter, 1988), pp. 258-287</description>
		<date>2010-09-14 11:11:02</date>
		<count>2</count>
		<journal>Perspectives of New Music</journal>
		<publisher>Perspectives of New Music</publisher>
		<year>1988</year>
		<url>http://www.jstor.org/stable/833330</url>
		<author>Dan Tudor Vuza</author>
		<authors>
			<first>Dan Tudor</first>
		</authors>
		<authors>
			<last>Vuza</last>
		</authors>
		<volume>26</volume>
		<number>1</number>
		<pages>pp. 258-287</pages>
		<issn>00316016</issn>
		<language>English</language>
		<copyright>Copyright © 1988 Perspectives of New Music</copyright>
		<title>Some Mathematical Aspects of David Lewin's Book: Generalized Musical Intervals and Transformations</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/211242197e760d37a067d64349efba424/maxirichter</id>
		<tags>music</tags>
		<tags>semantic_web</tags>
		<tags>nocache</tags>
		<tags>richterm_ba</tags>
		<tags>ontology</tags>
		<description></description>
		<date>2012-01-18 14:26:35</date>
		<count>1</count>
		<year>2012</year>
		<url>http://musicontology.com/</url>
		<author> musicontology.com</author>
		<authors>
			<first></first>
		</authors>
		<authors>
			<last>musicontology.com</last>
		</authors>
		<title>Music Ontology Specification</title>
		<pubtype>electronic</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22028f153bfbd163b6de597728f9e35f2/keinstein</id>
		<tags>DLfrage</tags>
		<tags>MaMu</tags>
		<tags>Musik</tags>
		<description>JSTOR: Journal of Music Theory, Vol. 42, No. 2 (Autumn, 1998), pp. 167-180</description>
		<date>2010-09-14 11:38:54</date>
		<count>2</count>
		<journal>Journal of Music Theory</journal>
		<publisher>Duke University Press on behalf of the Yale University Department of Music</publisher>
		<year>1998</year>
		<url>http://www.jstor.org/stable/843871</url>
		<author>Richard Cohn</author>
		<authors>
			<first>Richard</first>
		</authors>
		<authors>
			<last>Cohn</last>
		</authors>
		<volume>42</volume>
		<number>2</number>
		<pages>pp. 167-180</pages>
		<abstract>The papers collected in this issue represent an emerging species of transformational theory drawn together under the "Neo-Riemannian" designation. This introductory essay sketches the origins and recent development of neo-Riemannian theory, and positions it with respect to several other genera of music theory, as well as to an evolving post-structuralist critical practice. Along the way, it seeks to situate Hugo Riemann amidst the flurry of activity occuring beneath the banner bearing his name, and to provide comfort to the perplexed observer who, innocently prying apart these covers and peering within, spies the theoretical tradition of A. B. Marx and Oettingen romping merrily with that of Babbitt, Forte, and Morris. Strange fellows indeed!... or perhaps, upon reflection, not so strange....</abstract>
		<issn>00222909</issn>
		<language>English</language>
		<copyright>Copyright © 1998 Yale University Department of Music</copyright>
		<title>Introduction to Neo-Riemannian Theory: A Survey and a Historical Perspective</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c6b48317042af62970d7beb2202482a1/ks-plugin-devel</id>
		<tags>DLfrage</tags>
		<tags>MaMu</tags>
		<tags>Musik</tags>
		<description>JSTOR: Perspectives of New Music, Vol. 26, No. 1 (Winter, 1988), pp. 258-287</description>
		<date>2013-02-02 14:42:54</date>
		<count>2</count>
		<journal>Perspectives of New Music</journal>
		<publisher>Perspectives of New Music</publisher>
		<year>1988</year>
		<url>http://www.jstor.org/stable/833330</url>
		<author>Dan Tudor Vuza</author>
		<authors>
			<first>Dan Tudor</first>
		</authors>
		<authors>
			<last>Vuza</last>
		</authors>
		<volume>26</volume>
		<number>1</number>
		<pages>pp. 258-287</pages>
		<issn>00316016</issn>
		<language>English</language>
		<copyright>Copyright © 1988 Perspectives of New Music</copyright>
		<title>Some Mathematical Aspects of David Lewin's Book: Generalized Musical Intervals and Transformations</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/224e450096a6c4f243a12d0b70d137bfc/bauerc</id>
		<tags>myown</tags>
		<tags>music</tags>
		<tags>artist</tags>
		<tags>recsys</tags>
		<tags>recommender</tags>
		<tags>musicplatform</tags>
		<tags>equality</tags>
		<description></description>
		<date>2019-11-06 23:33:24</date>
		<count>2</count>
		<booktitle>1st Workshop on Designing Human-Centric MIR Systems (wsHCMIR 2019), satellite event to 20th annual conference of the International Society for Music Information Retrieval (ISMIR 2019)</booktitle>
		<series>wsHCMIR 2019</series>
		<year>2019</year>
		<url></url>
		<author>Christine Bauer</author>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<pages>16-18</pages>
		<abstract>Promoting diversity in the music sector is widely discussed on the media. While the major problem may lie deep in our society, music information retrieval contributes to promoting diversity or may create unequal opportunities for artists. For example, considering the known problem of popularity bias in music recommendation, it is important to investigate whether the short head of popular music artists and the long tail of less popular ones show similar patterns of diversity---in terms of, for example, age, gender, or ethnic origin---or the popularity bias amplifies a positive or negative effect.
I advocate for reasonable opportunities for artists---for (currently) popular artists and artists in the long-tail alike---in music recommender systems. In this work, I represent the position that we need to develop a deep understanding of the biases and inequalities because it is the essential basis to design approaches for music recommendation that provide reasonable opportunities. Thus, research needs to investigate the various reasons that hinder equal opportunity and diversity in music recommendation.</abstract>
		<language>English</language>
		<title>Allowing for equal opportunities for artists in music recommendation</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/224d1bbf83fe803f3178764d53504fd75/jaeschke</id>
		<tags>identification</tags>
		<tags>cover</tags>
		<tags>music</tags>
		<tags>uncovr</tags>
		<tags>mir</tags>
		<tags>similarity</tags>
		<tags>audio</tags>
		<tags>duplicate</tags>
		<tags>lyrics</tags>
		<description>Large-Scale Cover Song Detection in Digital Music Libraries Using Metadata, Lyrics and Audio Features</description>
		<date>2021-10-15 08:20:05</date>
		<count>4</count>
		<year>2018</year>
		<url>http://arxiv.org/abs/1808.10351</url>
		<author>Albin Andrew Correya</author>
		<author>Romain Hennequin</author>
		<author>Mickaël Arcos</author>
		<authors>
			<first>Albin Andrew</first>
		</authors>
		<authors>
			<last>Correya</last>
		</authors>
		<authors>
			<first>Romain</first>
		</authors>
		<authors>
			<last>Hennequin</last>
		</authors>
		<authors>
			<first>Mickaël</first>
		</authors>
		<authors>
			<last>Arcos</last>
		</authors>
		<abstract>Cover song detection is a very relevant task in Music Information Retrieval
(MIR) studies and has been mainly addressed using audio-based systems. Despite
its potential impact in industrial contexts, low performances and lack of
scalability have prevented such systems from being adopted in practice for
large applications. In this work, we investigate whether textual music
information (such as metadata and lyrics) can be used along with audio for
large-scale cover identification problem in a wide digital music library. We
benchmark this problem using standard text and state of the art audio
similarity measures. Our studies shows that these methods can significantly
increase the accuracy and scalability of cover detection systems on Million
Song Dataset (MSD) and Second Hand Song (SHS) datasets. By only leveraging
standard tf-idf based text similarity measures on song titles and lyrics, we
achieved 35.5% of absolute increase in mean average precision compared to the
current scalable audio content-based state of the art methods on MSD. These
experimental results suggests that new methodologies can be encouraged among
researchers to leverage and identify more sophisticated NLP-based techniques to
improve current cover song identification systems in digital music libraries
with metadata.</abstract>
		<title>Large-Scale Cover Song Detection in Digital Music Libraries Using Metadata, Lyrics and Audio Features</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f9737aae7c29d3f70c37e396eb632071/arminhs</id>
		<tags>tagging</tags>
		<tags>semantic</tags>
		<tags>music</tags>
		<description>PhD</description>
		<date>2009-10-08 23:11:38</date>
		<count>6</count>
		<booktitle>Proceedings of the 9th International Conference on Music Information Retrieval</booktitle>
		<address>Philadelphia, USA</address>
		<year>2008</year>
		<url></url>
		<author>D. Turnbull</author>
		<author>L. Barrington</author>
		<author>G. Lanckriet</author>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Turnbull</last>
		</authors>
		<authors>
			<first>L.</first>
		</authors>
		<authors>
			<last>Barrington</last>
		</authors>
		<authors>
			<first>G.</first>
		</authors>
		<authors>
			<last>Lanckriet</last>
		</authors>
		<pages>225--230</pages>
		<abstract>We compare five approaches to collecting tags for music:
conducting a survey, harvesting social tags, deploying annotation
games, mining web documents, and autotagging audio
content. The comparison includes a discussion of both scalability
(financial cost, human involvement, and computational
resources) and quality (the cold start problem & popularity
bias, strong vs. weak labeling, vocabulary structure & size,
and annotation accuracy). We then describe one state-ofthe-
art system for each approach. The performance of each
system is evaluated using a tag-based music information
retrieval task. Using this task, we are able to quantify the
effect of popularity bias on each approach by making use
of a subset of more popular (short-head) songs and a set of
less popular (long-tail) songs. Lastly, we propose a simple
hybrid context-content system that combines our individual
approaches and produces superior retrieval results.</abstract>
		<title>Five Approaches to Collecting Tags for Music</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e5b9856c5d266575c43739919edab830/zazi</id>
		<tags>imported</tags>
		<description>The Bibliography of my Belegarbeit</description>
		<date>2010-01-28 22:00:35</date>
		<count>1</count>
		<journal>Computing in Musicology</journal>
		<series>Computing in Musicology</series>
		<publisher>CCARH and The MIT Press</publisher>
		<year>2004</year>
		<url></url>
		<author>Micheline Lesaffre</author>
		<author>Dirk Moelants</author>
		<author>Marc Leman</author>
		<authors>
			<first>Micheline</first>
		</authors>
		<authors>
			<last>Lesaffre</last>
		</authors>
		<authors>
			<first>Dirk</first>
		</authors>
		<authors>
			<last>Moelants</last>
		</authors>
		<authors>
			<first>Marc</first>
		</authors>
		<authors>
			<last>Leman</last>
		</authors>
		<editor>Walter B. Hewlett</editor>
		<editor>Eleanor Selfridge-Field</editor>
		<editors>
			<first>Marc</first>
		</editors>
		<editors>
			<last>Leman</last>
		</editors>
		<editors>
			<first>Marc</first>
		</editors>
		<editors>
			<last>Leman</last>
		</editors>
		<volume>13</volume>
		<pages>129--146</pages>
		<title>Music Query: Methods, Models, and User Studies</title>
		<pubtype>inbook</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bf5c101c56f5dd69f531a4380631beb7/bfields</id>
		<tags>cognitive_categorization</tags>
		<description>initial imports</description>
		<date>2010-01-28 11:42:45</date>
		<count>1</count>
		<journal>Psychology of Music</journal>
		<year>2006</year>
		<url></url>
		<author>A. Ockelford</author>
		<authors>
			<first>A.</first>
		</authors>
		<authors>
			<last>Ockelford</last>
		</authors>
		<volume>34</volume>
		<number>1</number>
		<pages>81 - 142</pages>
		<abstract>This article examines implication and expectation in music, taking as its starting point music-theoretical and music-psychological work ranging from the seminal thinking of Meyer (1956, 1967, 1973) to its development in the theories of Narmour (1990, 1992) and subsequent empirical and theoretical investigation by, for example, Schellenberg (1996, 1997), Von Hippel and Huron (2000) and Aarden (2003). Other psychological approaches, such as those adopted by Jones (1981, 1982, 1992) and Bharucha (1987, 1999), are considered too. The most important contemporary reference point, however, is Huron's latest extended thinking on expectation (forthcoming), which summarizes, consolidates and develops a wide range of theoretical and empirical work in the field. These diverse perspectives on musical implication and expectation are analysed using the `zygonic' theory of musical understanding recently developed by Ockelford (for example, 1999, 2002, 2004, 2005a, 2005b). This holds that the cognition of structure stems from a sense of derivation arising from the presence of repetition in certain contexts. Using this framework, a new, composite theory of expectation in music is developed, which acknowledges the potential implications of three sources of regularity in music: patterns within groups of notes, and between them -- as encoded in short-term memory and long-term, both veridicallyand schematically.Finally, the phenomenological relevance of the new model to `typical' listening experiences is discussed, and the need for future empirical work is set out.</abstract>
		<title>Implication and expectation in music: a zygonic model</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2147ab976640457519c05c0f1ac27cb32/bfields</id>
		<tags>zygonic</tags>
		<tags>cognitive_categorization</tags>
		<description>initial imports</description>
		<date>2010-01-28 11:42:45</date>
		<count>1</count>
		<journal>Psychology of Music</journal>
		<year>2004</year>
		<url></url>
		<author>A. Ockelford</author>
		<authors>
			<first>A.</first>
		</authors>
		<authors>
			<last>Ockelford</last>
		</authors>
		<volume>34</volume>
		<number>1</number>
		<pages>23 - 74</pages>
		<abstract>Following a review of the model of similarity perception, cue abstraction and categorization, developed and empirically tested by Deliège and others over the last decade or so, it is proposed that the notion of perceived derivationmay be a key additional element in the cognition of musical structure. Evidence is sought in the re-analysis of recent empirical work and through the identification of structures that appear to challenge the sufficiency of Deliège's model. The issue is contextualized through a discussion of the concepts of similarity, sameness and salience, utilizing contemporary thinking in cognitive psychology, philosophy and music theory -- strands of thought which are drawn together through Ockelford's `zygonic' theory of music-structural understanding. This leads to the formulation of a new, composite account of how musical structure is processed, in which similarity, salience, derivation, categorization and schematization are shown to function in an integrated way.</abstract>
		<title>On similarity, derivation and the cognition of musical structure</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2df0a2bcc49b2dab96a0f96afc46d3458/ijtsrd</id>
		<tags>functionalapproach</tags>
		<tags>music</tags>
		<tags>piano</tags>
		<tags>pianoaccompaniment</tags>
		<tags>beginningmethod</tags>
		<description></description>
		<date>2021-01-28 10:15:59</date>
		<count>1</count>
		<journal>International Journal of Trend in Scientific Research and Development</journal>
		<year>2020</year>
		<url>https://www.ijtsrd.com/humanities-and-the-arts/music/35824/functional-approach-as-a-beginning-method-for-teaching-and-learning-piano-accompaniment-in-music/almighty-c-tabuena</url>
		<author>Almighty C. Tabuena</author>
		<authors>
			<first>Almighty C.</first>
		</authors>
		<authors>
			<last>Tabuena</last>
		</authors>
		<volume>5</volume>
		<number>1</number>
		<pages>51-54</pages>
		<abstract>Piano playing is an integral skill in music education in which students require mastery of the fundamental concepts and principles in terms of piano instruction, as well as in teaching learning processes. In the part of teaching, which is the piano pedagogy, facilitating learning in playing the piano serves as a ground to develop individuals on a particular skill in each grade level in accordance with the learning competencies enumerated in the Music Curriculum Guide by the Department of Education. This article aimed to describe the context of the functional approach of piano accompaniment to make an easy way of learning the piano accompaniment in the most convenient and fastest time, as well as to accompany certain music or composition in minimal learning time. The practical way of learning piano skills is the observation and use of musical elements through their functions. In this approach, there are four identified components in learning and playing the piano accompaniment musical elements and functions, musical patterns, musical piano accompaniment styles, and musical piano composition. By this process of learning, gradually, the manner of playing, the style, the elements, and other components of piano accompaniment will be combined to modify and to discover the aesthetic value and characteristics of music composition as well as the wisdom of the music itself. Almighty C. Tabuena "Functional Approach as a Beginning Method for Teaching and Learning Piano Accompaniment in Music" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-1 , December 2020, URL: https://www.ijtsrd.com/papers/ijtsrd35824.pdf Paper URL : https://www.ijtsrd.com/humanities-and-the-arts/music/35824/functional-approach-as-a-beginning-method-for-teaching-and-learning-piano-accompaniment-in-music/almighty-c-tabuena</abstract>
		<language>English</language>
		<issn>2456-6470</issn>
		<title>Functional Approach as a Beginning Method for Teaching and Learning Piano Accompaniment in Music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e22fefa1c7f4008218cb32723f9027c6/sapo</id>
		<tags>symbolic_music_analysis</tags>
		<description>About the Determination of Key of a Musical Excerpt | SpringerLink</description>
		<date>2020-12-10 10:55:49</date>
		<count>1</count>
		<booktitle>Computer Music Modeling and Retrieval</booktitle>
		<publisher>Springer Berlin Heidelberg</publisher>
		<address>Berlin, Heidelberg</address>
		<year>2006</year>
		<url></url>
		<author>Héctor Bellmann</author>
		<authors>
			<first>Héctor</first>
		</authors>
		<authors>
			<last>Bellmann</last>
		</authors>
		<editor>Richard Kronland-Martinet</editor>
		<editor>Thierry Voinier</editor>
		<editor>Sølvi Ystad</editor>
		<editors>
			<first>Héctor</first>
		</editors>
		<editors>
			<last>Bellmann</last>
		</editors>
		<editors>
			<first>Héctor</first>
		</editors>
		<editors>
			<last>Bellmann</last>
		</editors>
		<editors>
			<first>Héctor</first>
		</editors>
		<editors>
			<last>Bellmann</last>
		</editors>
		<pages>76--91</pages>
		<abstract>Knowledge of the key of a musical passage is a pre-requisite for all the analyses that require functional labelling. In the past, people from either a musical or AI background have tended to solve the problem by means of implementing a computerized version of musical analysis. Previous attempts are discussed and then attention is focused on a non-analytical solution first reported by J.A.Gabura. A practical way to carry it out is discussed as well as its limitations in relation to examples. References are made to the MusicXML format as needed.</abstract>
		<isbn>978-3-540-34028-7</isbn>
		<title>About the Determination of Key of a Musical Excerpt</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20c7c5f70aedd46285894605007e1a07b/kurtjx</id>
		<tags>similarity,</tags>
		<tags>music</tags>
		<tags>feature</tags>
		<tags>extraction</tags>
		<description>main bib file</description>
		<date>2008-03-17 17:08:40</date>
		<count>4</count>
		<booktitle>Proc. of Int. Symposium on Music Information Retrieval</booktitle>
		<year>2006</year>
		<url></url>
		<author>K. Jacobson</author>
		<authors>
			<first>K.</first>
		</authors>
		<authors>
			<last>Jacobson</last>
		</authors>
		<title>A Multifaceted Approach to Music Similarity</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bcf722766f63261d71f19db059747868/stefano</id>
		<tags>music</tags>
		<tags>creativity</tags>
		<tags>livecoding</tags>
		<tags>perfomance</tags>
		<description></description>
		<date>2007-03-15 13:13:11</date>
		<count>2</count>
		<journal>Contemporary Music Review</journal>
		<year>2003</year>
		<url>http://www.ingentaconnect.com/content/routledg/gcmr/2003/00000022/00000004/art00009</url>
		<author>Nick Collins</author>
		<authors>
			<first>Nick</first>
		</authors>
		<authors>
			<last>Collins</last>
		</authors>
		<volume>22</volume>
		<pages>67-79(13)</pages>
		<abstract>Live computer music is the perfect medium for generative music systems, for non-linear compositional constructions and for interactive manipulation of sound processing. Unfortunately, much of the complexity of these real-time systems is lost on a potential audience, excepting those few connoisseurs who sneak round the back to check the laptop screen. An artist using powerful software like SuperCollider or PD cannot be readily distinguished from someone checking their e-mail whilst DJ-ing with iTunes. Without a culture of understanding of both the laptop performer and current generation graphical and text-programming languages for audio, audiences tend to respond most to often gimmicky controllers, or to the tools they have had more exposure to - the (yawn) superstar DJs and their decks. This article attempts to convey the exciting things that are being explored with algorithmic composition and interactive synthesis techniques in live performance. The reasons for building generative music systems and the forms of control attainable over algorithmic processes are investigated. Direct manual control is set against the use of autonomous software agents. In line with this, four techniques for software control during live performance are introduced, namely presets, previewing, autopilot, and the powerful method of live coding. Finally, audio-visual collaboration is discussed.</abstract>
		<doi>doi:10.1080/0749446032000156919</doi>
		<title>Generative Music and Laptop Performance</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f32a2a7541c695083c21a64fc78cae92/iblis</id>
		<tags>imported</tags>
		<description></description>
		<date>2007-03-19 17:08:38</date>
		<count>1</count>
		<journal>Revue de musicologie</journal>
		<series>3</series>
		<publisher>Societe Francaise de Musicologie</publisher>
		<year>1985</year>
		<url>http://links.jstor.org/sici?sici=0035-1601%281985%293%3A71%3A1%2F2%3C79%3ATEPDLG%3E2.0.CO%3B2-V</url>
		<author>Jean During</author>
		<authors>
			<first>Jean</first>
		</authors>
		<authors>
			<last>During</last>
		</authors>
		<volume>71</volume>
		<number>1/2</number>
		<pages>79--118</pages>
		<abstract>From empiric observations, the great theorists of Irano-Arabian music have elaborated systems of divisions of the octave based upon priviledged intervals, showing a strong theoretical coherence. Examining the different stages of their reasoning, one can find out some slight deviations betwen the empiric data and their theoretical expression, allowing one to question the validity of their systems. The scientific analysis of intervals now used in contemporary Iran leads to regard all the attempts made up to now as more or less arbitrary reductions of data resisting any rigorous formulation.</abstract>
		<issn>0035-1601</issn>
		<jstor_date>1985</jstor_date>
		<language>fre</language>
		<copyright>Copyright 1985 Societe Francaise de Musicologie</copyright>
		<title>Theories et pratiques de la gamme iranienne</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2721f16b45cf2b508898cc3c3cb256c3c/brazovayeye</id>
		<tags>music,</tags>
		<tags>genetic</tags>
		<tags>algorithms,</tags>
		<tags>art,</tags>
		<tags>programming,</tags>
		<tags>jazz,</tags>
		<tags>breve</tags>
		<tags>genbebop,</tags>
		<tags>SuperDuperWalker,</tags>
		<description></description>
		<date>2008-06-19 17:46:40</date>
		<count>1</count>
		<journal>YLEM Journal (Artists Using Science and Technology)</journal>
		<address>YLEM Journal, Loren Means, 149 Evelyn Way, San
                 Francisco, CA, 94127 USA</address>
		<year>2005</year>
		<url>http://www.ylem.org/Journal/2005Iss06&08vol25.pdf</url>
		<author>Lee Spector</author>
		<author>Jon Klein</author>
		<author>Kyle Harrington</author>
		<authors>
			<first>Lee</first>
		</authors>
		<authors>
			<last>Spector</last>
		</authors>
		<authors>
			<first>Jon</first>
		</authors>
		<authors>
			<last>Klein</last>
		</authors>
		<authors>
			<first>Kyle</first>
		</authors>
		<authors>
			<last>Harrington</last>
		</authors>
		<volume>25</volume>
		<number>6 & 8</number>
		<pages>24--26</pages>
		<abstract>In this article we provide a brief introduction to the
                 use of evolutionary computation in the arts, focusing
                 on two approaches that we have taken in our own work on
                 the evolution of music-making systems.</abstract>
		<ssn>1057-2031</ssn>
		<notes>Audio and video available at:
                 http://hampshire.edu/lspector/selection-songs.html

                 Special CD edition of YLEM Journal on Computers and
                 Music

                 http://www.ylem.org/</notes>
		<title>Selection Songs: Evolutionary Music Computation</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e8ee42a951bde4e8b7cb8efc60be23da/muhe</id>
		<tags>Music.</tags>
		<tags>Consciousness,</tags>
		<tags>Altered</tags>
		<tags>Representation</tags>
		<tags>Spectrum,</tags>
		<tags>Cortical</tags>
		<tags>Auditory</tags>
		<tags>Selectivity,</tags>
		<tags>of</tags>
		<tags>State</tags>
		<tags>Symmetry</tags>
		<tags>Musicogenic</tags>
		<tags>Neural</tags>
		<tags>Epilepsy,</tags>
		<description></description>
		<date>2012-01-27 14:10:42</date>
		<count>1</count>
		<journal>Proceedings of the 20th Annual International Conference of the IEEE
	Engineering in Medicine and Biology Society</journal>
		<year>1998</year>
		<url></url>
		<author>S.R. Das</author>
		<author>G.C. Ray</author>
		<authors>
			<first>S.R.</first>
		</authors>
		<authors>
			<last>Das</last>
		</authors>
		<authors>
			<first>G.C.</first>
		</authors>
		<authors>
			<last>Ray</last>
		</authors>
		<volume>20</volume>
		<pages>1657-1660</pages>
		<abstract>It has been shown earlier that deep aesthetic appreciation of music
	can induce an altered state of consciousness, a phenomenon often
	referred to as musicogenic epilepsy. The change in EEG, during such
	a state, has been characterized by large number of spikes, symptomatic
	to epilepsy, along, with changes in background EEG. The fractal dimension
	of EEG was found to reduce during this (epoch) period. Morphological
	variations in ECG, causing a possible sympathetic burst on autonomic
	system (ANS), has also been reported. In the present study, the neural
	representation of the music, causing change in the state of mind,
	has been found out using a biophysically defensible model of auditory
	processing. This involves analysis, transduction and reduction in
	the cochlear mechanism generating the auditory spectrum. The latter
	is again taken as input to the response function (RF) of the neuron
	and the output of this filter is the neuronal representation of the
	input signal (music). This cortical representation turns out to be
	a ripple, i.e. near-sinusoid on the log-frequency axis, indicating
	a broad-based spike in the time domain. This might have deeper implications
	on the study of altered state of consciousness because spikes in
	EEG and the `clouding' of consciousness (say during grandmal/petitmal
	epilepsy) are established phenomena</abstract>
		<pdf>LFN-Datenbank\Brain\Neuronal Representation of Music During Musicogenic Epilepsy.pdf</pdf>
		<title>Neuronal Representation of Music During Musicogenic Epilepsy</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26b722a51bb777389fb358eac272d6b99/maxirichter</id>
		<tags>music</tags>
		<tags>dna</tags>
		<tags>audio</tags>
		<description></description>
		<date>2014-04-07 07:45:35</date>
		<count>1</count>
		<year>2014</year>
		<url>http://lab.aerotwist.com/canvas/music-dna/</url>
		<author> Music DNA</author>
		<authors>
			<first></first>
		</authors>
		<authors>
			<last>Music DNA</last>
		</authors>
		<title>Music DNA</title>
		<pubtype>electronic</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bb5feb4dafd211c60354919a54728eb6/andre@ismll</id>
		<tags>music</tags>
		<tags>similarity</tags>
		<tags>improvements</tags>
		<description>initial imports</description>
		<date>2010-05-06 09:17:56</date>
		<count>2</count>
		<booktitle>Proc. of Int. Symposium on Music Information Retrieval</booktitle>
		<year>2005</year>
		<url></url>
		<author>Elias Pampalk</author>
		<author>Arthur Flexer</author>
		<author>Gerhard Widmer</author>
		<authors>
			<first>Elias</first>
		</authors>
		<authors>
			<last>Pampalk</last>
		</authors>
		<authors>
			<first>Arthur</first>
		</authors>
		<authors>
			<last>Flexer</last>
		</authors>
		<authors>
			<first>Gerhard</first>
		</authors>
		<authors>
			<last>Widmer</last>
		</authors>
		<title>Improvements of Audio-Based Music Similarity and Genre Classification</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20c7c5f70aedd46285894605007e1a07b/andre@ismll</id>
		<tags>feature_extraction</tags>
		<tags>music_similarity</tags>
		<tags>feature</tags>
		<tags>itunes</tags>
		<tags>similarity</tags>
		<tags>genre</tags>
		<tags>mds</tags>
		<tags>extraction</tags>
		<description>initial imports</description>
		<date>2010-05-06 09:23:57</date>
		<count>4</count>
		<booktitle>Proc. of Int. Symposium on Music Information Retrieval</booktitle>
		<year>2006</year>
		<url></url>
		<author>K. Jacobson</author>
		<authors>
			<first>K.</first>
		</authors>
		<authors>
			<last>Jacobson</last>
		</authors>
		<title>A Multifaceted Approach to Music Similarity</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b325c81c2915e1d5f6383109332bb029/svrist</id>
		<tags>music;</tags>
		<tags>musical</tags>
		<tags>extensible</tags>
		<tags>electronic</tags>
		<tags>notation;</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>1</count>
		<journal>Aust. Comput. Sci. Commun. (Australia), Australian Computer Science Communications</journal>
		<year>1996</year>
		<url></url>
		<author>D. Bainbridge</author>
		<author>T. Bell</author>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Bainbridge</last>
		</authors>
		<authors>
			<first>T.</first>
		</authors>
		<authors>
			<last>Bell</last>
		</authors>
		<volume>18</volume>
		<number>1</number>
		<pages>308-17</pages>
		<abstract>Optical music recognition (OMR) is a form of structured document image analysis where symbols overlaid on the conventional five-line stave are isolated and identified so that the music can be played through a MIDI system, or edited in a music publishing-system. Traditionally OMR systems have had recognition techniques hard-coded in software. This paper describes a system that has been designed to be extensible without the need to change the systems source code. Extensibility is achieved by providing tools for music recognition that are used to tailor the system to suit the type of music being recognised. The tools include a selection of methods for identifying staves and isolating objects from them, methods for describing and identifying primitive musical shapes, and a grammar for specifying the relationships between the shapes that are recognised. The system is flexible enough to work with different publishers symbol sets, and even with different types of music notation, such as the square-note notation used in early music (13 Refs.) recognition; music; optical character recognition; pattern recognition</abstract>
		<title>An extensible optical music recognition system</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23901fdc2704af2a653e846d6b881edd2/keinstein</id>
		<tags>Informatik</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2004</date>
		<count>2</count>
		<journal>Computer Music Modeling and Retrieval</journal>
		<year>2004</year>
		<url>http://www.springerlink.com/content/chuqyybpguvjmxq7</url>
		<author>Gerald Friedland</author>
		<author>Kristian Jantz</author>
		<author>Lars Knipping</author>
		<authors>
			<first>Gerald</first>
		</authors>
		<authors>
			<last>Friedland</last>
		</authors>
		<authors>
			<first>Kristian</first>
		</authors>
		<authors>
			<last>Jantz</last>
		</authors>
		<authors>
			<first>Lars</first>
		</authors>
		<authors>
			<last>Knipping</last>
		</authors>
		<pages>89--123</pages>
		<abstract>This paper describes our approach for editing PSID files. PSID is a sound format that allows the original Commodore 64 synthesizer music to be played on modern computers and thus conserves a music subculture of the eighties. So far, editing PSID files required working directly with Commodore 64 machine language. The paper gives a small overview of sound synthesis with the Commodore 64, argues why this topic is still interesting for both musicians and computer scientists, and describes our editing approach.</abstract>
		<doi>10.1007/b12000</doi>
		<title>Conserving an Ancient Art of Music: Making SID Tunes Editable</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/223dea2e46ea772c30fdd1915f46fd7c8/knaevelboerrar</id>
		<tags>music</tags>
		<tags>mathematics</tags>
		<description>The Math Behind the Music with CD-ROM (Outlooks): Amazon.de: Leon Harkleroad: Fremdsprachige Bücher</description>
		<date>2015-12-22 16:05:31</date>
		<count>1</count>
		<publisher>Cambridge University Press</publisher>
		<address>Cambridge</address>
		<year>2003</year>
		<url>http://www.amazon.de/Math-Behind-Music-CD-ROM-Outlooks/dp/0521009359/ref=pd_sim_14_1?ie=UTF8&dpID=51ogyx5LOcL&dpSrc=sims&preST=_AC_UL160_SR106%2C160_&refRID=0FBZ3EKEMPCWGBFGSM5T</url>
		<author> na</author>
		<authors>
			<first></first>
		</authors>
		<authors>
			<last>na</last>
		</authors>
		<isbn>0521810957 9780521810951 0521009359 9780521009355</isbn>
		<title>Mathematical themes and variations : exploring mathematics in music</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2455b072eb5ba3ae96b129aadcf60a42c/nosebrain</id>
		<tags>overview</tags>
		<tags>music</tags>
		<tags>system</tags>
		<tags>recommendation</tags>
		<description>New Paths in Music Recommender Systems Research</description>
		<date>2017-09-17 23:41:40</date>
		<count>3</count>
		<booktitle>Proceedings of the Eleventh ACM Conference on Recommender Systems</booktitle>
		<series>RecSys '17</series>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2017</year>
		<url>http://doi.acm.org/10.1145/3109859.3109934</url>
		<author>Markus Schedl</author>
		<author>Peter Knees</author>
		<author>Fabien Gouyon</author>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<authors>
			<first>Peter</first>
		</authors>
		<authors>
			<last>Knees</last>
		</authors>
		<authors>
			<first>Fabien</first>
		</authors>
		<authors>
			<last>Gouyon</last>
		</authors>
		<editor>Paolo Cremonesi</editor>
		<editor>Francesco Ricci</editor>
		<editor>Shlomo Berkovsky</editor>
		<editor>Alexander Tuzhilin</editor>
		<editors>
			<first>Fabien</first>
		</editors>
		<editors>
			<last>Gouyon</last>
		</editors>
		<editors>
			<first>Fabien</first>
		</editors>
		<editors>
			<last>Gouyon</last>
		</editors>
		<editors>
			<first>Fabien</first>
		</editors>
		<editors>
			<last>Gouyon</last>
		</editors>
		<editors>
			<first>Fabien</first>
		</editors>
		<editors>
			<last>Gouyon</last>
		</editors>
		<pages>392--393</pages>
		<abstract>The particularities of musical data and its multiple modalities make original contributions possible in many core RecSys topics such as content-based and hybrid recommendation, user modeling, interfaces, and context-aware and mobile recommendations. But more urgently, the current revolution in the music industry represents major opportunities and challenges for recommendation systems in general. Recommendation systems are now central to music streaming platforms, which are rapidly increasing in listenership and becoming the top source of revenue for the music industry. It is increasingly more common for a music listener to simply access music than to purchase and own it in a personal collection. In this scenario, recommendation calls no longer for a one-shot recommendation for the purpose of a track or album purchase, but for a recommendation of a listening experience, comprising a very wide range of challenges, such as sequential recommendation, or conversational and contextual recommendations. Recommendation technologies now impact all actors in the rich and complex music industry ecosystem (listeners, labels, music makers and producers, concert halls, advertisers, etc.). To highlight these developments, we focus on three use cases: automatic playlist generation, context-aware music recommendation, and recommendation in the creative process of music making.</abstract>
		<isbn>978-1-4503-4652-8</isbn>
		<doi>10.1145/3109859.3109934</doi>
		<title>New Paths in Music Recommender Systems Research</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fdaed6c427ca4817ef26624e4fdd3750/ijtsrd</id>
		<tags>Social</tags>
		<tags>Violence</tags>
		<tags>Music</tags>
		<tags>video</tags>
		<tags>Representation</tags>
		<tags>violence</tags>
		<tags>Media</tags>
		<tags>Science</tags>
		<description></description>
		<date>2019-07-05 13:03:25</date>
		<count>1</count>
		<journal>International Journal of Trend in Scientific Research and Development</journal>
		<year>2019</year>
		<url>https://www.ijtsrd.com/humanities-and-the-arts/social-science/23723/representation-of-violence-in-music-videos-special-reference-to-most-viewed-ten-sinhala-music-videos-in-2016/e-m-t-t-ekanayake</url>
		<author>E. M. T. T. Ekanayake | E. W. M. S. Boyagoda</author>
		<authors>
			<first>E. M. T. T. Ekanayake | E. W. M. S.</first>
		</authors>
		<authors>
			<last>Boyagoda</last>
		</authors>
		<volume>3</volume>
		<number>4</number>
		<pages>575-577</pages>
		<abstract>This research seeks to find out representation of violence in Sri Lankan music videos. The purposive sampling method was used to select the sample and content analysis method performed to collect data. This research has found out that, Sri Lankan music videos have represented violence related content. Every music video has been portrayed at least one out of the nine violence. Majority of the music videos have been represented physical violence, emotional and psychological violence, sexual violence and verbal violence rather than other types such as spiritual, financial, neglect etc. more time has given to represent emotional violence, physical, cultural and verbal violence and less amount of time has given for spiritual violence, financial violence, neglect and other. E. M. T. T. Ekanayake | E. W. M. S. Boyagoda ""Representation of Violence in Music Videos (Special Reference to Most Viewed Ten Sinhala Music Videos in 2016)"" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-3 | Issue-4 , June 2019, URL: https://www.ijtsrd.com/papers/ijtsrd23723.pdf

 Paper URL: https://www.ijtsrd.com/humanities-and-the-arts/social-science/23723/representation-of-violence-in-music-videos-special-reference-to-most-viewed-ten-sinhala-music-videos-in-2016/e-m-t-t-ekanayake</abstract>
		<language>English</language>
		<issn>2456-6470</issn>
		<doi>https://doi.org/10.31142/ijtsrd23723</doi>
		<title>Representation of Violence in Music Videos Special Reference to Most Viewed Ten Sinhala Music Videos in 2016</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26a51fcac668daff84f5f92d228e30537/hotho</id>
		<tags>toread</tags>
		<description>Music Transformer</description>
		<date>2019-07-21 19:37:30</date>
		<count>1</count>
		<year>2018</year>
		<url>http://arxiv.org/abs/1809.04281</url>
		<author>Cheng-Zhi Anna Huang</author>
		<author>Ashish Vaswani</author>
		<author>Jakob Uszkoreit</author>
		<author>Noam Shazeer</author>
		<author>Ian Simon</author>
		<author>Curtis Hawthorne</author>
		<author>Andrew M. Dai</author>
		<author>Matthew D. Hoffman</author>
		<author>Monica Dinculescu</author>
		<author>Douglas Eck</author>
		<authors>
			<first>Cheng-Zhi Anna</first>
		</authors>
		<authors>
			<last>Huang</last>
		</authors>
		<authors>
			<first>Ashish</first>
		</authors>
		<authors>
			<last>Vaswani</last>
		</authors>
		<authors>
			<first>Jakob</first>
		</authors>
		<authors>
			<last>Uszkoreit</last>
		</authors>
		<authors>
			<first>Noam</first>
		</authors>
		<authors>
			<last>Shazeer</last>
		</authors>
		<authors>
			<first>Ian</first>
		</authors>
		<authors>
			<last>Simon</last>
		</authors>
		<authors>
			<first>Curtis</first>
		</authors>
		<authors>
			<last>Hawthorne</last>
		</authors>
		<authors>
			<first>Andrew M.</first>
		</authors>
		<authors>
			<last>Dai</last>
		</authors>
		<authors>
			<first>Matthew D.</first>
		</authors>
		<authors>
			<last>Hoffman</last>
		</authors>
		<authors>
			<first>Monica</first>
		</authors>
		<authors>
			<last>Dinculescu</last>
		</authors>
		<authors>
			<first>Douglas</first>
		</authors>
		<authors>
			<last>Eck</last>
		</authors>
		<abstract>Music relies heavily on repetition to build structure and meaning.
Self-reference occurs on multiple timescales, from motifs to phrases to reusing
of entire sections of music, such as in pieces with ABA structure. The
Transformer (Vaswani et al., 2017), a sequence model based on self-attention,
has achieved compelling results in many generation tasks that require
maintaining long-range coherence. This suggests that self-attention might also
be well-suited to modeling music. In musical composition and performance,
however, relative timing is critically important. Existing approaches for
representing relative positional information in the Transformer modulate
attention based on pairwise distance (Shaw et al., 2018). This is impractical
for long sequences such as musical compositions since their memory complexity
for intermediate relative information is quadratic in the sequence length. We
propose an algorithm that reduces their intermediate memory requirement to
linear in the sequence length. This enables us to demonstrate that a
Transformer with our modified relative attention mechanism can generate
minute-long compositions (thousands of steps, four times the length modeled in
Oore et al., 2018) with compelling structure, generate continuations that
coherently elaborate on a given motif, and in a seq2seq setup generate
accompaniments conditioned on melodies. We evaluate the Transformer with our
relative attention mechanism on two datasets, JSB Chorales and
Piano-e-Competition, and obtain state-of-the-art results on the latter.</abstract>
		<title>Music Transformer</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e3a9c01ad668f3000f8fb7532eeb5f3d/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>Proceedings of the Sound and Music Computing Conference 2013, SMC 2013, Stockholm, Sweden :</booktitle>
		<year>2013</year>
		<url>http://www.speech.kth.se/prod/publications/files/3880.pdf</url>
		<author>R. Parncutt</author>
		<author>E. Bisesi</author>
		<author>Anders Friberg</author>
		<authors>
			<first>R.</first>
		</authors>
		<authors>
			<last>Parncutt</last>
		</authors>
		<authors>
			<first>E.</first>
		</authors>
		<authors>
			<last>Bisesi</last>
		</authors>
		<authors>
			<first>Anders</first>
		</authors>
		<authors>
			<last>Friberg</last>
		</authors>
		<pages>335--340</pages>
		<abstract>We describe the first stage of a two-stage semialgorithmic approach
	to music performance rendering. In the first stage, we estimate the
	perceptual salience of immanent accents (phrasing, metrical, melodic,
	harmonic) in the musical score. In the second, we manipulate timing,
	dynamics and other performance parameters in the vicinity of immanent
	accents (e. g., getting slower and/or louder near an accent). Phrasing
	and metrical accents emerge from the hierarchical structure of phrasing
	and meter; their salience depends on the hierarchical levels that
	they demarcate, and their salience. Melodic accents follow melodic
	leaps; they are strongest at contour peaks and (to a lesser extent)
	valleys; and their salience depends on the leap interval and the
	distance of the target tone from the local mean pitch. Harmonic accents
	depend on local dissonance (roughness, non-harmonicity, non-diatonicity)
	and chord/key changes. The algorithm is under development and is
	being tested by comparing its predictions with music analyses, recorded
	performances and listener evaluations.</abstract>
		<title>A Preliminary Computational Model of Immanent Accent Salience in Tonal Music</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a75c4a8799b9ee3c2cca0522ad53d3e8/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>2</count>
		<booktitle>International Computer Music Conference</booktitle>
		<series>Proceedings of the International Computer Music Conference 2008</series>
		<year>2008</year>
		<url>http://sro.sussex.ac.uk/28543/</url>
		<author>Anna Jordanous</author>
		<authors>
			<first>Anna</first>
		</authors>
		<authors>
			<last>Jordanous</last>
		</authors>
		<abstract>Much polyphonic music is constructed from several melodic lines -
	known as voices - woven together. Identifying these constituent voices
	is useful for musicological analysis and music information retrieval;
	however, this voice-identification process is time-consuming for
	humans to carry out. Computational solutions have been proposed which
	automate voice segregation, but these rely heavily on human musical
	knowledge being encoded into the system. In this paper, a system
	is presented which is able to learn how to separate such polyphonic
	music into its individual parts. This system uses a training corpus
	of several similar pieces of music, in symbolic format (MIDI). It
	examines the note pitches in the training examples to make observations
	about the voice structures. Quantitative evaluation was carried out
	using 3-fold validation, a standard data mining evaluation method.
	This system offers a valid solution to this complex problem, with
	a 12\% improvement in performance compared to a baseline algorithm.
	It achieves an equal standard of performance to heuristic-based systems
	using simple statistical observations: demonstrating the power of
	applying data-driven techniques to the voice separation problem.</abstract>
		<title>Voice Separation in Polyphonic Music: A Data-driven Approach</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26f1552f97ccd06ccd3b8996762f24b4e/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>2</count>
		<publisher>International Society for Music Information Retrieval</publisher>
		<year>2010</year>
		<url>http://ismir2010.ismir.net/proceedings/ismir2010-108.pdf</url>
		<author>Michael Scott Cuthbert</author>
		<author>Christopher Ariza</author>
		<authors>
			<first>Michael Scott</first>
		</authors>
		<authors>
			<last>Cuthbert</last>
		</authors>
		<authors>
			<first>Christopher</first>
		</authors>
		<authors>
			<last>Ariza</last>
		</authors>
		<pages>637--642</pages>
		<abstract>Music21 is an object-oriented toolkit for analyzing, searching, and
	transforming music in symbolic (score- based) forms. The modular
	approach of the project allows musicians and researchers to write
	simple scripts rapidly and reuse them in other projects. The toolkit
	aims to pro- vide powerful software tools integrated with sophisticated
	musical knowledge to both musicians with little pro- gramming experience
	(especially musicologists) and to programmers with only modest music
	theory skills. This paper introduces the music21 system, demon- strating
	how to use it and the types of problems it is well- suited toward
	advancing. We include numerous examples of its power and flexibility,
	including demonstrations of graphing data and generating annotated
	musical scores. 1.</abstract>
		<journaltitle>11th International Society for Music Information Retrieval Conference (ismir 2010)2</journaltitle>
		<title>music21: A Toolkit for Computer-Aided Musicology and Symbolic Music Data</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f7dc17eda65e5b98e239971f25d78c73/ijtsrd</id>
		<tags>Pump</tags>
		<tags>Musical</tags>
		<tags>fountain</tags>
		<tags>&</tags>
		<tags>frequency</tags>
		<tags>Electronics</tags>
		<tags>Fourier</tags>
		<tags>water</tags>
		<tags>Amplifier</tags>
		<tags>Engineering</tags>
		<tags>Arduino</tags>
		<tags>Fast</tags>
		<tags>Transform</tags>
		<tags>Communication</tags>
		<tags>Signal</tags>
		<tags>Audio</tags>
		<description></description>
		<date>2019-09-10 12:11:30</date>
		<count>1</count>
		<journal>International Journal of Trend in Scientific Research and Development</journal>
		<year>2019</year>
		<url>https://www.ijtsrd.com/engineering/electronics-and-communication-engineering/25254/design-and-construction-of-musical-lighting-water-fountain/kyi-phyu-soe</url>
		<author>Kyi Phyu Soe | Win Zaw Hein</author>
		<authors>
			<first>Kyi Phyu Soe | Win Zaw</first>
		</authors>
		<authors>
			<last>Hein</last>
		</authors>
		<volume>3</volume>
		<number>5</number>
		<pages>161-165</pages>
		<abstract>This paper presents the design and construction of a musical water fountain system driven by Arduino using Fourier Transform technology. Audio signals were taken from sources such as MP3 player smart phone laptop etc and these signals were first amplified using LM386 IC and then transferred into an analog input of Arduino for FFT processing. To give out signals for DC pump motors digital output pins was used. With various frequency band turning on and off the DC motors and LED systems was created the lighting and water spray effects which should go well along with showed music. The response of this foundation is analysed for different audio frequency range and pump motor control was emphasised and fountain model is designed for domestic usages. Kyi Phyu Soe | Win Zaw Hein "Design and Construction of Musical Lighting Water Fountain" Published in International Journal of Trend in Scientific Research and Development (ijtsrd) ISSN: 2456-6470 Volume-3 | Issue-5  August 2019 URL: https://www.ijtsrd.com/papers/ijtsrd25254.pdfPaper URL: https://www.ijtsrd.com/engineering/electronics-and-communication-engineering/25254/design-and-construction-of-musical-lighting-water-fountain/kyi-phyu-soe</abstract>
		<language>English</language>
		<issn>2456-6470</issn>
		<doi>https://doi.org/10.31142/ijtsrd25254</doi>
		<title>Design and Construction of Musical Lighting Water Fountain</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ab2db6661b709f688fa26a54b771955b/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#ChengLC012</url>
		<author>Bo-Chao Cheng</author>
		<author>Guo-Tan Liao</author>
		<author>Chun-Yu Chen</author>
		<author>Huan Chen</author>
		<authors>
			<first>Bo-Chao</first>
		</authors>
		<authors>
			<last>Cheng</last>
		</authors>
		<authors>
			<first>Guo-Tan</first>
		</authors>
		<authors>
			<last>Liao</last>
		</authors>
		<authors>
			<first>Chun-Yu</first>
		</authors>
		<authors>
			<last>Chen</last>
		</authors>
		<authors>
			<first>Huan</first>
		</authors>
		<authors>
			<last>Chen</last>
		</authors>
		<pages>176-181</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>A Multi-level Fuzzy Comprehensive Evaluation Approach for Message Verification in VANETs.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/212525a48e0eee479170268f07aa735af/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#Wang12</url>
		<author>Tong-Chai Wang</author>
		<authors>
			<first>Tong-Chai</first>
		</authors>
		<authors>
			<last>Wang</last>
		</authors>
		<pages>66-71</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>An Adjustable Heuristic for Offset Assignment Problems in Embedded System Design.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29f03f5ca5fc003c1c838558a7cce3036/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#LiN12</url>
		<author>Celia Li</author>
		<author>Uyen Trang Nguyen</author>
		<authors>
			<first>Celia</first>
		</authors>
		<authors>
			<last>Li</last>
		</authors>
		<authors>
			<first>Uyen Trang</first>
		</authors>
		<authors>
			<last>Nguyen</last>
		</authors>
		<pages>101-108</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Efficient Group Key Management in Wireless LANs.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27350252dd83b5d6a843863cf3f5fae4b/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2018-06-25 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#OuZLZZ12</url>
		<author>Qinghai Ou</author>
		<author>Yan Zhen</author>
		<author>Xiangzhen Li</author>
		<author>Yiying Zhang</author>
		<author>Lingkang Zeng</author>
		<authors>
			<first>Qinghai</first>
		</authors>
		<authors>
			<last>Ou</last>
		</authors>
		<authors>
			<first>Yan</first>
		</authors>
		<authors>
			<last>Zhen</last>
		</authors>
		<authors>
			<first>Xiangzhen</first>
		</authors>
		<authors>
			<last>Li</last>
		</authors>
		<authors>
			<first>Yiying</first>
		</authors>
		<authors>
			<last>Zhang</last>
		</authors>
		<authors>
			<first>Lingkang</first>
		</authors>
		<authors>
			<last>Zeng</last>
		</authors>
		<pages>96-100</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Application of Internet of Things in Smart Grid Power Transmission.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26be855328ffb55379314420f330fe861/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#LiLL12</url>
		<author>Ruixue Li</author>
		<author>Jianming Liu</author>
		<author>Xiangzhen Li</author>
		<authors>
			<first>Ruixue</first>
		</authors>
		<authors>
			<last>Li</last>
		</authors>
		<authors>
			<first>Jianming</first>
		</authors>
		<authors>
			<last>Liu</last>
		</authors>
		<authors>
			<first>Xiangzhen</first>
		</authors>
		<authors>
			<last>Li</last>
		</authors>
		<pages>236-239</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>An Electric Power Asset Localization Scheme Based on LLA and ZigBee.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/249d2eeca9af758f774c0a6f61e8bd6b3/sapo</id>
		<tags>read_list</tags>
		<tags>music_demography</tags>
		<description>Inferring personal traits from music listening history | Proceedings of the second international ACM workshop on Music information retrieval with user-centered and multimodal strategies</description>
		<date>2020-05-14 09:43:13</date>
		<count>2</count>
		<booktitle>Proceedings of the second international ACM workshop on Music information retrieval with user-centered and multimodal strategies - MIRUM \textquotesingle12</booktitle>
		<publisher>ACM Press</publisher>
		<year>2012</year>
		<url>https://doi.org/10.1145%2F2390848.2390856</url>
		<author>Jen-Yu Liu</author>
		<author>Yi-Hsuan Yang</author>
		<authors>
			<first>Jen-Yu</first>
		</authors>
		<authors>
			<last>Liu</last>
		</authors>
		<authors>
			<first>Yi-Hsuan</first>
		</authors>
		<authors>
			<last>Yang</last>
		</authors>
		<editor>Cynthia C. S. Liem</editor>
		<editor>Meinard Müller</editor>
		<editor>Steven K. Tjoa</editor>
		<editor>George Tzanetakis</editor>
		<editors>
			<first>Yi-Hsuan</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<editors>
			<first>Yi-Hsuan</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<editors>
			<first>Yi-Hsuan</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<editors>
			<first>Yi-Hsuan</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<pages>31-36</pages>
		<doi>10.1145/2390848.2390856</doi>
		<title>Inferring personal traits from music listening history.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/227866b4ce195cb0db13e16d79315aca6/sapo</id>
		<tags>myown</tags>
		<tags>conference</tags>
		<description>On the Adoption of Standard Encoding Formats to Ensure Interoperability of Music Digital Archives: The IEEE 1599 Format | Zenodo</description>
		<date>2020-02-20 16:02:03</date>
		<count>2</count>
		<booktitle>6th International Conference on Digital Libraries for Musicology</booktitle>
		<publisher>ACM</publisher>
		<year>2019</year>
		<url>https://air.unimi.it/handle/2434/687286</url>
		<author>Luca Andrea Ludovico</author>
		<author>Adriano Baratè</author>
		<author>Federico Simonetta</author>
		<author>Davide Andrea Mauro</author>
		<authors>
			<first>Luca Andrea</first>
		</authors>
		<authors>
			<last>Ludovico</last>
		</authors>
		<authors>
			<first>Adriano</first>
		</authors>
		<authors>
			<last>Baratè</last>
		</authors>
		<authors>
			<first>Federico</first>
		</authors>
		<authors>
			<last>Simonetta</last>
		</authors>
		<authors>
			<first>Davide Andrea</first>
		</authors>
		<authors>
			<last>Mauro</last>
		</authors>
		<editor>David Rizo</editor>
		<editors>
			<first>Davide Andrea</first>
		</editors>
		<editors>
			<last>Mauro</last>
		</editors>
		<pages>20-24</pages>
		<abstract>With this paper, we want to stimulate the discussion about technologies for inter-operation between various music datasets and collections. Among the many standards for music representation, IEEE 1599 is the only one which was born with the exact purpose of representing the heterogeneous structures of music documents, granting full synchronization of all the different aspects of music (audio recordings, sheet music images, symbolic representations, musicological analysis, etc). We propose the adoption of IEEE 1599 as an interoperability framework between different collections for advanced music experience, musicological applications, and Music Information Retrieval (MIR). In the years to come, the format will undergo a review process aimed at providing an updated/improved version. It is now the perfect time, for all the stakeholders, to come together and discuss how the format can evolve to better support their requirements, enhancing its descriptive strength and available tools. Moreover, this standard can be profitably applied to any field that requires multi-layer and synchronized descriptions.</abstract>
		<doi>10.1145/3358664.3358665</doi>
		<title>On the Adoption of Standard Encoding Formats to Ensure Interoperability of Music Digital Archives: The IEEE 1599 Format.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2929ff10d1360f6396c5108354a33707a/inmantang</id>
		<tags>music</tags>
		<tags>aware</tags>
		<tags>recommendation</tags>
		<tags>context</tags>
		<description>Context-aware music recommendation based on latenttopic sequential patterns</description>
		<date>2014-04-05 09:15:59</date>
		<count>6</count>
		<booktitle>Proceedings of the sixth ACM conference on Recommender systems</booktitle>
		<series>RecSys '12</series>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2012</year>
		<url>http://doi.acm.org/10.1145/2365952.2365979</url>
		<author>Negar Hariri</author>
		<author>Bamshad Mobasher</author>
		<author>Robin Burke</author>
		<authors>
			<first>Negar</first>
		</authors>
		<authors>
			<last>Hariri</last>
		</authors>
		<authors>
			<first>Bamshad</first>
		</authors>
		<authors>
			<last>Mobasher</last>
		</authors>
		<authors>
			<first>Robin</first>
		</authors>
		<authors>
			<last>Burke</last>
		</authors>
		<pages>131--138</pages>
		<abstract>Contextual factors can greatly influence the users' preferences in listening to music. Although it is hard to capture these factors directly, it is possible to see their effects on the sequence of songs liked by the user in his/her current interaction with the system. In this paper, we present a context-aware music recommender system which infers contextual information based on the most recent sequence of songs liked by the user. Our approach mines the top frequent tags for songs from social tagging Web sites and uses topic modeling to determine a set of latent topics for each song, representing different contexts. Using a database of human-compiled playlists, each playlist is mapped into a sequence of topics and frequent sequential patterns are discovered among these topics. These patterns represent frequent sequences of transitions between the latent topics representing contexts. Given a sequence of songs in a user's current interaction, the discovered patterns are used to predict the next topic in the playlist. The predicted topics are then used to post-filter the initial ranking produced by a traditional recommendation algorithm. Our experimental evaluation suggests that our system can help produce better recommendations in comparison to a conventional recommender system based on collaborative or content-based filtering. Furthermore, the topic modeling approach proposed here is also useful in providing better insight into the underlying reasons for song selection and in applications such as playlist construction and context prediction.</abstract>
		<isbn>978-1-4503-1270-7</isbn>
		<doi>10.1145/2365952.2365979</doi>
		<title>Context-aware music recommendation based on latenttopic sequential patterns</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b967197b5cddea0e872e43851a6c4052/joaakive</id>
		<tags>ethnomusicology</tags>
		<tags>music</tags>
		<tags>theory</tags>
		<description></description>
		<date>2014-04-27 09:07:01</date>
		<count>1</count>
		<journal>FSTC Limited</journal>
		<year>2007</year>
		<url>http://www.muslimheritage.com/article/safi-al-din-al-urmawi-and-theory-music</url>
		<author>Fazli Arslan</author>
		<authors>
			<first>Fazli</first>
		</authors>
		<authors>
			<last>Arslan</last>
		</authors>
		<editor>Salim Ayduz Mohamed El-Gomati, Mohammed Abattouy</editor>
		<editors>
			<first>Fazli</first>
		</editors>
		<editors>
			<last>Arslan</last>
		</editors>
		<abstract>In this detailed and well documented article, Dr. Fazli Arslan describes the work of one of the most important figures of the history of Middle Eastern music, Safi al-Din 'Abd al-Mu'min al-Urmawi.</abstract>
		<title>Safī al-Dīn al-Urmawī and the Theory of Music: Al-Risāla al-sharafiyya fī al-nisab al-ta’līfiyya Content, Analysis, and Influences</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24adf9aa850fc2efdf22155da39b201c9/ks-plugin-devel</id>
		<tags>music_information_retrieval</tags>
		<tags>music</tags>
		<tags>Sci-MIDI_toolbox</tags>
		<tags>music_research</tags>
		<tags>mathematics_computing</tags>
		<tags>style_classification</tags>
		<tags>music_emotion_recognition</tags>
		<tags>open_source_Scilab_toolbox</tags>
		<tags>Scilab_computing_environment</tags>
		<description></description>
		<date>2013-02-02 14:40:30</date>
		<count>2</count>
		<booktitle>Open-source Software for Scientific Computation (OSSC), 2009 IEEE International Workshop on</booktitle>
		<year>2009</year>
		<url></url>
		<author>Zhigang Huang</author>
		<author>Changle Zhou</author>
		<authors>
			<first>Zhigang</first>
		</authors>
		<authors>
			<last>Huang</last>
		</authors>
		<authors>
			<first>Changle</first>
		</authors>
		<authors>
			<last>Zhou</last>
		</authors>
		<pages>159 -162</pages>
		<abstract>Sci-MIDI Toolbox, an open source Scilab toolbox for music research, provides a set of Scilab functions, which have versatile ways to analyze and compute MIDI files' data based on the Scilab computing environment. It contains extracting, computing and visualizing MIDI files data which is relating to musical melodic notes and others information, besides basic manipulation, filtering, statistics and some algorithms that are often used involved in musical research. The development of the Toolbox is suitable for some research of music information retrieval, music emotion recognition and style classification and others using with MIDI data.</abstract>
		<doi>10.1109/OSSC.2009.5416915</doi>
		<title>Sci-MIDI toolbox for music research</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23901fdc2704af2a653e846d6b881edd2/ks-plugin-devel</id>
		<tags>Informatik</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2004</date>
		<count>2</count>
		<journal>Computer Music Modeling and Retrieval</journal>
		<year>2004</year>
		<url>http://www.springerlink.com/content/chuqyybpguvjmxq7</url>
		<author>Gerald Friedland</author>
		<author>Kristian Jantz</author>
		<author>Lars Knipping</author>
		<authors>
			<first>Gerald</first>
		</authors>
		<authors>
			<last>Friedland</last>
		</authors>
		<authors>
			<first>Kristian</first>
		</authors>
		<authors>
			<last>Jantz</last>
		</authors>
		<authors>
			<first>Lars</first>
		</authors>
		<authors>
			<last>Knipping</last>
		</authors>
		<pages>89--123</pages>
		<abstract>This paper describes our approach for editing PSID files. PSID is a sound format that allows the original Commodore 64 synthesizer music to be played on modern computers and thus conserves a music subculture of the eighties. So far, editing PSID files required working directly with Commodore 64 machine language. The paper gives a small overview of sound synthesis with the Commodore 64, argues why this topic is still interesting for both musicians and computer scientists, and describes our editing approach.</abstract>
		<doi>10.1007/b12000</doi>
		<title>Conserving an Ancient Art of Music: Making SID Tunes Editable</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23a244bf0cd60252269e3c36530e34e8f/hotho</id>
		<tags>models</tags>
		<tags>music</tags>
		<tags>statistical</tags>
		<tags>toread</tags>
		<description>Statistical models of music-listening sessions in social media</description>
		<date>2012-09-19 10:32:38</date>
		<count>3</count>
		<booktitle>Proceedings of the 19th international conference on World wide web</booktitle>
		<series>WWW '10</series>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2010</year>
		<url>http://doi.acm.org/10.1145/1772690.1772794</url>
		<author>Elena Zheleva</author>
		<author>John Guiver</author>
		<author>Eduarda Mendes Rodrigues</author>
		<author>Natasa Milić-Frayling</author>
		<authors>
			<first>Elena</first>
		</authors>
		<authors>
			<last>Zheleva</last>
		</authors>
		<authors>
			<first>John</first>
		</authors>
		<authors>
			<last>Guiver</last>
		</authors>
		<authors>
			<first>Eduarda</first>
		</authors>
		<authors>
			<last>Mendes Rodrigues</last>
		</authors>
		<authors>
			<first>Natasa</first>
		</authors>
		<authors>
			<last>Milić-Frayling</last>
		</authors>
		<pages>1019--1028</pages>
		<abstract>User experience in social media involves rich interactions with the media content and other participants in the community. In order to support such communities, it is important to understand the factors that drive the users' engagement. In this paper we show how to define statistical models of different complexity to describe patterns of song listening in an online music community. First, we adapt the LDA model to capture music taste from listening activities across users and identify both the groups of songs associated with the specific taste and the groups of listeners who share the same taste. Second, we define a graphical model that takes into account listening sessions and captures the listening moods of users in the community. Our session model leads to groups of songs and groups of listeners with similar behavior across listening sessions and enables faster inference when compared to the LDA model. Our experiments with the data from an online media site demonstrate that the session model is better in terms of the perplexity compared to two other models: the LDA-based taste model that does not incorporate cross-session information and a baseline model that does not use latent groupings of songs.</abstract>
		<isbn>978-1-60558-799-8</isbn>
		<doi>10.1145/1772690.1772794</doi>
		<title>Statistical models of music-listening sessions in social media</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a67328ff553022857113ce7456a3d7cf/gromgull</id>
		<tags>recurrent-neural-networks</tags>
		<tags>music</tags>
		<tags>machine-learning</tags>
		<tags>neural-networks</tags>
		<tags>application</tags>
		<description>CiteSeerX — Finding temporal structure in music: Blues improvisation with LSTM recurrent networks</description>
		<date>2011-08-12 10:19:48</date>
		<count>3</count>
		<booktitle>NEURAL NETWORKS FOR SIGNAL PROCESSING XII, PROCEEDINGS OF THE 2002 IEEE WORKSHOP</booktitle>
		<publisher>IEEE</publisher>
		<year>2002</year>
		<url>http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.141.2656</url>
		<author>Douglas Eck</author>
		<author>Jürgen Schmidhuber</author>
		<authors>
			<first>Douglas</first>
		</authors>
		<authors>
			<last>Eck</last>
		</authors>
		<authors>
			<first>Jürgen</first>
		</authors>
		<authors>
			<last>Schmidhuber</last>
		</authors>
		<pages>747--756</pages>
		<abstract>Few types of signal streams are as ubiquitous as music. Here we consider the problem of extracting essential ingredients of music signals, such as well-defined global temporal structure in the form of nested periodicities (or meter). Can we construct an adaptive signal processing device that learns by example how to generate new instances of a given musical style? Because recurrent neural networks can in principle learn the temporal structure of a signal, they are good candidates for such a task. Unfortunately, music composed by standard recurrent neural networks (RNNs) often lacks global coherence. The reason for this failure seems to be that RNNs cannot keep track of temporally distant events that indicate global music structure. Long Short-Term Memory (LSTM) has succeeded in similar domains where other RNNs have failed, such as timing &amp; counting and learning of context sensitive languages. In the current study we show that LSTM is also a good mechanism for learning to compose music. We present experimental results showing that LSTM successfully learns a form of blues music and is able to compose novel (and we believe pleasing) melodies in that style. Remarkably, once the network has found the relevant structure it does not drift from it: LSTM is able to play the blues with good timing and proper structure as long as one is willing to listen.</abstract>
		<title>Finding temporal structure in music: Blues improvisation with LSTM recurrent networks</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25a0d64d648ac4b77b593a8047ea88beb/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-11-29 00:00:00</date>
		<count>1</count>
		<booktitle>Ubiquitous Music</booktitle>
		<series>Computational Music Science</series>
		<publisher>Springer</publisher>
		<year>2014</year>
		<url>http://dblp.uni-trier.de/db/series/music/um2014.html#KellerLP14</url>
		<author>Damián Keller</author>
		<author>Victor Lazzarini</author>
		<author>Marcelo Soares Pimenta</author>
		<authors>
			<first>Damián</first>
		</authors>
		<authors>
			<last>Keller</last>
		</authors>
		<authors>
			<first>Victor</first>
		</authors>
		<authors>
			<last>Lazzarini</last>
		</authors>
		<authors>
			<first>Marcelo Soares</first>
		</authors>
		<authors>
			<last>Pimenta</last>
		</authors>
		<editor>Damián Keller</editor>
		<editor>Victor Lazzarini</editor>
		<editor>Marcelo Soares Pimenta</editor>
		<editors>
			<first>Marcelo Soares</first>
		</editors>
		<editors>
			<last>Pimenta</last>
		</editors>
		<editors>
			<first>Marcelo Soares</first>
		</editors>
		<editors>
			<last>Pimenta</last>
		</editors>
		<editors>
			<first>Marcelo Soares</first>
		</editors>
		<editors>
			<last>Pimenta</last>
		</editors>
		<pages>3-23</pages>
		<isbn>978-3-319-11151-3</isbn>
		<title>Ubimus Through the Lens of Creativity Theories.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/240834a33012fd49a6f8fb2c8aa0d19e2/skasey</id>
		<tags>environment</tags>
		<tags>wearable</tags>
		<tags>design</tags>
		<tags>urban</tags>
		<tags>context</tags>
		<tags>bibtex-import</tags>
		<tags>interaction</tags>
		<tags>interactive</tags>
		<tags>mobile</tags>
		<description></description>
		<date>2008-08-21 00:12:13</date>
		<count>1</count>
		<booktitle>NIME '03: Proceedings of the 2003 Conference on New Interfaces for Musical Expression</booktitle>
		<publisher>National University of Singapore</publisher>
		<address>Singapore</address>
		<year>2003</year>
		<url>http://portal.acm.org/citation.cfm?id=1085714.1085741\#</url>
		<author>Lalya Gaye</author>
		<author>Ramia Maz\ a\copyright</author>
		<author>Lars E. Holmquist</author>
		<authors>
			<first>Lalya</first>
		</authors>
		<authors>
			<last>Gaye</last>
		</authors>
		<authors>
			<first>Ramia Maz\</first>
		</authors>
		<authors>
			<last>a\copyright</last>
		</authors>
		<authors>
			<first>Lars E.</first>
		</authors>
		<authors>
			<last>Holmquist</last>
		</authors>
		<abstract>In the project Sonic City, we have developed a system that enables users to create electronic music in real time by walking through and interacting with the urban environment. We explore the use of public space and everyday behaviours for creative purposes, in particular the city as an interface and mobilityas aninteractionmodel for electronic music making. A multi-disciplinary design process resulted in the implementation of a wearable, context-aware prototype. The system produces music by retrieving information about context anduser actionandmappingit toreal-timeprocessing of urbansounds. Potentials, constraints, and implications of thistypeof musiccreationarediscussed.</abstract>
		<title>Sonic City: the urban environment as a musical interface</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bf082668567a66f253b1bffb3442b040/lee_peck</id>
		<tags>Levy07musicTags</tags>
		<tags>tagging</tags>
		<tags>music</tags>
		<tags>social</tags>
		<tags>semantics</tags>
		<description>ISMIR 2007 - 8th International Conference on Music Information Retrieval</description>
		<date>2008-10-29 18:38:54</date>
		<count>6</count>
		<booktitle>8th International Conference on Music Information Retrieval (ISMIR 2007)</booktitle>
		<year>2007</year>
		<url>http://ismir2007.ismir.net/proceedings/ISMIR2007_p411_levy.pdf</url>
		<author>Mark Levy</author>
		<author>Mark Sandler</author>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Levy</last>
		</authors>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Sandler</last>
		</authors>
		<title>A Semantic Space for Music Derived from Social Tags</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c37e6c646f6df6fd2722510c79814e96/michael.sherbon</id>
		<tags>ancient</tags>
		<tags>sound</tags>
		<tags>pythagoras</tags>
		<tags>mythology</tags>
		<tags>history</tags>
		<tags>philosophy</tags>
		<tags>literature</tags>
		<tags>classic</tags>
		<tags>science</tags>
		<tags>music</tags>
		<tags>of</tags>
		<tags>amazon</tags>
		<tags>and</tags>
		<tags>greek</tags>
		<description>Amazon.com: The Music of Pythagoras: How an Ancient Brotherhood Cracked the Code of the Universe and Lit the Path from Antiquity to Oute: Kitty Ferguson: Books</description>
		<date>2008-09-08 21:32:07</date>
		<count>1</count>
		<publisher>Walker & Company</publisher>
		<year>2008</year>
		<url>http://www.amazon.com/Music-Pythagoras-Brotherhood-Universe-Antiquity/dp/0802716318%3FSubscriptionId%3D13CT5CVB80YFWJEPWS02%26tag%3Dws%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0802716318</url>
		<author>Kitty Ferguson</author>
		<authors>
			<first>Kitty</first>
		</authors>
		<authors>
			<last>Ferguson</last>
		</authors>
		<ean>9780802716316</ean>
		<asin>0802716318</asin>
		<isbn>0802716318</isbn>
		<dewey>182.2</dewey>
		<title>The Music of Pythagoras: How an Ancient Brotherhood Cracked the Code of the Universe and Lit the Path from Antiquity to Oute</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2daa312cc50ee2e53a48fe78f88586a8f/jaeschke</id>
		<tags>annotation</tags>
		<tags>game</tags>
		<tags>semantic</tags>
		<tags>music</tags>
		<description></description>
		<date>2008-10-15 13:59:44</date>
		<count>7</count>
		<booktitle>In 8th International Conference on Music Information Retrieval (ISMIR</booktitle>
		<year>2007</year>
		<url></url>
		<author>Douglas Turnbull</author>
		<author>Ruoran Liu</author>
		<author>Luke Barrington</author>
		<author>Gert Lanckriet</author>
		<authors>
			<first>Douglas</first>
		</authors>
		<authors>
			<last>Turnbull</last>
		</authors>
		<authors>
			<first>Ruoran</first>
		</authors>
		<authors>
			<last>Liu</last>
		</authors>
		<authors>
			<first>Luke</first>
		</authors>
		<authors>
			<last>Barrington</last>
		</authors>
		<authors>
			<first>Gert</first>
		</authors>
		<authors>
			<last>Lanckriet</last>
		</authors>
		<abstract>Games based on human computation are a valuable tool for collecting semantic information about images. We show how to transfer this idea into the music domain in order to collect high-quality semantic data for songs. We present Listen Game, a online, multiplayer game that measures the semantic relationship between music and words. In the normal mode, a player sees a list of semantically related words (e.g., ‘Instruments’, ‘Emotions ’ ‘Usages’) and is asked to pick the best and worst word to describe a song. In the freestyle mode, a user is asked to suggest a new word that describes the music. Each player receives realtime feedback on the agreement amongst all players. We show that we can use the data collected during a twoweek pilot study of Listen Game to learn a supervised multiclass labeling (SML) model which can annotate a novel song with meaningful words and retrieve relevant songs from a database of audio content.</abstract>
		<title>A game-based approach for collecting semantic annotations of music</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29990c26f59d1d12bd46ecacf61179e32/kurtjx</id>
		<tags>music</tags>
		<tags>genre</tags>
		<description></description>
		<date>2009-04-13 14:29:01</date>
		<count>9</count>
		<journal>Journal of New Music Research</journal>
		<year>2003</year>
		<url></url>
		<author>Jean julien Aucouturier</author>
		<author>François Pachet</author>
		<authors>
			<first>Jean</first>
		</authors>
		<authors>
			<last>julien Aucouturier</last>
		</authors>
		<authors>
			<first>François</first>
		</authors>
		<authors>
			<last>Pachet</last>
		</authors>
		<volume>32</volume>
		<pages>2003</pages>
		<abstract>Musical genre is probably the most popular music descriptor. In the context of large musical databases and Electronic Music Distribution, genre is therefore a crucial metadata for the description of music content. However, genre is intrinsically ill-defined and attempts at defining genre precisely have a strong tendency to end up in circular, ungrounded projections of fantasies. Is genre an intrinsic attribute of music titles, as, say, tempo? Or is genre a extrinsic description of the whole piece? In this article, we discuss the various approaches in representing musical genre, and propose to classify these approaches in three main categories: manual, prescriptive and emergent approaches. We discuss the pros and cons of each approach, and illustrate our study with results of the Cuidado IST project. 1.</abstract>
		<title>Representing musical genre: A state of the art</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23b741261fa8754b53692da344ab7ece5/tfalk</id>
		<tags>imported</tags>
		<description>The meaning of music in the lives of older people: a qualitative study
	-- Hays and Minichiello 33 (4): 437 -- Psychology of Music</description>
		<date>2009-02-28 21:01:39</date>
		<count>2</count>
		<journal>Psychology of Music</journal>
		<year>2005</year>
		<url>http://pom.sagepub.com/cgi/content/abstract/33/4/437</url>
		<author>Terrence Hays</author>
		<author>Victor Minichiello</author>
		<authors>
			<first>Terrence</first>
		</authors>
		<authors>
			<last>Hays</last>
		</authors>
		<authors>
			<first>Victor</first>
		</authors>
		<authors>
			<last>Minichiello</last>
		</authors>
		<volume>33</volume>
		<number>4</number>
		<pages>437-451</pages>
		<abstract>This qualitative study describes the experience of music and focuses
	on the emotional, social, intellectual and spiritual well-being roles
	that music plays in the lives of older people. In-depth interviews
	were used to explore the meaning, importance and function of music
	for 52 older Australians living in the community aged 60 years and
	older. The findings revealed that music provides people with ways
	of understanding and developing their self-identity; connecting with
	others; maintaining well-being; and experiencing and expressing spirituality.
	The results show how music contributes to positive ageing by providing
	ways for people to maintain positive self-esteem, feel competent,
	independent, and avoid feelings of isolation or loneliness. The study
	highlights the need to be better informed about how music can facilitate
	and sustain older people's well-being.</abstract>
		<doi>10.1177/0305735605056160</doi>
		<eprint>http://pom.sagepub.com/cgi/reprint/33/4/437.pdf</eprint>
		<title>The meaning of music in the lives of older people: a qualitative
	study</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c9ff56e22a484919c806e75a3ea2e503/tfalk</id>
		<tags>imported</tags>
		<description>The content and validity of music-genre stereotypes among college
	students -- Rentfrow and Gosling 35 (2): 306 -- Psychology of Music</description>
		<date>2009-02-28 21:01:39</date>
		<count>2</count>
		<journal>Psychology of Music</journal>
		<year>2007</year>
		<url>http://pom.sagepub.com/cgi/content/abstract/35/2/306</url>
		<author>Peter J. Rentfrow</author>
		<author>Samuel D. Gosling</author>
		<authors>
			<first>Peter J.</first>
		</authors>
		<authors>
			<last>Rentfrow</last>
		</authors>
		<authors>
			<first>Samuel D.</first>
		</authors>
		<authors>
			<last>Gosling</last>
		</authors>
		<volume>35</volume>
		<number>2</number>
		<pages>306-326</pages>
		<abstract>The present research examined the content and validity of stereotypes
	about fans of 14 different music genres (e.g. country, rap, rock).
	In particular, we focused on stereotypes concerning fans' personalities
	(e.g. extraversion, emotional stability), personal qualities (e.g.
	political beliefs, athleticism), values (e.g. for peace, for wisdom),
	and alcohol and drug preferences (e.g. wine, hallucinogens). Previous
	research has shown that music is linked to a variety of psychological
	characteristics, that music is used to convey information about oneself
	to observers, and that observers can infer personality on the basis
	of music preferences. Guided by such research, we predicted and found
	that individuals have robust and clearly defined stereotypes about
	the fans of various music genres (Study 1), and that many of these
	music-genre stereotypes possess a kernel of truth (Study 2). Discussion
	focuses on the potential role of music-genre stereotypes in self-expression
	and impression formation.</abstract>
		<doi>10.1177/0305735607070382</doi>
		<eprint>http://pom.sagepub.com/cgi/reprint/35/2/306.pdf</eprint>
		<title>The content and validity of music-genre stereotypes among college
	students</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bce5253aca1ef3cefc83a6536d05c77c/brazovayeye</id>
		<tags>genetic</tags>
		<tags>algorithms,</tags>
		<tags>Jazz,</tags>
		<tags>Charlie</tags>
		<tags>Parker</tags>
		<tags>programming,</tags>
		<tags>Music,</tags>
		<description></description>
		<date>2008-06-19 17:46:40</date>
		<count>1</count>
		<booktitle>Proceedings of International Joint Conference on
                 Artificial Intelligence, IJCAI'95 Workshop on Music and
                 AI</booktitle>
		<address>Montreal, Quebec, Canada</address>
		<year>1995</year>
		<url>http://hampshire.edu/lspector/pubs/IJCAI95mus-toappear.ps</url>
		<author>Lee Spector</author>
		<author>Adam Alpern</author>
		<authors>
			<first>Lee</first>
		</authors>
		<authors>
			<last>Spector</last>
		</authors>
		<authors>
			<first>Adam</first>
		</authors>
		<authors>
			<last>Alpern</last>
		</authors>
		<organisation>IJCAII,AAAI,CSCSI</organisation>
		<notes>Induce musical structure in ANN from body of musical
                 works, use resulting ANN as critic used to provide GP
                 fitness function. Uses 96 indexed memory cells plus
                 various block copy primitives and interation. Yeilds
                 änytime" GP. As in Spector:1994:ccaga high
                 fitness GP produced music does not appeal artistically
                 to the author.

                 See http://hampshire.edu/lspector/genbebop.html</notes>
		<title>Induction and Recapitulation of Deep Musical
                 Structure</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c9f9b9a2a09bedd26b21dd769b9ed305/yevb0</id>
		<tags>music,M1,M2,Music,Pitch</tags>
		<tags>Comparison,Humans,India,Indian</tags>
		<tags>Cross-Cultural</tags>
		<tags>Discrimination,Psychoacoustics,music,perception,scale</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Journal of Experimental Psychology: General</journal>
		<year>1984</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/6237169</url>
		<author>Mary A Castellano</author>
		<author>Jamshed J. Bharucha</author>
		<author>Carol L Krumhansl</author>
		<authors>
			<first>Mary A</first>
		</authors>
		<authors>
			<last>Castellano</last>
		</authors>
		<authors>
			<first>Jamshed J.</first>
		</authors>
		<authors>
			<last>Bharucha</last>
		</authors>
		<authors>
			<first>Carol L</first>
		</authors>
		<authors>
			<last>Krumhansl</last>
		</authors>
		<volume>113</volume>
		<number>3</number>
		<pages>394--412</pages>
		<abstract>Cross-culturally, most music is tonal in the sense that one particular
	tone, called the tonic, provides a focus around which the other tones
	are organized. The specific organizational structures around the
	tonic show considerable diversity. Previous studies of the perceptual
	response to Western tonal music have shown that listeners familiar
	with this musical tradition have internalized a great deal about
	its underlying organization. Krumhansl and Shepard (1979) developed
	a probe tone method for quantifying the perceived hierarchy of stability
	of tones. When applied to Western tonal contexts, the measured hierarchies
	were found to be consistent with music-theoretic accounts. In the
	present study, the probe tone method was used to quantify the perceived
	hierarchy of tones of North Indian music. Indian music is tonal and
	has many features in common with Western music. One of the most significant
	differences is that the primary means of expressing tonality in Indian
	music is through melody, whereas in Western music it is through harmony
	(the use of chords). Indian music is based on a standard set of melodic
	forms (called rags), which are themselves built on a large set of
	scales (thats). The tones within a rag are thought to be organized
	in a hierarchy of importance. Probe tone ratings were given by Indian
	and Western listeners in the context of 10 North Indian rags. These
	ratings confirmed the predicted hierarchical ordering. Both groups
	of listeners gave the highest ratings to the tonic and the fifth
	degree of the scale. These tones are considered by Indian music theorists
	to be structurally significant, as they are immovable tones around
	which the scale system is constructed, and they are sounded continuously
	in the drone. Relatively high ratings were also given to the vadi
	tone, which is designated for each rag and is given emphasis in the
	melody. The ratings of both groups of listeners generally reflected
	the pattern of tone durations in the musical contexts. This result
	suggests that the distribution of tones in music is a psychologically
	effective means of conveying the tonal hierarchy to listeners whether
	they are familiar with the musical tradition. Beyond this, only the
	Indian listeners were sensitive to the scales (thats) underlying
	the rags. For Indian listeners, multidimensional scaling of the correlations
	between the rating profiles recovered the theoretical representation
	of scales described by theorists of Indian music.(ABSTRACT TRUNCATED
	AT 400 WORDS)</abstract>
		<issn>0096-3445</issn>
		<title>Tonal hierarchies in the music of north India</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21a6011ebb325c27b9ea43decf38443bc/yevb0</id>
		<tags>music,musicality,neuro,perception,pitch</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Music Perception</journal>
		<publisher>University of California Press</publisher>
		<year>2001</year>
		<url>http://caliber.ucpress.net/doi/abs/10.1525/mp.2001.19.2.199</url>
		<author>Elvira Brattico</author>
		<author>Risto Näätänen</author>
		<author>Mari Tervaniemi</author>
		<authors>
			<first>Elvira</first>
		</authors>
		<authors>
			<last>Brattico</last>
		</authors>
		<authors>
			<first>Risto</first>
		</authors>
		<authors>
			<last>Näätänen</last>
		</authors>
		<authors>
			<first>Mari</first>
		</authors>
		<authors>
			<last>Tervaniemi</last>
		</authors>
		<volume>19</volume>
		<number>2</number>
		<pages>199--222</pages>
		<issn>0730-7829</issn>
		<title>Context effects on pitch perception in musicians and nonmusicians:
	evidence from event-related-potential recordings</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f9a8a7cdf8af05120503eba47bf735e7/yevb0</id>
		<tags>Adolescent,Adult,Auditory</tags>
		<tags>Perception,Auditory</tags>
		<tags>Perception:</tags>
		<tags>Potentials,Evoked</tags>
		<tags>Potentials:</tags>
		<tags>physiology,Brain,Brain:</tags>
		<tags>physiology,Electroencephalography,Evoked</tags>
		<tags>physiology,Female,Humans,Male,Music,music,musicality,neuro</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Neuroscience Letters</journal>
		<year>1997</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/9153627</url>
		<author>Mari Tervaniemi</author>
		<author>T Ilvonen</author>
		<author>K Karma</author>
		<author>K Alho</author>
		<author>Risto Näätänen</author>
		<authors>
			<first>Mari</first>
		</authors>
		<authors>
			<last>Tervaniemi</last>
		</authors>
		<authors>
			<first>T</first>
		</authors>
		<authors>
			<last>Ilvonen</last>
		</authors>
		<authors>
			<first>K</first>
		</authors>
		<authors>
			<last>Karma</last>
		</authors>
		<authors>
			<first>K</first>
		</authors>
		<authors>
			<last>Alho</last>
		</authors>
		<authors>
			<first>Risto</first>
		</authors>
		<authors>
			<last>Näätänen</last>
		</authors>
		<volume>226</volume>
		<number>1</number>
		<pages>1--4</pages>
		<abstract>To reveal neurophysiological prerequisites of musicality, auditory
	event-related potentials (ERPs) were recorded from musical and non-musical
	subjects, musicality being here defined as the ability to temporally
	structure auditory information. Instructed to read a book and to
	ignore sounds, subjects were presented with a repetitive sound pattern
	with occasional changes in its temporal structure. The mismatch negativity
	(MMN) component of ERPs, indexing the cortical preattentive detection
	of change in these stimulus patterns, was larger in amplitude in
	musical than non-musical subjects. This amplitude enhancement, indicating
	more accurate sensory memory function in musical subjects, suggests
	that even the cognitive component of musicality, traditionally regarded
	as depending on attention-related brain processes, in fact, is based
	on neural mechanisms present already at the preattentive level.</abstract>
		<issn>0304-3940</issn>
		<title>The musical brain: brain waves reveal the neurophysiological basis
	of musicality in human subjects</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28bf7731297142f821bf7fb25d51a38c3/ijtsrd</id>
		<tags>Women</tags>
		<tags>Entrepreneurs</tags>
		<tags>Music</tags>
		<tags>Stress</tags>
		<tags>Management</tags>
		<tags>Responsibilities</tags>
		<description></description>
		<date>2018-08-31 12:53:24</date>
		<count>1</count>
		<journal>International Journal of Trend in Scientific Research and Development</journal>
		<year>2017</year>
		<url>http://www.ijtsrd.com/humanities-and-the-arts/music/138/musical-stress-management-for-women-entrepreneurs/shveata-misra</url>
		<author>Shveata Misra Ina Shastri</author>
		<authors>
			<first>Shveata Misra Ina</first>
		</authors>
		<authors>
			<last>Shastri</last>
		</authors>
		<volume>1</volume>
		<number>4</number>
		<pages>380-383</pages>
		<abstract>Today's global economic development has brought many changes in the lives of women. The women in today's time have broken away from the beaten track and are exploring new vistas of economic participation as entrepreneurs. They at times get largely discouraged by their own family members and peers who cannot see a woman's worth. In the last few decades there have been many extensive literature's on work stress but among women entrepreneurs, knowledge about coping with work stress is still inadequate in comparison. The present study is designed to extend the influential sources on stress among women entrepreneurs and how music can help to heal stress amongst them. Shveata Misra | Ina Shastri"Musical Stress Management for Women Entrepreneurs" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-1 | Issue-4 , June 2017, URL: http://www.ijtsrd.com/papers/ijtsrd138.pdf  http://www.ijtsrd.com/humanities-and-the-arts/music/138/musical-stress-management-for-women-entrepreneurs/shveata-misra</abstract>
		<language>English</language>
		<issn>2456-6470</issn>
		<title>Musical Stress Management for Women Entrepreneurs</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/247e470a2eea5fb6795fd371ea05956cc/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-09-20 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#AboodiW12</url>
		<author>Ahed Aboodi</author>
		<author>Tat Chee Wan</author>
		<authors>
			<first>Ahed</first>
		</authors>
		<authors>
			<last>Aboodi</last>
		</authors>
		<authors>
			<first>Tat Chee</first>
		</authors>
		<authors>
			<last>Wan</last>
		</authors>
		<pages>260-264</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Evaluation of WiFi-Based Indoor (WBI) Positioning Algorithm.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2632e08d4967f408ecbf67f1c0c387afa/annakrause</id>
		<tags>music</tags>
		<tags>embeddings</tags>
		<description>[2010.08091] PiRhDy: Learning Pitch-, Rhythm-, and Dynamics-aware Embeddings for Symbolic Music</description>
		<date>2021-01-19 11:17:49</date>
		<count>4</count>
		<year>2020</year>
		<url>http://arxiv.org/abs/2010.08091</url>
		<author>Hongru Liang</author>
		<author>Wenqiang Lei</author>
		<author>Paul Yaozhu Chan</author>
		<author>Zhenglu Yang</author>
		<author>Maosong Sun</author>
		<author>Tat-Seng Chua</author>
		<authors>
			<first>Hongru</first>
		</authors>
		<authors>
			<last>Liang</last>
		</authors>
		<authors>
			<first>Wenqiang</first>
		</authors>
		<authors>
			<last>Lei</last>
		</authors>
		<authors>
			<first>Paul Yaozhu</first>
		</authors>
		<authors>
			<last>Chan</last>
		</authors>
		<authors>
			<first>Zhenglu</first>
		</authors>
		<authors>
			<last>Yang</last>
		</authors>
		<authors>
			<first>Maosong</first>
		</authors>
		<authors>
			<last>Sun</last>
		</authors>
		<authors>
			<first>Tat-Seng</first>
		</authors>
		<authors>
			<last>Chua</last>
		</authors>
		<abstract>Definitive embeddings remain a fundamental challenge of computational
musicology for symbolic music in deep learning today. Analogous to natural
language, music can be modeled as a sequence of tokens. This motivates the
majority of existing solutions to explore the utilization of word embedding
models to build music embeddings. However, music differs from natural languages
in two key aspects: (1) musical token is multi-faceted -- it comprises of
pitch, rhythm and dynamics information; and (2) musical context is
two-dimensional -- each musical token is dependent on both melodic and harmonic
contexts. In this work, we provide a comprehensive solution by proposing a
novel framework named PiRhDy that integrates pitch, rhythm, and dynamics
information seamlessly. PiRhDy adopts a hierarchical strategy which can be
decomposed into two steps: (1) token (i.e., note event) modeling, which
separately represents pitch, rhythm, and dynamics and integrates them into a
single token embedding; and (2) context modeling, which utilizes melodic and
harmonic knowledge to train the token embedding. A thorough study was made on
each component and sub-strategy of PiRhDy. We further validate our embeddings
in three downstream tasks -- melody completion, accompaniment suggestion, and
genre classification. Results indicate a significant advancement of the neural
approach towards symbolic music as well as PiRhDy's potential as a pretrained
tool for a broad range of symbolic music applications.</abstract>
		<doi>10.1145/3394171.3414032</doi>
		<title>PiRhDy: Learning Pitch-, Rhythm-, and Dynamics-aware Embeddings for
  Symbolic Music</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21d7b8346ae032f188732409bf2ff8086/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#SheikhhaM12</url>
		<author>Fatemeh Sheikhha</author>
		<author>Mohsen Ebrahimi Moghaddam</author>
		<authors>
			<first>Fatemeh</first>
		</authors>
		<authors>
			<last>Sheikhha</last>
		</authors>
		<authors>
			<first>Mohsen Ebrahimi</first>
		</authors>
		<authors>
			<last>Moghaddam</last>
		</authors>
		<pages>230-235</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Service-Oriented Wireless Multimedia Sensor Network Middleware Using Infra-Red Cameras.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/288ced7098532e50c929269e1bdb6779c/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#NadaliGGA12</url>
		<author>Ahmad Nadali</author>
		<author>Pouria Goldasteh</author>
		<author>Mohammad Ghazivakili</author>
		<author>Sami Ahmari</author>
		<authors>
			<first>Ahmad</first>
		</authors>
		<authors>
			<last>Nadali</last>
		</authors>
		<authors>
			<first>Pouria</first>
		</authors>
		<authors>
			<last>Goldasteh</last>
		</authors>
		<authors>
			<first>Mohammad</first>
		</authors>
		<authors>
			<last>Ghazivakili</last>
		</authors>
		<authors>
			<first>Sami</first>
		</authors>
		<authors>
			<last>Ahmari</last>
		</authors>
		<pages>78-83</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Evaluating Success of Innovation Ideas in Social Computing Technologies Using Fuzzy Expert System.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29df4405bfc4e4e3eeff18c029be18242/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#YiLZZ12</url>
		<author>Xiaowei Yi</author>
		<author>Mingyu Li</author>
		<author>Gang Zheng</author>
		<author>Changwen Zheng</author>
		<authors>
			<first>Xiaowei</first>
		</authors>
		<authors>
			<last>Yi</last>
		</authors>
		<authors>
			<first>Mingyu</first>
		</authors>
		<authors>
			<last>Li</last>
		</authors>
		<authors>
			<first>Gang</first>
		</authors>
		<authors>
			<last>Zheng</last>
		</authors>
		<authors>
			<first>Changwen</first>
		</authors>
		<authors>
			<last>Zheng</last>
		</authors>
		<pages>148-153</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Quality-Optimized Authentication of Scalable Media Streams with Flexible Transcoding over Wireless Networks.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b9158df951e91f1477a018da9c5d79b1/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#LeeHJJ12</url>
		<author>Kee-Sung Lee</author>
		<author>Myung-Duk Hong</author>
		<author>Jin-Guk Jung</author>
		<author>GeunSik Jo</author>
		<authors>
			<first>Kee-Sung</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Myung-Duk</first>
		</authors>
		<authors>
			<last>Hong</last>
		</authors>
		<authors>
			<first>Jin-Guk</first>
		</authors>
		<authors>
			<last>Jung</last>
		</authors>
		<authors>
			<first>GeunSik</first>
		</authors>
		<authors>
			<last>Jo</last>
		</authors>
		<pages>90-95</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Building a Semantic Social Network Based on Interpersonal Relationships.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/281e8c90b800fd69975aa4da7d91c61f9/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-09-16 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#KhalajLP12</url>
		<author>Azade Khalaj</author>
		<author>Hanan Lutfiyya</author>
		<author>Mark Perry</author>
		<authors>
			<first>Azade</first>
		</authors>
		<authors>
			<last>Khalaj</last>
		</authors>
		<authors>
			<first>Hanan</first>
		</authors>
		<authors>
			<last>Lutfiyya</last>
		</authors>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Perry</last>
		</authors>
		<pages>21-28</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>A Proxy-Based Mobile Computing Infrastructure.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d6f6ad100289bedcc93e0224131d3122/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#YuZ12</url>
		<author>Qian Yu</author>
		<author>Chang N. Zhang</author>
		<authors>
			<first>Qian</first>
		</authors>
		<authors>
			<last>Yu</last>
		</authors>
		<authors>
			<first>Chang N.</first>
		</authors>
		<authors>
			<last>Zhang</last>
		</authors>
		<pages>158-163</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>A Secure Multicast Scheme for Wireless Sensor Networks.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/244ff7fc30d71d278e306b390fccbd52b/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#MendezL12</url>
		<author>Diego Mendez</author>
		<author>Miguel A. Labrador</author>
		<authors>
			<first>Diego</first>
		</authors>
		<authors>
			<last>Mendez</last>
		</authors>
		<authors>
			<first>Miguel A.</first>
		</authors>
		<authors>
			<last>Labrador</last>
		</authors>
		<pages>35-40</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Density Maps: Determining Where to Sample in Participatory Sensing Systems.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c904c650a5f3f89b3f5573345986f7bd/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#Schefer-WenzlS12</url>
		<author>Sigrid Schefer-Wenzl</author>
		<author>Mark Strembeck</author>
		<authors>
			<first>Sigrid</first>
		</authors>
		<authors>
			<last>Schefer-Wenzl</last>
		</authors>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Strembeck</last>
		</authors>
		<pages>126-131</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Modeling Context-Aware RBAC Models for Business Processes in Ubiquitous Computing Environments.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/200fd99ba1c9d34e1ac510244576eb042/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#HuangLCH12</url>
		<author>Yo-Ping Huang</author>
		<author>Shin-Liang Lai</author>
		<author>Tsun-Wei Chang</author>
		<author>Maw-Sheng Horng</author>
		<authors>
			<first>Yo-Ping</first>
		</authors>
		<authors>
			<last>Huang</last>
		</authors>
		<authors>
			<first>Shin-Liang</first>
		</authors>
		<authors>
			<last>Lai</last>
		</authors>
		<authors>
			<first>Tsun-Wei</first>
		</authors>
		<authors>
			<last>Chang</last>
		</authors>
		<authors>
			<first>Maw-Sheng</first>
		</authors>
		<authors>
			<last>Horng</last>
		</authors>
		<pages>41-46</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Query-by-Humming/Singing of MIDI and Audio Files by Fuzzy Inference System.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2faaf66ea454bb63026667efa25b123dc/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-16 00:00:00</date>
		<count>1</count>
		<booktitle>Ubiquitous Music</booktitle>
		<series>Computational Music Science</series>
		<publisher>Springer</publisher>
		<year>2014</year>
		<url>http://dblp.uni-trier.de/db/series/music/um2014.html</url>
		<editor>Damián Keller</editor>
		<editor>Victor Lazzarini</editor>
		<editor>Marcelo Soares Pimenta</editor>
		<editors>
			<first>Maw-Sheng</first>
		</editors>
		<editors>
			<last>Horng</last>
		</editors>
		<editors>
			<first>Maw-Sheng</first>
		</editors>
		<editors>
			<last>Horng</last>
		</editors>
		<editors>
			<first>Maw-Sheng</first>
		</editors>
		<editors>
			<last>Horng</last>
		</editors>
		<isbn>978-3-319-11151-3</isbn>
		<title>Ubiquitous Music</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29ddd9fdc30970b844564fcdd5536ca29/sapo</id>
		<tags>Genre_recognition</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>2</count>
		<booktitle>Proceedings of the 12th International Society for Music Information Retrieval Conference (ISMIR 2012)</booktitle>
		<publisher>University of Miami</publisher>
		<year>2011</year>
		<url>http://publik.tuwien.ac.at/files/PubDat_205736.pdf</url>
		<author>Rudolf Mayer</author>
		<author>Andreas Rauber</author>
		<authors>
			<first>Rudolf</first>
		</authors>
		<authors>
			<last>Mayer</last>
		</authors>
		<authors>
			<first>Andreas</first>
		</authors>
		<authors>
			<last>Rauber</last>
		</authors>
		<pages>675--680</pages>
		<abstract>Algorithms that can understand and interpret characteristics of music,
	and organise them for and recommend them to their users can be of
	great assistance in handling the ever growing size of both private
	and commercial collections. Music is an inherently multi-modal type
	of data, and the lyrics associated with the music are as essential
	to the reception and the message of a song as is the audio. In this
	paper, we present advanced methods on how the lyrics domain of music
	can be combined with the acoustic domain. We evaluate our approach
	by means of a common task in music information retrieval, musical
	genre classification. Advancing over previous work that showed improvements
	with simple feature fusion, we apply the more sophisticated approach
	of result (or late) fusion. We achieve results superior to the best
	choice of a single algorithm on a single feature set.</abstract>
		<isbn>978-0-615-54865-4</isbn>
		<title>Musical Genre Classification by Ensembles of Audio and Lyrics Features</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24285d7112b085212e19abec4b4df2c8f/anak</id>
		<tags>musicinformationretrieval</tags>
		<tags>musicretrievalsystems</tags>
		<tags>musicretrieval</tags>
		<description></description>
		<date>2021-04-05 11:21:28</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2004</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj28.html#Downie04</url>
		<author>Stephen Downie</author>
		<authors>
			<first>Stephen</first>
		</authors>
		<authors>
			<last>Downie</last>
		</authors>
		<volume>28</volume>
		<number>2</number>
		<pages>12-23</pages>
		<abstract>Music Information Retrieval (MIR) is a multidisciplinary research endeavor that strives to develop
innovative content-based searching schemes, novel
interfaces, and evolving networked delivery mechanisms in an effort to make the world’s vast store of
music accessible to all.</abstract>
		<title>The Scientific Evaluation of Music Information Retrieval Systems: Foundations and Future</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/250eb357ffca409bfa50048500e6dbc87/sapo</id>
		<tags>read_list</tags>
		<tags>music_demography</tags>
		<description>Predicting the Geographical Origin of Music - IEEE Conference Publication</description>
		<date>2020-05-14 09:35:29</date>
		<count>1</count>
		<booktitle>2014 IEEE International Conference on Data Mining</booktitle>
		<year>2014</year>
		<url>https://ieeexplore.ieee.org/abstract/document/7023456</url>
		<author>F. Zhou</author>
		<author>Q. Claire</author>
		<author>R. D. King</author>
		<authors>
			<first>F.</first>
		</authors>
		<authors>
			<last>Zhou</last>
		</authors>
		<authors>
			<first>Q.</first>
		</authors>
		<authors>
			<last>Claire</last>
		</authors>
		<authors>
			<first>R. D.</first>
		</authors>
		<authors>
			<last>King</last>
		</authors>
		<pages>1115-1120</pages>
		<abstract>Traditional research into the arts has almost always been based around the subjective judgment of human critics. The use of data mining tools to understand art has great promise as it is objective and operational. We investigate the distribution of music from around the world: geographical ethnomusicology. We cast the problem as training a machine learning program to predict the geographical origin of pieces of music. This is a technically interesting problem as it has features of both classification and regression, and because of the spherical geometry of the surface of the Earth. Because of these characteristics of the representation of geographical positions, most standard classification/regression methods cannot be directly used. Two applicable methods are K-Nearest Neighbors and Random forest regression, which are robust to the non-standard structure of data. We also investigated improving performance through use of bagging. We collected 1,142 pieces of music from 73 countries/areas, and described them using 2 different sets of standard audio descriptors using MARSYAS. 10-fold cross validation was used in all experiments. The experimental results indicate that Random forest regression produces significantly better results than KNN, and the use of bagging improves the performance of KNN. The best performing algorithm achieved a mean great circle distance error of 3,113 km.</abstract>
		<issn>2374-8486</issn>
		<doi>10.1109/ICDM.2014.73</doi>
		<title>Predicting the Geographical Origin of Music</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/207577692a6b16dcc5bbe6a1f16139fb2/sapo</id>
		<tags>music_performance</tags>
		<description></description>
		<date>2020-08-19 17:32:20</date>
		<count>2</count>
		<journal>Journal of Musicological Research</journal>
		<publisher>Routledge</publisher>
		<year>2020</year>
		<url>/brokenurl#         https://doi.org/10.1080/01411896.2020.1775087    </url>
		<author>Laurence Dreyfus</author>
		<authors>
			<first>Laurence</first>
		</authors>
		<authors>
			<last>Dreyfus</last>
		</authors>
		<volume>0</volume>
		<number>0</number>
		<pages>1-26</pages>
		<abstract>ABSTRACT The idea of a musical performance as an “interpretation” cannot be dated before the 1840s, yet we use the term unthinkingly as a synonym for a privileged performance of any music from the past. Tracing a history and pre-history of the metaphor and its usage sheds light on the eclipse of more richly textured models of music-making from previous eras as well as on an esthetic predicament within contemporary “historicist” performance. The limitations of the metaphor suggest moving “beyond interpretation” in favor of more experiential and intuitive notions of making music.</abstract>
		<eprint>https://doi.org/10.1080/01411896.2020.1775087</eprint>
		<doi>10.1080/01411896.2020.1775087</doi>
		<title>Beyond the Interpretation of Music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20b28b2d21e9286c51eacb3b1a6e3fdac/sapo</id>
		<tags>music_information_retrieval</tags>
		<description>Tonal Description of Polyphonic Audio for Music Content Processing | Semantic Scholar</description>
		<date>2021-11-30 16:08:15</date>
		<count>2</count>
		<journal>INFORMS Journal on Computing, Special Cluster on Computation in Music</journal>
		<year>2006</year>
		<url></url>
		<author>Emilia Gómez</author>
		<authors>
			<first>Emilia</first>
		</authors>
		<authors>
			<last>Gómez</last>
		</authors>
		<volume>18</volume>
		<number>3</number>
		<pages>294-304</pages>
		<title>Tonal Description of Polyphonic Audio for Music Content Processing</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25256c3d1a0205d579fbd5ab60e2e8711/svrist</id>
		<tags>segmentation;</tags>
		<tags>music</tags>
		<tags>primitives</tags>
		<tags>low-level</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>2</count>
		<journal>BMVC92. Proceedings of the British Machine Vision Conference</journal>
		<year>1992</year>
		<url></url>
		<author>K. C. Ng</author>
		<author>R. D. Boyle</author>
		<authors>
			<first>K. C.</first>
		</authors>
		<authors>
			<last>Ng</last>
		</authors>
		<authors>
			<first>R. D.</first>
		</authors>
		<authors>
			<last>Boyle</last>
		</authors>
		<pages>472-80</pages>
		<abstract>Low-level knowledge-directed pre-processing and segmentation of music scores are presented. We discuss some of the problems that have been overlooked by existing research, but have proved to be major obstacles for robust optical music recognisers, to help in entering music into a computer, including sub-segmentation of interconnected primitives and identification of nonstraight stave lines, and we present solutions to these problems. We conclude that, with knowledge, a significant improvement in low-level segmentations can be achieved (10 Refs.)</abstract>
		<title>Segmentation of music primitives</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23fbc3ae4af183713dc722bc43e3684b9/brazovayeye</id>
		<tags>genetic</tags>
		<tags>algorithms,</tags>
		<tags>complearn</tags>
		<tags>programming,</tags>
		<description></description>
		<date>2008-06-19 17:35:00</date>
		<count>2</count>
		<journal>Computer Music Journal</journal>
		<year>2004</year>
		<url>http://homepages.cwi.nl/~paulv/papers/music.pdf</url>
		<author>Rudi Cilibrasi</author>
		<author>Paul Vitanyi</author>
		<author>Ronald de Wolf</author>
		<authors>
			<first>Rudi</first>
		</authors>
		<authors>
			<last>Cilibrasi</last>
		</authors>
		<authors>
			<first>Paul</first>
		</authors>
		<authors>
			<last>Vitanyi</last>
		</authors>
		<authors>
			<first>Ronald</first>
		</authors>
		<authors>
			<last>de Wolf</last>
		</authors>
		<volume>28</volume>
		<number>4</number>
		<pages>49--67</pages>
		<abstract>All musical pieces are similar, but some are more
                 similar than others. Apart from serving as an infinite
                 source of discussion (''Haydn is just like Mozart No,
                 he's not!''), such similarities are also crucial for
                 the design of efficient music information retrieval
                 systems. The amount of digitised music available on the
                 Internet has grown dramatically in recent years, both
                 in the public domain and on commercial sites; Napster
                 and its clones are prime examples.</abstract>
		<notes>C 2004 Massachusetts Institute of Technology

                 Earlier version at cs.SD/0303025
                 http://arxiv.org/abs/cs.SD/0303025</notes>
		<title>Algorithmic Clustering of Music Based on String
                 Compression</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d04cfaade177b192e2c7014ef175d6d7/schmitz</id>
		<tags>music</tags>
		<tags>multimedia</tags>
		<tags>genre</tags>
		<tags>classification</tags>
		<description>CiteULike: Automatic genre classification of music content: a survey</description>
		<date>2007-04-02 14:06:29</date>
		<count>2</count>
		<journal>Signal Processing Magazine, IEEE</journal>
		<year>2006</year>
		<url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598089</url>
		<author>N. Scaringella</author>
		<author>G. Zoia</author>
		<author>D. Mlynek</author>
		<authors>
			<first>N.</first>
		</authors>
		<authors>
			<last>Scaringella</last>
		</authors>
		<authors>
			<first>G.</first>
		</authors>
		<authors>
			<last>Zoia</last>
		</authors>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Mlynek</last>
		</authors>
		<volume>23</volume>
		<number>2</number>
		<pages>133--141</pages>
		<abstract>This paper reviews the state-of-the-art in automatic genre classification of music collections through three main paradigms: expert systems, unsupervised classification, and supervised classification. The paper discusses the importance of music genres with their definitions and hierarchies. It also presents techniques to extract meaningful information from audio data to characterize musical excerpts. The paper also presents the results of new emerging research fields and techniques that investigate the proximity of music genres.</abstract>
		<title>Automatic genre classification of music content: a survey</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25c474cb65a93bcb89675e855dcc711ad/taspel</id>
		<tags>web2.0</tags>
		<tags>music_industry</tags>
		<tags>semanticweb</tags>
		<description>This paper introduces various ways to suggest music-related
content on the Web thanks to Semantic Web technologies.</description>
		<date>2009-02-20 02:07:56</date>
		<count>4</count>
		<year>2008</year>
		<url>http://ceur-ws.org/Vol-405/paper3.pdf</url>
		<author>Alexandre Passant</author>
		<author>Yves Raimond</author>
		<authors>
			<first>Alexandre</first>
		</authors>
		<authors>
			<last>Passant</last>
		</authors>
		<authors>
			<first>Yves</first>
		</authors>
		<authors>
			<last>Raimond</last>
		</authors>
		<title>Combining Social Music and Semantic Web for Music-related Recommender Systems</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c9ff56e22a484919c806e75a3ea2e503/snarc</id>
		<tags>imported</tags>
		<description>The content and validity of music-genre stereotypes among college students -- Rentfrow and Gosling 35 (2): 306 -- Psychology of Music</description>
		<date>2009-02-23 16:43:09</date>
		<count>2</count>
		<journal>Psychology of Music</journal>
		<year>2007</year>
		<url>http://pom.sagepub.com/cgi/content/abstract/35/2/306</url>
		<author>Peter J. Rentfrow</author>
		<author>Samuel D. Gosling</author>
		<authors>
			<first>Peter J.</first>
		</authors>
		<authors>
			<last>Rentfrow</last>
		</authors>
		<authors>
			<first>Samuel D.</first>
		</authors>
		<authors>
			<last>Gosling</last>
		</authors>
		<volume>35</volume>
		<number>2</number>
		<pages>306-326</pages>
		<abstract>The present research examined the content and validity of stereotypes about fans of 14 different music genres (e.g. country, rap, rock). In particular, we focused on stereotypes concerning fans' personalities (e.g. extraversion, emotional stability), personal qualities (e.g. political beliefs, athleticism), values (e.g. for peace, for wisdom), and alcohol and drug preferences (e.g. wine, hallucinogens). Previous research has shown that music is linked to a variety of psychological characteristics, that music is used to convey information about oneself to observers, and that observers can infer personality on the basis of music preferences. Guided by such research, we predicted and found that individuals have robust and clearly defined stereotypes about the fans of various music genres (Study 1), and that many of these music-genre stereotypes possess a kernel of truth (Study 2). Discussion focuses on the potential role of music-genre stereotypes in self-expression and impression formation.</abstract>
		<doi>10.1177/0305735607070382</doi>
		<eprint>http://pom.sagepub.com/cgi/reprint/35/2/306.pdf</eprint>
		<title>The content and validity of music-genre stereotypes among college students</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ecbe976655fb6d64cd091c0bff839be2/mediadigits</id>
		<tags>music</tags>
		<tags>psychology</tags>
		<description></description>
		<date>2010-12-17 17:01:44</date>
		<count>1</count>
		<journal>Psychology of Music</journal>
		<publisher>SAGE Publications</publisher>
		<year>2007</year>
		<url></url>
		<author>N. Dibben</author>
		<author>V.J. Williamson</author>
		<authors>
			<first>N.</first>
		</authors>
		<authors>
			<last>Dibben</last>
		</authors>
		<authors>
			<first>V.J.</first>
		</authors>
		<authors>
			<last>Williamson</last>
		</authors>
		<volume>35</volume>
		<number>4</number>
		<pages>571</pages>
		<issn>0305-7356</issn>
		<title>An exploratory survey of in-vehicle music listening</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a93b46e3f4d166627214ef4c8f76b8f8/yevb0</id>
		<tags>music,gamelan,music,perception,pitch,scale,tonality</tags>
		<tags>Indian</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>Cognitive foundations of musical pitch</booktitle>
		<publisher>Oxford University Press</publisher>
		<address>New York</address>
		<year>1990</year>
		<url></url>
		<author>Carol L Krumhansl</author>
		<authors>
			<first>Carol L</first>
		</authors>
		<authors>
			<last>Krumhansl</last>
		</authors>
		<title>Cognitive foundations of musical pitch</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20d9036d03c0ae367a0ea32c06db0e144/yevb0</id>
		<tags>System:</tags>
		<tags>Adult,Analysis</tags>
		<tags>System,Immune</tags>
		<tags>metabolism,Immunodiffusion,Immunoglobulin</tags>
		<tags>of</tags>
		<tags>Secretory,Immunoglobulin</tags>
		<tags>Secretory:</tags>
		<tags>A,</tags>
		<tags>metabolism,Male,Music,Saliva,Saliva:</tags>
		<tags>immunology</tags>
		<tags>Variance,Female,Humans,Immune</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Journal of music therapy</journal>
		<year>2002</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/12015810</url>
		<author>Dawn Kuhn</author>
		<authors>
			<first>Dawn</first>
		</authors>
		<authors>
			<last>Kuhn</last>
		</authors>
		<volume>39</volume>
		<number>1</number>
		<pages>30--9</pages>
		<abstract>The purposes of this study were (a) to determine if musical activity
	would produce a significant change in the immune system as measured
	by Salivary Immunoglobulin A (SIgA), and (b) to determine if active
	participation in musical activity had a significantly different effect
	on the immune system than passive participation. Thirty-three participants
	(28 women and 5 men) were randomly assigned to one of 3 groups, 2
	experimental and 1 control. Active group participants participated
	in a 30-minute session where they played various percussive instruments
	and sang. Passive group participants listened to 30 minutes worth
	of live music. Saliva samples were taken before and after sessions
	and SIgA concentrations were determined using radial immunodiffusion
	technique. All groups were found to be significantly different from
	each other. SIgA levels of the active group showed a significantly
	greater increase than those of the passive group and the control
	group, suggesting that active participation in musical activity produces
	a greater effect on the immune system than passive participation.</abstract>
		<issn>0022-2917</issn>
		<title>The effects of active and passive participation in musical activity
	on the immune system as measured by salivary immunoglobulin A (SIgA).</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2dfaadab086109bcc322751a0db14e6e8/yevb0</id>
		<tags>culture,perception,perceptual</tags>
		<tags>Indian</tags>
		<tags>memory,tension</tags>
		<tags>music,M1,M2,biculturalism,music,musical</tags>
		<tags>learning,recognition</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Music Perception</journal>
		<publisher>University of California Press Journals Division, 2000 Center Street,
	\#303, Berkeley, CA 94704-1223 USA journals@ucpress.edu</publisher>
		<year>2009</year>
		<url>http://caliber.ucpress.net.proxy.nss.udel.edu/doi/abs/10.1525/mp.2009.27.2.81</url>
		<author>Patrick C.M. Wong</author>
		<author>Anil K Roy</author>
		<author>Elizabeth Hellmuth Margulis</author>
		<authors>
			<first>Patrick C.M.</first>
		</authors>
		<authors>
			<last>Wong</last>
		</authors>
		<authors>
			<first>Anil K</first>
		</authors>
		<authors>
			<last>Roy</last>
		</authors>
		<authors>
			<first>Elizabeth Hellmuth</first>
		</authors>
		<authors>
			<last>Margulis</last>
		</authors>
		<volume>27</volume>
		<number>2</number>
		<pages>81--88</pages>
		<abstract>One prominent example of globalization and mass cultural exchange
	is bilingualism, whereby world citizens learn to understand and speak
	multiple languages. Music, similar to language, is a human universal,
	and subject to the effects of globalization. In two experiments,
	we asked whether bimusicalism exists as a phenomenon, and whether
	it can occur even without explicit formal training and extensive
	music-making. Everyday music listeners who had significant exposure
	to music of both Indian (South Asian) and Westerners traditions (IW
	listeners) and listeners who had experience with only Indian or Western
	culture (I or W listeners) participated in recognition memory and
	tension judgment experiments where they listened to Western and Indian
	music. We found that while I and W listeners showed an in-culture
	bias, IW listeners showed equal responses to music from both cultures,
	suggesting that dual mental and affective sensitivities can be extended
	to a nonlinguistic domain.</abstract>
		<issn>1533-8312</issn>
		<title>Bimusicalism: The implicit dual enculturation of cognitive and affective
	systems</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/219407c6bd282e1316f7860efb880cd6a/yevb0</id>
		<tags>music,musicality,neuro</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Science</journal>
		<year>1995</year>
		<url>http://gottfriedschlaug.org/musicianbrain.backup/papers/Schlaug\_HemDominance\_1995a.pdf</url>
		<author>Gottfried Schlaug</author>
		<author>Lutz Jäncke</author>
		<author>Yanxiong Huang</author>
		<author>Helmuth Steinmetz</author>
		<authors>
			<first>Gottfried</first>
		</authors>
		<authors>
			<last>Schlaug</last>
		</authors>
		<authors>
			<first>Lutz</first>
		</authors>
		<authors>
			<last>Jäncke</last>
		</authors>
		<authors>
			<first>Yanxiong</first>
		</authors>
		<authors>
			<last>Huang</last>
		</authors>
		<authors>
			<first>Helmuth</first>
		</authors>
		<authors>
			<last>Steinmetz</last>
		</authors>
		<volume>267</volume>
		<number>5198</number>
		<pages>699--701</pages>
		<title>In vivo evidence of structural brain asymmetry in musicians</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26ceef684858cc75dc982ee8dfba82171/yevb0</id>
		<tags>music,musicality,perception</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Music Perception</journal>
		<year>1997</year>
		<url></url>
		<author>J David Smith</author>
		<authors>
			<first>J David</first>
		</authors>
		<authors>
			<last>Smith</last>
		</authors>
		<volume>14</volume>
		<number>3</number>
		<pages>227-- 262</pages>
		<title>The place of musical novices in music science</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20b4def91a2faaeb4afc602d90641e453/yevb0</id>
		<tags>music,musicality,perception</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Hearing Research</journal>
		<publisher>Elsevier B.V.</publisher>
		<year>2010</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/20018234</url>
		<author>Dana L Strait</author>
		<author>Nina Kraus</author>
		<author>Alexandra Parbery-Clark</author>
		<author>Richard Ashley</author>
		<authors>
			<first>Dana L</first>
		</authors>
		<authors>
			<last>Strait</last>
		</authors>
		<authors>
			<first>Nina</first>
		</authors>
		<authors>
			<last>Kraus</last>
		</authors>
		<authors>
			<first>Alexandra</first>
		</authors>
		<authors>
			<last>Parbery-Clark</last>
		</authors>
		<authors>
			<first>Richard</first>
		</authors>
		<authors>
			<last>Ashley</last>
		</authors>
		<volume>261</volume>
		<number>1-2</number>
		<pages>22--9</pages>
		<abstract>A growing body of research suggests that cognitive functions, such
	as attention and memory, drive perception by tuning sensory mechanisms
	to relevant acoustic features. Long-term musical experience also
	modulates lower-level auditory function, although the mechanisms
	by which this occurs remain uncertain. In order to tease apart the
	mechanisms that drive perceptual enhancements in musicians, we posed
	the question: do well-developed cognitive abilities fine-tune auditory
	perception in a top-down fashion? We administered a standardized
	battery of perceptual and cognitive tests to adult musicians and
	non-musicians, including tasks either more or less susceptible to
	cognitive control (e.g., backward versus simultaneous masking) and
	more or less dependent on auditory or visual processing (e.g., auditory
	versus visual attention). Outcomes indicate lower perceptual thresholds
	in musicians specifically for auditory tasks that relate with cognitive
	abilities, such as backward masking and auditory attention. These
	enhancements were observed in the absence of group differences for
	the simultaneous masking and visual attention tasks. Our results
	suggest that long-term musical practice strengthens cognitive functions
	and that these functions benefit auditory skills. Musical training
	bolsters higher-level mechanisms that, when impaired, relate to language
	and literacy deficits. Thus, musical training may serve to lessen
	the impact of these deficits by strengthening the corticofugal system
	for hearing.</abstract>
		<issn>1878-5891</issn>
		<doi>10.1016/j.heares.2009.12.021</doi>
		<title>Musical experience shapes top-down auditory mechanisms: evidence
	from masking and auditory attention performance</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24adf9aa850fc2efdf22155da39b201c9/keinstein</id>
		<tags>music_information_retrieval</tags>
		<tags>music</tags>
		<tags>Sci-MIDI_toolbox</tags>
		<tags>music_research</tags>
		<tags>mathematics_computing</tags>
		<tags>style_classification</tags>
		<tags>music_emotion_recognition</tags>
		<tags>Scilab_computing_environment</tags>
		<tags>open_source_Scilab_toolbox</tags>
		<description></description>
		<date>2012-09-21 10:16:07</date>
		<count>2</count>
		<booktitle>Open-source Software for Scientific Computation (OSSC), 2009 IEEE International Workshop on</booktitle>
		<year>2009</year>
		<url></url>
		<author>Zhigang Huang</author>
		<author>Changle Zhou</author>
		<authors>
			<first>Zhigang</first>
		</authors>
		<authors>
			<last>Huang</last>
		</authors>
		<authors>
			<first>Changle</first>
		</authors>
		<authors>
			<last>Zhou</last>
		</authors>
		<pages>159 -162</pages>
		<abstract>Sci-MIDI Toolbox, an open source Scilab toolbox for music research, provides a set of Scilab functions, which have versatile ways to analyze and compute MIDI files' data based on the Scilab computing environment. It contains extracting, computing and visualizing MIDI files data which is relating to musical melodic notes and others information, besides basic manipulation, filtering, statistics and some algorithms that are often used involved in musical research. The development of the Toolbox is suitable for some research of music information retrieval, music emotion recognition and style classification and others using with MIDI data.</abstract>
		<doi>10.1109/OSSC.2009.5416915</doi>
		<title>Sci-MIDI toolbox for music research</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2628b490883a32d63128e0f3eca7260f0/ks-plugin-devel</id>
		<tags>Mathematik</tags>
		<tags>Gruppen</tags>
		<tags>Musiktheorie</tags>
		<tags>MaMu</tags>
		<description>Musical Actions of Dihedral Groups</description>
		<date>2013-02-02 14:42:39</date>
		<count>2</count>
		<year>2007</year>
		<url>http://arxiv.org/abs/0711.1873</url>
		<author>Alissa S. Crans</author>
		<author>Thomas M. Fiore</author>
		<author>Ramon Satyendra</author>
		<authors>
			<first>Alissa S.</first>
		</authors>
		<authors>
			<last>Crans</last>
		</authors>
		<authors>
			<first>Thomas M.</first>
		</authors>
		<authors>
			<last>Fiore</last>
		</authors>
		<authors>
			<first>Ramon</first>
		</authors>
		<authors>
			<last>Satyendra</last>
		</authors>
		<abstract>The sequence of pitches which form a musical melody can be transposed orinverted. Since the 1970s, music theorists have modeled musical transpositionand inversion in terms of an action of the dihedral group of order 24. Morerecently music theorists have found an intriguing second way that the dihedralgroup of order 24 acts on the set of major and minor chords. We illustrate bothgeometrically and algebraically how these two actions are dual. Bothactions and their duality have been used to analyze works of music as diverseas Hindemith and the Beatles.</abstract>
		<title>Musical Actions of Dihedral Groups</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/294d2e364215c1f00b9e0681abe90d243/maxirichter</id>
		<tags>music</tags>
		<tags>algorithms</tags>
		<description>Discovering novel computer music techniques by exploring the space of
  short computer programs</description>
		<date>2011-12-07 14:22:39</date>
		<count>1</count>
		<year>2011</year>
		<url>http://arxiv.org/PS_cache/arxiv/pdf/1112/1112.1368v1.pdf</url>
		<author>Ville-Matias Heikkil&#xe4;</author>
		<authors>
			<first>Ville-Matias</first>
		</authors>
		<authors>
			<last>Heikkil&#xe4;</last>
		</authors>
		<abstract>Very short computer programs, sometimes consisting of as few as three
arithmetic operations in an infinite loop, can generate data that sounds like
music when output as raw PCM audio. The space of such programs was recently
explored by dozens of individuals within various on-line communities. This
paper discusses the programs resulting from this exploratory work and
highlights some rather unusual methods they use for synthesizing sound and
generating musical structure.</abstract>
		<title>Discovering novel computer music techniques by exploring the space of
  short computer programs</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2078056c332d91c311a0b8ccfe2ca4c10/rcb</id>
		<tags>autoencoder</tags>
		<tags>music</tags>
		<tags>long-term</tags>
		<description>[1803.05428] A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music</description>
		<date>2018-03-16 15:06:47</date>
		<count>3</count>
		<year>2018</year>
		<url>http://arxiv.org/abs/1803.05428</url>
		<author>Adam Roberts</author>
		<author>Jesse Engel</author>
		<author>Colin Raffel</author>
		<author>Curtis Hawthorne</author>
		<author>Douglas Eck</author>
		<authors>
			<first>Adam</first>
		</authors>
		<authors>
			<last>Roberts</last>
		</authors>
		<authors>
			<first>Jesse</first>
		</authors>
		<authors>
			<last>Engel</last>
		</authors>
		<authors>
			<first>Colin</first>
		</authors>
		<authors>
			<last>Raffel</last>
		</authors>
		<authors>
			<first>Curtis</first>
		</authors>
		<authors>
			<last>Hawthorne</last>
		</authors>
		<authors>
			<first>Douglas</first>
		</authors>
		<authors>
			<last>Eck</last>
		</authors>
		<abstract>The Variational Autoencoder (VAE) has proven to be an effective model for
producing semantically meaningful latent representations for natural data.
However, it has thus far seen limited application to sequential data, and, as
we demonstrate, existing recurrent VAE models have difficulty modeling
sequences with long-term structure. To address this issue, we propose the use
of a hierarchical decoder, which first outputs embeddings for subsequences of
the input and then uses these embeddings to generate each subsequence
independently. This structure encourages the model to utilize its latent code,
thereby avoiding the "posterior collapse" problem which remains an issue for
recurrent VAEs. We apply this architecture to modeling sequences of musical
notes and find that it exhibits dramatically better sampling, interpolation,
and reconstruction performance than a "flat" baseline model. An implementation
of our "MusicVAE" is available online at http://g.co/magenta/musicvae-colab.</abstract>
		<title>A Hierarchical Latent Vector Model for Learning Long-Term Structure in
  Music</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23430cc0031810ba7e27ab584db01328f/prophe</id>
		<tags>MUSIC</tags>
		<tags>EDUCATION,</tags>
		<tags>MUSIC,</tags>
		<description></description>
		<date>2018-06-19 15:18:45</date>
		<count>1</count>
		<year>1990</year>
		<url></url>
		<author>Todd Lindsay Fallis</author>
		<authors>
			<first>Todd Lindsay</first>
		</authors>
		<authors>
			<last>Fallis</last>
		</authors>
		<abstract>Purpose. The purpose of this study was to: (1) determine current practices in music education enabling high school students to meet high school graduation as well as admission requirements for California's public universities; and (2) provide guidance to school districts as they develop courses designed for that purpose. Methodology. A questionnaire was designed to gather data concerning: (1) the number of non-ensemble courses offered at any given high school and the current approach or mode of instruction of each course offered; (2) teacher familiarity with the 1987 university admission requirements; (3) status of submitted curricula, both ensemble and non-ensemble at each high school; (4) teacher beliefs about whether or not schools should offer non-ensemble courses for the purpose of meeting the requirements; and (5) teacher values which pertain to the approach and mode of instruction of such non-ensemble courses and their willingness to teach such courses. This questionnaire was distributed to a random sample of 200 teachers taken from the California Music Educators Association's mailing list of 842 high school teachers. Of the 200 questionnaires distributed, 124 (62.0\%) of the questionnaires were returned. Conclusions. (1) Although non-ensemble courses are offered in many of the schools, such courses are often performance oriented, including courses for guitar and keyboard. Many students have difficulty finding non-ensemble courses that are not geared toward performance. (2) Most non-ensemble courses are geared toward students who already have a strong background in music since they include members of ensemble courses. Consequently, there are few opportunities for students who are not in ensembles to receive a musical education. (3) Teachers are aware of policies regarding admission requirements, but few have submitted curricula to the institutions for approval. Courses submitted were ensembles adapted to meet requirements without consideration for general non-participating students. (4) Teachers maintain that high schools should offer non-ensemble courses for general students. Most favored by teachers are courses emphasizing historically oriented music listening and music fundamentals taught via Keyboard/Technology. (5) A majority of teachers indicated that they are interested in teaching non-ensemble courses. (Copies available exclusively from Micrographics Department, Doheny Library, USC, Los Angeles, CA 90089-0182.)</abstract>
		<shorttitle>The impact of graduation requirements and college admission policies on the music curriculum in California public and private high schools</shorttitle>
		<title>The impact of graduation requirements and college admission policies on the music curriculum in California public and private high schools</title>
		<pubtype>phdthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bbe492585a02b20324365d31620911ac/rcb</id>
		<tags>music</tags>
		<tags>lstm</tags>
		<tags>generation</tags>
		<description>[1804.07300] Generating Music using an LSTM Network</description>
		<date>2018-04-23 12:41:29</date>
		<count>2</count>
		<year>2018</year>
		<url>http://arxiv.org/abs/1804.07300</url>
		<author>Nikhil Kotecha</author>
		<author>Paul Young</author>
		<authors>
			<first>Nikhil</first>
		</authors>
		<authors>
			<last>Kotecha</last>
		</authors>
		<authors>
			<first>Paul</first>
		</authors>
		<authors>
			<last>Young</last>
		</authors>
		<abstract>A model of music needs to have the ability to recall past details and have a
clear, coherent understanding of musical structure. Detailed in the paper is a
neural network architecture that predicts and generates polyphonic music
aligned with musical rules. The probabilistic model presented is a Bi-axial
LSTM trained with a kernel reminiscent of a convolutional kernel. When analyzed
quantitatively and qualitatively, this approach performs well in composing
polyphonic music. Link to the code is provided.</abstract>
		<title>Generating Music using an LSTM Network</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22702cb5245db1972af9bef5be40611d4/ijtsrd</id>
		<tags>memory</tags>
		<tags>music</tags>
		<tags>therapy</tags>
		<tags>suggestibility</tags>
		<tags>attention</tags>
		<description></description>
		<date>2021-04-13 11:54:13</date>
		<count>1</count>
		<journal>International Journal of Trend in Scientific Research and Development</journal>
		<year>2021</year>
		<url>https://www.ijtsrd.com/medicine/other/38421/music-therapy-and-suggestibility-–-methods-of-activating-mechanisms-to-improve-cognitive-processes/dr-liliana-neagu</url>
		<author>Dr. Liliana Neagu</author>
		<authors>
			<first>Dr. Liliana</first>
		</authors>
		<authors>
			<last>Neagu</last>
		</authors>
		<volume>5</volume>
		<number>2</number>
		<pages>277-283</pages>
		<abstract>There are numerous research studies focused on the brain’s processing of information and on finding patterns and strategies to improve cognitive processes. To increase the capacity of memory and concentration we need to understand both the physiological and biochemical mechanisms, and the role of the external factors on these processes. The suggestions and harmonic combinations of music have proven their effectiveness by acting as a major influence in the field of neurophysiology, ameliorating a wide spectrum of memory and attention issues. Dr. Liliana Neagu "Music Therapy and Suggestibility – Methods of Activating Mechanisms to Improve Cognitive Processes" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-2 , February 2021, URL: https://www.ijtsrd.com/papers/ijtsrd38421.pdf Paper Url: https://www.ijtsrd.com/medicine/other/38421/music-therapy-and-suggestibility-–-methods-of-activating-mechanisms-to-improve-cognitive-processes/dr-liliana-neagu</abstract>
		<language>English</language>
		<issn>2456-6470</issn>
		<title>Music Therapy and Suggestibility – Methods of Activating Mechanisms to Improve Cognitive Processes</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2851621487b584a19c8d5a56731054d65/sapo</id>
		<tags>read_list</tags>
		<tags>music_demography</tags>
		<description>Predicting user demographics from music listening information | SpringerLink</description>
		<date>2020-05-14 09:37:16</date>
		<count>2</count>
		<journal>Multimedia Tools and Applications</journal>
		<year>2019</year>
		<url>https://doi.org/10.1007/s11042-018-5980-y</url>
		<author>Thomas Krismayer</author>
		<author>Markus Schedl</author>
		<author>Peter Knees</author>
		<author>Rick Rabiser</author>
		<authors>
			<first>Thomas</first>
		</authors>
		<authors>
			<last>Krismayer</last>
		</authors>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<authors>
			<first>Peter</first>
		</authors>
		<authors>
			<last>Knees</last>
		</authors>
		<authors>
			<first>Rick</first>
		</authors>
		<authors>
			<last>Rabiser</last>
		</authors>
		<volume>78</volume>
		<number>3</number>
		<pages>2897--2920</pages>
		<abstract>Online activities such as social networking, online shopping, and consuming multi-media create digital traces, which are often analyzed and used to improve user experience and increase revenue, e. g., through better-fitting recommendations and more targeted marketing. Analyses of digital traces typically aim to find user traits such as age, gender, and nationality to derive common preferences. We investigate to which extent the music listening habits of users of the social music platform Last.fm can be used to predict their age, gender, and nationality. We propose a feature modeling approach building on Term Frequency-Inverse Document Frequency (TF-IDF) for artist listening information and artist tags combined with additionally extracted features. We show that we can substantially outperform a baseline majority voting approach and can compete with existing approaches. Further, regarding prediction accuracy vs. available listening data we show that even one single listening event per user is enough to outperform the baseline in all prediction tasks. We also compare the performance of our algorithm for different user groups and discuss possible prediction errors and how to mitigate them. We conclude that personal information can be derived from music listening information, which indeed can help better tailoring recommendations, as we illustrate with the use case of a music recommender system that can directly utilize the user attributes predicted by our algorithm to increase the quality of it's recommendations.</abstract>
		<issn>1573-7721</issn>
		<doi>10.1007/s11042-018-5980-y</doi>
		<title>Predicting user demographics from music listening information.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25dd6930184e28657cb3dfc8cfbad1eb7/simonha94</id>
		<tags>cover</tags>
		<tags>retrieval</tags>
		<tags>music</tags>
		<tags>detection</tags>
		<tags>mir</tags>
		<tags>melody</tags>
		<tags>information</tags>
		<tags>extraction</tags>
		<description>Melody Extraction from Polyphonic Music Signals: Approaches, applications, and challenges | IEEE Journals & Magazine | IEEE Xplore</description>
		<date>2021-06-03 14:22:06</date>
		<count>2</count>
		<journal>IEEE Signal Processing Magazine</journal>
		<year>2014</year>
		<url>https://ieeexplore.ieee.org/abstract/document/6739213</url>
		<author>Justin Salamon</author>
		<author>Emilia Gomez</author>
		<author>Daniel P. W. Ellis</author>
		<author>Gael Richard</author>
		<authors>
			<first>Justin</first>
		</authors>
		<authors>
			<last>Salamon</last>
		</authors>
		<authors>
			<first>Emilia</first>
		</authors>
		<authors>
			<last>Gomez</last>
		</authors>
		<authors>
			<first>Daniel P. W.</first>
		</authors>
		<authors>
			<last>Ellis</last>
		</authors>
		<authors>
			<first>Gael</first>
		</authors>
		<authors>
			<last>Richard</last>
		</authors>
		<volume>31</volume>
		<number>2</number>
		<pages>118-134</pages>
		<abstract>Melody extraction algorithms aim to produce a sequence of frequency values corresponding to the pitch of the dominant melody from a musical recording. Over the past decade, melody extraction has emerged as an active research topic, comprising a large variety of proposed algorithms spanning a wide range of techniques. This article provides an overview of these techniques, the applications for which melody extraction is useful, and the challenges that remain. We start with a discussion of ?melody? from both musical and signal processing perspectives and provide a case study that interprets the output of a melody extraction algorithm for specific excerpts. We then provide a comprehensive comparative analysis of melody extraction algorithms based on the results of an international evaluation campaign. We discuss issues of algorithm design, evaluation, and applications that build upon melody extraction. Finally, we discuss some of the remaining challenges in melody extraction research in terms of algorithmic performance, development, and evaluation methodology.</abstract>
		<issn>1558-0792</issn>
		<doi>10.1109/MSP.2013.2271648</doi>
		<title>Melody Extraction from Polyphonic Music Signals: Approaches, applications, and challenges</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/287341d27a60c7940a4dfd7687cc6b7a2/sapo</id>
		<tags>concatenative_synthesis</tags>
		<description>k-Best Unit Selection Strategies for Musical Concatenative Synthesis | SpringerLink</description>
		<date>2019-06-30 12:40:28</date>
		<count>1</count>
		<booktitle>Music Technology with Swing</booktitle>
		<publisher>Springer International Publishing</publisher>
		<address>Cham</address>
		<year>2018</year>
		<url></url>
		<author>Cárthach Ó Nuanáin</author>
		<author>Perfecto Herrera</author>
		<author>Sergi Jordá</author>
		<authors>
			<first>Cárthach Ó</first>
		</authors>
		<authors>
			<last>Nuanáin</last>
		</authors>
		<authors>
			<first>Perfecto</first>
		</authors>
		<authors>
			<last>Herrera</last>
		</authors>
		<authors>
			<first>Sergi</first>
		</authors>
		<authors>
			<last>Jordá</last>
		</authors>
		<editor>Mitsuko Aramaki</editor>
		<editor>Matthew E. P. Davies</editor>
		<editor>Richard Kronland-Martinet</editor>
		<editor>Sølvi Ystad</editor>
		<editors>
			<first>Sergi</first>
		</editors>
		<editors>
			<last>Jordá</last>
		</editors>
		<editors>
			<first>Sergi</first>
		</editors>
		<editors>
			<last>Jordá</last>
		</editors>
		<editors>
			<first>Sergi</first>
		</editors>
		<editors>
			<last>Jordá</last>
		</editors>
		<editors>
			<first>Sergi</first>
		</editors>
		<editors>
			<last>Jordá</last>
		</editors>
		<pages>76--97</pages>
		<abstract>Concatenative synthesis is a sample-based approach to sound creation used frequently in speech synthesis and, increasingly, in musical contexts. Unit selection, a key component, is the process by which sounds are chosen from the corpus of samples. With their ability to match target units as well as preserve continuity, Hidden Markov Models are often chosen for this task, but one common criticism is its singular path output which is considered too restrictive when variations are desired. In this article, we propose considering the problem in terms of k-Best path solving for generating alternative lists of candidate solutions and summarise our implementations along with some practical examples.</abstract>
		<isbn>978-3-030-01692-0</isbn>
		<title>k-Best Unit Selection Strategies for Musical Concatenative Synthesis</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22a7b5668029eead580e26263b5cb4966/sapo</id>
		<tags>performance_analysis</tags>
		<description></description>
		<date>2019-09-13 13:07:34</date>
		<count>3</count>
		<booktitle>Prooceedings of the International Society for Music Information Retrieval Conference</booktitle>
		<address>Delft, The Netherlands</address>
		<year>2019</year>
		<url>http://arxiv.org/abs/1907.00178</url>
		<author>Alexander Lerch</author>
		<author>Claire Arthur</author>
		<author>Ashis Pati</author>
		<author>Siddharth Gururani</author>
		<authors>
			<first>Alexander</first>
		</authors>
		<authors>
			<last>Lerch</last>
		</authors>
		<authors>
			<first>Claire</first>
		</authors>
		<authors>
			<last>Arthur</last>
		</authors>
		<authors>
			<first>Ashis</first>
		</authors>
		<authors>
			<last>Pati</last>
		</authors>
		<authors>
			<first>Siddharth</first>
		</authors>
		<authors>
			<last>Gururani</last>
		</authors>
		<abstract>Music Information Retrieval (MIR) tends to focus on the analysis of audio
signals. Often, a single music recording is used as representative of a "song"
even though different performances of the same song may reveal different
properties. A performance is distinct in many ways from a (arguably more
abstract) representation of a "song," "piece," or musical score. The
characteristics of the (recorded) performance -- as opposed to the score or
musical idea -- can have a major impact on how a listener perceives music. The
analysis of music performance, however, has been traditionally only a
peripheral topic for the MIR research community. This paper surveys the field
of Music Performance Analysis (MPA) from various perspectives, discusses its
significance to the field of MIR, and points out opportunities for future
research in this field.</abstract>
		<title>Music Performance Analysis: A Survey</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d50343a5429efe750f06ccc3360fbbe4/sapo</id>
		<tags>Sound_Synthesis</tags>
		<tags>Electroencephalogram</tags>
		<tags>Biological_signal</tags>
		<tags>BCI</tags>
		<tags>EEG</tags>
		<tags>Music</tags>
		<tags>eNTERFACE'05</tags>
		<tags>Sound_Mapping.</tags>
		<tags>Electromyogram</tags>
		<tags>Brain_Computer_In-_terface</tags>
		<tags>EMG</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>Pdfs.Semanticscholar.Org</booktitle>
		<year>2005</year>
		<url>https://pdfs.semanticscholar.org/eec3/2ef086655648bd7879a93f572f68a64f74a5.pdf</url>
		<author>B. Arslan</author>
		<author>A. Brouse</author>
		<author>J. Castet</author>
		<author>J. J. Filatriau</author>
		<author>R. Lehembre</author>
		<authors>
			<first>B.</first>
		</authors>
		<authors>
			<last>Arslan</last>
		</authors>
		<authors>
			<first>A.</first>
		</authors>
		<authors>
			<last>Brouse</last>
		</authors>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Castet</last>
		</authors>
		<authors>
			<first>J. J.</first>
		</authors>
		<authors>
			<last>Filatriau</last>
		</authors>
		<authors>
			<first>R.</first>
		</authors>
		<authors>
			<last>Lehembre</last>
		</authors>
		<pages>14</pages>
		<abstract>This project proposes to use the analysis of phys- iological signals
	(electroencephalogram (EEG), electromyogram (EMG), heart beats) to
	control sound synthesis algorithms in order to build a biologically
	driven musical instrument. This project took place during the eNTERFACE'05
	summer workshop in Mons, Belgium. Over four weeks specialists from
	the fields of brain computer interfaces and sound synthesis worked
	together to produce playable biologically controlled musical instruments.
	Indeed, a ”bio- orchestra”, with three new digital musical instru-
	ments controlled by physiological signals of two bio-musicians on
	stage, was offered to a live audience.</abstract>
		<title>Biologically-driven Musical Instrument</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bca64703d11b0602dc5fbd70c02de292/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>Proceedings of the 10th International Society for Music Information Retrieval Conference (ISMIR 2009)</booktitle>
		<publisher>International Society for Music Information Retrieval</publisher>
		<year>2009</year>
		<url>http://ismir2009.ismir.net/proceedings/OS7-1.pdf</url>
		<author>Nicola Orio</author>
		<author>Antonio Rodà</author>
		<authors>
			<first>Nicola</first>
		</authors>
		<authors>
			<last>Orio</last>
		</authors>
		<authors>
			<first>Antonio</first>
		</authors>
		<authors>
			<last>Rodà</last>
		</authors>
		<editor>Keiji Hirata</editor>
		<editor>George Tzanetakis</editor>
		<editor>Kazuyoshi Yoshii</editor>
		<editors>
			<first>Antonio</first>
		</editors>
		<editors>
			<last>Rodà</last>
		</editors>
		<editors>
			<first>Antonio</first>
		</editors>
		<editors>
			<last>Rodà</last>
		</editors>
		<editors>
			<first>Antonio</first>
		</editors>
		<editors>
			<last>Rodà</last>
		</editors>
		<pages>543--548</pages>
		<abstract>Content-based music retrieval requires to define a similar- ity measure
	between music documents. In this paper, we propose a novel similarity
	measure between melodic con- tent, as represented in symbolic notation,
	that takes into accountmusicological aspects on the structural function
	of the melodic elements. The approach is based on the rep- resentation
	of a collection of music scores with a graph structure, where terminal
	nodes directly describe the mu- sic content, internal nodes represent
	its incremental gen- eralization, and arcs denote the relationships
	among them. The similarity between two melodies can be computed by
	analyzing the graph structure and finding the shortest path between
	the corresponding nodes inside the graph. Pre- liminary results in
	terms of music similarity are presented using a small test collection.</abstract>
		<bibsource>dblp computer science bibliography, https://dblp.org</bibsource>
		<title>A Measure of Melodic Similarity Based on a Graph Representation of the Music Structure</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/284278133345bb2a81f8316cc180e5b4a/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#PanLC12</url>
		<author>Susan Juan Pan</author>
		<author>Li Li</author>
		<author>Wu Chou</author>
		<authors>
			<first>Susan Juan</first>
		</authors>
		<authors>
			<last>Pan</last>
		</authors>
		<authors>
			<first>Li</first>
		</authors>
		<authors>
			<last>Li</last>
		</authors>
		<authors>
			<first>Wu</first>
		</authors>
		<authors>
			<last>Chou</last>
		</authors>
		<pages>29-34</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Real-Time Collaborative Video Watching on Mobile Devices with REST Services.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25dc573462d154f7bc7aaea2d73040e29/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#ZhengNN12</url>
		<author>Yi Zheng</author>
		<author>Uyen Trang Nguyen</author>
		<author>Hoang Lan Nguyen</author>
		<authors>
			<first>Yi</first>
		</authors>
		<authors>
			<last>Zheng</last>
		</authors>
		<authors>
			<first>Uyen Trang</first>
		</authors>
		<authors>
			<last>Nguyen</last>
		</authors>
		<authors>
			<first>Hoang Lan</first>
		</authors>
		<authors>
			<last>Nguyen</last>
		</authors>
		<pages>154-157</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Data Overhead Impact of Multipath Routing for Multicast in Wireless Mesh Networks.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26489789898acd147e36827729ac1a789/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#KimLPP12</url>
		<author>Joo-Hyung Kim</author>
		<author>Ho-Dong Lee</author>
		<author>Gwi-Tae Park</author>
		<author>Jung-Min Park</author>
		<authors>
			<first>Joo-Hyung</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Ho-Dong</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Gwi-Tae</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<authors>
			<first>Jung-Min</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<pages>254-259</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Implementation of Network Framework for Development and Integration of Network-Based Humanoid Robot System.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27084811eb568bf753d3b06a68dc84741/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#Chen12</url>
		<author>Jen-Shiun Chen</author>
		<authors>
			<first>Jen-Shiun</first>
		</authors>
		<authors>
			<last>Chen</last>
		</authors>
		<pages>164-169</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Target Identification Using Multifrequency Radar Sensor Networks.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b705f285dc2bf7beeb11ea83c255d738/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#JeunJLW12</url>
		<author>Inkyung Jeun</author>
		<author>Hyun-Chul Jung</author>
		<author>Nan Ki Lee</author>
		<author>Dongho Won</author>
		<authors>
			<first>Inkyung</first>
		</authors>
		<authors>
			<last>Jeun</last>
		</authors>
		<authors>
			<first>Hyun-Chul</first>
		</authors>
		<authors>
			<last>Jung</last>
		</authors>
		<authors>
			<first>Nan Ki</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Dongho</first>
		</authors>
		<authors>
			<last>Won</last>
		</authors>
		<pages>109-113</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Database Encryption Implementation and Analysis Using Graphics Processing Unit.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2cc62bd326810d00d3dbae076ebcdd8af/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#NsohB12</url>
		<author>Stephen Atambire Nsoh</author>
		<author>Robert Benkoczi</author>
		<authors>
			<first>Stephen Atambire</first>
		</authors>
		<authors>
			<last>Nsoh</last>
		</authors>
		<authors>
			<first>Robert</first>
		</authors>
		<authors>
			<last>Benkoczi</last>
		</authors>
		<pages>214-219</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Towards Global Connectivity by Joint Routing and Scheduling in Wireless Mesh Networks.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a324dd9ae4f7e73fb787b4183a6fcf9c/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-06-04 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#WongK12</url>
		<author>Kok-Seng Wong</author>
		<author>Myung Ho Kim</author>
		<authors>
			<first>Kok-Seng</first>
		</authors>
		<authors>
			<last>Wong</last>
		</authors>
		<authors>
			<first>Myung Ho</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<pages>120-125</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>A Privacy-Preserving Biometric Matching Protocol for Iris Codes Verification.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c9f7096febd986e4a0b741cb3048e52e/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2018-03-14 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#MaSWWYZH12</url>
		<author>Yanpeng Ma</author>
		<author>Jinshu Su</author>
		<author>Chunqing Wu</author>
		<author>Xiaofeng Wang</author>
		<author>Wanrong Yu</author>
		<author>Baokang Zhao</author>
		<author>Xiaofeng Hu</author>
		<authors>
			<first>Yanpeng</first>
		</authors>
		<authors>
			<last>Ma</last>
		</authors>
		<authors>
			<first>Jinshu</first>
		</authors>
		<authors>
			<last>Su</last>
		</authors>
		<authors>
			<first>Chunqing</first>
		</authors>
		<authors>
			<last>Wu</last>
		</authors>
		<authors>
			<first>Xiaofeng</first>
		</authors>
		<authors>
			<last>Wang</last>
		</authors>
		<authors>
			<first>Wanrong</first>
		</authors>
		<authors>
			<last>Yu</last>
		</authors>
		<authors>
			<first>Baokang</first>
		</authors>
		<authors>
			<last>Zhao</last>
		</authors>
		<authors>
			<first>Xiaofeng</first>
		</authors>
		<authors>
			<last>Hu</last>
		</authors>
		<pages>240-245</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>A Source-Based Share-Tree Like Multicast Routing in Satellite Constellation Networks.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2220f828c103df8b8a5be168cedda7a73/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#GardaziSS12</url>
		<author>Syeda Uzma Gardazi</author>
		<author>Arshad Ali Shahid</author>
		<author>Christine Salimbene</author>
		<authors>
			<first>Syeda Uzma</first>
		</authors>
		<authors>
			<last>Gardazi</last>
		</authors>
		<authors>
			<first>Arshad Ali</first>
		</authors>
		<authors>
			<last>Shahid</last>
		</authors>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Salimbene</last>
		</authors>
		<pages>246-253</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>HIPAA and QMS Based Architectural Requirements to Cope with the OCR Audit Program.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/200fa9f675ecf03fce46b3472df672bbd/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#AlowaisS12</url>
		<author>Mohammed Ibrahim Alowais</author>
		<author>Lay-Ki Soon</author>
		<authors>
			<first>Mohammed Ibrahim</first>
		</authors>
		<authors>
			<last>Alowais</last>
		</authors>
		<authors>
			<first>Lay-Ki</first>
		</authors>
		<authors>
			<last>Soon</last>
		</authors>
		<pages>114-119</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Credit Card Fraud Detection: Personalized or Aggregated Model.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/278b9718c356df6936555db336ebd543d/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#WangCZL12</url>
		<author>Jin-Yao Wang</author>
		<author>Yu-Lun Chang</author>
		<author>Wei-Sheng Zeng</author>
		<author>Shian-Hua Lin</author>
		<authors>
			<first>Jin-Yao</first>
		</authors>
		<authors>
			<last>Wang</last>
		</authors>
		<authors>
			<first>Yu-Lun</first>
		</authors>
		<authors>
			<last>Chang</last>
		</authors>
		<authors>
			<first>Wei-Sheng</first>
		</authors>
		<authors>
			<last>Zeng</last>
		</authors>
		<authors>
			<first>Shian-Hua</first>
		</authors>
		<authors>
			<last>Lin</last>
		</authors>
		<pages>84-89</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Recommend Significant Tags to Travel Photos Based on Web Mining.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23a697f21d411b9cbba35ccc21482c8fb/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#HoLLS12</url>
		<author>Kevin I.-J. Ho</author>
		<author>Wei-Bin Lee</author>
		<author>Siu-Chung Lau</author>
		<author>John Sum</author>
		<authors>
			<first>Kevin I.-J.</first>
		</authors>
		<authors>
			<last>Ho</last>
		</authors>
		<authors>
			<first>Wei-Bin</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Siu-Chung</first>
		</authors>
		<authors>
			<last>Lau</last>
		</authors>
		<authors>
			<first>John</first>
		</authors>
		<authors>
			<last>Sum</last>
		</authors>
		<pages>182-187</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Convergence Time of Wang's kWTA Network with Stochastic Output Nodes.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b9261af4e1d88058892511c9c4b29906/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#Al-FagihAHA12</url>
		<author>Ashraf E. Al-Fagih</author>
		<author>Fadi M. Al-Turjman</author>
		<author>Hossam S. Hassanein</author>
		<author>Waleed Alsalih</author>
		<authors>
			<first>Ashraf E.</first>
		</authors>
		<authors>
			<last>Al-Fagih</last>
		</authors>
		<authors>
			<first>Fadi M.</first>
		</authors>
		<authors>
			<last>Al-Turjman</last>
		</authors>
		<authors>
			<first>Hossam S.</first>
		</authors>
		<authors>
			<last>Hassanein</last>
		</authors>
		<authors>
			<first>Waleed</first>
		</authors>
		<authors>
			<last>Alsalih</last>
		</authors>
		<pages>220-224</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Coverage-Based Placement in RFID Networks: An Overview.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c5fc2a48493339b7351128bea1724f9b/sapo</id>
		<tags>datasets</tags>
		<description>[2010.07061] GiantMIDI-Piano: A large-scale MIDI dataset for classical piano music</description>
		<date>2020-11-18 16:14:04</date>
		<count>2</count>
		<journal>Transactions of the International Society for Music Information Retrieval</journal>
		<year>2020</year>
		<url>http://arxiv.org/abs/2010.07061</url>
		<author>Qiuqiang Kong</author>
		<author>Bochen Li</author>
		<author>Jitong Chen</author>
		<author>Yuxuan Wang</author>
		<authors>
			<first>Qiuqiang</first>
		</authors>
		<authors>
			<last>Kong</last>
		</authors>
		<authors>
			<first>Bochen</first>
		</authors>
		<authors>
			<last>Li</last>
		</authors>
		<authors>
			<first>Jitong</first>
		</authors>
		<authors>
			<last>Chen</last>
		</authors>
		<authors>
			<first>Yuxuan</first>
		</authors>
		<authors>
			<last>Wang</last>
		</authors>
		<volume>V</volume>
		<abstract>Symbolic music datasets are important for music information retrieval and
musical analysis. However, there is a lack of large-scale symbolic dataset for
classical piano music. In this article, we create a GiantMIDI-Piano dataset
containing 10,854 unique piano solo pieces composed by 2,786 composers. The
dataset is collected as follows, we extract music piece names and composer
names from the International Music Score Library Project (IMSLP). We search and
download their corresponding audio recordings from the internet. We apply a
convolutional neural network to detect piano solo pieces. Then, we transcribe
those piano solo recordings to Musical Instrument Digital Interface (MIDI)
files using our recently proposed high-resolution piano transcription system.
Each transcribed MIDI file contains onset, offset, pitch and velocity
attributes of piano notes, and onset and offset attributes of sustain pedals.
GiantMIDI-Piano contains 34,504,873 transcribed notes, and contains metadata
information of each music piece. To our knowledge, GiantMIDI-Piano is the
largest classical piano MIDI dataset so far. We analyses the statistics of
GiantMIDI-Piano including the nationalities, the number and duration of works
of composers. We show the chroma, interval, trichord and tetrachord frequencies
of six composers from different eras to show that GiantMIDI-Piano can be used
for musical analysis. Our piano solo detection system achieves an accuracy of
89\%, and the piano note transcription achieves an onset F1 of 96.72\%
evaluated on the MAESTRO dataset. GiantMIDI-Piano achieves an alignment error
rate (ER) of 0.154 to the manually input MIDI files, comparing to MAESTRO with
an alignment ER of 0.061 to the manually input MIDI files. We release the
source code of acquiring the GiantMIDI-Piano dataset at
https://github.com/bytedance/GiantMIDI-Piano.</abstract>
		<title>GiantMIDI-Piano: A large-scale MIDI dataset for classical piano music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>http://hdl.handle.net/10230/34086</id>
		<tags>source_separation</tags>
		<description>Monaural score-informed source separation for classical music using convolutional neural networks</description>
		<date>2020-11-12 18:21:14</date>
		<count>1</count>
		<publisher>International Society for Music Information Retrieval (ISMIR)</publisher>
		<year>2018</year>
		<url>https://repositori.upf.edu/handle/10230/34086</url>
		<author>Marius Miron</author>
		<author>Jordi Janer Mestres</author>
		<author>1975 Gómez Gutiérrez, Emilia</author>
		<authors>
			<first>Marius</first>
		</authors>
		<authors>
			<last>Miron</last>
		</authors>
		<authors>
			<first>Jordi</first>
		</authors>
		<authors>
			<last>Janer Mestres</last>
		</authors>
		<authors>
			<first>1975</first>
		</authors>
		<authors>
			<last>Gómez Gutiérrez, Emilia</last>
		</authors>
		<abstract>Comunicació presentada a la 18th International Society for Music Information Retrieval Conference (ISMIR 2017), celebrada els dies 23 a 27 d'octubre de 2017 a Suzhou, Xina., Score information has been shown to improve music source separation when included into non-negative matrix factorization (NMF) frameworks. Recently, deep learning approaches have outperformed NMF methods in terms of separation quality and processing time, and there is scope to extend them with score information. In this paper, we propose a score-informed separation system for classical music that is based on deep learning. We propose a method to derive training features from audio files and the corresponding coarsely aligned scores for a set of classical music pieces. Additionally, we introduce a convolutional neural network architecture (CNN) with the goal of estimating time-frequency masks for source separation. Our system is trained with synthetic renditions derived from the original scores and can be used to separate real-life performances based on the same scores, provided a coarse audio-to-score alignment. The proposed system achieves better performance (SDR and SIR) and is less computationally intensive than a score-informed NMF system on a dataset comprising Bach chorales., This work is partially supported by the Spanish Ministry of Economy and Competitiveness under CASAS project (TIN2015-70816-R) and by the Spanish Ministry of Economy and Competitiveness under the Maria de Maeztu Units of Excellence Programme (MDM2015-0502).</abstract>
		<title>Monaural score-informed source separation for classical music using convolutional neural networks</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c742d0944ec8a064c30fa0ba7c1129ca/sitdhibong</id>
		<tags>emotion</tags>
		<tags>music</tags>
		<description>Transformer-based approach towards music emotion recognition from lyrics</description>
		<date>2021-10-22 04:19:12</date>
		<count>1</count>
		<year>2021</year>
		<url>http://arxiv.org/abs/2101.02051</url>
		<author>Yudhik Agrawal</author>
		<author>Ramaguru Guru Ravi Shanker</author>
		<author>Vinoo Alluri</author>
		<authors>
			<first>Yudhik</first>
		</authors>
		<authors>
			<last>Agrawal</last>
		</authors>
		<authors>
			<first>Ramaguru Guru Ravi</first>
		</authors>
		<authors>
			<last>Shanker</last>
		</authors>
		<authors>
			<first>Vinoo</first>
		</authors>
		<authors>
			<last>Alluri</last>
		</authors>
		<abstract>The task of identifying emotions from a given music track has been an active
pursuit in the Music Information Retrieval (MIR) community for years. Music
emotion recognition has typically relied on acoustic features, social tags, and
other metadata to identify and classify music emotions. The role of lyrics in
music emotion recognition remains under-appreciated in spite of several studies
reporting superior performance of music emotion classifiers based on features
extracted from lyrics. In this study, we use the transformer-based approach
model using XLNet as the base architecture which, till date, has not been used
to identify emotional connotations of music based on lyrics. Our proposed
approach outperforms existing methods for multiple datasets. We used a robust
methodology to enhance web-crawlers' accuracy for extracting lyrics. This study
has important implications in improving applications involved in playlist
generation of music based on emotions in addition to improving music
recommendation systems.</abstract>
		<doi>10.1007/978-3-030-72240-1_12</doi>
		<title>Transformer-based approach towards music emotion recognition from lyrics</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c9fb4a7583c4d61dbb622ec9041df060/brusilovsky</id>
		<tags>music-recommendation</tags>
		<tags>user-control</tags>
		<tags>interactive-recommender</tags>
		<tags>iui2021</tags>
		<tags>information-exploration</tags>
		<description>Interactive Music Genre Exploration with Visualization and Mood Control | 26th International Conference on Intelligent User Interfaces</description>
		<date>2022-01-16 21:03:40</date>
		<count>1</count>
		<booktitle>26th International Conference on Intelligent User Interfaces</booktitle>
		<publisher>ACM</publisher>
		<year>2021</year>
		<url>https://doi.org/10.1145%2F3397481.3450700</url>
		<author>Yu Liang</author>
		<author>Martijn C. Willemsen</author>
		<authors>
			<first>Yu</first>
		</authors>
		<authors>
			<last>Liang</last>
		</authors>
		<authors>
			<first>Martijn C.</first>
		</authors>
		<authors>
			<last>Willemsen</last>
		</authors>
		<pages>175-185</pages>
		<abstract>Recommender systems can be used to help users discover novel items and explore new tastes, for example in music genre exploration. However, little work has studied how to improve users’ understandability and acceptance of the novel items as well as support users to explore a new domain. In this paper, we investigate how two different visualizations and mood control influence the perceived control, informativeness and understandability of a music genre exploration tool, and further to improve the helpfulness for new music genre exploration. Specifically, we compare a bar chart visualization used by earlier work to a contour plot which allows users to compare their musical preferences with both the recommended tracks as well as the new genre. Mood control is implemented with two sliders to set a preferred mood on energy and valence features (that correlate with psychological mood dimensions). In the online user study, mood control was manipulated between subjects, and the visualizations were compared within subjects. During the study (N=102), we measured users’ subjective perceptions, experiences and the interactions with the system. Our results show that the contour plot visualization is perceived more helpful to explore new genres than the bar chart visualization, as the contour plot is perceived to be more informative and understandable. Users spent significantly more time and used the mood control more in the contour plot than in the bar chart visualization. Overall, our results show that the contour plot visualization combined with mood control serves as the most helpful way for new music genre exploration, because the mood control is easier to understand and use when made transparent via an informative visualization.</abstract>
		<doi>10.1145/3397481.3450700</doi>
		<title>Interactive Music Genre Exploration with Visualization and Mood Control</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2703f0a69fb23b159c22dd602af2ce219/brazovayeye</id>
		<tags>music,</tags>
		<tags>genetic</tags>
		<tags>algorithms,</tags>
		<tags>programming,</tags>
		<tags>coevolution</tags>
		<description></description>
		<date>2008-06-19 17:46:40</date>
		<count>1</count>
		<booktitle>Musical Networks: Parallel Distributed Perception and
                 Performance</booktitle>
		<publisher>MIT Press</publisher>
		<year>1999</year>
		<url>http://www-abc.mpib-berlin.mpg.de/users/ptodd/publications/99evmus/99evmus.pdf</url>
		<author>Peter M. Todd</author>
		<author>Gregory M. Werner</author>
		<authors>
			<first>Peter M.</first>
		</authors>
		<authors>
			<last>Todd</last>
		</authors>
		<authors>
			<first>Gregory M.</first>
		</authors>
		<authors>
			<last>Werner</last>
		</authors>
		<editor>Niall Griffith</editor>
		<editor>Peter M. Todd</editor>
		<editors>
			<first>Gregory M.</first>
		</editors>
		<editors>
			<last>Werner</last>
		</editors>
		<editors>
			<first>Gregory M.</first>
		</editors>
		<editors>
			<last>Werner</last>
		</editors>
		<pages>313--340</pages>
		<abstract>Victor Frankenstein sought to create an intelligent
                 being imbued with the rules of civilized human conduct,
                 who could further learn how to behave and possibly even
                 evolve through successive generations into a more
                 perfect form. Modern human composers similarly strive
                 to create intelligent algorithmic music composition
                 systems that can follow prespecified rules, learn
                 appropriate patterns from a collection of melodies, or
                 evolve to produce output more perfectly matched to some
                 aesthetic criteria. Here we review recent efforts aimed
                 at each of these three types of algorithmic
                 composition. We focus particularly on evolutionary
                 methods, and indicate how monstrous many of the results
                 have been. We present a new method that uses
                 coevolution to create linked artificial music critics
                 and music composers, and describe how this method can
                 attach the separate parts of rules, learning, and
                 evolution together into one coherent body.</abstract>
		<isbn>0-262-07181-9</isbn>
		<notes>Survey. Mentions Spector:1994:ccaga and
                 Spector:1995:irdms cf Lee Spector's email to GP
                 list Wed, 27 Apr 2005 21:21:24 EDT</notes>
		<title>Frankensteinian approaches to evolutionary music
                 composition</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2cb6e11b60289161d81cc61fe7990d882/roddyhawkins</id>
		<tags>music</tags>
		<tags>mcclary</tags>
		<tags>history</tags>
		<tags>pedagogy</tags>
		<tags>taruskin</tags>
		<description>Project MUSE - Music and Letters - The World According to Taruskin</description>
		<date>2009-05-20 02:30:30</date>
		<count>1</count>
		<journal>Music and Letters</journal>
		<year>2006</year>
		<url>http://muse.jhu.edu/journals/music_and_letters/v087/87.3mcclary.html</url>
		<author>Susan. McClary</author>
		<authors>
			<first>Susan.</first>
		</authors>
		<authors>
			<last>McClary</last>
		</authors>
		<volume>87</volume>
		<pages>408-415</pages>
		<issn>1477-4631</issn>
		<issue>3</issue>
		<title>The World According to Taruskin</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bb5feb4dafd211c60354919a54728eb6/bfields</id>
		<tags>music_classification</tags>
		<tags>music_similarity</tags>
		<description>initial imports</description>
		<date>2010-01-28 11:42:45</date>
		<count>2</count>
		<booktitle>Proc. of Int. Symposium on Music Information Retrieval</booktitle>
		<year>2005</year>
		<url></url>
		<author>Elias Pampalk</author>
		<author>Arthur Flexer</author>
		<author>Gerhard Widmer</author>
		<authors>
			<first>Elias</first>
		</authors>
		<authors>
			<last>Pampalk</last>
		</authors>
		<authors>
			<first>Arthur</first>
		</authors>
		<authors>
			<last>Flexer</last>
		</authors>
		<authors>
			<first>Gerhard</first>
		</authors>
		<authors>
			<last>Widmer</last>
		</authors>
		<title>Improvements of Audio-Based Music Similarity and Genre Classification</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/204a9dc41d1b4203537b2d64a0a45adf7/bfields</id>
		<tags>feature_extraction</tags>
		<description>initial imports</description>
		<date>2010-01-28 11:42:45</date>
		<count>1</count>
		<year>2005?</year>
		<url>http://www.music.mcgill.ca/~cmckay/papers/musictech/jAudio.pdf</url>
		<author>C McKay</author>
		<authors>
			<first>C</first>
		</authors>
		<authors>
			<last>McKay</last>
		</authors>
		<title>jAudio: Towards a standardized extensible audio music feature extraction system</title>
		<pubtype>techreport</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28711c7ba88e8f7b2dfcfefa55e8eb7d6/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>INTERCULTURAL MUSIC</journal>
		<series>The intercultural nexus between the music of Motown and the Beatles.</series>
		<year>2001</year>
		<url>http://ezproxy.library.yorku.ca/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=mah&AN=MAH0001368011&site=ehost-live</url>
		<author>KIMASI L. BROWNE</author>
		<author>KIMASI L. BROWNE</author>
		<authors>
			<first>KIMASI L.</first>
		</authors>
		<authors>
			<last>BROWNE</last>
		</authors>
		<authors>
			<first>KIMASI L.</first>
		</authors>
		<authors>
			<last>BROWNE</last>
		</authors>
		<volume>4</volume>
		<pages>57--71</pages>
		<doi>Article</doi>
		<title>The intercultural nexus between the music of Motown and the Beatles.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ac30b7b1add11a83164af97c4a576f46/svrist</id>
		<tags>Music</tags>
		<tags>Optical</tags>
		<tags>analysis,</tags>
		<tags>Grammar</tags>
		<tags>DCG,</tags>
		<tags>Document</tags>
		<tags>Recognition,</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>1</count>
		<journal>International Conference on the Practical Application of Prolog</journal>
		<year>1995</year>
		<url></url>
		<author>B. Co�asnon</author>
		<author>P. Brisset</author>
		<author>I. Stephan</author>
		<authors>
			<first>B.</first>
		</authors>
		<authors>
			<last>Co�asnon</last>
		</authors>
		<authors>
			<first>P.</first>
		</authors>
		<authors>
			<last>Brisset</last>
		</authors>
		<authors>
			<first>I.</first>
		</authors>
		<authors>
			<last>Stephan</last>
		</authors>
		<pages>115-34</pages>
		<abstract>Optical Music Recognition is a particular form of document analysis in
which there is much knowledge about document structure. Indeed there exists
an important set of rules for musical notation, but current systems do not
fully use them. We propose a new solution using a grammar to guide the
segmentation of the graphical objects and their recognition. The grammar is
essentially a description of the relations (relative position and size,
adjacency,
etc) between the graphical objects.
Inspired by Definite Clause Grammar techniques, the grammar can be directly
implemented in Prolog, a higher-order dialect of Prolog. Moreover, the
translation from the grammar into Prolog code can be done automatically.
Our approach is justified by the first encouraging results obtained with a
prototype for music score recognition.</abstract>
		<title>Using logic programming languages for optical music recognition</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/249758d95a2da54f0be7966bc08a270a9/svrist</id>
		<tags>music</tags>
		<tags>scores</tags>
		<tags>detection;</tags>
		<tags>Kalman</tags>
		<tags>segment</tags>
		<tags>filtering;</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>2</count>
		<journal>Proceedings of the 12th IAPR International Conference on Pattern Recognition (Cat. No.94CH3440-5)</journal>
		<year>1994</year>
		<url></url>
		<author>V. P. d'Andecy</author>
		<author>J. Camillerapp</author>
		<author>I. Leplumey</author>
		<authors>
			<first>V. P.</first>
		</authors>
		<authors>
			<last>d'Andecy</last>
		</authors>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Camillerapp</last>
		</authors>
		<authors>
			<first>I.</first>
		</authors>
		<authors>
			<last>Leplumey</last>
		</authors>
		<volume>1</volume>
		<pages>301-5</pages>
		<abstract>Many symbols in music scores are linear segments. In this context, we designed an extractor of segments. It is robust towards problems of quality within binary images (scale factor, curvature, bias and noises). It is based on Kalman filtering technique. By splitting music scores into layers of detectable symbols and by applying methodically to the defined layers both this extractor and simple rules of classification for the detected segments, we were able to recognize staves, stems, slurs, beams, bar lines, black note heads and then quarters and note groups (9 Refs.) filters; optical character recognition</abstract>
		<title>Kalman filtering for segment detection: application to music scores analysis</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/228c1db42279323f89cabdb48bae9404c/svrist</id>
		<tags>image</tags>
		<tags>distributed</tags>
		<tags>symbols;</tags>
		<tags>techniques)</tags>
		<tags>optical</tags>
		<tags>printed</tags>
		<tags>binary</tags>
		<tags>recognition);</tags>
		<tags>processing</tags>
		<tags>ADAM;</tags>
		<tags>C1230D</tags>
		<tags>C1250B</tags>
		<tags>vision</tags>
		<tags>C5290</tags>
		<tags>memory</tags>
		<tags>techniques);</tags>
		<tags>network;</tags>
		<tags>neural</tags>
		<tags>advanced</tags>
		<tags>(Character</tags>
		<tags>score</tags>
		<tags>music</tags>
		<tags>character</tags>
		<tags>recognition;</tags>
		<tags>computing</tags>
		<tags>and</tags>
		<tags>associative</tags>
		<tags>nets);</tags>
		<tags>(Neural</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>1</count>
		<journal>Machine Learning and Data Mining in Pattern Recognition. First International Workshop, MLDM'99. Proceedings. (Lecture Notes in Artificial Intelligence Vol.1715)</journal>
		<year>1999</year>
		<url></url>
		<author>T. Beran</author>
		<author>T. Macek</author>
		<authors>
			<first>T.</first>
		</authors>
		<authors>
			<last>Beran</last>
		</authors>
		<authors>
			<first>T.</first>
		</authors>
		<authors>
			<last>Macek</last>
		</authors>
		<pages>174-9</pages>
		<abstract>This article describes our implementation of the optical music recognition system (OMR). The system implemented in our project is based on the binary neural network ADAM which has been used for recognition of music symbols. Preprocessing was implemented by conventional techniques. We decomposed the OMR process into several phases. The results of these phases are summarized (12 Refs.) pattern classification</abstract>
		<title>Recognition of printed music score</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e93c338cea3818d2450a46e1ff8374b7/ajd54</id>
		<tags>reward</tags>
		<tags>emotion</tags>
		<tags>music</tags>
		<description>Intensely pleasurable responses to music correlate... [Proc Natl Acad Sci U S A. 2001] - PubMed result</description>
		<date>2010-03-27 17:49:44</date>
		<count>1</count>
		<journal>Proc Natl Acad Sci U S A</journal>
		<year>2001</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/11573015</url>
		<author>A J Blood</author>
		<author>R J Zatorre</author>
		<authors>
			<first>A J</first>
		</authors>
		<authors>
			<last>Blood</last>
		</authors>
		<authors>
			<first>R J</first>
		</authors>
		<authors>
			<last>Zatorre</last>
		</authors>
		<volume>98</volume>
		<number>20</number>
		<pages>11818-11823</pages>
		<abstract>We used positron emission tomography to study neural mechanisms underlying intensely pleasant emotional responses to music. Cerebral blood flow changes were measured in response to subject-selected music that elicited the highly pleasurable experience of "shivers-down-the-spine" or "chills." Subjective reports of chills were accompanied by changes in heart rate, electromyogram, and respiration. As intensity of these chills increased, cerebral blood flow increases and decreases were observed in brain regions thought to be involved in reward/motivation, emotion, and arousal, including ventral striatum, midbrain, amygdala, orbitofrontal cortex, and ventral medial prefrontal cortex. These brain structures are known to be active in response to other euphoria-inducing stimuli, such as food, sex, and drugs of abuse. This finding links music with biologically relevant, survival-related stimuli via their common recruitment of brain circuitry involved in pleasure and reward.</abstract>
		<doi>10.1073/pnas.191355898</doi>
		<title>Intensely pleasurable responses to music correlate with activity in brain regions implicated in reward and emotion</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29dff30a4f4288fc80bc748dce9cd04be/mediadigits</id>
		<tags>music</tags>
		<tags>psychology</tags>
		<description></description>
		<date>2010-12-27 16:08:46</date>
		<count>1</count>
		<journal>Musicae Scientiae</journal>
		<publisher>European Society for the Cognitive Sciences of Music</publisher>
		<year>2001</year>
		<url></url>
		<author>J.A. Sloboda</author>
		<author>S.A. O'Neill</author>
		<author>A. Ivaldi</author>
		<authors>
			<first>J.A.</first>
		</authors>
		<authors>
			<last>Sloboda</last>
		</authors>
		<authors>
			<first>S.A.</first>
		</authors>
		<authors>
			<last>O'Neill</last>
		</authors>
		<authors>
			<first>A.</first>
		</authors>
		<authors>
			<last>Ivaldi</last>
		</authors>
		<issn>1029-8649</issn>
		<title>Functions of music in everyday life: An exploratory study using the Experience Sampling Method.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28feefd8687a79106c84181ca820a25b4/yevb0</id>
		<tags>music,perception,pitch</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>The psychology of music</booktitle>
		<publisher>Academic Press</publisher>
		<address>New York</address>
		<year>1982</year>
		<url></url>
		<author>R A Rasch</author>
		<author>R Plomp</author>
		<authors>
			<first>R A</first>
		</authors>
		<authors>
			<last>Rasch</last>
		</authors>
		<authors>
			<first>R</first>
		</authors>
		<authors>
			<last>Plomp</last>
		</authors>
		<editor>Diana Deutsch</editor>
		<editors>
			<first>R</first>
		</editors>
		<editors>
			<last>Plomp</last>
		</editors>
		<pages>1--21</pages>
		<title>The perception of musical tones</title>
		<pubtype>inbook</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f37a976ccc1beaf3ff06172e1b4e5aaa/mediadigits</id>
		<tags>semantic</tags>
		<tags>music</tags>
		<tags>taxonomy</tags>
		<description></description>
		<date>2008-03-20 14:48:24</date>
		<count>1</count>
		<address>Stockholm, Sweden</address>
		<year>2003</year>
		<url>http://citeseer.ist.psu.edu/cache/papers/cs/30732/http:zSzzSzwww.ipem.ugent.bezSzmamizSzpubliczSzPaperszSzLesaffreEtAlSMAC03.pdf/lesaffre03userdependent.pdf</url>
		<author>Micheline Lesaffre</author>
		<author>Marc Leman</author>
		<author>Koen Tanghe</author>
		<author>Bernard De Baet</author>
		<author>Hans De Meyer</author>
		<author>Jean-Pierre Martens</author>
		<authors>
			<first>Micheline</first>
		</authors>
		<authors>
			<last>Lesaffre</last>
		</authors>
		<authors>
			<first>Marc</first>
		</authors>
		<authors>
			<last>Leman</last>
		</authors>
		<authors>
			<first>Koen</first>
		</authors>
		<authors>
			<last>Tanghe</last>
		</authors>
		<authors>
			<first>Bernard De</first>
		</authors>
		<authors>
			<last>Baet</last>
		</authors>
		<authors>
			<first>Hans De</first>
		</authors>
		<authors>
			<last>Meyer</last>
		</authors>
		<authors>
			<first>Jean-Pierre</first>
		</authors>
		<authors>
			<last>Martens</last>
		</authors>
		<editor>Stockholm Music Acoustics Conference</editor>
		<editors>
			<first>Jean-Pierre</first>
		</editors>
		<editors>
			<last>Martens</last>
		</editors>
		<abstract>Musical audio-mining technology allows users to search and retrieve music by means of content-based text and audio queries. Though these systems are promising, there is a need for a better understanding of the role of user preferences and user profiles. The development of taxonomies for different aspects of a musical audio-mining system aims at bridging the gap between system development and user interactive interfacing. In the first part of this paper, the need for user-dependent taxonomy development is addressed and an experiment in spontaneous user behavior is described, based on 72 users and 1148 vocal queries. Statistical analysis provides insight into the characteristics of vocal querying and possible useful concepts. In the second part of the paper, it is described how categories and concepts from the statistical analysis have been used for the refinement of taxonomies that address user interactive interfacing and feature extraction.</abstract>
		<title>User-Dependent Taxonomy of Musical Features As a Conceptual Framework for Musical Audio-Mining Technology</title>
		<pubtype>proceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fa1bf1e293d9c266710659f73d2e3c19/keinstein</id>
		<tags>Intervall</tags>
		<tags>Musiktheorie</tags>
		<tags>MaMu</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2012-02-16 17:12:46</date>
		<count>2</count>
		<journal>Journal of Music Theory</journal>
		<year>2009</year>
		<url>http://jmt.dukejournals.org/cgi/content/abstract/53/2/227</url>
		<author>Dmitri Tymoczko</author>
		<authors>
			<first>Dmitri</first>
		</authors>
		<authors>
			<last>Tymoczko</last>
		</authors>
		<volume>53</volume>
		<number>2</number>
		<pages>227-254</pages>
		<abstract>Taking David Lewin's work as a point of departure, this essay uses geometry to reexamine familiar music-theoretical assumptions about intervals and transformations. Section 1 introduces the problem of "transportability," noting that it is sometimes impossible to say whether two different directions--located at two different points in a geometrical space--are "the same" or not. Relevant examples include the surface of the earth and the geometrical spaces representing n-note chords. Section 2 argues that we should not require that every interval be defined at every point in a space, since some musical spaces have natural boundaries. It also notes that there are spaces, including the familiar pitch-class circle, in which there are multiple paths between any two points. This leads to the suggestion that we might sometimes want to replace traditional pitch-class intervals with paths in pitch-class space, a more fine-grained alternative that specifies how one pitch class moves to another. Section 3 argues that group theory alone cannot represent the intuition that intervals have quantifiable sizes, proposing an extension to Lewin's formalism that accomplishes this goal. Finally, Section 4 considers the analytical implications of the preceding points, paying particular attention to questions about voice leading.</abstract>
		<doi>10.1215/00222909-2010-003</doi>
		<eprint>http://jmt.dukejournals.org/cgi/reprint/53/2/227.pdf</eprint>
		<title>Generalizing Musical Intervals</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2628b490883a32d63128e0f3eca7260f0/keinstein</id>
		<tags>Mathematik</tags>
		<tags>Gruppen</tags>
		<tags>Musiktheorie</tags>
		<tags>MaMu</tags>
		<description>Musical Actions of Dihedral Groups</description>
		<date>2011-09-13 14:10:01</date>
		<count>2</count>
		<year>2007</year>
		<url>http://arxiv.org/abs/0711.1873</url>
		<author>Alissa S. Crans</author>
		<author>Thomas M. Fiore</author>
		<author>Ramon Satyendra</author>
		<authors>
			<first>Alissa S.</first>
		</authors>
		<authors>
			<last>Crans</last>
		</authors>
		<authors>
			<first>Thomas M.</first>
		</authors>
		<authors>
			<last>Fiore</last>
		</authors>
		<authors>
			<first>Ramon</first>
		</authors>
		<authors>
			<last>Satyendra</last>
		</authors>
		<abstract>The sequence of pitches which form a musical melody can be transposed orinverted. Since the 1970s, music theorists have modeled musical transpositionand inversion in terms of an action of the dihedral group of order 24. Morerecently music theorists have found an intriguing second way that the dihedralgroup of order 24 acts on the set of major and minor chords. We illustrate bothgeometrically and algebraically how these two actions are dual. Bothactions and their duality have been used to analyze works of music as diverseas Hindemith and the Beatles.</abstract>
		<title>Musical Actions of Dihedral Groups</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2376bf34a2a479234735f19336a029955/ks-plugin-devel</id>
		<tags>MaMu</tags>
		<description>doi: 10.1080/17459730601137716</description>
		<date>2013-02-02 14:39:57</date>
		<count>2</count>
		<journal>Journal of Mathematics and Music</journal>
		<booktitle>Journal of Mathematics and Music</booktitle>
		<publisher>Taylor & Francis</publisher>
		<year>2007</year>
		<url>http://dx.doi.org/10.1080/17459730601137716</url>
		<author>Guerino Mazzola</author>
		<author>Moreno Andreatta</author>
		<authors>
			<first>Guerino</first>
		</authors>
		<authors>
			<last>Mazzola</last>
		</authors>
		<authors>
			<first>Moreno</first>
		</authors>
		<authors>
			<last>Andreatta</last>
		</authors>
		<volume>1</volume>
		<number>1</number>
		<pages>23--46</pages>
		<abstract>Abstract This paper shows an interplay of music and mathematics which strongly differs from the usual scheme reducing mathematics to a toolbox of formal models for music. Using the topos of directed graphs as a common base category, we develop a comprising framework for mathematical music theory, which ramifies into an algebraic and a topological branch. Whereas the algebraic component comprises the universe of formulae, transformations, and functional constraints as they are described by functorial diagrammatic limits, the topological branch covers the continuous aspects of the creative dynamics of musical gestures and their multilayered articulation. These two branches unfold in a surprisingly parallel manner, although the concrete structures (homotopy versus representation theory) are fairly heterogeneous. However, the unity of the underlying musical substance suggests that these two apparently divergent strategies should find a common point of unification, an idea that we describe in terms of a conjectural diamond of categories which suggests a number of unification points. In particular, the passage from the topological to the algebraic branch is achieved by the idea of the gestoid, an ?algebraic? category associated with the fundamental groupoid of a gesture.</abstract>
		<issn>17459737</issn>
		<doi>10.1080/17459730601137716</doi>
		<title>Diagrams, gestures and formulae in music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2376bf34a2a479234735f19336a029955/keinstein</id>
		<tags>MaMu</tags>
		<description>doi: 10.1080/17459730601137716</description>
		<date>2012-11-22 21:20:40</date>
		<count>2</count>
		<journal>Journal of Mathematics and Music</journal>
		<booktitle>Journal of Mathematics and Music</booktitle>
		<publisher>Taylor & Francis</publisher>
		<year>2007</year>
		<url>http://dx.doi.org/10.1080/17459730601137716</url>
		<author>Guerino Mazzola</author>
		<author>Moreno Andreatta</author>
		<authors>
			<first>Guerino</first>
		</authors>
		<authors>
			<last>Mazzola</last>
		</authors>
		<authors>
			<first>Moreno</first>
		</authors>
		<authors>
			<last>Andreatta</last>
		</authors>
		<volume>1</volume>
		<number>1</number>
		<pages>23--46</pages>
		<abstract>Abstract This paper shows an interplay of music and mathematics which strongly differs from the usual scheme reducing mathematics to a toolbox of formal models for music. Using the topos of directed graphs as a common base category, we develop a comprising framework for mathematical music theory, which ramifies into an algebraic and a topological branch. Whereas the algebraic component comprises the universe of formulae, transformations, and functional constraints as they are described by functorial diagrammatic limits, the topological branch covers the continuous aspects of the creative dynamics of musical gestures and their multilayered articulation. These two branches unfold in a surprisingly parallel manner, although the concrete structures (homotopy versus representation theory) are fairly heterogeneous. However, the unity of the underlying musical substance suggests that these two apparently divergent strategies should find a common point of unification, an idea that we describe in terms of a conjectural diamond of categories which suggests a number of unification points. In particular, the passage from the topological to the algebraic branch is achieved by the idea of the gestoid, an ?algebraic? category associated with the fundamental groupoid of a gesture.</abstract>
		<issn>17459737</issn>
		<doi>10.1080/17459730601137716</doi>
		<title>Diagrams, gestures and formulae in music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/209b2696374131c39d2d4da6d7d3a25b2/schmidt2</id>
		<tags>music</tags>
		<tags>fma</tags>
		<tags>archive</tags>
		<tags>free</tags>
		<tags>open_data</tags>
		<description>FMA: A Dataset For Music Analysis</description>
		<date>2017-05-13 17:46:29</date>
		<count>3</count>
		<year>2016</year>
		<url>http://arxiv.org/abs/1612.01840</url>
		<author>Michaël Defferrard</author>
		<author>Kirell Benzi</author>
		<author>Pierre Vandergheynst</author>
		<author>Xavier Bresson</author>
		<authors>
			<first>Michaël</first>
		</authors>
		<authors>
			<last>Defferrard</last>
		</authors>
		<authors>
			<first>Kirell</first>
		</authors>
		<authors>
			<last>Benzi</last>
		</authors>
		<authors>
			<first>Pierre</first>
		</authors>
		<authors>
			<last>Vandergheynst</last>
		</authors>
		<authors>
			<first>Xavier</first>
		</authors>
		<authors>
			<last>Bresson</last>
		</authors>
		<abstract>We introduce the Free Music Archive (FMA), an open and easily accessible
dataset which can be used to evaluate several tasks in music information
retrieval (MIR), a field concerned with browsing, searching, and organizing
large music collections. The community's growing interest in feature and
end-to-end learning is however restrained by the limited availability of large
audio datasets. By releasing the FMA, we hope to foster research which will
improve the state-of-the-art and hopefully surpass the performance ceiling
observed in e.g. genre recognition (MGR). The data is made of 106,574 tracks,
16,341 artists, 14,854 albums, arranged in a hierarchical taxonomy of 161
genres, for a total of 343 days of audio and 917 GiB, all under permissive
Creative Commons licenses. It features metadata like song title, album, artist
and genres; user data like play counts, favorites, and comments; free-form text
like description, biography, and tags; together with full-length, high-quality
audio, and some pre-computed features. We propose a train/validation/test split
and three subsets: a genre-balanced set of 8,000 tracks from 8 major genres, a
genre-unbalanced set of 25,000 tracks from 16 genres, and a 98 GiB version with
clips trimmed to 30s. This paper describes the dataset and how it was created,
proposes some tasks like music classification and annotation or recommendation,
and evaluates some baselines for MGR. Code, data, and usage examples are
available at https://github.com/mdeff/fma.</abstract>
		<title>FMA: A Dataset For Music Analysis</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d76be6b156cefc288164fd71f56acbfb/ar0berts</id>
		<tags>Disorders;</tags>
		<tags>Child;</tags>
		<tags>Humans;</tags>
		<tags>Developmental</tags>
		<tags>Factors;</tags>
		<tags>Adolescent;</tags>
		<tags>Cerebral</tags>
		<tags>Male;</tags>
		<tags>Children;</tags>
		<tags>Therapy;</tags>
		<tags>Disabilities;</tags>
		<tags>Time</tags>
		<tags>Communication;</tags>
		<tags>Palsy;</tags>
		<tags>Female;</tags>
		<tags>Music</tags>
		<tags>Video</tags>
		<tags>Development</tags>
		<tags>Recording</tags>
		<tags>Language</tags>
		<tags>Disabled</tags>
		<description></description>
		<date>2014-07-19 21:00:18</date>
		<count>1</count>
		<journal>J Music Ther</journal>
		<year>2003</year>
		<url></url>
		<author>Mary M Rainey Perry</author>
		<authors>
			<first>Mary M Rainey</first>
		</authors>
		<authors>
			<last>Perry</last>
		</authors>
		<volume>40</volume>
		<number>3</number>
		<pages>227--246</pages>
		<abstract>The effect of different levels of preintentional and intentional communication development on musical interaction with children with severe and multiple disabilities has not been explored in the music therapy literature. Aside from stage of communication development, what are the particular influences of disability on musical interaction with children who have preintentional and early intentional communication? A qualitative research project explored these issues. Ten school-aged children with severe and multiple disabilities participated in the project. The most common medical diagnosis was cerebral palsy. Analysis of video recordings and other data confirmed that the children's level of communication development was reflected in individual music therapy. Specifically, children at different levels of communication development varied in their abilities to initiate, anticipate, and sustain participation in turn taking, and to maintain attention to and engagement in the interaction. Both turn taking and playing and singing together were found to be important forms of communication during music therapy. Communication problems related to disability included: difficulties in using objects as a focus of joint attention, difficulties in interpreting the interactive environment, being sufficiently motivated to communicate, severely limited means of interaction, attaining and maintaining an appropriate level of arousal, and lack of interest in interaction and the outside environment. Further study of how music therapy can be related to general issues in communication for individuals with severe and multiple disabilities is recommended.</abstract>
		<title>Relating improvisational music therapy with severely and multiply disabled children to communication development.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/290fdbdb5de9404a712fc304b69b8fb85/shubhanshugupta</id>
		<tags>Social</tags>
		<tags>Theory</tags>
		<tags>Semantic</tags>
		<tags>Retrieval</tags>
		<tags>Data</tags>
		<tags>Information</tags>
		<tags>Open</tags>
		<tags>Machine</tags>
		<tags>Analysis</tags>
		<tags>Web</tags>
		<tags>Learning</tags>
		<tags>Linked</tags>
		<tags>Network</tags>
		<tags>Music</tags>
		<tags>Big</tags>
		<description>Music Data Analysis: A State-of-the-art Survey</description>
		<date>2015-01-06 21:37:37</date>
		<count>2</count>
		<year>2014</year>
		<url>http://arxiv.org/abs/1411.5014</url>
		<author>Shubhanshu Gupta</author>
		<authors>
			<first>Shubhanshu</first>
		</authors>
		<authors>
			<last>Gupta</last>
		</authors>
		<abstract>Music accounts for a significant chunk of interest among various online
activities. This is reflected by wide array of alternatives offered in music
related web/mobile apps, information portals, featuring millions of artists,
songs and events attracting user activity at similar scale. Availability of
large scale structured and unstructured data has attracted similar level of
attention by data science community. This paper attempts to offer current
state-of-the-art in music related analysis. Various approaches involving
machine learning, information theory, social network analysis, semantic web and
linked open data are represented in the form of taxonomy along with data
sources and use cases addressed by the research community.</abstract>
		<title>Music Data Analysis: A State-of-the-art Survey</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24f4a6bdf9805d207918ae7e2db2cf594/sapo</id>
		<tags>read_list</tags>
		<tags>music_demography</tags>
		<description>The relation of culture, socio-economics, and friendship to music preferences: A large-scale, cross-country study</description>
		<date>2020-05-14 09:44:38</date>
		<count>1</count>
		<journal>PLOS ONE</journal>
		<publisher>Public Library of Science</publisher>
		<year>2018</year>
		<url>https://doi.org/10.1371/journal.pone.0208186</url>
		<author>Meijun Liu</author>
		<author>Xiao Hu</author>
		<author>Markus Schedl</author>
		<authors>
			<first>Meijun</first>
		</authors>
		<authors>
			<last>Liu</last>
		</authors>
		<authors>
			<first>Xiao</first>
		</authors>
		<authors>
			<last>Hu</last>
		</authors>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<volume>13</volume>
		<number>12</number>
		<pages>1-29</pages>
		<abstract>Music listening is an inherently cultural behavior, which may be shaped by users’ backgrounds and contextual characteristics. Due to geographical, socio-economic, linguistic, and cultural factors as well as friendship networks, users in different countries may have different music preferences. Investigating cultural-socio-economic factors that might be associated with between-country differences in music preferences can facilitate music information retrieval, contribute to the prediction of users’ music preferences, and improve music recommendation in cross-country contexts. However, previous literature provides limited empirical evidence of the relationships between possible cross-country differences on a wide range of socio-economic aspects and those in music preferences. To bridge this research gap, and drawing on a large-scale dataset, LFM-1b, this study examines the possible relationship between cross-country differences in artist, album, and genre listening frequencies as well as the cross-country distance in geographical, socio-economic, linguistic, cultural, and friendship connections using the Quadratic Assignment Procedure. Results indicate: (1) there is no significant relationship between geographical and economic distance on album, artist, and genre preferences’ distance at the country-level; (2) the cross-country distance of three cultural dimensions (masculinity, long-term orientation, and indulgence) is positively associated with both the album and artist preferences distances; (3) the between-country distance in main languages has a positive relationship with the album, artist, and genre preferences distances across countries; (4) the density of friendship connections among countries negatively correlates to the cross-country preference distances in terms of artist and genre. Findings from this study not only expand knowledge of factors related to music preferences at the country level, but also can be integrated into real-world music recommendation systems that consider country-level music preferences.</abstract>
		<doi>10.1371/journal.pone.0208186</doi>
		<title>The relation of culture, socio-economics, and friendship to music preferences: A large-scale, cross-country study</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2623370b0802b1cda8ba0d2d87f078183/lbalby</id>
		<tags>myown</tags>
		<tags>music</tags>
		<tags>novelty</tags>
		<tags>preferences</tags>
		<description></description>
		<date>2013-09-13 20:03:54</date>
		<count>1</count>
		<booktitle>Proceedings of the 14th International Society for Music Information Retrieval Conference</booktitle>
		<year>2013</year>
		<url>http://ismir2013.ismir.net/wp-content/uploads/2013/09/257_Paper.pdf</url>
		<author>Andryw Marques</author>
		<author>Nazareno Andrade</author>
		<author>Leandro Balby</author>
		<authors>
			<first>Andryw</first>
		</authors>
		<authors>
			<last>Marques</last>
		</authors>
		<authors>
			<first>Nazareno</first>
		</authors>
		<authors>
			<last>Andrade</last>
		</authors>
		<authors>
			<first>Leandro</first>
		</authors>
		<authors>
			<last>Balby</last>
		</authors>
		<pages>407-412</pages>
		<abstract>The discovery of new music, e.g. song tracks and artists, is a central aspect of music consumption. In order to assist users in this task, several mechanisms have been proposed to incorporate novelty awareness into music recommender systems. In this paper, we complement these efforts by investigating how the music preferences of users are affected by two different aspects of novel artists, namely familiarity and mainstreamness. We collected historical data from Last.fm users, a popular online music discovery service, to investigate how these aspects of novel artists relate to the preferences of music listeners for novel artists. The results of this analysis suggests that the users tend to cluster according to their novelty related preferences. We then
conducted a comprehensive study on these groups, from where we derive implications and useful insights for developers of music retrieval services.</abstract>
		<title>Exploring the Relation Between Novelty Aspects And Preferences in Music Listening</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/279c96ccb37989ed94c062552ea228ad1/weibel</id>
		<tags>surfaces</tags>
		<tags>multi-sensory</tags>
		<tags>stimulation,</tags>
		<tags>musical</tags>
		<tags>interactive</tags>
		<tags>children,</tags>
		<description></description>
		<date>2019-02-18 07:11:07</date>
		<count>2</count>
		<booktitle>Proceedings of the 10th EAI International Conference on Pervasive Computing Technologies for Healthcare</booktitle>
		<series>PervasiveHealth '16</series>
		<publisher>ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)</publisher>
		<address>ICST, Brussels, Belgium, Belgium</address>
		<year>2016</year>
		<url>http://dl.acm.org/citation.cfm?id=3021319.3021357</url>
		<author>Franceli L. Cibrian</author>
		<author>Monica Tentori</author>
		<author>Nadir Weibel</author>
		<authors>
			<first>Franceli L.</first>
		</authors>
		<authors>
			<last>Cibrian</last>
		</authors>
		<authors>
			<first>Monica</first>
		</authors>
		<authors>
			<last>Tentori</last>
		</authors>
		<authors>
			<first>Nadir</first>
		</authors>
		<authors>
			<last>Weibel</last>
		</authors>
		<pages>241--244</pages>
		<abstract>Interactive surfaces are promising to support multi-sensory stimulation during early development of children and for therapy for children with autism, as they offer a casual and engaging experience in a multi-sensory environment. In this paper, we present preliminary results of two field deployment studies of the use of BendableSound, a fabric-based interactive surfaces that allows children to play music. The first study was conducted at a school-clinic specialized in the care of children with autism in north-western Mexico, and the second study was conducted at an early education center in San Diego, California. Our preliminary results indicate that BendableSound is successful when used in both contexts to support specific development areas. We discuss implications of the introduction of BendableSound and we illustrate future work.</abstract>
		<isbn>978-1-63190-051-8</isbn>
		<title>A Musical Interactive Surface to Support the Multi-sensory Stimulation of Children</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20c9e1e92ee324946445b62e8be5190ea/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>Proceedings of the International Workshop on Artificial Intelligence and Music (MUSIC-AI 2007), Hyderabad, India, 6-12 January 2007</booktitle>
		<series>Proceedings of the International Workshop on Artificial Intelligence and Music</series>
		<year>2007</year>
		<url>http://www.ofai.at/{~}soren.madsen/pub/music-ai07.pdf</url>
		<author>S. T. Madsen</author>
		<author>G. Widmer</author>
		<authors>
			<first>S. T.</first>
		</authors>
		<authors>
			<last>Madsen</last>
		</authors>
		<authors>
			<first>G.</first>
		</authors>
		<authors>
			<last>Widmer</last>
		</authors>
		<number>January</number>
		<pages>6--12</pages>
		<abstract>n this paper, we will examine the importance of music com- plexity
	as a factor for melody recognition in multi-voiced popular music.
	The assumption is that the melody (or lead instrument) will contain
	the largest amount of information -- that it will be the least redundant
	voice. Measures of melodic complexity calculated from pitch and timing
	information are proposed. We test the different complexity measures
	and different prediction strategies, and evaluate them on the task
	of predict- ing which track of a MIDI file contains the main melody.
	Filtering out melody tracks can be useful when searching large databases
	for simi- lar songs. 108 melody track annotated pop songs were included
	in the experiment.</abstract>
		<title>A Complexity-based Approach to Melody Track Identification in Midi Files</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b4671d8a43b273da33c3986164683dc1/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>Proceedings of the 1st ACM workshop on Audio and music computing multimedia - AMCMM '06</booktitle>
		<series>Proceedings of the 1st ACM workshop on Audio and music computing multimedia</series>
		<publisher>ACM</publisher>
		<year>2006</year>
		<url>http://portal.acm.org/citation.cfm?doid=1178723.1178727</url>
		<author>Christopher Harte</author>
		<author>Mark Sandler</author>
		<author>Martin Gasser</author>
		<authors>
			<first>Christopher</first>
		</authors>
		<authors>
			<last>Harte</last>
		</authors>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Sandler</last>
		</authors>
		<authors>
			<first>Martin</first>
		</authors>
		<authors>
			<last>Gasser</last>
		</authors>
		<pages>21</pages>
		<abstract>We propose a novel method for detecting changes in the harmonic content
	of musical audio signals. Our method uses a new model for Equal Tempered
	Pitch Class Space. This model maps 12-bin chroma vectors to the interior
	space of a 6-D polytope; pitch classes are mapped onto the vertices
	of this polytope. Close harmonic relations such as fifths and thirds
	appear as small Euclidian distances. We calculate the Euclidian distance
	between analysis frames n +1 and n 1 to develop a harmonic change
	measure for frame n. A peak in the detection function denotes a transi-
	tion from one harmonically stable region to another. Initial experiments
	show that the algorithm can successfully detect harmonic changes
	such as chord boundaries in polyphonic audio recordings.</abstract>
		<doi>10.1145/1178723.1178727</doi>
		<title>Detecting harmonic change in musical audio</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20367b6b77fcfda32a45390b3207b8974/jpooley</id>
		<tags>postcolonial</tags>
		<tags>music</tags>
		<tags>popular-culture</tags>
		<tags>said</tags>
		<description></description>
		<date>2019-08-29 01:56:31</date>
		<count>1</count>
		<journal>Popular Music and Society</journal>
		<year>2017</year>
		<url></url>
		<author>Wouter Capitain</author>
		<authors>
			<first>Wouter</first>
		</authors>
		<authors>
			<last>Capitain</last>
		</authors>
		<volume>40</volume>
		<number>1</number>
		<pages>49--60</pages>
		<abstract>AbstractAlthough Edward Said, generally known as one of the founders of postcolonial studies, has written extensively on music, he almost completely ignores popular music. However, the few moments in which he does reflect on popular music are highly revealing. In this article I provide a comprehensive overview and a critical analysis of Said's public statements on popular music, and argue that these strongly create dissonance with his interventions in postcolonial theory and politics. More specifically, I argue that in these reflections on popular music Said voices problematic elitist, orientalist, and universalist claims. Consequently, Said's notion of popular music constitutes perhaps the most antagonistic aspect of his oeuvre.</abstract>
		<doi>10.1080/03007766.2016.1228097</doi>
		<title>Edward Said on Popular Music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23905286204e8ab427456876655e6a14b/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#TsaiCZ12</url>
		<author>Chang-Lung Tsai</author>
		<author>Chun-Jung Chen</author>
		<author>Deng-Jie Zhuang</author>
		<authors>
			<first>Chang-Lung</first>
		</authors>
		<authors>
			<last>Tsai</last>
		</authors>
		<authors>
			<first>Chun-Jung</first>
		</authors>
		<authors>
			<last>Chen</last>
		</authors>
		<authors>
			<first>Deng-Jie</first>
		</authors>
		<authors>
			<last>Zhuang</last>
		</authors>
		<pages>138-141</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Secure OTP and Biometric Verification Scheme for Mobile Banking.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ab6c15431793e58da7040ba549864bb4/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#GuoLTL12</url>
		<author>Zhengbiao Guo</author>
		<author>Zhitang Li</author>
		<author>Hao Tu</author>
		<author>Long Li</author>
		<authors>
			<first>Zhengbiao</first>
		</authors>
		<authors>
			<last>Guo</last>
		</authors>
		<authors>
			<first>Zhitang</first>
		</authors>
		<authors>
			<last>Li</last>
		</authors>
		<authors>
			<first>Hao</first>
		</authors>
		<authors>
			<last>Tu</last>
		</authors>
		<authors>
			<first>Long</first>
		</authors>
		<authors>
			<last>Li</last>
		</authors>
		<pages>60-65</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Characterizing User Behavior in Weibo.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2926aad7c40d0c9c9e3b88c39d35c7851/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#ChoiK12</url>
		<author>Ji-Woong Choi</author>
		<author>Myung Ho Kim</author>
		<authors>
			<first>Ji-Woong</first>
		</authors>
		<authors>
			<last>Choi</last>
		</authors>
		<authors>
			<first>Myung Ho</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<pages>53-59</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Generating OWL Ontology from Relational Database.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2383438fff02ea858c6a0487a30f50f1b/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#WerthEC12</url>
		<author>Dirk Werth</author>
		<author>Andreas Emrich</author>
		<author>Alexandra Chapko</author>
		<authors>
			<first>Dirk</first>
		</authors>
		<authors>
			<last>Werth</last>
		</authors>
		<authors>
			<first>Andreas</first>
		</authors>
		<authors>
			<last>Emrich</last>
		</authors>
		<authors>
			<first>Alexandra</first>
		</authors>
		<authors>
			<last>Chapko</last>
		</authors>
		<pages>142-147</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>An Architecture Proposal for User-Generated Mobile Services.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d9013930566181d16672aa847f27366e/ijtsrd</id>
		<tags>music</tags>
		<tags>traditions</tags>
		<tags>culture</tags>
		<tags>songs</tags>
		<tags>and</tags>
		<tags>mentality</tags>
		<tags>national</tags>
		<tags>customs</tags>
		<tags>ideology</tags>
		<tags>moral</tags>
		<description></description>
		<date>2021-05-06 12:16:20</date>
		<count>1</count>
		<journal>International Journal of Trend in Scientific Research and Development</journal>
		<year>2021</year>
		<url>https://www.ijtsrd.com/humanities-and-the-arts/music/38724/form-mind-and-thinking-through-music/gulnoza-siddikova</url>
		<author>Gulnoza Siddikova</author>
		<authors>
			<first>Gulnoza</first>
		</authors>
		<authors>
			<last>Siddikova</last>
		</authors>
		<volume>5</volume>
		<number>2</number>
		<pages>76-77</pages>
		<abstract>In this article the formation of national consciousness and thinking in the hearts of every young person growing up is the goal of today the national ideology a system of ideas and views justifying the existence and development of a particular nation as an ethno social unit national values people There is information about the role of folk songs folk festivals as well as folk songs sayings and yallas. Gulnoza Siddikova "Form Mind and Thinking through Music" Published in International Journal of Trend in Scientific Research and Development (ijtsrd) ISSN: 2456-6470 Special Issue | International Research Development and Scientific Excellence in Academic Life  March 2021 URL: https://www.ijtsrd.com/papers/ijtsrd38724.pdf Paper Url: https://www.ijtsrd.com/humanities-and-the-arts/music/38724/form-mind-and-thinking-through-music/gulnoza-siddikova</abstract>
		<language>English</language>
		<issn>2456-6470</issn>
		<title>Form Mind and Thinking through Music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f538b27fec294d31233d44898e13faeb/svrist</id>
		<tags>music</tags>
		<tags>handwritten</tags>
		<tags>character</tags>
		<tags>recognition;</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>2</count>
		<journal>Pattern Recognition</journal>
		<year>1988</year>
		<url></url>
		<author>J. W. Roach</author>
		<author>J. E. Tatem</author>
		<authors>
			<first>J. W.</first>
		</authors>
		<authors>
			<last>Roach</last>
		</authors>
		<authors>
			<first>J. E.</first>
		</authors>
		<authors>
			<last>Tatem</last>
		</authors>
		<volume>21</volume>
		<number>1</number>
		<pages>33-44</pages>
		<abstract>Turning handwritten scores into engraved scores consumes a significant portion of music publishing companies' budgets. Pattern recognition is the major bottleneck holding up automation of this process. Human beings who know music can easily read a handwritten score, but without musical knowledge, even people cannot correctly perceive the markings in a handwritten score. This paper reports an experiment in which knowledge of music, a highly structured domain is applied to extract primitive musical features. This experiment shows that if the domain of image processing is well defined, significant improvements in low-level segmentations can be achieved (17 Refs.) recognition; computerised picture processing; expert systems; music</abstract>
		<title>Using domain knowledge in low-level visual processing to interpret handwritten music: an experiment</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25c474cb65a93bcb89675e855dcc711ad/boehr</id>
		<tags>semantic</tags>
		<tags>music</tags>
		<tags>web2.0</tags>
		<tags>recommendation</tags>
		<description>This paper introduces various ways to suggest music-related
content on the Web thanks to Semantic Web technologies.</description>
		<date>2009-01-25 11:44:21</date>
		<count>4</count>
		<year>2008</year>
		<url>http://ceur-ws.org/Vol-405/paper3.pdf</url>
		<author>Alexandre Passant</author>
		<author>Yves Raimond</author>
		<authors>
			<first>Alexandre</first>
		</authors>
		<authors>
			<last>Passant</last>
		</authors>
		<authors>
			<first>Yves</first>
		</authors>
		<authors>
			<last>Raimond</last>
		</authors>
		<title>Combining Social Music and Semantic Web for Music-related Recommender Systems</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23b741261fa8754b53692da344ab7ece5/snarc</id>
		<tags>imported</tags>
		<description>The meaning of music in the lives of older people: a qualitative study -- Hays and Minichiello 33 (4): 437 -- Psychology of Music</description>
		<date>2009-02-23 16:43:09</date>
		<count>2</count>
		<journal>Psychology of Music</journal>
		<year>2005</year>
		<url>http://pom.sagepub.com/cgi/content/abstract/33/4/437</url>
		<author>Terrence Hays</author>
		<author>Victor Minichiello</author>
		<authors>
			<first>Terrence</first>
		</authors>
		<authors>
			<last>Hays</last>
		</authors>
		<authors>
			<first>Victor</first>
		</authors>
		<authors>
			<last>Minichiello</last>
		</authors>
		<volume>33</volume>
		<number>4</number>
		<pages>437-451</pages>
		<abstract>This qualitative study describes the experience of music and focuses on the emotional, social, intellectual and spiritual well-being roles that music plays in   the lives of older people. In-depth interviews were used to explore the meaning, importance and function of music for 52 older Australians living in the community   aged 60 years and older. The findings revealed that music provides people with ways of understanding and developing their self-identity; connecting with others;  maintaining well-being; and experiencing and expressing spirituality. The results show how music contributes to positive ageing by providing ways for people to maintain positive self-esteem, feel competent, independent, and avoid feelings of isolation or loneliness. The study highlights the need to be better informed about how music can facilitate and sustain older people's well-being.</abstract>
		<doi>10.1177/0305735605056160</doi>
		<eprint>http://pom.sagepub.com/cgi/reprint/33/4/437.pdf</eprint>
		<title>The meaning of music in the lives of older people: a qualitative study</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2375be6730f161365fe05d3d7e0c49a85/stefano</id>
		<tags>music</tags>
		<tags>ontology</tags>
		<description></description>
		<date>2007-06-18 05:29:03</date>
		<count>7</count>
		<booktitle>ISMIR 2007: 8th International Conference on Music Information Retrieval</booktitle>
		<address>Vienna, Austria</address>
		<year>2007</year>
		<url>http://moustaki.org/pubs/Raimond-ISMIR2007-Submitted.pdf</url>
		<author>Yves Raimond</author>
		<author>Samer Abdallah</author>
		<author>Mark Sandler</author>
		<author>Frederick Giasson</author>
		<authors>
			<first>Yves</first>
		</authors>
		<authors>
			<last>Raimond</last>
		</authors>
		<authors>
			<first>Samer</first>
		</authors>
		<authors>
			<last>Abdallah</last>
		</authors>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Sandler</last>
		</authors>
		<authors>
			<first>Frederick</first>
		</authors>
		<authors>
			<last>Giasson</last>
		</authors>
		<abstract>In this paper, we overview some Semantic Web technologies
allowing to create a web of data. We then detail the
Music Ontology: a formal framework for dealing with
music-related information on the Semantic Web, including
editorial, cultural and acoustic information. We detail
how this ontology can act as a grounding for more
domain-specific knowledge representation. In addition,
we describe current projects involving the Music Ontology
and interlinked repositories of music-related knowledge.</abstract>
		<title>The Music Ontology</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bc2e6e073de62ff922aa43f833ebbdea/yevb0</id>
		<tags>acquisition,music,musicality,perception</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Psychology of Music</journal>
		<year>1992</year>
		<url>http://pom.sagepub.com/cgi/content/abstract/20/1/29</url>
		<author>B. A. Morrongiello</author>
		<authors>
			<first>B. A.</first>
		</authors>
		<authors>
			<last>Morrongiello</last>
		</authors>
		<volume>20</volume>
		<number>1</number>
		<pages>29--41</pages>
		<abstract>In this article research examining the effects of formal training
	on children's perception of music is reviewed. Consideration is given
	both to perception of rhythmic and tonal properties of melodies.
	It is concluded that training in children serves to enhance perception
	of more detailed aspects of melodies, to facilitate the speed at
	which this information is encoded, and to result in better memory
	for this material. Moreover, it seems to affect not only perception
	of rhythmic details but also organisational strategies, and to accelerate
	the emergence of sensitivity to diatonic scale structure and the
	ease of application of this knowledge to music perception tasks.
	Future research might be aimed at determining if these auditory processing
	effects are specific to musical sequences or generalise to the perception
	of other patterned stimuli such as speech. Empirical efforts aimed
	at increasing our understanding of the mechanisms mediating the effects
	of training are also sorely needed.</abstract>
		<issn>0305-7356</issn>
		<title>Effects of training on children's perception of music: A review</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ee499db2b69f5c73e80de923557e4905/yevb0</id>
		<tags>music,gamelan,music,perception,tonality</tags>
		<tags>M1,Western</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Music Perception</journal>
		<year>1984</year>
		<url>http://www.jstor.org/stable/40285289</url>
		<author>E J Kessler</author>
		<author>Christa Hansen</author>
		<author>Roger N Shepard</author>
		<authors>
			<first>E J</first>
		</authors>
		<authors>
			<last>Kessler</last>
		</authors>
		<authors>
			<first>Christa</first>
		</authors>
		<authors>
			<last>Hansen</last>
		</authors>
		<authors>
			<first>Roger N</first>
		</authors>
		<authors>
			<last>Shepard</last>
		</authors>
		<volume>2</volume>
		<number>2</number>
		<pages>131-- 165</pages>
		<title>Tonal schemata in the perception of music in Bali and in the West</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20c7c5f70aedd46285894605007e1a07b/bfields</id>
		<tags>feature_extraction</tags>
		<tags>music_similarity</tags>
		<description>initial imports</description>
		<date>2010-01-28 11:42:45</date>
		<count>4</count>
		<booktitle>Proc. of Int. Symposium on Music Information Retrieval</booktitle>
		<year>2006</year>
		<url></url>
		<author>K. Jacobson</author>
		<authors>
			<first>K.</first>
		</authors>
		<authors>
			<last>Jacobson</last>
		</authors>
		<title>A Multifaceted Approach to Music Similarity</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24268980cfcb3fdfdddf0f4d470c61070/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>College music symposium. Vol. 37</journal>
		<year>1997</year>
		<url></url>
		<author>John Wallace author White</author>
		<authors>
			<first>John Wallace</first>
		</authors>
		<authors>
			<last>author White</last>
		</authors>
		<volume>37</volume>
		<pages>1--12</pages>
		<abstract>Cover versions of past hit songs have long been a staple of commercial popular music. In covers, the style of the original is usually transformed to reflect current radio formats and audience tastes. Stylistic repackaging mainly involves musical parameters such as dynamics, sound processing, instrumentation, and timbre. Dolly Parton's 1984 remake of Downtown is compared with Petula Clark's original 1965 hit recording, which was written and produced by Tony Hatch.</abstract>
		<issn>0069-5696</issn>
		<shorttitle>Radio formats and the transformation of musical style</shorttitle>
		<title>Radio formats and the transformation of musical style: Codes and cultural values in the remaking of tunes</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f98f3b44632975319129cb3ffffb4db4/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<publisher>General Music Publishing Co.</publisher>
		<address>New York, NY</address>
		<year>2000</year>
		<url></url>
		<author>Walter ed Everett</author>
		<authors>
			<first>Walter</first>
		</authors>
		<authors>
			<last>ed Everett</last>
		</authors>
		<abstract>A collection of essays presents a wide range of scholarly approaches to understanding artistic expression in rock music, with some taking a primarily analytical approach to the craft of composition, whereas others are interested in general stylistic concerns, and others still are focused on the listener's role in the process. The following contributions are cited separately in RILM: Jonathan W. BERNARD, "The musical world(s?) of Frank Zappa: Some observations of his "crossover" pieces" (2000-33815); James BORDERS, "Frank Zappa's "The black page"--A case of musical "conceptual continuity"" (2000-33814); Lori BURNS, "Harmonic and voice-leading strategies in Tori Amos's "Crucify"" (2000-33816); John COVACH, "Jazz-rock? Rock-jazz? Stylistic crossover in late-1970s American progressive rock" (2000-33813); Walter EVERETT, "Confessions from Blueberry Hell, or, pitch can be a sticky substance" (2000-33818); Susan FAST, "Music, contexts, and meaning in U2" (2000-33810); Ellie M. HISAMA, "From "L'Étranger" to "Killing an Arab": Representing the other in a Cure song" (2000-33811); Nadine HUBBS, "The imagination of pop-rock criticism" (2000-33809); Timothy KOOZIN, "Fumbling towards ectasy: Voice-leading, tonal structure, and the theme of self-realization in the music of Sarah McLachlan" (2000-33817); Mark S. SPICER, "Large-scale strategy and compositional design in the early music of Genesis" (2000-33812).</abstract>
		<isbn>0815331606</isbn>
		<shorttitle>Expression in pop-rock music</shorttitle>
		<title>Expression in pop-rock music: A collection of critical and analytical essays</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f8a5a6603c92c137cb8cf1327ccd3287/joaakive</id>
		<tags>ethnomusicology</tags>
		<tags>music</tags>
		<tags>theory</tags>
		<description></description>
		<date>2014-04-27 09:15:19</date>
		<count>1</count>
		<journal>South Indian Music</journal>
		<year>1963</year>
		<url></url>
		<author>P. Sambamurthy</author>
		<authors>
			<first>P.</first>
		</authors>
		<authors>
			<last>Sambamurthy</last>
		</authors>
		<volume>Book V</volume>
		<abstract>This commentary explaining the classical just intonation tuning system
is copied from the chapter ‘Early Experiments in Music’ of the great
anthology ‘South Indian Music’ Book V by musicologist
Prof. P. Sambamurthy, first published in 1963, and in May 1999 in the Seventh Edition by The Indian Music Publishing House, Royapettah, Chennai</abstract>
		<title>Early Experiments in Music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2929ff10d1360f6396c5108354a33707a/schwemmlein</id>
		<tags>music</tags>
		<tags>recommendation</tags>
		<tags>context-aware</tags>
		<tags>lda</tags>
		<description>Context-aware music recommendation based on latenttopic sequential patterns</description>
		<date>2012-10-09 11:01:11</date>
		<count>6</count>
		<booktitle>Proceedings of the sixth ACM conference on Recommender systems</booktitle>
		<series>RecSys '12</series>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2012</year>
		<url>http://doi.acm.org/10.1145/2365952.2365979</url>
		<author>Negar Hariri</author>
		<author>Bamshad Mobasher</author>
		<author>Robin Burke</author>
		<authors>
			<first>Negar</first>
		</authors>
		<authors>
			<last>Hariri</last>
		</authors>
		<authors>
			<first>Bamshad</first>
		</authors>
		<authors>
			<last>Mobasher</last>
		</authors>
		<authors>
			<first>Robin</first>
		</authors>
		<authors>
			<last>Burke</last>
		</authors>
		<pages>131--138</pages>
		<abstract>Contextual factors can greatly influence the users' preferences in listening to music. Although it is hard to capture these factors directly, it is possible to see their effects on the sequence of songs liked by the user in his/her current interaction with the system. In this paper, we present a context-aware music recommender system which infers contextual information based on the most recent sequence of songs liked by the user. Our approach mines the top frequent tags for songs from social tagging Web sites and uses topic modeling to determine a set of latent topics for each song, representing different contexts. Using a database of human-compiled playlists, each playlist is mapped into a sequence of topics and frequent sequential patterns are discovered among these topics. These patterns represent frequent sequences of transitions between the latent topics representing contexts. Given a sequence of songs in a user's current interaction, the discovered patterns are used to predict the next topic in the playlist. The predicted topics are then used to post-filter the initial ranking produced by a traditional recommendation algorithm. Our experimental evaluation suggests that our system can help produce better recommendations in comparison to a conventional recommender system based on collaborative or content-based filtering. Furthermore, the topic modeling approach proposed here is also useful in providing better insight into the underlying reasons for song selection and in applications such as playlist construction and context prediction.</abstract>
		<isbn>978-1-4503-1270-7</isbn>
		<doi>10.1145/2365952.2365979</doi>
		<title>Context-aware music recommendation based on latenttopic sequential patterns</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fa1bf1e293d9c266710659f73d2e3c19/ks-plugin-devel</id>
		<tags>Intervall</tags>
		<tags>Musiktheorie</tags>
		<tags>MaMu</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2013-02-02 14:42:13</date>
		<count>2</count>
		<journal>Journal of Music Theory</journal>
		<year>2009</year>
		<url>http://jmt.dukejournals.org/cgi/content/abstract/53/2/227</url>
		<author>Dmitri Tymoczko</author>
		<authors>
			<first>Dmitri</first>
		</authors>
		<authors>
			<last>Tymoczko</last>
		</authors>
		<volume>53</volume>
		<number>2</number>
		<pages>227-254</pages>
		<abstract>Taking David Lewin's work as a point of departure, this essay uses geometry to reexamine familiar music-theoretical assumptions about intervals and transformations. Section 1 introduces the problem of "transportability," noting that it is sometimes impossible to say whether two different directions--located at two different points in a geometrical space--are "the same" or not. Relevant examples include the surface of the earth and the geometrical spaces representing n-note chords. Section 2 argues that we should not require that every interval be defined at every point in a space, since some musical spaces have natural boundaries. It also notes that there are spaces, including the familiar pitch-class circle, in which there are multiple paths between any two points. This leads to the suggestion that we might sometimes want to replace traditional pitch-class intervals with paths in pitch-class space, a more fine-grained alternative that specifies how one pitch class moves to another. Section 3 argues that group theory alone cannot represent the intuition that intervals have quantifiable sizes, proposing an extension to Lewin's formalism that accomplishes this goal. Finally, Section 4 considers the analytical implications of the preceding points, paying particular attention to questions about voice leading.</abstract>
		<doi>10.1215/00222909-2010-003</doi>
		<eprint>http://jmt.dukejournals.org/cgi/reprint/53/2/227.pdf</eprint>
		<title>Generalizing Musical Intervals</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c247da308e74eedf4c7cad4a8b338202/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-11-29 00:00:00</date>
		<count>3</count>
		<booktitle>Ubiquitous Music</booktitle>
		<series>Computational Music Science</series>
		<publisher>Springer</publisher>
		<year>2014</year>
		<url>http://dblp.uni-trier.de/db/series/music/um2014.html#ZawackiJ14</url>
		<author>Lucas Fialho Zawacki</author>
		<author>Marcelo de Oliveira Johann</author>
		<authors>
			<first>Lucas Fialho</first>
		</authors>
		<authors>
			<last>Zawacki</last>
		</authors>
		<authors>
			<first>Marcelo</first>
		</authors>
		<authors>
			<last>de Oliveira Johann</last>
		</authors>
		<editor>Damián Keller</editor>
		<editor>Victor Lazzarini</editor>
		<editor>Marcelo Soares Pimenta</editor>
		<editors>
			<first>Marcelo</first>
		</editors>
		<editors>
			<last>de Oliveira Johann</last>
		</editors>
		<editors>
			<first>Marcelo</first>
		</editors>
		<editors>
			<last>de Oliveira Johann</last>
		</editors>
		<editors>
			<first>Marcelo</first>
		</editors>
		<editors>
			<last>de Oliveira Johann</last>
		</editors>
		<pages>83-107</pages>
		<isbn>978-3-319-11151-3</isbn>
		<title>Analogue Audio Recording Using Remote Servers.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ee05cb02beb956f32efd73f60a96b27f/asmelash</id>
		<tags>music</tags>
		<tags>timeseries</tags>
		<tags>evolution</tags>
		<description>[1502.05417] The Evolution of Popular Music: USA 1960-2010</description>
		<date>2015-05-08 18:41:35</date>
		<count>3</count>
		<year>2015</year>
		<url>http://arxiv.org/abs/1502.05417</url>
		<author>Matthias Mauch</author>
		<author>Robert M. MacCallum</author>
		<author>Mark Levy</author>
		<author>Armand M. Leroi</author>
		<authors>
			<first>Matthias</first>
		</authors>
		<authors>
			<last>Mauch</last>
		</authors>
		<authors>
			<first>Robert M.</first>
		</authors>
		<authors>
			<last>MacCallum</last>
		</authors>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Levy</last>
		</authors>
		<authors>
			<first>Armand M.</first>
		</authors>
		<authors>
			<last>Leroi</last>
		</authors>
		<abstract>In modern societies, cultural change seems ceaseless. The flux of fashion is
especially obvious for popular music. While much has been written about the
origin and evolution of pop, most claims about its history are anecdotal rather
than scientific in nature. To rectify this we investigate the US Billboard Hot
100 between 1960 and 2010. Using Music Information Retrieval (MIR) and
text-mining tools we analyse the musical properties of ~17,000 recordings that
appeared in the charts and demonstrate quantitative trends in their harmonic
and timbral properties. We then use these properties to produce an audio-based
classification of musical styles and study the evolution of musical diversity
and disparity, testing, and rejecting, several classical theories of cultural
change. Finally, we investigate whether pop musical evolution has been gradual
or punctuated. We show that, although pop music has evolved continuously, it
did so with particular rapidity during three stylistic "revolutions" around
1964, 1983 and 1991. We conclude by discussing how our study points the way to
a quantitative science of cultural change.</abstract>
		<title>The Evolution of Popular Music: USA 1960-2010</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/299a59408f7e63cdf64856ee6c67808b1/asmelash</id>
		<tags>music</tags>
		<tags>timeseries</tags>
		<tags>changepoint</tags>
		<description>The evolution of popular music: USA 1960–2010 | Open Science</description>
		<date>2016-02-09 13:58:18</date>
		<count>2</count>
		<journal>Royal Society Open Science</journal>
		<publisher>The Royal Society</publisher>
		<year>2015</year>
		<url>http://rsos.royalsocietypublishing.org/content/2/5/150081</url>
		<author>Matthias Mauch</author>
		<author>Robert M. MacCallum</author>
		<author>Mark Levy</author>
		<author>Armand M. Leroi</author>
		<authors>
			<first>Matthias</first>
		</authors>
		<authors>
			<last>Mauch</last>
		</authors>
		<authors>
			<first>Robert M.</first>
		</authors>
		<authors>
			<last>MacCallum</last>
		</authors>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Levy</last>
		</authors>
		<authors>
			<first>Armand M.</first>
		</authors>
		<authors>
			<last>Leroi</last>
		</authors>
		<volume>2</volume>
		<number>5</number>
		<abstract>In modern societies, cultural change seems ceaseless. The flux of fashion is especially obvious for popular music. While much has been written about the origin and evolution of pop, most claims about its history are anecdotal rather than scientific in nature. To rectify this, we investigate the US Billboard Hot 100 between 1960 and 2010. Using music information retrieval and text-mining tools, we analyse the musical properties of approximately~17 000 recordings that appeared in the charts and demonstrate quantitative trends in their harmonic and timbral properties. We then use these properties to produce an audio-based classification of musical styles and study the evolution of musical diversity and disparity, testing, and rejecting, several classical theories of cultural change. Finally, we investigate whether pop musical evolution has been gradual or punctuated. We show that, although pop music has evolved continuously, it did so with particular rapidity during three stylistic ‘revolutions’ around 1964, 1983 and 1991. We conclude by discussing how our study points the way to a quantitative science of cultural change.</abstract>
		<eprint>http://rsos.royalsocietypublishing.org/content/2/5/150081.full.pdf</eprint>
		<doi>10.1098/rsos.150081</doi>
		<title>The evolution of popular music: USA 1960–2010</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2cbc1af51a9f8adf832c5f908a9fcdf6b/sapo</id>
		<tags>source_separation</tags>
		<description>ISMIR 2020: Score-informed Source Separation of Choral Music</description>
		<date>2020-11-12 18:51:39</date>
		<count>1</count>
		<booktitle>Proceedings of the 21th International Society for Music Information Retrieval Conference</booktitle>
		<year>2020</year>
		<url>https://program.ismir2020.net/poster_2-09.html</url>
		<author>Matan Gover</author>
		<author>Philippe Depalle</author>
		<authors>
			<first>Matan</first>
		</authors>
		<authors>
			<last>Gover</last>
		</authors>
		<authors>
			<first>Philippe</first>
		</authors>
		<authors>
			<last>Depalle</last>
		</authors>
		<abstract>Choral music recordings are a particularly challenging target for source separation due to the choral blend and the inherent acoustical complexity of the ‘choral timbre’. Due to the scarcity of publicly available multi-track choir recordings, we create a dataset of synthesized Bach chorales. We apply data augmentation to alter the chorales so that they more faithfully represent music from a broader range of choral genres. For separation we employ Wave-U-Net, a time-domain convolutional neural network (CNN) originally proposed for vocals and accompaniment separation. We show that Wave-U-Net outperforms a baseline implemented using score-informed NMF (non-negative matrix factorization). We introduce score-informed Wave-U-Net to incorporate the musical score into the separation process. We experiment with different score conditioning methods and show that conditioning on the score leads to improved separation results. We propose a ‘score-guided’ model variant in which separation is guided by the score alone, bypassing the need to specify the identity of the extracted source. Finally, we evaluate our models (trained on synthetic data only) on real choir recordings and find that in the absence of a large training set of real recordings, NMF still performs better than Wave-U-Net in this setting. To our knowledge, this paper is the first to study source separation of choral music.</abstract>
		<title>Score-informed Source Separation of Choral Music</title>
		<pubtype>conference</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ce73ce7ac45f07fb849a9acbcb3ddd54/ijtsrd</id>
		<tags>Instruments</tags>
		<tags>Assam</tags>
		<tags>Music</tags>
		<tags>Tribes</tags>
		<tags>Culture</tags>
		<description></description>
		<date>2020-11-26 07:47:30</date>
		<count>1</count>
		<journal>International Journal of Trend in Scientific Research and Development</journal>
		<year>2020</year>
		<url>https://www.ijtsrd.com/humanities-and-the-arts/music/33351/paragon-of-musical-fiesta-in-assam/richa-gogoi</url>
		<author>Richa Gogoi | Jyoti Senapati</author>
		<authors>
			<first>Richa Gogoi | Jyoti</first>
		</authors>
		<authors>
			<last>Senapati</last>
		</authors>
		<volume>4</volume>
		<number>6</number>
		<pages>287-291</pages>
		<abstract>Assam is a land where great many tribes commingle. Traces of several ethnic groups like Negrito, Austric, Alpine, Aryan, Dravidian and Vedic Aryans are residing in Assam. Few tribes of Assam are Ahom, Karbi, Boro, Tiwa, Dimasa, Deori, Mising and so on. These tribes have a rich heritage associated with with music, art and culture. However music and instruments are the nucleus of entertainment for these tribes. An air of melancholy surrounds the valleys of Assam as proper study of the tribal music and instruments haven’t been laid hold of. This paper endeavours the musical interest of the many tribes found in Assam along with their musical instruments. Richa Gogoi | Jyoti Senapati "Paragon of Musical Fiesta in Assam" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-4 | Issue-6 , October 2020, URL: https://www.ijtsrd.com/papers/ijtsrd33351.pdf Paper Url: https://www.ijtsrd.com/humanities-and-the-arts/music/33351/paragon-of-musical-fiesta-in-assam/richa-gogoi</abstract>
		<language>English</language>
		<issn>2456-6470</issn>
		<title>Paragon of Musical Fiesta in Assam</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2896c7cb469d6bf260e9282bd855294c5/loretoparisi</id>
		<tags>emotion</tags>
		<tags>music</tags>
		<tags>deep</tags>
		<tags>audio</tags>
		<tags>recognition</tags>
		<tags>learning</tags>
		<tags>lyrics</tags>
		<description>Exploiting Synchronized Lyrics And Vocal Features For Music Emotion Detection</description>
		<date>2019-01-16 17:43:52</date>
		<count>2</count>
		<year>2019</year>
		<url>http://arxiv.org/abs/1901.04831</url>
		<author>Loreto Parisi</author>
		<author>Simone Francia</author>
		<author>Silvio Olivastri</author>
		<author>Maria Stella Tavella</author>
		<authors>
			<first>Loreto</first>
		</authors>
		<authors>
			<last>Parisi</last>
		</authors>
		<authors>
			<first>Simone</first>
		</authors>
		<authors>
			<last>Francia</last>
		</authors>
		<authors>
			<first>Silvio</first>
		</authors>
		<authors>
			<last>Olivastri</last>
		</authors>
		<authors>
			<first>Maria Stella</first>
		</authors>
		<authors>
			<last>Tavella</last>
		</authors>
		<abstract>One of the key points in music recommendation is authoring engaging playlists
according to sentiment and emotions. While previous works were mostly based on
audio for music discovery and playlists generation, we take advantage of our
synchronized lyrics dataset to combine text representations and music features
in a novel way; we therefore introduce the Synchronized Lyrics Emotion Dataset.
Unlike other approaches that randomly exploited the audio samples and the whole
text, our data is split according to the temporal information provided by the
synchronization between lyrics and audio. This work shows a comparison between
text-based and audio-based deep learning classification models using different
techniques from Natural Language Processing and Music Information Retrieval
domains. From the experiments on audio we conclude that using vocals only,
instead of the whole audio data improves the overall performances of the audio
classifier. In the lyrics experiments we exploit the state-of-the-art word
representations applied to the main Deep Learning architectures available in
literature. In our benchmarks the results show how the Bilinear LSTM classifier
with Attention based on fastText word embedding performs better than the CNN
applied on audio.</abstract>
		<title>Exploiting Synchronized Lyrics And Vocal Features For Music Emotion
  Detection</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27f53f1e7e4ec5a603ef727b17afca63e/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>Proceedings of the 4th Sound and Music Computing Conference</booktitle>
		<series>Proceedings 4th Sound and Music Computing Conference (SMC'2007)</series>
		<year>2007</year>
		<url></url>
		<author>Ioannis Karydis</author>
		<author>Alexandros Nanopoulos</author>
		<author>Apostolos Papadopoulos</author>
		<author>Emilios Cambouropoulos</author>
		<authors>
			<first>Ioannis</first>
		</authors>
		<authors>
			<last>Karydis</last>
		</authors>
		<authors>
			<first>Alexandros</first>
		</authors>
		<authors>
			<last>Nanopoulos</last>
		</authors>
		<authors>
			<first>Apostolos</first>
		</authors>
		<authors>
			<last>Papadopoulos</last>
		</authors>
		<authors>
			<first>Emilios</first>
		</authors>
		<authors>
			<last>Cambouropoulos</last>
		</authors>
		<number>July</number>
		<pages>11--13</pages>
		<abstract>Listeners are thought to be capable of perceiving multiple voices
	in music. Adopting a perceptual view of musical 'voice' that corresponds
	to the notion of auditory stream, a computational model is developed
	that splits a musical score (symbolic musical data) into different
	voices. A single 'voice' may consist of more than one synchronous
	notes that are perceived as belonging to the same auditory stream;
	in this sense, the proposed algorithm, may separate a given musical
	work into fewer voices than the maximum number of notes in the greatest
	chord (e.g. a piece consisting of four or more concurrent notes may
	be separated simply into melody and accompaniment). This is paramount,
	not only in the study of auditory streaming per se, but also for
	developing MIR systems that enable pattern recognition and extraction
	within musically pertinent 'voices' (e.g. melodic lines). The algorithm
	is tested qualitatively and quantitatively against a small dataset
	that acts as groundtruth.</abstract>
		<title>Horizontal and Vertical Integration / Segregation in Auditory Streaming : A Voice Separation Algorithm for Symbolic Musical Data</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24ffd5f56a4e99600c45158c558598641/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>Computer Music Modeling and Retrieval</booktitle>
		<series>International Symposium on Computer Music Modeling and Retrieval</series>
		<publisher>Springer</publisher>
		<year>2005</year>
		<url></url>
		<author>Elaine Chew</author>
		<author>Xiaodan Wu</author>
		<authors>
			<first>Elaine</first>
		</authors>
		<authors>
			<last>Chew</last>
		</authors>
		<authors>
			<first>Xiaodan</first>
		</authors>
		<authors>
			<last>Wu</last>
		</authors>
		<volume>3310</volume>
		<pages>1--20</pages>
		<abstract>Voice separation is a critical component of music information retrieval,
	music analysis and automated transcription systems. We present a
	contig mapping approach to voice separation based on perceptual principles.
	The algorithm runs in O(n 2 ) time, uses only pitch height and event
	boundaries, and requires no user-defined parameters. The method segments
	a piece into contigs according to voice count, then reconnects fragments
	in adjacent contigs using a shortest distance strategy. The order
	of connection is by distance from maximal voice contigs, where the
	voice ordering is known. This contig-mapping algorithm has been implemented
	in VoSA, a Java-based voice separation analyzer software. The algorithm
	performed well when applied to J. S. Bach's Twoand Three-Part Inventions
	and the forty-eight Fugues from the WellTempered Clavier. We report
	an overall average fragment consistency of 99.75\%, correct fragment
	connection rate of 94.50\% and average voice consistency of 88.98\%,
	metrics which we propose to measure voice separation performance.</abstract>
		<eprinttype>arXiv</eprinttype>
		<eprint>9780201398298</eprint>
		<doi>10.1007/b105507</doi>
		<title>Computer Music Modeling and Retrieval</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d5c26381b636b8dc026f740afb133883/sapo</id>
		<tags>perceptual_evaluation</tags>
		<tags>excerpt_duration</tags>
		<description></description>
		<date>2020-03-24 12:50:44</date>
		<count>2</count>
		<journal>Bulletin of the Council for Research in Music Education</journal>
		<publisher>University of Illinois Press, Council for Research in Music Education</publisher>
		<year>2009</year>
		<url>http://www.jstor.org/stable/40319327</url>
		<author>Jessica Napoles</author>
		<authors>
			<first>Jessica</first>
		</authors>
		<authors>
			<last>Napoles</last>
		</authors>
		<number>179</number>
		<pages>21--32</pages>
		<abstract>The purpose of this study was to examine the effect of excerpt length on music education majors' ratings of children's choral performances. A secondary purpose was to determine whether there were any differences in ratings between choral and instrumental music education majors. Participants (28 choral music education majors and 44 instrumental music education majors from a large western university) listened to 10 excerpts of high-quality children's choral performances and rated them on pitch accuracy rhythmic accuracy, tone quality expressiveness, and overall impression, using a 7-point Likert-type scale. Participants were assigned to one of two groups. The only difference between the groups was that, if Group 1 listened to an excerpt that was 20 seconds long Group 2 listened to that same excerpt for 60 seconds, and vice versa. Results of a three-way ANOVA with repeated measures indicate there were significant interactions between major and excerpt length (F (1, 64) = 6.06, p = .01, partial Î·Â² = .08) and between piece and excerpt length (F (9, 576) = 2.39, p = .01, partial Î·Â² -.03). Participants rated excerpts that were 60 seconds long slightly higher than excerpts that were 20 seconds long There were no significant differences between the ratings of instrumental and choral music education majors, although instrumental majors tended to rate excerpts higher than choral majors.</abstract>
		<issn>00109894</issn>
		<title>The Effect of Excerpt Duration and Music Education Emphasis on Ratings of High Quality Children's Choral Performances</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b0d839fb9c27712b94e8c63a876999e4/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#RahmanA12</url>
		<author>Md. Azizur Rahman</author>
		<author>Alex Aravind</author>
		<authors>
			<first>Md. Azizur</first>
		</authors>
		<authors>
			<last>Rahman</last>
		</authors>
		<authors>
			<first>Alex</first>
		</authors>
		<authors>
			<last>Aravind</last>
		</authors>
		<pages>5-12</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Connectivity Analysis of Mobile Ad Hoc Networks Using Destination Guided Mobility Models.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f85f9d944c3dd7be369a9b3c4dcc523b/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#Liao12</url>
		<author>Hui-Wen Liao</author>
		<authors>
			<first>Hui-Wen</first>
		</authors>
		<authors>
			<last>Liao</last>
		</authors>
		<pages>132-137</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>A Multiple Watermarking Scheme for Color Images.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29c7e7140c4ab7048e8216a275d1ca65a/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#KofmanNN12</url>
		<author>Inna Kofman</author>
		<author>Uyen Trang Nguyen</author>
		<author>Hoang Lan Nguyen</author>
		<authors>
			<first>Inna</first>
		</authors>
		<authors>
			<last>Kofman</last>
		</authors>
		<authors>
			<first>Uyen Trang</first>
		</authors>
		<authors>
			<last>Nguyen</last>
		</authors>
		<authors>
			<first>Hoang Lan</first>
		</authors>
		<authors>
			<last>Nguyen</last>
		</authors>
		<pages>188-194</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>A Node Control Model for the Charging and Accounting Problem in MANETs.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/283ed8d0c9d945688a5cebca6d235bc1f/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#HoskinsonN12</url>
		<author>Reynald Hoskinson</author>
		<author>Etienne Naugle</author>
		<authors>
			<first>Reynald</first>
		</authors>
		<authors>
			<last>Hoskinson</last>
		</authors>
		<authors>
			<first>Etienne</first>
		</authors>
		<authors>
			<last>Naugle</last>
		</authors>
		<pages>1-4</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>A Mobile Head-Mounted Display for Action Sports.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/213c297c771af41f24bb2b9605a88c0c0/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2018-06-16 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#HuLX12</url>
		<author>Gang Hu</author>
		<author>Lixia Liu</author>
		<author>Ming Xu</author>
		<authors>
			<first>Gang</first>
		</authors>
		<authors>
			<last>Hu</last>
		</authors>
		<authors>
			<first>Lixia</first>
		</authors>
		<authors>
			<last>Liu</last>
		</authors>
		<authors>
			<first>Ming</first>
		</authors>
		<authors>
			<last>Xu</last>
		</authors>
		<pages>170-175</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>A SINR Guaranteed Power and Spectrum Allocation Algorithm for Cognitive Networks.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23451e50915471c011769d9f64234ea23/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#GallegoH12</url>
		<author>Daniel Gallego</author>
		<author>Gabriel Huecas</author>
		<authors>
			<first>Daniel</first>
		</authors>
		<authors>
			<last>Gallego</last>
		</authors>
		<authors>
			<first>Gabriel</first>
		</authors>
		<authors>
			<last>Huecas</last>
		</authors>
		<pages>13-20</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>An Empirical Case of a Context-Aware Mobile Recommender System in a Banking Environment.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f3330f9cec6ff34ee1828ae513d39f2f/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#KimHR12</url>
		<author>Daehoon Kim</author>
		<author>Eenjun Hwang</author>
		<author>Seungmin Rho</author>
		<authors>
			<first>Daehoon</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Eenjun</first>
		</authors>
		<authors>
			<last>Hwang</last>
		</authors>
		<authors>
			<first>Seungmin</first>
		</authors>
		<authors>
			<last>Rho</last>
		</authors>
		<pages>47-52</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Location-Based Large-Scale Landmark Image Recognition Scheme for Mobile Devices.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a0fae2331560e812b206a264b4086616/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#KhaledBC12</url>
		<author>Shah Mostafa Khaled</author>
		<author>Robert Benkoczi</author>
		<author>Yuanzhu Peter Chen</author>
		<authors>
			<first>Shah Mostafa</first>
		</authors>
		<authors>
			<last>Khaled</last>
		</authors>
		<authors>
			<first>Robert</first>
		</authors>
		<authors>
			<last>Benkoczi</last>
		</authors>
		<authors>
			<first>Yuanzhu Peter</first>
		</authors>
		<authors>
			<last>Chen</last>
		</authors>
		<pages>207-213</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Mesh Network Deployment to Ensure Global Reachability.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bf54c8f27ca096875489123151a4478f/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2019-10-10 00:00:00</date>
		<count>1</count>
		<booktitle>Ubiquitous Music</booktitle>
		<series>Computational Music Science</series>
		<publisher>Springer</publisher>
		<year>2014</year>
		<url>http://dblp.uni-trier.de/db/series/music/um2014.html#Troyer14</url>
		<author>Akito van Troyer</author>
		<authors>
			<first>Akito</first>
		</authors>
		<authors>
			<last>van Troyer</last>
		</authors>
		<editor>Damián Keller</editor>
		<editor>Victor Lazzarini</editor>
		<editor>Marcelo Soares Pimenta</editor>
		<editors>
			<first>Akito</first>
		</editors>
		<editors>
			<last>van Troyer</last>
		</editors>
		<editors>
			<first>Akito</first>
		</editors>
		<editors>
			<last>van Troyer</last>
		</editors>
		<editors>
			<first>Akito</first>
		</editors>
		<editors>
			<last>van Troyer</last>
		</editors>
		<pages>51-63</pages>
		<isbn>978-3-319-11151-3</isbn>
		<title>Repertoire Remix in the Context of Festival City.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2889f851f126fe726d5049811956b51bd/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2019-10-19 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#MahmudRK12</url>
		<author>Arif Mahmud</author>
		<author>Rahim Rahmani</author>
		<author>Theo Kanter</author>
		<authors>
			<first>Arif</first>
		</authors>
		<authors>
			<last>Mahmud</last>
		</authors>
		<authors>
			<first>Rahim</first>
		</authors>
		<authors>
			<last>Rahmani</last>
		</authors>
		<authors>
			<first>Theo</first>
		</authors>
		<authors>
			<last>Kanter</last>
		</authors>
		<pages>195-200</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Deployment of Flow-Sensors in Internet of Things' Virtualization via OpenFlow.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/274f2f92153cf7eeba7f5bf75191ee816/jpooley</id>
		<tags>popular-music</tags>
		<tags>music</tags>
		<tags>music-education</tags>
		<tags>norway</tags>
		<description></description>
		<date>2019-08-29 01:56:31</date>
		<count>1</count>
		<journal>Music Education Research</journal>
		<year>2017</year>
		<url></url>
		<author>Petter Dyndahl</author>
		<author>Sidsel Karlsen</author>
		<author>Siw G. Nielsen</author>
		<author>Odd Sk\aarberg</author>
		<authors>
			<first>Petter</first>
		</authors>
		<authors>
			<last>Dyndahl</last>
		</authors>
		<authors>
			<first>Sidsel</first>
		</authors>
		<authors>
			<last>Karlsen</last>
		</authors>
		<authors>
			<first>Siw G.</first>
		</authors>
		<authors>
			<last>Nielsen</last>
		</authors>
		<authors>
			<first>Odd</first>
		</authors>
		<authors>
			<last>Sk\aarberg</last>
		</authors>
		<volume>19</volume>
		<number>4</number>
		<pages>438--454</pages>
		<abstract>With a hundred years (1912–2012) of Norwegian master's and doctoral theses written within the field of music as a backdrop, this article reports from an extensive study of the academisation of popular music in higher music education and research in Norway. Theoretically, the study builds on the sociology of culture and education in the tradition of Bourdieu and some of his successors, and its methodological design is that of a comprehensive survey of the entire corpus of academic theses produced within the Norwegian music field. On this basis, the authors examine what forms of popular music have been included and excluded respectively, how this aesthetic and cultural expansion has found its legitimate scholarly expression, and which structural forces seem to govern the processes of academisation of popular music in the Norwegian context. The results show that popular music to a large extent has been successfully academised, but also that this process has led to some limitations of academic openness as well as the emergence of new power hierarchies.</abstract>
		<doi>10.1080/14613808.2016.1204280</doi>
		<title>The Academisation of Popular Music in Higher Music Education: The Case of Norway</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b3ea4e85605988f6ac530f62eaa67523/sapo</id>
		<tags>phdthesis</tags>
		<tags>performance_analysis</tags>
		<description>The Performance of Music - ScienceDirect https://www-sciencedirect-com.pros2.lib.unimi.it/science/article/pii/B9780122135644500159</description>
		<date>2021-12-17 12:34:33</date>
		<count>1</count>
		<booktitle>The Psychology of Music (Second Edition)</booktitle>
		<series>Cognition and Perception</series>
		<publisher>Academic Press</publisher>
		<address>San Diego</address>
		<year>1999</year>
		<url>https://www.sciencedirect.com/science/article/pii/B9780122135644500159</url>
		<author>Alf Gabrielsson</author>
		<authors>
			<first>Alf</first>
		</authors>
		<authors>
			<last>Gabrielsson</last>
		</authors>
		<editor>Diana Deutsch</editor>
		<editors>
			<first>Alf</first>
		</editors>
		<editors>
			<last>Gabrielsson</last>
		</editors>
		<pages>501-602</pages>
		<abstract>Publisher Summary
Music performance is a large subject that can be approached in many different ways. This chapter focuses on empirical research of music performance and related matters. Most of this research is concerned with Western tonal music and mainly art music. Excellence in music performance involves two major components like a genuine understanding of what the music is about, its structure and meaning, and a complete mastery of the instrumental technique. Evaluation of performance included many studies which are reviewed earlier. Evaluation occurs in the everyday activity of music critics, music teachers, and musicians. An overall evaluation is considered as a weighted function of the evaluations in the specific aspects. In order to maintain the tempo and to achieve perceived synchrony, musicians should therefore play a small amount ahead of the beat they hear. With sharp attacks the delay is less, and instruments with sharp attacks may therefore serve as “beat-definers” for the rest of an ensemble. In addition, some attempts are made to predict evaluation of music performances from the physical characteristics of the performances.</abstract>
		<isbn>978-0-12-213564-4</isbn>
		<doi>https://doi.org/10.1016/B978-012213564-4/50015-9</doi>
		<title>The Performance of Music</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21f0d4a9974eb6affaa8752e63e5dd942/ocelma</id>
		<tags>PhD</tags>
		<description>PhD</description>
		<date>2009-01-08 17:06:55</date>
		<count>2</count>
		<journal>Proceedings of the 8th International Conference on Music Information Retrieval</journal>
		<address>Vienna, Austria</address>
		<year>2007</year>
		<url>files/publications/7c086c-ISMIR-2007-msordo-claurier.pdf</url>
		<author>M. Sordo</author>
		<author>C. Laurier</author>
		<author>O. Celma</author>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Sordo</last>
		</authors>
		<authors>
			<first>C.</first>
		</authors>
		<authors>
			<last>Laurier</last>
		</authors>
		<authors>
			<first>O.</first>
		</authors>
		<authors>
			<last>Celma</last>
		</authors>
		<abstract>In this paper we present a way to annotate music collections by exploiting
	audio similarity. In this sense, similarity is used to propose labels
	(tags) to yet unlabeled songs, based on the content-based distance
	between them. The main goal of our work is to ease the process of
	annotating huge music collections, by using content-based similarity
	distances as a way to propagate labels among songs.
	We present two different experiments. The first one propagates labels
	that are related with the style of the piece, whereas the second
	experiment deals with mood labels. On the one hand, our approach
	shows that using a music collection annotated at 40% with styles,
	and using content-based, the collection can be automatically annotated
	up to 78% (that is, 40% already annotated and the rest, 38%, only
	using propagation), with a recall greater than 0.4. On the other
	hand, for a smaller music collection annotated at 30% with moods,
	the collection can be automatically annotated up to 65% (e.g. 30%
	plus 35% using propagation).</abstract>
		<title>Annotating Music Collections How content-based similarity helps to
	propagate labels</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20344e9251df13d3fca026f5e76b8f4e3/aucelum</id>
		<tags>harmony</tags>
		<tags>music</tags>
		<tags>mem</tags>
		<tags>analysis</tags>
		<tags>algorithm</tags>
		<description></description>
		<date>2010-02-02 17:03:01</date>
		<count>2</count>
		<journal>Computer Music Journal</journal>
		<year>2002</year>
		<url></url>
		<author>Bryan Pardo</author>
		<author>William P. Birmingham</author>
		<authors>
			<first>Bryan</first>
		</authors>
		<authors>
			<last>Pardo</last>
		</authors>
		<authors>
			<first>William P.</first>
		</authors>
		<authors>
			<last>Birmingham</last>
		</authors>
		<volume>26</volume>
		<number>2</number>
		<pages>27--49</pages>
		<abstract>Presents an evaluation of the algorithms that perform harmonic analysis using a corpus of excerpts of tonal music.  Major tasks in harmonic analysis; Comparison of the labeling performance of algorithm using tie-breaking rules against its performance without the use of tie breaking; Classes that resulted from labeling errors.</abstract>
		<issn>01489267</issn>
		<title>Algorithms for Chordal Analysis</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24ae3ebda0b46bdba0267dcbf34bfc03b/tb2332</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>1</count>
		<booktitle>The Proceedings of the Eighth International Conference on Music
	Perception and Cognition (ICMPC8)</booktitle>
		<publisher>Causal Productions</publisher>
		<address>Adelaide</address>
		<year>2004</year>
		<url></url>
		<author>D. Eck</author>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Eck</last>
		</authors>
		<editor>S.D. Lipscomb</editor>
		<editor>R. Ashley</editor>
		<editor>R.O. Gjerdingen</editor>
		<editor>P. Webster</editor>
		<editors>
			<first>D.</first>
		</editors>
		<editors>
			<last>Eck</last>
		</editors>
		<editors>
			<first>D.</first>
		</editors>
		<editors>
			<last>Eck</last>
		</editors>
		<editors>
			<first>D.</first>
		</editors>
		<editors>
			<last>Eck</last>
		</editors>
		<editors>
			<first>D.</first>
		</editors>
		<editors>
			<last>Eck</last>
		</editors>
		<pages>542-543</pages>
		<abstract>One major challenge in using statistical sequence learning methods
	in the domain of music lies in bridging the long timelags that separate
	important musical events. Consider, for example, the chord changes
	that convey the basic structure of a pop song. A sequence learner
	that cannot predict chord changes will almost certainly not be able
	to generate new examples in a musical style or to categorize songs
	by style. Yet, it is surprisingly difficult for a sequence learner
	to bridge the long timelags necessary to identify when a chord change
	will occur and what its new value will be. This is the case because
	chord changes can be separated by dozens or hundreds of intervening
	notes. One could solve this problem by treating chords as being special
	(as did Mozer, NIPS 1991). But this is impractical---it requires
	chords to be labeled specially in the dataset, limiting the applicability
	of the model to non-labeled examples---and furthermore does not address
	the general issue of nested temporal structure in music. I will briefly
	describe this temporal structure (known commonly as "meter") and
	present a model that uses to its advantage an assumption that sequences
	are metrical. The model consists of an autocorrelation-based filtration
	that estimates online the most likely metrical tree (i.e. the frequency
	and phase of beat, measure, phrase &etc.) and uses that to generate
	a series of sequences varying at different rates. These sequences
	correspond to each level in the hierarchy. Multiple learners can
	be used to treat each series separately and their predictions can
	be combined to perform composition and categorization. I will present
	preliminary results that demonstrate the usefulness of this approach.
	Time permitting I will also compare the model to alternate approaches.</abstract>
		<source>OwnPublication</source>
		<title>A Machine-Learning Approach to Musical Sequence Induction That Uses
	Autocorrelation to Bridge Long Timelags</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d6b353059673fe05bef0c70167c56213/syslogd</id>
		<tags>Retrieval,</tags>
		<tags>Popular</tags>
		<tags>folksonomy</tags>
		<tags>Music</tags>
		<tags>Information</tags>
		<tags>Folksonomies,</tags>
		<tags>Music,</tags>
		<tags>last.fm,</tags>
		<description></description>
		<date>2010-01-07 13:57:09</date>
		<count>1</count>
		<publisher>School of Information and Library Science</publisher>
		<year>2008</year>
		<url>http://etd.ils.unc.edu:8080/dspace/handle/1901/535?mode=full</url>
		<author>Abbey E. Thompson</author>
		<authors>
			<first>Abbey E.</first>
		</authors>
		<authors>
			<last>Thompson</last>
		</authors>
		<abstract>The cataloging of musical materials has always posed challenges for librarians, requiring special treatment for classification and organization. Accurate description is critical for achieving high-recall retrieval and access by patrons. This study considers the challenge of music description through content analysis of the popular music websites AllMusic.com and Last.fm. The goal of the research was to gain a better understanding of how users describe their music collections. The findings in this study illustrate that hierarchical vocabulary structures are clearly evident within the Last.fm folksonomy. The findings also show that tagging data is more reliable in representing musical genre/subject than previously speculated, indicating that with proper analysis and coding, social tag data could be harvested to provide genre-level metadata for popular music titles. The work presented here contributes a methodology for further study of this topic, specific to music folksonomies and vocabularies, which may also be useful for other disciplines.</abstract>
		<title>Playing Tag: An Analysis of Vocabulary Patterns and Relationships Within a Popular Music Folksonomy</title>
		<pubtype>mastersthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25b37af53d5e95897fdd917b93c0db253/bfields</id>
		<tags>social_networks</tags>
		<tags>music_similarity</tags>
		<tags>ground-truth</tags>
		<description>initial imports</description>
		<date>2010-01-28 11:42:45</date>
		<count>4</count>
		<booktitle>Proc. of the 2002 International Computer Music Confernce</booktitle>
		<year>2002</year>
		<url></url>
		<author>Brian Whitman</author>
		<author>Steve Lawrence</author>
		<authors>
			<first>Brian</first>
		</authors>
		<authors>
			<last>Whitman</last>
		</authors>
		<authors>
			<first>Steve</first>
		</authors>
		<authors>
			<last>Lawrence</last>
		</authors>
		<title>Inferring Descriptions and Similarity for Music from Community Metadata</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f0d968b1da9bb231bf326d25e0ecfd90/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>DISSERTATION ABSTRACTS INTERNATIONAL, Section A: The Humanities and Social Sciences</journal>
		<series>The Abbey Road medley: Extended forms in popular music (Ph.D., New York Univ., 2005).</series>
		<year>2005</year>
		<url>http://ezproxy.library.yorku.ca/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=mah&AN=MAH0001448377&site=ehost-live</url>
		<author>THOMAS MacFARLANE</author>
		<author>THOMAS MacFARLANE</author>
		<authors>
			<first>THOMAS</first>
		</authors>
		<authors>
			<last>MacFARLANE</last>
		</authors>
		<authors>
			<first>THOMAS</first>
		</authors>
		<authors>
			<last>MacFARLANE</last>
		</authors>
		<volume>65</volume>
		<pages>4042</pages>
		<issn>0419-4209</issn>
		<shorttitle>The Abbey Road medley</shorttitle>
		<doi>Dissertation</doi>
		<title>The Abbey Road medley: Extended forms in popular music (Ph.D., New York Univ., 2005).</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/277fd52a4db1e775fe346480a4c191591/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>Popular Music</journal>
		<year>2006</year>
		<url>http://journals.cambridge.org/action/displayIssue?jid=PMU&volumeId=26&issueId=01&iid=637328</url>
		<author>TONY RUSSELL</author>
		<authors>
			<first>TONY</first>
		</authors>
		<authors>
			<last>RUSSELL</last>
		</authors>
		<volume>26</volume>
		<number>01</number>
		<pages>23--31</pages>
		<shorttitle>Country Music on Location</shorttitle>
		<doi>10.1017/S0261143007001109</doi>
		<title>Country Music on Location: Before Bristol</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21485f6521c6ae2db520d1a7c3c429f07/sdo</id>
		<tags>tagging</tags>
		<tags>retrieval</tags>
		<tags>folksonomy</tags>
		<tags>usage</tags>
		<tags>last.fm</tags>
		<description>Analysis of Music Tagging and Listening Patterns: Do Tags Really Function as Retrieval Aids? - Springer</description>
		<date>2015-04-15 18:15:07</date>
		<count>2</count>
		<booktitle>Social Computing, Behavioral-Cultural Modeling, and Prediction</booktitle>
		<series>Lecture Notes in Computer Science</series>
		<publisher>Springer International Publishing</publisher>
		<year>2015</year>
		<url>http://dx.doi.org/10.1007/978-3-319-16268-3_15</url>
		<author>Jared Lorince</author>
		<author>Kenneth Joseph</author>
		<author>PeterM. Todd</author>
		<authors>
			<first>Jared</first>
		</authors>
		<authors>
			<last>Lorince</last>
		</authors>
		<authors>
			<first>Kenneth</first>
		</authors>
		<authors>
			<last>Joseph</last>
		</authors>
		<authors>
			<first>PeterM.</first>
		</authors>
		<authors>
			<last>Todd</last>
		</authors>
		<editor>Nitin Agarwal</editor>
		<editor>Kevin Xu</editor>
		<editor>Nathaniel Osgood</editor>
		<editors>
			<first>PeterM.</first>
		</editors>
		<editors>
			<last>Todd</last>
		</editors>
		<editors>
			<first>PeterM.</first>
		</editors>
		<editors>
			<last>Todd</last>
		</editors>
		<editors>
			<first>PeterM.</first>
		</editors>
		<editors>
			<last>Todd</last>
		</editors>
		<volume>9021</volume>
		<pages>141-152</pages>
		<abstract>In collaborative tagging systems, it is generally assumed that users assign tags to facilitate retrieval of content at a later time. There is, however, little behavioral evidence that tags actually serve this purpose. Using a large-scale dataset from the social music website Last.fm, we explore how patterns of music tagging and subsequent listening interact to determine if there exist measurable signals of tags functioning as retrieval aids. Specifically, we describe our methods for testing if the assignment of a tag tends to lead to an increase in listening behavior. Results suggest that tagging, on average, leads to only very small increases in listening rates, and overall the data do</abstract>
		<isbn>978-3-319-16267-6</isbn>
		<language>English</language>
		<doi>10.1007/978-3-319-16268-3_15</doi>
		<title>Analysis of Music Tagging and Listening Patterns: Do Tags Really Function as Retrieval Aids?</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/215a34259e768be60f788fc39caf3a7da/researchparks</id>
		<tags>musical-choreographicaction</tags>
		<tags>performinggroups</tags>
		<tags>ceremony</tags>
		<description></description>
		<date>2021-03-03 06:30:24</date>
		<count>1</count>
		<journal>International Journal on Integrated Education</journal>
		<year>2020</year>
		<url>https://journals.researchparks.org/index.php/IJIE/article/view/1045</url>
		<author>Tursunpulatova Gulrukhsor</author>
		<authors>
			<first>Tursunpulatova</first>
		</authors>
		<authors>
			<last>Gulrukhsor</last>
		</authors>
		<volume>3</volume>
		<number>12</number>
		<pages>415-417</pages>
		<abstract>The world significance of Chinese or Japanese art is now more or less universally recognized. Russian readers are well aware of articles and monographs on Japanese GA-Gaku music. The term gagaku is the Japanese pronunciation of two Chinese characters: ya + Yue, and this so-called gagaku music in Japan basically share a common ancestry with Chinese yayue. All of them mean "perfect", "correct music Tursunpulatova Gulrukhsor 2020. O "PERFECT MUSIC&quot; IN THE FAR EAST. International Journal on Integrated Education. 3, 12 (Dec. 2020), 415-417. DOI:https://doi.org/10.31149/ijie.v3i12.1045 Pdf Url : https://journals.researchparks.org/index.php/IJIE/article/view/1045/993 Paper Url : https://journals.researchparks.org/index.php/IJIE/article/view/1045</abstract>
		<language>English</language>
		<issn>2620-3502</issn>
		<title>O "PERFECT MUSIC" IN THE FAR EAST</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a9edc807f4db7a71fceff519b73dfe8a/sapo</id>
		<tags>score-to-audio_alignment</tags>
		<description>A diff procedure for music score files | 6th International Conference on Digital Libraries for Musicology</description>
		<date>2021-03-01 17:44:15</date>
		<count>2</count>
		<booktitle>6th International Conference on Digital Libraries for Musicology</booktitle>
		<publisher>ACM</publisher>
		<year>2019</year>
		<url>https://doi.org/10.1145%2F3358664.3358671</url>
		<author>Francesco Foscarin</author>
		<author>Florent Jacquemard</author>
		<author>Raphaël Fournier-S'niehotta</author>
		<authors>
			<first>Francesco</first>
		</authors>
		<authors>
			<last>Foscarin</last>
		</authors>
		<authors>
			<first>Florent</first>
		</authors>
		<authors>
			<last>Jacquemard</last>
		</authors>
		<authors>
			<first>Raphaël</first>
		</authors>
		<authors>
			<last>Fournier-S'niehotta</last>
		</authors>
		<editor>David Rizo</editor>
		<editors>
			<first>Raphaël</first>
		</editors>
		<editors>
			<last>Fournier-S'niehotta</last>
		</editors>
		<doi>10.1145/3358664.3358671</doi>
		<title>A diff procedure for music score files.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ab9cbbc670b2a322d0dad4843d990a6a/ijtsrd</id>
		<tags>Cepstral</tags>
		<tags>k-Nearest</tags>
		<tags>Mel</tags>
		<tags>Learning</tags>
		<tags>Support</tags>
		<tags>Neighbors</tags>
		<tags>Vector</tags>
		<tags>Frequency</tags>
		<tags>Coefficients</tags>
		<tags>Machine</tags>
		<description></description>
		<date>2021-07-15 10:25:53</date>
		<count>1</count>
		<journal>International Journal of Trend in Scientific Research and Development</journal>
		<year>2021</year>
		<url>https://www.ijtsrd.comcomputer-science/data-processing/41263/music-genre-classification-using-machine-learning/seethal-v</url>
		<author>Seethal V | Dr. A. Vijayakumar</author>
		<authors>
			<first>Seethal V | Dr. A.</first>
		</authors>
		<authors>
			<last>Vijayakumar</last>
		</authors>
		<volume>5</volume>
		<number>4</number>
		<pages>829-831</pages>
		<abstract>Music genre classification has been a toughest task in the area of music information retrieval MIR . Classification of genre can be important to clarify some genuine fascinating issues, such as, making songs references, discovering related songs, finding societies who will like that particular song. The inspiration behind the research is to find the appropriate machine learning algorithm that predict the genres of music utilizing k nearest neighbor k NN and Support Vector Machine SVM . GTZAN dataset is the frequently used dataset for the classification music genre. The Mel Frequency cepstral coefficients MFCC is utilized to extricate features for the dataset. From results we found that k NN classifier gave more exact results compared to support vector machine classifier. If the training data is bigger than number of features, k NN gives better outcomes than SVM. SVM can only identify limited set of patterns. KNN classifier is more powerful for the classification of music genre. Seethal V | Dr. A. Vijayakumar "Music Genre Classification using Machine Learning" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https://www.ijtsrd.compapers/ijtsrd41263.pdf Paper URL: https://www.ijtsrd.comcomputer-science/data-processing/41263/music-genre-classification-using-machine-learning/seethal-v</abstract>
		<language>english</language>
		<issn>2456-6470</issn>
		<title>Music Genre Classification using Machine Learning</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22f3cd4914c1913b02f887fad3345a212/bauerc</id>
		<tags>imported</tags>
		<tags>myown</tags>
		<tags>music</tags>
		<tags>artists</tags>
		<tags>recsys</tags>
		<description></description>
		<date>2019-04-27 18:22:14</date>
		<count>2</count>
		<booktitle>30th Bled eConference</booktitle>
		<publisher>University of Maribor Press</publisher>
		<address>Maribor, Slovenia</address>
		<year>2017</year>
		<url>http://dblp.uni-trier.de/db/conf/bled/bled2017.html#0001KS17</url>
		<author>Christine Bauer</author>
		<author>Marta Kholodylo</author>
		<author>Christine Strauss</author>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<authors>
			<first>Marta</first>
		</authors>
		<authors>
			<last>Kholodylo</last>
		</authors>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Strauss</last>
		</authors>
		<editor>Andreja Pucihar</editor>
		<editor>Mirjana Kljajić Borštnar</editor>
		<editor>Christian Kittl</editor>
		<editor>Pascal Ravesteijn</editor>
		<editor>Roger Clarke</editor>
		<editor>Roger Bons</editor>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Strauss</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Strauss</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Strauss</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Strauss</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Strauss</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Strauss</last>
		</editors>
		<pages>21-32</pages>
		<abstract>Music Recommender Systems (MRS) are important drivers in music industry and are widely adopted by music platforms. Other than most MRS research exploring MRS from a technical or from a consumers’ perspective, this work focuses on the impact, value generation, challenges and opportunities for those, who contribute the core value, i.e. the artists. We outline the non-superstar artist’s perspective on MRS, and explore the question if and how non-superstar artists may benefit from MRS to foster their professional advancement. Thereby, we explain several techniques how MRS generate recommendations and discuss their impact on non-superstar artists.</abstract>
		<isbn>978-961-286-043-1</isbn>
		<language>English</language>
		<doi>10.18690/978-961-286-043-1.3</doi>
		<title>Music Recommender Systems: Challenges and Opportunities for Non-Superstar Artists</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2046b09124815e492be761827b4e5c1ba/bauerc</id>
		<tags>imported</tags>
		<tags>running</tags>
		<tags>myown</tags>
		<tags>music</tags>
		<tags>jogging</tags>
		<description></description>
		<date>2019-04-27 18:22:14</date>
		<count>2</count>
		<booktitle>ACM SIGCHI Extended Abstracts of Conference on Human Factors in Computing Systems (CHI 2015)</booktitle>
		<series>CHI 2015</series>
		<publisher>ACM</publisher>
		<year>2015</year>
		<url></url>
		<author>Christine Bauer</author>
		<author>Anna Kratschmar</author>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<authors>
			<first>Anna</first>
		</authors>
		<authors>
			<last>Kratschmar</last>
		</authors>
		<editor>Bo Begole</editor>
		<editor>Jinwoo Kim</editor>
		<editor>Kori Inkpen</editor>
		<editor>Woontack Woo</editor>
		<editors>
			<first>Anna</first>
		</editors>
		<editors>
			<last>Kratschmar</last>
		</editors>
		<editors>
			<first>Anna</first>
		</editors>
		<editors>
			<last>Kratschmar</last>
		</editors>
		<editors>
			<first>Anna</first>
		</editors>
		<editors>
			<last>Kratschmar</last>
		</editors>
		<editors>
			<first>Anna</first>
		</editors>
		<editors>
			<last>Kratschmar</last>
		</editors>
		<pages>1379-1384</pages>
		<abstract>Music has long been acknowledged for its effects on participants in sports and exercise. For casual runners music may act as a motivator and distractor of physical strain. It may also serve as a training guide, when sensing technology is used as an enabler for adapting music to a runner’s situation in real-time. While many effects of music are known from sports science and psychology, application designers lack a consolidated knowledge base that guides them in designing a running application. This work synthesizes findings from the involved disciplines and provides 7 requirements for an application that increases casual runners’ motivation and controls training.</abstract>
		<isbn>978-1-4503-3146-3</isbn>
		<language>English</language>
		<doi>10.1145/2702613.2732736</doi>
		<title>Designing a Music-controlled Running Application: a Sports Science and Psychological Perspective</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/273403c1488f7cdb4710c43cdb4e5b2ab/bauerc</id>
		<tags>imported</tags>
		<tags>myown</tags>
		<tags>music</tags>
		<tags>mainstreaminess</tags>
		<tags>recsys</tags>
		<description></description>
		<date>2019-04-27 18:20:03</date>
		<count>2</count>
		<booktitle>2nd Workshop on Surprise, Opposition, and Obstruction in Adaptive and Personalized Systems, in conjunction with 25th International Conference on User Modeling, Adaptation and Personalization (UMAP 2017)</booktitle>
		<series>SOAP 2017</series>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2017</year>
		<url>http://dblp.uni-trier.de/db/conf/um/umap2017a.html#SchedlB17</url>
		<author>Markus Schedl</author>
		<author>Christine Bauer</author>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<editor>Mária Bieliková</editor>
		<editor>Eelco Herder</editor>
		<editor>Federica Cena</editor>
		<editor>Michel C. Desmarais</editor>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<pages>364-367</pages>
		<abstract>A music listener's mainstreaminess indicates the extent to which her listening preferences correspond to those of the population at large. However, formal definitions to quantify the level of mainstreaminess of a listener are rare and those available define mainstreaminess based on fractions between some kind of individual and global listening profiles. We argue, in contrast, that measures based on a modified version of the well-established Kullback-Leibler (KL) divergence as well as rank-order correlation coefficient may be better suited to capture the mainstreaminess of listeners. We therefore propose two measures adopting KL divergence and rank-order correlation and show, on a real-world dataset of over one billion user-generated listening events (LFM-1b), that music recommender systems can notably benefit when grouping users according to their level of mainstreaminess with respect to these two measures. This particularly holds for the frequently neglected listener group which is characterized by low mainstreaminess.</abstract>
		<isbn>978-1-4503-5067-9/17/07</isbn>
		<language>English</language>
		<doi>10.1145/3099023.3099098</doi>
		<title>Distance- and Rank-based Music Mainstreaminess Measurement</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27e97173f14752c184547b6c54b945e13/researchparks</id>
		<tags>music</tags>
		<tags>culture</tags>
		<tags>folkmusic</tags>
		<tags>education</tags>
		<description></description>
		<date>2021-03-01 10:19:54</date>
		<count>1</count>
		<journal>International Journal on Integrated Education</journal>
		<year>2020</year>
		<url>https://journals.researchparks.org/index.php/IJIE/article/view/338</url>
		<author>Abduraxmonova Manzura Batirovna</author>
		<authors>
			<first>Abduraxmonova Manzura</first>
		</authors>
		<authors>
			<last>Batirovna</last>
		</authors>
		<volume>3</volume>
		<number>3</number>
		<pages>19-22</pages>
		<abstract>This article deals with the Uzbek music, and the legacy of our ancestors is based on the masterpieces they left behind. It has been reported that music has evolved not only today but also for centuries Abduraxmonova Manzura Batirovna 2020. Ancestors’ heritage in the Uzbek musical art. International Journal on Integrated Education. 3, 4 (Apr. 2020), 19-22. DOI:https://doi.org/10.31149/ijie.v3i4.338. Pdf Url : https://journals.researchparks.org/index.php/IJIE/article/view/338/331 Paper Url : https://journals.researchparks.org/index.php/IJIE/article/view/338</abstract>
		<language>English</language>
		<issn>2620-3502</issn>
		<title>Ancestors' heritage in the Uzbek musical art</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b9190cafec61b031c90c690b3ce4f2d8/jpooley</id>
		<tags>music</tags>
		<tags>disciplinarian</tags>
		<tags>music-education</tags>
		<tags>film-studies</tags>
		<description></description>
		<date>2021-02-27 18:14:49</date>
		<count>1</count>
		<journal>The Journal of Film Music</journal>
		<year>2009</year>
		<url>https://journal.equinoxpub.com/JFM/article/view/4071</url>
		<author>William H. Rosar</author>
		<authors>
			<first>William H.</first>
		</authors>
		<authors>
			<last>Rosar</last>
		</authors>
		<volume>2</volume>
		<number>2-4</number>
		<pages>99-125</pages>
		<title>Film Studies In Musicology: Disciplinarity vs. Interdisciplinarity</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a6f1f9fd87c01892dceef9e3a54bd356/bauerc</id>
		<tags>fairness</tags>
		<tags>myown</tags>
		<tags>music</tags>
		<tags>gender</tags>
		<tags>recsys</tags>
		<tags>recommender</tags>
		<description></description>
		<date>2021-03-14 17:51:27</date>
		<count>1</count>
		<booktitle>Proceedings of the 6th ACM SIGIR Conference on Human Information Interaction and Retrieval</booktitle>
		<series>CHIIR 2021</series>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2021</year>
		<url>https://doi.org/10.1145/3406522.3446033</url>
		<author>Andrés Ferraro</author>
		<author>Xavier Serra</author>
		<author>Christine Bauer</author>
		<authors>
			<first>Andrés</first>
		</authors>
		<authors>
			<last>Ferraro</last>
		</authors>
		<authors>
			<first>Xavier</first>
		</authors>
		<authors>
			<last>Serra</last>
		</authors>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<pages>249-254</pages>
		<abstract>As recommender systems play an important role in everyday life, there is an increasing pressure that such systems are fair. Besides serving diverse groups of users, recommenders need to represent and serve item providers fairly as well. In interviews with music artists, we identified that gender fairness is one of the artists’ main concerns. They emphasized that female artists should be given more exposure in music recommendations. We analyze a widely-used collaborative filtering approach with two public datasets—enriched with gender information—to understand how this approach per-forms with respect to the artists’ gender. To achieve gender balance, we propose a progressive re-ranking method that is based on the insights from the interviews. For the evaluation, we rely on a simulation of feedback loops and provide an in-depth analysis using state-of-the-art performance measures and metrics concerning gender fairness.</abstract>
		<isbn>978-1-4503-8055-3/21/03</isbn>
		<language>English</language>
		<doi>10.1145/3406522.3446033</doi>
		<title>Break the Loop: Gender Imbalance in Music Recommenders</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27d4c468a1dfbff89c5dd28d0706ef5a2/simonha94</id>
		<tags>Content-based</tags>
		<tags>musicsearch</tags>
		<tags>music</tags>
		<tags>uncovr</tags>
		<tags>mir</tags>
		<tags>retrieval,Personalization,Query</tags>
		<tags>refinement,User-filtering</tags>
		<description></description>
		<date>2021-10-15 08:46:33</date>
		<count>2</count>
		<journal>Proceedings - 2018 Conference on Technologies and Applications of Artificial Intelligence, TAAI 2018</journal>
		<publisher>Institute of Electrical and Electronics Engineers Inc.</publisher>
		<year>2018</year>
		<url></url>
		<author>Ja Hwung Su</author>
		<author>Tzung Pei Hong</author>
		<author>Jyun Yu Li</author>
		<author>Jung Jui Su</author>
		<authors>
			<first>Ja Hwung</first>
		</authors>
		<authors>
			<last>Su</last>
		</authors>
		<authors>
			<first>Tzung Pei</first>
		</authors>
		<authors>
			<last>Hong</last>
		</authors>
		<authors>
			<first>Jyun Yu</first>
		</authors>
		<authors>
			<last>Li</last>
		</authors>
		<authors>
			<first>Jung Jui</first>
		</authors>
		<authors>
			<last>Su</last>
		</authors>
		<pages>177-180</pages>
		<abstract>In recent years, music is an important media because it can relax us in our daily life. Therefore, most people listen to music frequently and current music websites offer online listening services. However, because the semantic gap, it is not easy to effectively retrieve the user preferred music especially from a huge amount of music data. For this issue, this paper presents a personalized content-based music retrieval system that integrates techniques of user-filtering and query-refinement to achieve high quality of music retrieval. In terms of user-filtering, the new user interest can be inferred by the user similarities. In terms of query-refinement, the user interest can be guided to the potential search space by iterative feedbacks. The experimental results show the proposed method does improve the retrieval quality significantly.</abstract>
		<doi>10.1109/TAAI.2018.00047</doi>
		<title>Personalized Content-Based Music Retrieval by User-Filtering and Query-Refinement</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25b9b3e0773beeb45c0c273bc5382c0e1/sapo</id>
		<tags>phdthesis</tags>
		<tags>synthesis</tags>
		<description>Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders http://proceedings.mlr.press/v70/engel17a.html</description>
		<date>2021-11-22 18:16:35</date>
		<count>4</count>
		<booktitle>Proceedings of the 34th International Conference on Machine Learning</booktitle>
		<series>Proceedings of Machine Learning Research</series>
		<publisher>PMLR</publisher>
		<year>2017</year>
		<url>https://proceedings.mlr.press/v70/engel17a.html</url>
		<author>Jesse Engel</author>
		<author>Cinjon Resnick</author>
		<author>Adam Roberts</author>
		<author>Sander Dieleman</author>
		<author>Mohammad Norouzi</author>
		<author>Douglas Eck</author>
		<author>Karen Simonyan</author>
		<authors>
			<first>Jesse</first>
		</authors>
		<authors>
			<last>Engel</last>
		</authors>
		<authors>
			<first>Cinjon</first>
		</authors>
		<authors>
			<last>Resnick</last>
		</authors>
		<authors>
			<first>Adam</first>
		</authors>
		<authors>
			<last>Roberts</last>
		</authors>
		<authors>
			<first>Sander</first>
		</authors>
		<authors>
			<last>Dieleman</last>
		</authors>
		<authors>
			<first>Mohammad</first>
		</authors>
		<authors>
			<last>Norouzi</last>
		</authors>
		<authors>
			<first>Douglas</first>
		</authors>
		<authors>
			<last>Eck</last>
		</authors>
		<authors>
			<first>Karen</first>
		</authors>
		<authors>
			<last>Simonyan</last>
		</authors>
		<editor>Doina Precup</editor>
		<editor>Yee Whye Teh</editor>
		<editors>
			<first>Karen</first>
		</editors>
		<editors>
			<last>Simonyan</last>
		</editors>
		<editors>
			<first>Karen</first>
		</editors>
		<editors>
			<last>Simonyan</last>
		</editors>
		<volume>70</volume>
		<pages>1068--1077</pages>
		<abstract>Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.</abstract>
		<pdf>http://proceedings.mlr.press/v70/engel17a/engel17a.pdf</pdf>
		<title>Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/254b4f2d29735a79764184bcbb5ca6f38/sapo</id>
		<tags>score-informed</tags>
		<description>A separate and restore approach to score-informed music decomposition | IEEE Conference Publication | IEEE Xplore https://ieeexplore.ieee.org/abstract/document/7336883</description>
		<date>2021-11-23 14:19:34</date>
		<count>1</count>
		<booktitle>2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</booktitle>
		<year>2015</year>
		<url>https://ieeexplore.ieee.org/abstract/document/7336883</url>
		<author>Christian Dittmar</author>
		<author>Jonathan Driedger</author>
		<author>Meinard Muller</author>
		<authors>
			<first>Christian</first>
		</authors>
		<authors>
			<last>Dittmar</last>
		</authors>
		<authors>
			<first>Jonathan</first>
		</authors>
		<authors>
			<last>Driedger</last>
		</authors>
		<authors>
			<first>Meinard</first>
		</authors>
		<authors>
			<last>Muller</last>
		</authors>
		<pages>1-5</pages>
		<abstract>Our goal is to improve the perceptual quality of signal components extracted in the context of music source separation. Specifically, we focus on decomposing polyphonic, mono-timbral piano recordings into the sound events that correspond to the individual notes of the underlying composition. Our separation technique is based on score-informed Non-Negative Matrix Factorization (NMF) that has been proposed in earlier works as an effective means to enforce a musically meaningful decomposition of piano music. However, the method still has certain shortcomings for complex mixtures where the tones strongly overlap in frequency and time. As the main contribution of this paper, we propose a restoration stage based on refined Wiener filter masks to score-informed NMF. Our idea is to introduce notewise soft masks created from a dictionary of perfectly isolated piano tones, which are then adapted to match the timbre of the target components. A basic experiment with mixtures of piano tones shows improvements of our novel reconstruction method with regard to perceptually motivated separation quality metrics. A second experiment with more complex piano recordings shows that further investigations into the concept are necessary for real-world applicability.</abstract>
		<doi>10.1109/WASPAA.2015.7336883</doi>
		<title>A separate and restore approach to score-informed music decomposition</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/256120a5f4563c044f6df5ff5385d6b03/anak</id>
		<tags>musicinformationretrieval</tags>
		<tags>musicretrievalsystems</tags>
		<tags>musicretrieval</tags>
		<description></description>
		<date>2021-04-05 11:17:34</date>
		<count>1</count>
		<year>2003</year>
		<url></url>
		<author>Stephen J. Downie</author>
		<authors>
			<first>Stephen J.</first>
		</authors>
		<authors>
			<last>Downie</last>
		</authors>
		<abstract>This paper outlines the findings-to-date of a project
to assist in the efforts being made to establish a
TREC-like evaluation paradigm within the Music
Information Retrieval (MIR) research community.
The findings and recommendations are based upon
expert opinion garnered from members of the
Information Retrieval (IR), Music Digital Library
(MDL) and MIR communities with regard to the
construction and implementation of scientifically
valid evaluation frameworks. Proposed
recommendations include the creation of data-rich
query records that are both grounded in real-world
requirements and neutral with respect to retrieval
technique(s) being examined; adoption, and
subsequent validation, of a “reasonable person”
approach to “relevance” assessment; and, the
development of a secure, yet accessible, research
environment that allows researchers to remotely
access the large-scale testbed collection.</abstract>
		<title>Toward the Scientific Evaluation of Music Information Retrieval Systems</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22cdc9e2774b9e63c265ac38def007e81/mediadigits</id>
		<tags>music</tags>
		<tags>use</tags>
		<description></description>
		<date>2011-03-10 16:04:49</date>
		<count>3</count>
		<booktitle>Proceedings of the 5th International Symposium on Music Information Retrieval</booktitle>
		<year>2004</year>
		<url></url>
		<author>S.J. Cunningham</author>
		<author>M. Jones</author>
		<author>S. Jones</author>
		<authors>
			<first>S.J.</first>
		</authors>
		<authors>
			<last>Cunningham</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Jones</last>
		</authors>
		<authors>
			<first>S.</first>
		</authors>
		<authors>
			<last>Jones</last>
		</authors>
		<title>Organizing digital music for use: an examination of personal music collections</title>
		<pubtype>conference</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2020dd1383391148a32f54347d5fd8eab/enricostano</id>
		<tags>music</tags>
		<tags>psychology</tags>
		<description></description>
		<date>2011-08-30 01:51:32</date>
		<count>1</count>
		<journal>Journal of Consciousness Studies</journal>
		<year>2006</year>
		<url>http://www.ingentaconnect.com/content/imp/jcs/2006/00000013/00000006/art00003</url>
		<author>Steven Brown</author>
		<authors>
			<first>Steven</first>
		</authors>
		<authors>
			<last>Brown</last>
		</authors>
		<volume>13</volume>
		<number>6</number>
		<pages>43-62</pages>
		<abstract>The perpetual music track is a new concept that describes a condition of constant or near-constant musical imagery. This condition appears to be very rare even among composers and musicians. I present here a detailed self-analysis of musical imagery for the purpose of defining the psychological features of a perpetual music track. I have music running through my head almost constantly during waking hours, consisting of a combination of recently-heard pieces and distant pieces that spontaneously pop into the head. Imagery consists mainly of short musical fragments that get looped repeatedly upon themselves. Corporeal manifestations of imagery occur in the form of unconscious finger movements whose patterns correspond to the melodic contour of the imagined piece. Musical dreams occur every week or two, and contain a combination of familiar and originally- composed music. These results are discussed in light of theories of imagery, consciousness, hallucination, obsessive cognition, and most especially the notion that acoustic consciousness can be split into multiple parallel streams.</abstract>
		<title>The Perpetual Music Track: The Phenomenon of Constant Musical Imagery</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/237dfa1581c9de0e63c85629d6d798f66/ocelma</id>
		<tags>imported</tags>
		<description></description>
		<date>2008-02-17 16:51:33</date>
		<count>4</count>
		<booktitle>Proceedings of 5th International Conference on Music Information Retrieval</booktitle>
		<address>Barcelona, Spain</address>
		<year>2004</year>
		<url></url>
		<author>E. Pampalk</author>
		<authors>
			<first>E.</first>
		</authors>
		<authors>
			<last>Pampalk</last>
		</authors>
		<abstract>A Matlab toolbox implementing music similarity measures 
      for audio is presented. The implemented measures focus on aspects 
      related to timbre and periodicities in the signal. This paper gives 
      an overview of the implemented functions. In particular, the basics 
      of the similarity measures are reviewed and some visualizations 
      are discussed.</abstract>
		<title>A Matlab Toolbox to Compute Music Similarity from Audio</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/249143afd75baf525e30c3a154ede2965/iglesia</id>
		<tags>music</tags>
		<tags>art</tags>
		<tags>toread</tags>
		<tags>fernleihe</tags>
		<tags>priorität:2</tags>
		<tags>presence</tags>
		<description></description>
		<date>2011-11-07 16:52:38</date>
		<count>1</count>
		<series>Pictographic index ; 2</series>
		<publisher>Pepin Press</publisher>
		<address>Amsterdam</address>
		<year>2011</year>
		<url></url>
		<author>Hans Lijklema</author>
		<authors>
			<first>Hans</first>
		</authors>
		<authors>
			<last>Lijklema</last>
		</authors>
		<volume>2</volume>
		<pages>320 S.</pages>
		<title>Design for music</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bf082668567a66f253b1bffb3442b040/snarc</id>
		<tags>imported</tags>
		<description>ISMIR 2007 - 8th International Conference on Music Information Retrieval</description>
		<date>2009-02-23 16:43:09</date>
		<count>6</count>
		<booktitle>8th International Conference on Music Information Retrieval (ISMIR 2007)</booktitle>
		<year>2007</year>
		<url>http://ismir2007.ismir.net/proceedings/ISMIR2007_p411_levy.pdf</url>
		<author>Mark Levy</author>
		<author>Mark Sandler</author>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Levy</last>
		</authors>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Sandler</last>
		</authors>
		<title>A Semantic Space for Music Derived from Social Tags</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25ef228fe2024e1eae9833d4312e774dc/ivan</id>
		<tags>music</tags>
		<tags>semantic_web</tags>
		<tags>ontology</tags>
		<description></description>
		<date>2009-02-19 13:28:52</date>
		<count>7</count>
		<address>Vienna, Austria</address>
		<year>2007</year>
		<url>http://moustaki.org/pubs/Raimond-ISMIR2007-Submitted.pdf</url>
		<author>Yves Raimond</author>
		<author>Samer Abdallah</author>
		<author>Mark Sandler</author>
		<author>Frederick Giasson</author>
		<authors>
			<first>Yves</first>
		</authors>
		<authors>
			<last>Raimond</last>
		</authors>
		<authors>
			<first>Samer</first>
		</authors>
		<authors>
			<last>Abdallah</last>
		</authors>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Sandler</last>
		</authors>
		<authors>
			<first>Frederick</first>
		</authors>
		<authors>
			<last>Giasson</last>
		</authors>
		<abstract>In this paper, we overview some Semantic Web technologies
allowing to create a web of data. We then detail the
Music Ontology: a formal framework for dealing with
music-related information on the Semantic Web, including
editorial, cultural and acoustic information. We detail
how this ontology can act as a grounding for more
domain-specific knowledge representation. In addition,
we describe current projects involving the Music Ontology
and interlinked repositories of music-related knowledge.</abstract>
		<comment>The guys forgot to mention me even in the acknowledgments part. A bit, well,... Also, first author should have been Frederick:-(</comment>
		<title>The Music Ontology</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d5b6376f0fe1a9c7daad5134bdb3f915/baltazhar</id>
		<tags>music</tags>
		<tags>tambora</tags>
		<description></description>
		<date>2008-10-09 01:40:21</date>
		<count>1</count>
		<journal>Theory Cultue Sociey</journal>
		<year>2001</year>
		<url>http://tcs.sagepub.com/cgi/reprint/18/5/1.pdf</url>
		<author>Antoine Henion</author>
		<authors>
			<first>Antoine</first>
		</authors>
		<authors>
			<last>Henion</last>
		</authors>
		<volume>18</volume>
		<number>5</number>
		<pages>1-22</pages>
		<abstract>This article presents the implications, objectives and initial results of a current ethnographic research project on music lovers. It looks at problems of theory and method posed by such research if it is not conceived only as the explanation of external determinisms, relating taste to the social origins of the amateur or to the aesthetic properties of the works. Our aim is, on the contrary, from long interviews and observations undertaken with music lovers, mostly in the classical field, to concentrate on gestures, objects, mediums, devices and relations engaged in a form of playing or listening, which amounts to more than the actualization of a taste `already there', for they are redefined during the action, with a result that is partly uncertain. This is why amateurs' attachments and ways of doing things can both engage and form subjectivities, rather than merely recording social labels, and have a history, irreducible to that of the taste for works.</abstract>
		<title>Music lovers: Taste as performance</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2daa312cc50ee2e53a48fe78f88586a8f/michi</id>
		<tags>master_thesis</tags>
		<description></description>
		<date>2008-12-17 12:56:28</date>
		<count>7</count>
		<booktitle>In 8th International Conference on Music Information Retrieval (ISMIR</booktitle>
		<year>2007</year>
		<url></url>
		<author>Douglas Turnbull</author>
		<author>Ruoran Liu</author>
		<author>Luke Barrington</author>
		<author>Gert Lanckriet</author>
		<authors>
			<first>Douglas</first>
		</authors>
		<authors>
			<last>Turnbull</last>
		</authors>
		<authors>
			<first>Ruoran</first>
		</authors>
		<authors>
			<last>Liu</last>
		</authors>
		<authors>
			<first>Luke</first>
		</authors>
		<authors>
			<last>Barrington</last>
		</authors>
		<authors>
			<first>Gert</first>
		</authors>
		<authors>
			<last>Lanckriet</last>
		</authors>
		<abstract>Games based on human computation are a valuable tool for collecting semantic information about images. We show how to transfer this idea into the music domain in order to collect high-quality semantic data for songs. We present Listen Game, a online, multiplayer game that measures the semantic relationship between music and words. In the normal mode, a player sees a list of semantically related words (e.g., ‘Instruments’, ‘Emotions ’ ‘Usages’) and is asked to pick the best and worst word to describe a song. In the freestyle mode, a user is asked to suggest a new word that describes the music. Each player receives realtime feedback on the agreement amongst all players. We show that we can use the data collected during a twoweek pilot study of Listen Game to learn a supervised multiclass labeling (SML) model which can annotate a novel song with meaningful words and retrieve relevant songs from a database of audio content.</abstract>
		<title>A game-based approach for collecting semantic annotations of music</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c3c9c42901ef397297fd9ca9618327e0/mediadigits</id>
		<tags>imported</tags>
		<tags>music</tags>
		<tags>algorithms</tags>
		<tags>similarity</tags>
		<tags>programming</tags>
		<description></description>
		<date>2009-11-26 15:54:55</date>
		<count>2</count>
		<address>Vienna, Austria</address>
		<year>2006</year>
		<url>http://www.ofai.at/~elias.pampalk/publications/pampalk06thesis.pdf</url>
		<author>Elias Pampalk</author>
		<authors>
			<first>Elias</first>
		</authors>
		<authors>
			<last>Pampalk</last>
		</authors>
		<abstract>This thesis aims at developing techniques which support users in accessing and discovering music. The main part consists of two chapters. Chapter 2 gives an introduction to computational models of music similarity. The combination of different approaches is optimized and the largest evaluation of music similarity measures published to date is presented. The best combination performs significantly better than the baseline approach in most of the evaluation categories. A particular effort is made to avoid overfitting. To cross-check the results from the evaluation based on genre classification a listening test is conducted. The test confirms that genre-based evaluations are suitable to efficiently evaluate large parameter spaces. Chapter 2 ends with recommendations on the use of similarity measures. Chapter 3 describes three applications of such similarity measures. The first application demonstrates how music collections can be organized and visualized so that users can control the aspect of similarity they are interested in. The second application demonstrates how music collections can be organized hierarchically into overlapping groups at the artist level. These groups are summarized using words from web pages associated with the respective artists. The third application demonstrates how playlists can be generated which require minimum user input.</abstract>
		<title>Computational Models of Music Similarity and their Application in Music Information Retrieval</title>
		<pubtype>phdthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23575ae1d84c3b86c5c888f81c0963e92/mediadigits</id>
		<tags>feedback</tags>
		<tags>music</tags>
		<tags>similarity</tags>
		<tags>machinelearning</tags>
		<description></description>
		<date>2009-11-25 14:36:53</date>
		<count>1</count>
		<booktitle>AMCMM '06: Proceedings of the 1st ACM workshop on Audio and music computing multimedia</booktitle>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2006</year>
		<url></url>
		<author>Kris West</author>
		<author>Stephen Cox</author>
		<author>Paul Lamere</author>
		<authors>
			<first>Kris</first>
		</authors>
		<authors>
			<last>West</last>
		</authors>
		<authors>
			<first>Stephen</first>
		</authors>
		<authors>
			<last>Cox</last>
		</authors>
		<authors>
			<first>Paul</first>
		</authors>
		<authors>
			<last>Lamere</last>
		</authors>
		<pages>89--96</pages>
		<isbn>1-59593-501-0</isbn>
		<doi>http://doi.acm.org/10.1145/1178723.1178737</doi>
		<title>Incorporating machine-learning into music similarity estimation</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/294d966b61d20932c1ac1df234d74ec44/yevb0</id>
		<tags>Uganda,amadinda,music</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>African Music</journal>
		<year>1970</year>
		<url>http://www.jstor.org/stable/30249682</url>
		<author>Peter Cooke</author>
		<authors>
			<first>Peter</first>
		</authors>
		<authors>
			<last>Cooke</last>
		</authors>
		<volume>4</volume>
		<number>4</number>
		<pages>62--80</pages>
		<title>Ganda xylophone music: Another approach</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e90f9891be35dd011a7da93fa1db9a05/yevb0</id>
		<tags>physiology,Electroencephalography,Electroencephalography:</tags>
		<tags>Variance,Auditory,Auditory:</tags>
		<tags>Stimulation,Acoustic</tags>
		<tags>Variation,Contingent</tags>
		<tags>Acoustic</tags>
		<tags>physiology,Speech</tags>
		<tags>Perception,Pitch</tags>
		<tags>methods,Adult,Analysis</tags>
		<tags>Adult,language,music,musicality,neuro,perception,pitch,tone</tags>
		<tags>Time,Reaction</tags>
		<tags>physiology,Contingent</tags>
		<tags>Time:</tags>
		<tags>Performance,Psychomotor</tags>
		<tags>Stimulation:</tags>
		<tags>Performance:</tags>
		<tags>methods,Evoked</tags>
		<tags>Perception,Speech</tags>
		<tags>Variation:</tags>
		<tags>Potentials,Female,Humans,L1,L2,Language,Male,Music,Phonetics,Pitch</tags>
		<tags>physiology,Psychomotor</tags>
		<tags>Perception:</tags>
		<tags>Negative</tags>
		<tags>physiology,Young</tags>
		<tags>of</tags>
		<tags>physiology,Reaction</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Brain and Language</journal>
		<year>2009</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/18343493</url>
		<author>Bharath Chandrasekaran</author>
		<author>Ananthanarayan Krishnan</author>
		<author>Jackson T. Gandour</author>
		<authors>
			<first>Bharath</first>
		</authors>
		<authors>
			<last>Chandrasekaran</last>
		</authors>
		<authors>
			<first>Ananthanarayan</first>
		</authors>
		<authors>
			<last>Krishnan</last>
		</authors>
		<authors>
			<first>Jackson T.</first>
		</authors>
		<authors>
			<last>Gandour</last>
		</authors>
		<volume>108</volume>
		<number>1</number>
		<pages>1--9</pages>
		<abstract>To assess domain specificity of experience-dependent pitch representation
	we evaluated the mismatch negativity (MMN) and discrimination judgments
	of English musicians, English nonmusicians, and native Chinese for
	pitch contours presented in a nonspeech context using a passive oddball
	paradigm. Stimuli consisted of homologues of Mandarin high rising
	(T2) and high level (T1) tones, and a linear rising ramp (T2L). One
	condition involved a between-category contrast (T1/T2), the other,
	a within-category contrast (T2L/T2). Irrespective of condition, musicians
	and Chinese showed larger MMN responses than nonmusicians; Chinese
	larger than musicians. Chinese, however, were less accurate than
	nonnatives in overt discrimination of T2L and T2. Taken together,
	these findings suggest that experience-dependent effects to pitch
	contours are domain-general and not driven by linguistic categories.
	Yet specific differences in long-term experience in pitch processing
	between domains (music vs. language) may lead to gradations in cortical
	plasticity to pitch contours.</abstract>
		<issn>1090-2155</issn>
		<doi>10.1016/j.bandl.2008.02.001</doi>
		<title>Relative influence of musical and linguistic experience on early
	cortical processing of pitch contours</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25057a01626d806a4123f60f20a342bda/yevb0</id>
		<tags>Animal</tags>
		<tags>Communication,Animals,Archaeology,Birds,Evolution,Genetic,Humans,Language,Mammals,Music,Selection,Sociobiology,evolution,music,neuro</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Cognition</journal>
		<year>2006</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/16412411</url>
		<author>W Tecumseh Fitch</author>
		<authors>
			<first>W Tecumseh</first>
		</authors>
		<authors>
			<last>Fitch</last>
		</authors>
		<volume>100</volume>
		<number>1</number>
		<pages>173--215</pages>
		<abstract>Studies of the biology of music (as of language) are highly interdisciplinary
	and demand the integration of diverse strands of evidence. In this
	paper, I present a comparative perspective on the biology and evolution
	of music, stressing the value of comparisons both with human language,
	and with those animal communication systems traditionally termed
	"song". A comparison of the "design features" of music with those
	of language reveals substantial overlap, along with some important
	differences. Most of these differences appear to stem from semantic,
	rather than structural, factors, suggesting a shared formal core
	of music and language. I next review various animal communication
	systems that appear related to human music, either by analogy (bird
	and whale "song") or potential homology (great ape bimanual drumming).
	A crucial comparative distinction is between learned, complex signals
	(like language, music and birdsong) and unlearned signals (like laughter,
	ape calls, or bird calls). While human vocalizations clearly build
	upon an acoustic and emotional foundation shared with other primates
	and mammals, vocal learning has evolved independently in our species
	since our divergence with chimpanzees. The convergent evolution of
	vocal learning in other species offers a powerful window into psychological
	and neural constraints influencing the evolution of complex signaling
	systems (including both song and speech), while ape drumming presents
	a fascinating potential homology with human instrumental music. I
	next discuss the archeological data relevant to music evolution,
	concluding on the basis of prehistoric bone flutes that instrumental
	music is at least 40,000 years old, and perhaps much older. I end
	with a brief review of adaptive functions proposed for music, concluding
	that no one selective force (e.g., sexual selection) is adequate
	to explaining all aspects of human music. I suggest that questions
	about the past function of music are unlikely to be answered definitively
	and are thus a poor choice as a research focus for biomusicology.
	In contrast, a comparative approach to music promises rich dividends
	for our future understanding of the biology and evolution of music.</abstract>
		<issn>0010-0277</issn>
		<doi>10.1016/j.cognition.2005.11.009</doi>
		<title>The biology and evolution of music: a comparative perspective</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2902419ccc37f479b7d794f4542f46527/yevb0</id>
		<tags>English,British</tags>
		<tags>English,English,L1,acquisition,interval,language,music,perception,pitch,scale</tags>
		<tags>American</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Music Perception</journal>
		<year>1991</year>
		<url></url>
		<author>Diana Deutsch</author>
		<authors>
			<first>Diana</first>
		</authors>
		<authors>
			<last>Deutsch</last>
		</authors>
		<volume>8</volume>
		<pages>335--347</pages>
		<title>The tritone paradox: An influence of language on music perception</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29754b831b6ae35247150f8dbd0fedad6/yevb0</id>
		<tags>perception,interval,melody,music,perception</tags>
		<tags>Humans,Judgment,Music,Pitch</tags>
		<tags>Time,categorical</tags>
		<tags>(Psychology),Reaction</tags>
		<tags>Discrimination,Practice</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>The Journal of the Acoustical Society of America</journal>
		<year>1978</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/670543</url>
		<author>Edward M. Burns</author>
		<author>W. Dixon Ward</author>
		<authors>
			<first>Edward M.</first>
		</authors>
		<authors>
			<last>Burns</last>
		</authors>
		<authors>
			<first>W. Dixon</first>
		</authors>
		<authors>
			<last>Ward</last>
		</authors>
		<volume>63</volume>
		<number>2</number>
		<pages>456--68</pages>
		<abstract>Categorical perception was investigated in a series of experiments
	on the perception of melodic musical intervals (sequential frequency
	ratios). When procedures equivalent to those typically used in speech-perception
	experiments were employed, i.e., determination of identification
	and discrimination functions for stimuli separated by equal physical
	increments), musical intervals were perceived categorically by trained
	musicians. When a variable-step-size (adaptive) discrimination procedure
	was used, evidence of categorical perception (in the form of smaller
	interval-width DL's for ratios at identification category boundaries
	than for ratios within categories), although present initially, largely
	disappeared after subjects had reached asymptotic performance. However,
	equal-step-size discrimination functions obtained after observers
	had reached asymptotic performance in the adaptive paradigm were
	not substantially different from those initially obtained. The results
	of other experiments imply that this dependence of categorical perception
	on procedure may be related to differences in stimulus uncertainty
	between the procedures. An experiment on the perception of melodic
	intervals by musically untrained observers showed no evidence for
	the existence of "natural" categories for musical intervals.</abstract>
		<issn>0001-4966</issn>
		<title>Categorical perception--phenomenon or epiphenomenon: evidence from
	experiments in the perception of melodic musical intervals</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25903dda29bfbffde9b8ba295395507e5/yevb0</id>
		<tags>psychology,Pattern</tags>
		<tags>Science,Communication,Emotions,Humans,Models,Motion</tags>
		<tags>Perception,Cognition,Cognitive</tags>
		<tags>Auditory</tags>
		<tags>Perception,Music,Music:</tags>
		<tags>Recognition,Physiological,Psychoacoustics,Psychological</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Cognition</journal>
		<year>2006</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/16412410</url>
		<author>Jamshed J Bharucha</author>
		<author>Meagan Curtis</author>
		<author>Kaivon Paroo</author>
		<authors>
			<first>Jamshed J</first>
		</authors>
		<authors>
			<last>Bharucha</last>
		</authors>
		<authors>
			<first>Meagan</first>
		</authors>
		<authors>
			<last>Curtis</last>
		</authors>
		<authors>
			<first>Kaivon</first>
		</authors>
		<authors>
			<last>Paroo</last>
		</authors>
		<volume>100</volume>
		<number>1</number>
		<pages>131--72</pages>
		<abstract>In this paper, we argue that music cognition involves the use of acoustic
	and auditory codes to evoke a variety of conscious experiences. The
	variety of domains that are encompassed by music is so diverse that
	it is unclear whether a single domain of structure or experience
	is defining. Music is best understood as a form of communication
	in which formal codes (acoustic patterns and their auditory representations)
	are employed to elicit a variety of conscious experiences. After
	proposing our theoretical perspective we offer three prominent examples
	of conscious experiences elicited by the code of music: the recognition
	of structure itself, affect, and the experience of motion.</abstract>
		<issn>0010-0277</issn>
		<doi>10.1016/j.cognition.2005.11.008</doi>
		<title>Varieties of musical experience</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ccb631ae6dc4d96a36f33d35a0970933/yevb0</id>
		<tags>Development,Child</tags>
		<tags>Behavior,Social</tags>
		<tags>(Psychology),Emotions,Humans,Infant,Music,Social</tags>
		<tags>Perception,Auditory</tags>
		<tags>Psychology,Discrimination</tags>
		<tags>Perception:</tags>
		<tags>psychology,acquisition,music,musicality</tags>
		<tags>Animals,Auditory</tags>
		<tags>Environment,Triplets,Triplets:</tags>
		<tags>Development:</tags>
		<tags>physiology,Child</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Nature Neuroscience</journal>
		<year>2003</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/12830157</url>
		<author>Sandra E. Trehub</author>
		<authors>
			<first>Sandra E.</first>
		</authors>
		<authors>
			<last>Trehub</last>
		</authors>
		<volume>6</volume>
		<number>7</number>
		<pages>669--73</pages>
		<abstract>The study of musical abilities and activities in infancy has the potential
	to shed light on musical biases or dispositions that are rooted in
	nature rather than nurture. The available evidence indicates that
	infants are sensitive to a number of sound features that are fundamental
	to music across cultures. Their discrimination of pitch and timing
	differences and their perception of equivalence classes are similar,
	in many respects, to those of listeners who have had many years of
	exposure to music. Whether these perceptual skills are unique to
	human listeners is not known. What is unique is the intense human
	interest in music, which is evident from the early days of life.
	Also unique is the importance of music in social contexts. Current
	ideas about musical timing and interpersonal synchrony are considered
	here, along with proposals for future research.</abstract>
		<issn>1097-6256</issn>
		<doi>10.1038/nn1084</doi>
		<title>The developmental origins of musicality</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2034855777a167b387c09eb75ae13b20e/yevb0</id>
		<tags>L2,language,music,musicality,neuro</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>Proceedings of the 7th Triennial Conference of European Society for
	the Cognitive Sciences of Music</booktitle>
		<address>Jyväskylä, Finland</address>
		<year>2009</year>
		<url></url>
		<author>Riia Milovanov</author>
		<authors>
			<first>Riia</first>
		</authors>
		<authors>
			<last>Milovanov</last>
		</authors>
		<editor>Jukka Louhivuori</editor>
		<editor>Tuomas Eerola</editor>
		<editor>Suvi Saarikallio</editor>
		<editor>Tommi Himberg</editor>
		<editor>Päivi-Sisko Eerola</editor>
		<editors>
			<first>Riia</first>
		</editors>
		<editors>
			<last>Milovanov</last>
		</editors>
		<editors>
			<first>Riia</first>
		</editors>
		<editors>
			<last>Milovanov</last>
		</editors>
		<editors>
			<first>Riia</first>
		</editors>
		<editors>
			<last>Milovanov</last>
		</editors>
		<editors>
			<first>Riia</first>
		</editors>
		<editors>
			<last>Milovanov</last>
		</editors>
		<editors>
			<first>Riia</first>
		</editors>
		<editors>
			<last>Milovanov</last>
		</editors>
		<number>Escom</number>
		<pages>338--342</pages>
		<title>Musical aptitude and foreign language learning skills: Neural and
	behavioural evidence about their connections</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/265f2be8e1a694273ab6e8333ff1813b0/yevb0</id>
		<tags>methods,Evoked</tags>
		<tags>Acoustic</tags>
		<tags>psychology,Neocortex,Neocortex:</tags>
		<tags>\&</tags>
		<tags>physiology,Preschool,Radiation,acquisition,music,musicality,neuro</tags>
		<tags>Potentials,Female,Humans,Life</tags>
		<tags>Relationship,Electroencephalography,Electroencephalography:</tags>
		<tags>Change</tags>
		<tags>development,Neocortex:</tags>
		<tags>Events,Male,Music,Music:</tags>
		<tags>Mapping,Case-Control</tags>
		<tags>Studies,Dose-Response</tags>
		<tags>growth</tags>
		<tags>Stimulation,Age</tags>
		<tags>physiology,Brain</tags>
		<tags>Factors,Auditory,Auditory:</tags>
		<tags>Studies,Child,Cross-Sectional</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Neuroreport</journal>
		<year>2004</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/15305137</url>
		<author>Antoine Shahin</author>
		<author>Larry E Roberts</author>
		<author>Laurel J. Trainor</author>
		<authors>
			<first>Antoine</first>
		</authors>
		<authors>
			<last>Shahin</last>
		</authors>
		<authors>
			<first>Larry E</first>
		</authors>
		<authors>
			<last>Roberts</last>
		</authors>
		<authors>
			<first>Laurel J.</first>
		</authors>
		<authors>
			<last>Trainor</last>
		</authors>
		<volume>15</volume>
		<number>12</number>
		<pages>1917--21</pages>
		<abstract>Auditory evoked potentials (AEPs) express the development of mature
	synaptic connections in the upper neocortical laminae known to occur
	between 4 and 15 years of age. AEPs evoked by piano, violin, and
	pure tones were measured twice in a group of 4- to 5-year-old children
	enrolled in Suzuki music lessons and in non-musician controls. P1
	was larger in the Suzuki pupils for all tones whereas P2 was enhanced
	specifically for the instrument of practice (piano or violin). AEPs
	observed for the instrument of practice were comparable to those
	of non-musician children about 3 years older in chronological age.
	The findings set into relief a general process by which the neocortical
	synaptic matrix is shaped by an accumulation of specific auditory
	experiences.</abstract>
		<issn>0959-4965</issn>
		<title>Enhancement of auditory cortical development by musical experience
	in children</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bc126eafaa410954f4626dc557a9f8ac/yevb0</id>
		<tags>Analysis,Time</tags>
		<tags>Perceptual</tags>
		<tags>Potentials,Female,Functional</tags>
		<tags>Stimulation,Acoustic</tags>
		<tags>physiology,Auditory</tags>
		<tags>Acoustic</tags>
		<tags>Factors,language,modularity,music,perception</tags>
		<tags>Variance,Auditory,Auditory</tags>
		<tags>Recall,Mental</tags>
		<tags>complications,Male,Mental</tags>
		<tags>Laterality,Humans,Intelligence,Language,Language</tags>
		<tags>Disorders,Auditory</tags>
		<tags>Mapping,Child,Electroencephalography,Electroencephalography:</tags>
		<tags>physiology,Music,Preschool,Principal</tags>
		<tags>methods,Analysis</tags>
		<tags>Stimulation:</tags>
		<tags>Disorders:</tags>
		<tags>methods,Evoked</tags>
		<tags>complications,Brain</tags>
		<tags>Recall:</tags>
		<tags>Perception,Auditory</tags>
		<tags>Disorders,Language</tags>
		<tags>Perception:</tags>
		<tags>of</tags>
		<tags>Development</tags>
		<tags>Component</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>2</count>
		<journal>Journal of Cognitive Neuroscience</journal>
		<year>2008</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/18416683</url>
		<author>Sebastian Jentschke</author>
		<author>Stefan Koelsch</author>
		<author>Stephan Sallat</author>
		<author>Angela D. Friederici</author>
		<authors>
			<first>Sebastian</first>
		</authors>
		<authors>
			<last>Jentschke</last>
		</authors>
		<authors>
			<first>Stefan</first>
		</authors>
		<authors>
			<last>Koelsch</last>
		</authors>
		<authors>
			<first>Stephan</first>
		</authors>
		<authors>
			<last>Sallat</last>
		</authors>
		<authors>
			<first>Angela D.</first>
		</authors>
		<authors>
			<last>Friederici</last>
		</authors>
		<volume>20</volume>
		<number>11</number>
		<pages>1940--51</pages>
		<abstract>Both language and music consist of sequences that are structured according
	to syntactic regularities. We used two specific event-related brain
	potential (ERP) components to investigate music-syntactic processing
	in children: the ERAN (early right anterior negativity) and the N5.
	The neural resources underlying these processes have been posited
	to overlap with those involved in the processing of linguistic syntax.
	Thus, we expected children with specific language impairment (SLI,
	which is characterized by deficient processing of linguistic syntax)
	to demonstrate difficulties with music-syntactic processing. Such
	difficulties were indeed observed in the neural correlates of music-syntactic
	processing: neither an ERAN nor an N5 was elicited in children with
	SLI, whereas both components were evoked in age-matched control children
	with typical language development. Moreover, the amplitudes of ERAN
	and N5 were correlated with subtests of a language development test.
	These data provide evidence for a strong interrelation between the
	language and the music processing system, thereby setting the ground
	for possible effects of musical training in SLI therapy.</abstract>
		<issn>0898-929X</issn>
		<doi>10.1162/jocn.2008.20135</doi>
		<title>Children with specific language impairment also show impairment of
	music-syntactic processing</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23e04e4a985c56de5c9c3b9903aff8484/yevb0</id>
		<tags>physiology,Time</tags>
		<tags>Adult,language,music,musicality,neuro,perception,speech</tags>
		<tags>Acoustic</tags>
		<tags>Factors,Young</tags>
		<tags>Stimulation,Adult,Analysis</tags>
		<tags>Variance,Auditory,Auditory</tags>
		<tags>Perception,Speech</tags>
		<tags>Stem,Brain</tags>
		<tags>Perception,Auditory</tags>
		<tags>Perception:</tags>
		<tags>Acoustics,Speech</tags>
		<tags>of</tags>
		<tags>physiology,Evoked</tags>
		<tags>Stem:</tags>
		<tags>Tests,Humans,Male,Music,Noise,Psychoacoustics,Speech</tags>
		<tags>Potentials,Female,Hearing</tags>
		<tags>physiology,Brain</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Journal of Neuroscience</journal>
		<year>2009</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/19906958</url>
		<author>Alexandra Parbery-Clark</author>
		<author>Erika Skoe</author>
		<author>Nina Kraus</author>
		<authors>
			<first>Alexandra</first>
		</authors>
		<authors>
			<last>Parbery-Clark</last>
		</authors>
		<authors>
			<first>Erika</first>
		</authors>
		<authors>
			<last>Skoe</last>
		</authors>
		<authors>
			<first>Nina</first>
		</authors>
		<authors>
			<last>Kraus</last>
		</authors>
		<volume>29</volume>
		<number>45</number>
		<pages>14100--7</pages>
		<abstract>Musicians have lifelong experience parsing melodies from background
	harmonies, which can be considered a process analogous to speech
	perception in noise. To investigate the effect of musical experience
	on the neural representation of speech-in-noise, we compared subcortical
	neurophysiological responses to speech in quiet and noise in a group
	of highly trained musicians and nonmusician controls. Musicians were
	found to have a more robust subcortical representation of the acoustic
	stimulus in the presence of noise. Specifically, musicians demonstrated
	faster neural timing, enhanced representation of speech harmonics,
	and less degraded response morphology in noise. Neural measures were
	associated with better behavioral performance on the Hearing in Noise
	Test (HINT) for which musicians outperformed the nonmusician controls.
	These findings suggest that musical experience limits the negative
	effects of competing background noise, thereby providing the first
	biological evidence for musicians' perceptual advantage for speech-in-noise.</abstract>
		<issn>1529-2401</issn>
		<doi>10.1523/JNEUROSCI.3256-09.2009</doi>
		<title>Musical experience limits the degradative effects of background noise
	on the neural processing of sound</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2924d2683e666db91041683d8c13c1f15/yevb0</id>
		<tags>Skills,Music,Neuronal</tags>
		<tags>Plasticity,Prospective</tags>
		<tags>anatomy</tags>
		<tags>Adult,Cerebellum,Cerebellum:</tags>
		<tags>histology,Female,Humans,Learning,Magnetic</tags>
		<tags>Resonance</tags>
		<tags>\&</tags>
		<tags>Studies,Sex</tags>
		<tags>Characteristics,music,musicality,neuro</tags>
		<tags>Imaging,Male,Motor</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Cerebral Cortex</journal>
		<year>2003</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/12902393</url>
		<author>Siobhan Hutchinson</author>
		<author>Leslie Hui-Lin Lee</author>
		<author>Nadine Gaab</author>
		<author>Gottfried Schlaug</author>
		<authors>
			<first>Siobhan</first>
		</authors>
		<authors>
			<last>Hutchinson</last>
		</authors>
		<authors>
			<first>Leslie Hui-Lin</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Nadine</first>
		</authors>
		<authors>
			<last>Gaab</last>
		</authors>
		<authors>
			<first>Gottfried</first>
		</authors>
		<authors>
			<last>Schlaug</last>
		</authors>
		<volume>13</volume>
		<number>9</number>
		<pages>943--9</pages>
		<abstract>There is evidence that the cerebellum is involved in motor learning
	and cognitive function in humans. Animal experiments have found structural
	changes in the cerebellum in response to long-term motor skill activity.
	We investigated whether professional keyboard players, who learn
	specialized motor skills early in life and practice them intensely
	throughout life, have larger cerebellar volumes than matched non-musicians
	by analyzing high-resolution T(1)-weighted MR images from a large
	prospectively acquired database (n = 120). Significantly greater
	absolute (P = 0.018) and relative (P = 0.006) cerebellar volume but
	not total brain volume was found in male musicians compared to male
	non-musicians. Lifelong intensity of practice correlated with relative
	cerebellar volume in the male musician group (r = 0.595, P = 0.001).
	In the female group, there was no significant difference noted in
	volume measurements between musicians and non-musicians. The significant
	main effect for gender on relative cerebellar volume (F = 10.41,
	P < 0.01), with females having a larger relative cerebellar volume,
	may mask the effect of musicianship in the female group. We propose
	that the significantly greater cerebellar volume in male musicians
	and the positive correlation between relative cerebellar volume and
	lifelong intensity of practice represents structural adaptation to
	long-term motor and cognitive functional demands in the human cerebellum.</abstract>
		<issn>1047-3211</issn>
		<title>Cerebellar volume of musicians</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28d57a2f3e4fe5ee6f2137d446b49d7e2/yevb0</id>
		<tags>Cortex:</tags>
		<tags>Adult,Auditory</tags>
		<tags>physiology,Auditory</tags>
		<tags>Perception,Auditory</tags>
		<tags>Perception:</tags>
		<tags>Cortex,Auditory</tags>
		<tags>Discrimination,music,musicality,neuro,plasticity</tags>
		<tags>physiology,Humans,Magnetoencephalography,Music,Occupations,Pitch</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Nature</journal>
		<year>1998</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/9572139</url>
		<author>Christo Pantev</author>
		<author>R Oostenveld</author>
		<author>A Engelien</author>
		<author>Bernhard Ross</author>
		<author>Larry E Roberts</author>
		<author>M Hoke</author>
		<authors>
			<first>Christo</first>
		</authors>
		<authors>
			<last>Pantev</last>
		</authors>
		<authors>
			<first>R</first>
		</authors>
		<authors>
			<last>Oostenveld</last>
		</authors>
		<authors>
			<first>A</first>
		</authors>
		<authors>
			<last>Engelien</last>
		</authors>
		<authors>
			<first>Bernhard</first>
		</authors>
		<authors>
			<last>Ross</last>
		</authors>
		<authors>
			<first>Larry E</first>
		</authors>
		<authors>
			<last>Roberts</last>
		</authors>
		<authors>
			<first>M</first>
		</authors>
		<authors>
			<last>Hoke</last>
		</authors>
		<volume>392</volume>
		<number>6678</number>
		<pages>811--814</pages>
		<abstract>Acoustic stimuli are processed throughout the auditory projection
	pathway, including the neocortex, by neurons that are aggregated
	into 'tonotopic' maps according to their specific frequency tunings.
	Research on animals has shown that tonotopic representations are
	not statically fixed in the adult organism but can reorganize after
	damage to the cochlea or after training the intact subject to discriminate
	between auditory stimuli. Here we used functional magnetic source
	imaging (single dipole model) to measure cortical representations
	in highly skilled musicians. Dipole moments for piano tones, but
	not for pure tones of similar fundamental frequency (matched in loudness),
	were found to be enlarged by about 25\% in musicians compared with
	control subjects who had never played an instrument. Enlargement
	was correlated with the age at which musicians began to practise
	and did not differ between musicians with absolute or relative pitch.
	These results, when interpreted with evidence for modified somatosensory
	representations of the fingering digits in skilled violinists, suggest
	that use-dependent functional reorganization extends across the sensory
	cortices to reflect the pattern of sensory input processed by the
	subject during development of musical skill.</abstract>
		<issn>0028-0836</issn>
		<doi>10.1038/33918</doi>
		<title>Increased auditory cortical representation in musicians</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/299dbd1dd6d21fcb200f0c6b5053c7b93/yevb0</id>
		<tags>language,music,perception,stress</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Music Perception</journal>
		<year>2009</year>
		<url>http://caliber.ucpress.net/doi/abs/10.1525/mp.2009.26.3.235</url>
		<author>Régine Kolinsky</author>
		<author>Héléne Cuvelier</author>
		<author>Vincent Goetry</author>
		<author>Isabelle Peretz</author>
		<author>José Morais</author>
		<authors>
			<first>Régine</first>
		</authors>
		<authors>
			<last>Kolinsky</last>
		</authors>
		<authors>
			<first>Héléne</first>
		</authors>
		<authors>
			<last>Cuvelier</last>
		</authors>
		<authors>
			<first>Vincent</first>
		</authors>
		<authors>
			<last>Goetry</last>
		</authors>
		<authors>
			<first>Isabelle</first>
		</authors>
		<authors>
			<last>Peretz</last>
		</authors>
		<authors>
			<first>José</first>
		</authors>
		<authors>
			<last>Morais</last>
		</authors>
		<volume>26</volume>
		<number>3</number>
		<pages>235--246</pages>
		<issn>0730-7829</issn>
		<doi>10.1525/mp.2009.26.3.235</doi>
		<title>Music training facilitates lexical stress processing</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2611517d7b0d3382835e691c9306b3563/yevb0</id>
		<tags>reasoning</tags>
		<tags>Attention,Child,Computers,Education,Female,Humans,Male,Music,Music:</tags>
		<tags>Scales,acquisition,music,musicality,spatial</tags>
		<tags>Performance,Space</tags>
		<tags>Perception,Thinking,Wechsler</tags>
		<tags>psychology,Preschool,Psychomotor</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Neurological Research</journal>
		<year>1997</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/9090630</url>
		<author>F H Rauscher</author>
		<author>G L Shaw</author>
		<author>L J Levine</author>
		<author>E L Wright</author>
		<author>W R Dennis</author>
		<author>R L Newcomb</author>
		<authors>
			<first>F H</first>
		</authors>
		<authors>
			<last>Rauscher</last>
		</authors>
		<authors>
			<first>G L</first>
		</authors>
		<authors>
			<last>Shaw</last>
		</authors>
		<authors>
			<first>L J</first>
		</authors>
		<authors>
			<last>Levine</last>
		</authors>
		<authors>
			<first>E L</first>
		</authors>
		<authors>
			<last>Wright</last>
		</authors>
		<authors>
			<first>W R</first>
		</authors>
		<authors>
			<last>Dennis</last>
		</authors>
		<authors>
			<first>R L</first>
		</authors>
		<authors>
			<last>Newcomb</last>
		</authors>
		<volume>19</volume>
		<number>1</number>
		<pages>2--8</pages>
		<abstract>Predictions from a structured cortical model led us to test the hypothesis
	that music training enhances young children's spatial-temporal reasoning.
	Seventy-eight preschool children participated in this study. Thirty-four
	children received private piano keyboard lessons, 20 children received
	private computer lessons, and 24 children provided other controls.
	Four standard, age-calibrated, spatial reasoning tests were given
	before and after training; one test assessed spatial-temporal reasoning
	and three tests assessed spatial recognition. Significant improvement
	on the spatial-temporal test was found for the keyboard group only.
	No group improved significantly on the spatial recognition tests.
	The magnitude of the spatial-temporal improvement from keyboard training
	was greater than one standard deviation of the standardized test
	and lasted at least one day, a duration traditionally classified
	as long term. This represents an increase in time by a factor of
	over 100 compared to a previous study in which listening to a Mozart
	piano sonata primed spatial-temporal reasoning in college students.
	This suggests that music training produces long-term modifications
	in underlying neural circuitry in regions not primarily concerned
	with music and might be investigated using EEG. We propose that an
	improvement of the magnitude reported may enhance the learning of
	standard curricula, such as mathematics and science, that draw heavily
	upon spatial-temporal reasoning.</abstract>
		<issn>0161-6412</issn>
		<title>Music training causes long-term enhancement of preschool children's
	spatial-temporal reasoning</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25bd511485f36870ee7ba207529de8072/yevb0</id>
		<tags>no</tags>
		<tags>agreement</tags>
		<tags>a,auditory</tags>
		<tags>timing,music,musicality,perception,rhythm,rhythm</tags>
		<tags>clear</tags>
		<tags>is</tags>
		<tags>processing,interval</tags>
		<tags>structure</tags>
		<tags>fusion,dimensional</tags>
		<tags>of</tags>
		<tags>perception,there</tags>
		<tags>temporal,information</tags>
		<tags>on</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Music Perception</journal>
		<year>2006</year>
		<url>http://caliber.ucpress.net/doi/abs/10.1525/mp.2006.24.1.37</url>
		<author>Thomas Rammsayer</author>
		<author>Eckart O Altenmüller</author>
		<authors>
			<first>Thomas</first>
		</authors>
		<authors>
			<last>Rammsayer</last>
		</authors>
		<authors>
			<first>Eckart O</first>
		</authors>
		<authors>
			<last>Altenmüller</last>
		</authors>
		<volume>24</volume>
		<number>1</number>
		<pages>37--48</pages>
		<issn>0730-7829</issn>
		<doi>10.1525/mp.2006.24.1.37</doi>
		<title>Temporal information processing in musicians and nonmusicians</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29a7d00374086e31849a441b4732a46eb/yevb0</id>
		<tags>physiology,Cross-Cultural</tags>
		<tags>Sami,melody,music,nonwestern,perception,tonality,typology</tags>
		<tags>Adult,Cognition,Cognition:</tags>
		<tags>Comparison,Culture,Female,Finland,Humans,M1,Male,Music,North</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Cognition</journal>
		<year>2000</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/10822042</url>
		<author>Carol L Krumhansl</author>
		<author>Petri Toiviainen</author>
		<author>T Eerola</author>
		<author>T Järvinen</author>
		<author>J Louhivuori</author>
		<authors>
			<first>Carol L</first>
		</authors>
		<authors>
			<last>Krumhansl</last>
		</authors>
		<authors>
			<first>Petri</first>
		</authors>
		<authors>
			<last>Toiviainen</last>
		</authors>
		<authors>
			<first>T</first>
		</authors>
		<authors>
			<last>Eerola</last>
		</authors>
		<authors>
			<first>T</first>
		</authors>
		<authors>
			<last>Järvinen</last>
		</authors>
		<authors>
			<first>J</first>
		</authors>
		<authors>
			<last>Louhivuori</last>
		</authors>
		<volume>76</volume>
		<number>1</number>
		<pages>13--58</pages>
		<abstract>This article is a study of melodic expectancy in North Sami yoiks,
	a style of music quite distinct from Western tonal music. Three different
	approaches were taken. The first approach was a statistical style
	analysis of tones in a representative corpus of 18 yoiks. The analysis
	determined the relative frequencies of tone onsets and two- and three-tone
	transitions. It also identified style characteristics, such as pentatonic
	orientation, the presence of two reference pitches, the frequency
	of large consonant intervals, and a relatively large set of possible
	melodic continuations. The second approach was a behavioral experiment
	in which listeners made judgments about melodic continuations. Three
	groups of listeners participated. One group was from the Sami culture,
	the second group consisted of Finnish music students who had learned
	some yoiks, and the third group consisted of Western musicians unfamiliar
	with yoiks. Expertise was associated with stronger veridical expectations
	(for the correct next tone) than schematic expectations (based on
	general style characteristics). Familiarity with the particular yoiks
	was found to compensate for lack of experience with the musical culture.
	The third approach simulated melodic expectancy with neural network
	models of the self-organizing map (SOM) type (Kohonen, T. (1997).
	Self-organizing maps (2nd ed.). Berlin: Springer). One model was
	trained on the excerpts of yoiks used in the behavioral experiment
	including the correct continuation tone, while another was trained
	with a set of Finnish folk songs and Lutheran hymns. The convergence
	of the three approaches showed that both listeners and the SOM model
	are influenced by the statistical distributions of tones and tone
	sequences. The listeners and SOM models also provided evidence supporting
	a core set of psychological principles underlying melody formation
	whose relative weights appear to differ across musical styles.</abstract>
		<issn>0010-0277</issn>
		<title>Cross-cultural music cognition: cognitive methodology applied to
	North Sami yoiks</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/234e1152b95f6a18ce26fa8a8d628bd00/yevb0</id>
		<tags>Variation,Psychomotor</tags>
		<tags>Laterality:</tags>
		<tags>Acoustic</tags>
		<tags>physiology,Recognition</tags>
		<tags>Stimulation,Adolescent,Adult,Auditory</tags>
		<tags>Plasticity:</tags>
		<tags>Adult,music,musicality,neuro,perception</tags>
		<tags>Circulation:</tags>
		<tags>Cortex:</tags>
		<tags>psychology,Neuronal</tags>
		<tags>Mapping,Cerebral</tags>
		<tags>physiology,Female,Functional</tags>
		<tags>Resonance</tags>
		<tags>histology,Cerebral</tags>
		<tags>Plasticity,Neuronal</tags>
		<tags>(Psychology),Recognition</tags>
		<tags>physiology,Brain</tags>
		<tags>Performance,Psychomotor</tags>
		<tags>Aged,Music,Music:</tags>
		<tags>Performance:</tags>
		<tags>anatomy</tags>
		<tags>Circulation,Cerebrovascular</tags>
		<tags>physiology,Neuropsychological</tags>
		<tags>Tests,Observer</tags>
		<tags>physiology,Magnetic</tags>
		<tags>\&</tags>
		<tags>(Psychology):</tags>
		<tags>physiology,Humans,Learning,Learning:</tags>
		<tags>Perception,Auditory</tags>
		<tags>Perception:</tags>
		<tags>Imaging,Male,Middle</tags>
		<tags>physiology,Young</tags>
		<tags>Cortex,Cerebral</tags>
		<tags>physiology,Cerebrovascular</tags>
		<tags>Laterality,Functional</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Human Brain Mapping</journal>
		<year>2009</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/18072277</url>
		<author>Elizabeth Hellmuth Margulis</author>
		<author>Lauren M Mlsna</author>
		<author>Ajith K Uppunda</author>
		<author>Todd B Parrish</author>
		<author>Patrick C.M. Wong</author>
		<authors>
			<first>Elizabeth Hellmuth</first>
		</authors>
		<authors>
			<last>Margulis</last>
		</authors>
		<authors>
			<first>Lauren M</first>
		</authors>
		<authors>
			<last>Mlsna</last>
		</authors>
		<authors>
			<first>Ajith K</first>
		</authors>
		<authors>
			<last>Uppunda</last>
		</authors>
		<authors>
			<first>Todd B</first>
		</authors>
		<authors>
			<last>Parrish</last>
		</authors>
		<authors>
			<first>Patrick C.M.</first>
		</authors>
		<authors>
			<last>Wong</last>
		</authors>
		<volume>30</volume>
		<number>1</number>
		<pages>267--75</pages>
		<abstract>To appropriately adapt to constant sensory stimulation, neurons in
	the auditory system are tuned to various acoustic characteristics,
	such as center frequencies, frequency modulations, and their combinations,
	particularly those combinations that carry species-specific communicative
	functions. The present study asks whether such tunings extend beyond
	acoustic and communicative functions to auditory self-relevance and
	expertise. More specifically, we examined the role of the listening
	biography--an individual's long term experience with a particular
	type of auditory input--on perceptual-neural plasticity. Two groups
	of expert instrumentalists (violinists and flutists) listened to
	matched musical excerpts played on the two instruments (J.S. Bach
	Partitas for solo violin and flute) while their cerebral hemodynamic
	responses were measured using fMRI. Our experimental design allowed
	for a comprehensive investigation of the neurophysiology (cerebral
	hemodynamic responses as measured by fMRI) of auditory expertise
	(i.e., when violinists listened to violin music and when flutists
	listened to flute music) and nonexpertise (i.e., when subjects listened
	to music played on the other instrument). We found an extensive cerebral
	network of expertise, which implicates increased sensitivity to musical
	syntax (BA 44), timbre (auditory association cortex), and sound-motor
	interactions (precentral gyrus) when listening to music played on
	the instrument of expertise (the instrument for which subjects had
	a unique listening biography). These findings highlight auditory
	self-relevance and expertise as a mechanism of perceptual-neural
	plasticity, and implicate neural tuning that includes and extends
	beyond acoustic and communication-relevant structures.</abstract>
		<issn>1097-0193</issn>
		<doi>10.1002/hbm.20503</doi>
		<title>Selective neurophysiologic responses to music in instrumentalists
	with different listening biographies</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27fa19545956a672507f147a39677bd2a/yevb0</id>
		<tags>harmony,language,music,rhythm,syntax</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>3</count>
		<booktitle>A generative theory of tonal music</booktitle>
		<publisher>The MIT Press</publisher>
		<address>Cambridge. MA</address>
		<year>1983</year>
		<url></url>
		<author>Fred Lerdahl</author>
		<author>Ray Jackendoff</author>
		<authors>
			<first>Fred</first>
		</authors>
		<authors>
			<last>Lerdahl</last>
		</authors>
		<authors>
			<first>Ray</first>
		</authors>
		<authors>
			<last>Jackendoff</last>
		</authors>
		<isbn>0262120941</isbn>
		<title>A generative theory of tonal music</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/220c2ac0ea5d6c259d47a6295bf66c630/yevb0</id>
		<tags>music,musicality,neuro</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Annals of the New York Academy of Sciences</journal>
		<year>2001</year>
		<url></url>
		<author>Gottfried Schlaug</author>
		<authors>
			<first>Gottfried</first>
		</authors>
		<authors>
			<last>Schlaug</last>
		</authors>
		<volume>930</volume>
		<pages>281--299</pages>
		<title>The brain of musicians</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a2fd5067f39fb8c3d0e636cef655d4d1/yevb0</id>
		<tags>Imaging,Middle</tags>
		<tags>Fitness:</tags>
		<tags>Skills:</tags>
		<tags>Laterality:</tags>
		<tags>anatomy</tags>
		<tags>ultrastructure,Nerve</tags>
		<tags>Adult,Brain,Brain</tags>
		<tags>Mapping,Brain:</tags>
		<tags>physiology,Humans,Magnetic</tags>
		<tags>physiology,Myelinated:</tags>
		<tags>\&</tags>
		<tags>physiology,Music,Myelinated,Myelinated:</tags>
		<tags>Fibers,Physical</tags>
		<tags>physiology,music,musicality,neuro</tags>
		<tags>physiology,Functional</tags>
		<tags>Skills,Motor</tags>
		<tags>physiology,Cognition,Cognition:</tags>
		<tags>histology,Brain:</tags>
		<tags>Aged,Motor</tags>
		<tags>Resonance</tags>
		<tags>Fitness,Physical</tags>
		<tags>Laterality,Functional</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Neuroscience Letters</journal>
		<year>2002</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/11872256</url>
		<author>Vincent J Schmithorst</author>
		<author>Marko Wilke</author>
		<authors>
			<first>Vincent J</first>
		</authors>
		<authors>
			<last>Schmithorst</last>
		</authors>
		<authors>
			<first>Marko</first>
		</authors>
		<authors>
			<last>Wilke</last>
		</authors>
		<volume>321</volume>
		<number>1-2</number>
		<pages>57--60</pages>
		<abstract>Previous studies found structural brain differences between musicians
	and non-musicians. In order to determine possible differences in
	white matter architecture, diffusion tensor imaging was performed
	on five adult subjects with musical training since early childhood,
	and seven adult controls. The musicians displayed significantly greater
	fractional anisotropy (FA) in the genu of the corpus callosum, while
	significantly less FA was found in the corona radiata and the internal
	capsule bilaterally. Further areas also showed significant differences.
	We hypothesize that these changes are due to the cognitive and motor
	effects, respectively, of musical training.</abstract>
		<issn>0304-3940</issn>
		<title>Differences in white matter architecture between musicians and non-musicians:
	a diffusion tensor imaging study</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c87e4e2d140864a94b573d00cbbe6570/joaakive</id>
		<tags>ethnomusicology</tags>
		<tags>music</tags>
		<tags>theory</tags>
		<description>BibSonomy :: search</description>
		<date>2014-04-27 08:58:50</date>
		<count>1</count>
		<journal>Asian Music</journal>
		<year>1993</year>
		<url></url>
		<author>Scott Marcus</author>
		<authors>
			<first>Scott</first>
		</authors>
		<authors>
			<last>Marcus</last>
		</authors>
		<volume>Vol. 24</volume>
		<number>No. 2</number>
		<title>The Interface between Theory and Practice: Intonation in Arab Music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20b8ac8469242a02446d16da2dac181a7/andre@ismll</id>
		<tags>feature</tags>
		<tags>toget</tags>
		<description>Design and comparison of different evolution strategies for feature selection and consolidation in music classification</description>
		<date>2010-05-06 10:46:04</date>
		<count>2</count>
		<booktitle>CEC'09: Proceedings of the Eleventh conference on Congress on Evolutionary Computation</booktitle>
		<publisher>IEEE Press</publisher>
		<address>Piscataway, NJ, USA</address>
		<year>2009</year>
		<url>http://portal.acm.org/citation.cfm?id=1689622&dl=GUIDE&coll=GUIDE&CFID=87319572&CFTOKEN=29577946</url>
		<author>I. Vatolkin</author>
		<author>W. Theimer</author>
		<author>G. Rudolph</author>
		<authors>
			<first>I.</first>
		</authors>
		<authors>
			<last>Vatolkin</last>
		</authors>
		<authors>
			<first>W.</first>
		</authors>
		<authors>
			<last>Theimer</last>
		</authors>
		<authors>
			<first>G.</first>
		</authors>
		<authors>
			<last>Rudolph</last>
		</authors>
		<pages>174--181</pages>
		<abstract>Music classification is a complex problem which has gained high relevance for organizing large music collections. Different parameters concerning feature extraction, selection, processing and classification have a strong impact on the categorization quality. Since it is very difficult to design a deterministic approach which provides the efficient parameter tuning, we haven chosen a heuristic approach. In our work we apply and compare different evolution strategies for the optimization of feature selection and consolidation using three pre-defined personal user categories. Concepts of local search operators with domain-specific knowledge and self-adaptation are examined. Several suggestions based on an empirical study are discussed and ideas for future work are given.</abstract>
		<isbn>978-1-4244-2958-5</isbn>
		<title>Design and comparison of different evolution strategies for feature selection and consolidation in music classification</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27f57bf239e8acf91fd17002775317e83/nasi</id>
		<tags>tobbac</tags>
		<tags>emotion-based</tags>
		<tags>emotional</tags>
		<tags>glass</tags>
		<tags>descriptor</tags>
		<tags>retrieval</tags>
		<tags>emotion</tags>
		<tags>music</tags>
		<tags>web</tags>
		<tags>2.0</tags>
		<tags>engine</tags>
		<tags>wisrep10</tags>
		<tags>information</tags>
		<description>This paper presents a study on emotional music retrieval. The study used music by glass (a musician) that was already indexed in matters of which emotions are evoked and classic music pieces that can be considered emotion-provoking. These prototype music-pieces were presented to a group of people who were then asked to assign specific emotions.</description>
		<date>2010-05-16 11:41:52</date>
		<count>2</count>
		<journal>Proceedings of the American Society for Information Science and Technology</journal>
		<year>2007</year>
		<url></url>
		<author>Hyuk-Jin Lee</author>
		<author>Diane Neal</author>
		<authors>
			<first>Hyuk-Jin</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Diane</first>
		</authors>
		<authors>
			<last>Neal</last>
		</authors>
		<volume>44</volume>
		<number>1</number>
		<pages>1-34</pages>
		<title>Toward Web 2.0 music information retrieval: Utilizing emotion-based, user-assigned descriptors</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/229f12072a9b62cdc460c033782d86316/svrist</id>
		<tags>imported</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>1</count>
		<year>1993</year>
		<url></url>
		<author>N. P. Carter</author>
		<authors>
			<first>N. P.</first>
		</authors>
		<authors>
			<last>Carter</last>
		</authors>
		<number>STAN-M-87</number>
		<title>A generalized approach to automatic recognition of music scores</title>
		<pubtype>techreport</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22c8c59e9965cf360442e9b273ec490e5/svrist</id>
		<tags>imported</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>1</count>
		<journal>Procceedings of the International  Computer Music Conference</journal>
		<year> 2002 </year>
		<url></url>
		<author>MacMillan K.</author>
		<author>M. Droettboom</author>
		<author>I. Fujinaga I.</author>
		<authors>
			<first>MacMillan</first>
		</authors>
		<authors>
			<last>K.</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Droettboom</last>
		</authors>
		<authors>
			<first>I. Fujinaga</first>
		</authors>
		<authors>
			<last>I.</last>
		</authors>
		<pages>482-485</pages>
		<title>Gamera: Optical music recognition in a new shell</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/259c2e7da3af9069f4a8181d3571b1923/torsten_peh</id>
		<tags>software</tags>
		<tags>Music</tags>
		<tags>e-Learning</tags>
		<description></description>
		<date>2012-10-08 12:53:30</date>
		<count>1</count>
		<journal>British Journal of Music Education</journal>
		<year>2009</year>
		<url>http://journals.cambridge.org/action/displayFulltext?pageCode=100101&type=1&fid=4012272&jid=BME&volumeId=26&issueId=01&aid=4012264</url>
		<author>Crispin Dale Steve Cooper</author>
		<author>Steve Spencer</author>
		<authors>
			<first>Crispin Dale</first>
		</authors>
		<authors>
			<last>Steve Cooper</last>
		</authors>
		<authors>
			<first>Steve</first>
		</authors>
		<authors>
			<last>Spencer</last>
		</authors>
		<volume>26</volume>
		<pages>85-97</pages>
		<title>A tutor in your back pocket: reflections on the use of iPods and podcasting in an undergraduate popular music programme</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/244b26899136055207f575931773b258f/keinstein</id>
		<tags>MaMu</tags>
		<description></description>
		<date>2012-11-22 21:53:50</date>
		<count>2</count>
		<journal>Memoirs of the Fourth International Seminar on Mathematical Music Theory</journal>
		<year>2011</year>
		<url>http://www.smm.org.mx/smm/PEMemorias</url>
		<author>Julio Estrada</author>
		<authors>
			<first>Julio</first>
		</authors>
		<authors>
			<last>Estrada</last>
		</authors>
		<volume>4</volume>
		<pages>113–145</pages>
		<title>La teoría d1, MúSIIC-Win y algunas aplicaciones al análisis musical: Seis piezas para piano, de Arnold Schoenberg</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/275caaf2a8880bb8726ffd5f77a7fda95/keinstein</id>
		<tags>Informatik</tags>
		<tags>Musiktheorie</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2009</date>
		<count>2</count>
		<journal>Computer Music Modeling and Retrieval. Sense of Sounds</journal>
		<booktitle>The Usability of Music Theory Software: The Analysis of Twelve-Tone Music as a Case Study</booktitle>
		<series>Lecture Notes in Computer Science</series>
		<publisher>Springer</publisher>
		<address>Berlin, Heidelberg</address>
		<year>2009</year>
		<url></url>
		<editor>Richard Kronland-Martinet</editor>
		<editor>Sølvi Ystad</editor>
		<editor>Kristoffer Jensen</editor>
		<editors>
			<first>Julio</first>
		</editors>
		<editors>
			<last>Estrada</last>
		</editors>
		<editors>
			<first>Julio</first>
		</editors>
		<editors>
			<last>Estrada</last>
		</editors>
		<editors>
			<first>Julio</first>
		</editors>
		<editors>
			<last>Estrada</last>
		</editors>
		<volume>4969</volume>
		<issn>1611-3349</issn>
		<doi>10.1007/978-3-540-85035-9</doi>
		<title>The Usability of Music Theory Software: The Analysis of Twelve-Tone Music as a Case Study</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25902dd2c90fb1fcdcbca4ba264c9ff48/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#XuZ13a</url>
		<author>Bingqing Xu</author>
		<author>Lichen Zhang</author>
		<authors>
			<first>Bingqing</first>
		</authors>
		<authors>
			<last>Xu</last>
		</authors>
		<authors>
			<first>Lichen</first>
		</authors>
		<authors>
			<last>Zhang</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Lichen</first>
		</editors>
		<editors>
			<last>Zhang</last>
		</editors>
		<editors>
			<first>Lichen</first>
		</editors>
		<editors>
			<last>Zhang</last>
		</editors>
		<editors>
			<first>Lichen</first>
		</editors>
		<editors>
			<last>Zhang</last>
		</editors>
		<editors>
			<first>Lichen</first>
		</editors>
		<editors>
			<last>Zhang</last>
		</editors>
		<volume>274</volume>
		<pages>137-142</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Formal Descriptions of Cyber Physical Systems Using Clock Theory.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2de82588ece9fb138940e0b44e64234f1/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#LinCLWC13</url>
		<author>Pei-Jung Lin</author>
		<author>Sheng-Chang Chen</author>
		<author>Yi-Hsung Li</author>
		<author>Meng-Syue Wu</author>
		<author>Shih-Yue Chen</author>
		<authors>
			<first>Pei-Jung</first>
		</authors>
		<authors>
			<last>Lin</last>
		</authors>
		<authors>
			<first>Sheng-Chang</first>
		</authors>
		<authors>
			<last>Chen</last>
		</authors>
		<authors>
			<first>Yi-Hsung</first>
		</authors>
		<authors>
			<last>Li</last>
		</authors>
		<authors>
			<first>Meng-Syue</first>
		</authors>
		<authors>
			<last>Wu</last>
		</authors>
		<authors>
			<first>Shih-Yue</first>
		</authors>
		<authors>
			<last>Chen</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Shih-Yue</first>
		</editors>
		<editors>
			<last>Chen</last>
		</editors>
		<editors>
			<first>Shih-Yue</first>
		</editors>
		<editors>
			<last>Chen</last>
		</editors>
		<editors>
			<first>Shih-Yue</first>
		</editors>
		<editors>
			<last>Chen</last>
		</editors>
		<editors>
			<first>Shih-Yue</first>
		</editors>
		<editors>
			<last>Chen</last>
		</editors>
		<volume>274</volume>
		<pages>509-514</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>An Implementation of Augmented Reality and Location Awareness Services in Mobile Devices.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/215cc40c0c1872061308cdd54c0e8b0c9/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#SonKSOCK13</url>
		<author>Jeonghan Son</author>
		<author>Keon Uk Kim</author>
		<author>Yeon gyeong Seo</author>
		<author>Wonyeong Oh</author>
		<author>Seowon Choi</author>
		<author>Ana Kang</author>
		<authors>
			<first>Jeonghan</first>
		</authors>
		<authors>
			<last>Son</last>
		</authors>
		<authors>
			<first>Keon Uk</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Yeon</first>
		</authors>
		<authors>
			<last>gyeong Seo</last>
		</authors>
		<authors>
			<first>Wonyeong</first>
		</authors>
		<authors>
			<last>Oh</last>
		</authors>
		<authors>
			<first>Seowon</first>
		</authors>
		<authors>
			<last>Choi</last>
		</authors>
		<authors>
			<first>Ana</first>
		</authors>
		<authors>
			<last>Kang</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Ana</first>
		</editors>
		<editors>
			<last>Kang</last>
		</editors>
		<editors>
			<first>Ana</first>
		</editors>
		<editors>
			<last>Kang</last>
		</editors>
		<editors>
			<first>Ana</first>
		</editors>
		<editors>
			<last>Kang</last>
		</editors>
		<editors>
			<first>Ana</first>
		</editors>
		<editors>
			<last>Kang</last>
		</editors>
		<volume>274</volume>
		<pages>99-104</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>A Research Based on the Effect of Smart Phone Use on Consumption Life of Teenagers in a Smart Era.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/299dcd7d5026a71f648c463be8ed67a9a/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#HanKCSGL13</url>
		<author>Seungho Han</author>
		<author>Myoungjin Kim</author>
		<author>Yun Cui</author>
		<author>Seunghyun Seo</author>
		<author>Yi Gu</author>
		<author>Hanku Lee</author>
		<authors>
			<first>Seungho</first>
		</authors>
		<authors>
			<last>Han</last>
		</authors>
		<authors>
			<first>Myoungjin</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Yun</first>
		</authors>
		<authors>
			<last>Cui</last>
		</authors>
		<authors>
			<first>Seunghyun</first>
		</authors>
		<authors>
			<last>Seo</last>
		</authors>
		<authors>
			<first>Yi</first>
		</authors>
		<authors>
			<last>Gu</last>
		</authors>
		<authors>
			<first>Hanku</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Hanku</first>
		</editors>
		<editors>
			<last>Lee</last>
		</editors>
		<editors>
			<first>Hanku</first>
		</editors>
		<editors>
			<last>Lee</last>
		</editors>
		<editors>
			<first>Hanku</first>
		</editors>
		<editors>
			<last>Lee</last>
		</editors>
		<editors>
			<first>Hanku</first>
		</editors>
		<editors>
			<last>Lee</last>
		</editors>
		<volume>274</volume>
		<pages>359-363</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Load Distribution Method for Ensuring QoS of Social Media Streaming Services in Cloud Environment.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d6990f623aeb4c8d43d239ae2e2c224b/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#HsiehYTL13</url>
		<author>Meng-Yen Hsieh</author>
		<author>Ching-Hung Yeh</author>
		<author>Yin-Te Tsai</author>
		<author>Kuan-Ching Li</author>
		<authors>
			<first>Meng-Yen</first>
		</authors>
		<authors>
			<last>Hsieh</last>
		</authors>
		<authors>
			<first>Ching-Hung</first>
		</authors>
		<authors>
			<last>Yeh</last>
		</authors>
		<authors>
			<first>Yin-Te</first>
		</authors>
		<authors>
			<last>Tsai</last>
		</authors>
		<authors>
			<first>Kuan-Ching</first>
		</authors>
		<authors>
			<last>Li</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Kuan-Ching</first>
		</editors>
		<editors>
			<last>Li</last>
		</editors>
		<editors>
			<first>Kuan-Ching</first>
		</editors>
		<editors>
			<last>Li</last>
		</editors>
		<editors>
			<first>Kuan-Ching</first>
		</editors>
		<editors>
			<last>Li</last>
		</editors>
		<editors>
			<first>Kuan-Ching</first>
		</editors>
		<editors>
			<last>Li</last>
		</editors>
		<volume>274</volume>
		<pages>93-98</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Toward a Mobile Application for Social Sharing Context.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2723b34871f50883237630c3ddfd1f0fd/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#UmJCLJ13</url>
		<author>Jung-Ho Um</author>
		<author>Chang-Hoo Jeong</author>
		<author>Sung-Pil Choi</author>
		<author>Seungwoo Lee</author>
		<author>Hanmin Jung</author>
		<authors>
			<first>Jung-Ho</first>
		</authors>
		<authors>
			<last>Um</last>
		</authors>
		<authors>
			<first>Chang-Hoo</first>
		</authors>
		<authors>
			<last>Jeong</last>
		</authors>
		<authors>
			<first>Sung-Pil</first>
		</authors>
		<authors>
			<last>Choi</last>
		</authors>
		<authors>
			<first>Seungwoo</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Hanmin</first>
		</authors>
		<authors>
			<last>Jung</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Hanmin</first>
		</editors>
		<editors>
			<last>Jung</last>
		</editors>
		<editors>
			<first>Hanmin</first>
		</editors>
		<editors>
			<last>Jung</last>
		</editors>
		<editors>
			<first>Hanmin</first>
		</editors>
		<editors>
			<last>Jung</last>
		</editors>
		<editors>
			<first>Hanmin</first>
		</editors>
		<editors>
			<last>Jung</last>
		</editors>
		<volume>274</volume>
		<pages>267-271</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Fast Big Textual Data Parsing in Distributed and Parallel Computing Environment.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29d1fa1416d75147ed71d9ed91459dbdc/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#KimKP13</url>
		<author>Youngsoo Kim</author>
		<author>Ikkyun Kim</author>
		<author>Namje Park</author>
		<authors>
			<first>Youngsoo</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Ikkyun</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Namje</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Namje</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Namje</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Namje</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Namje</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<volume>274</volume>
		<pages>489-494</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Analysis of Cyber Attacks and Security Intelligence.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2051292760b8f04a7652aad073882d6b1/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#AkhmedjanovKSK13</url>
		<author>Umid Akhmedjanov</author>
		<author>Eunjeong Ko</author>
		<author>Yunhee Shin</author>
		<author>Eun Yi Kim</author>
		<authors>
			<first>Umid</first>
		</authors>
		<authors>
			<last>Akhmedjanov</last>
		</authors>
		<authors>
			<first>Eunjeong</first>
		</authors>
		<authors>
			<last>Ko</last>
		</authors>
		<authors>
			<first>Yunhee</first>
		</authors>
		<authors>
			<last>Shin</last>
		</authors>
		<authors>
			<first>Eun Yi</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Eun Yi</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Eun Yi</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Eun Yi</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Eun Yi</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<volume>274</volume>
		<pages>347-351</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Finding Relationships between Human Affects and Colors Using SVD and pLSA.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/244c3d92802a3f24a4b7ceb87f1f5154a/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#ChoHM13</url>
		<author>Shung Han Cho</author>
		<author>Sangjin Hong</author>
		<author>Nammee Moon</author>
		<authors>
			<first>Shung Han</first>
		</authors>
		<authors>
			<last>Cho</last>
		</authors>
		<authors>
			<first>Sangjin</first>
		</authors>
		<authors>
			<last>Hong</last>
		</authors>
		<authors>
			<first>Nammee</first>
		</authors>
		<authors>
			<last>Moon</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Nammee</first>
		</editors>
		<editors>
			<last>Moon</last>
		</editors>
		<editors>
			<first>Nammee</first>
		</editors>
		<editors>
			<last>Moon</last>
		</editors>
		<editors>
			<first>Nammee</first>
		</editors>
		<editors>
			<last>Moon</last>
		</editors>
		<editors>
			<first>Nammee</first>
		</editors>
		<editors>
			<last>Moon</last>
		</editors>
		<volume>274</volume>
		<pages>165-171</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Effective Object Identification through RFID Reader Power Control.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/227891b0d9782dd4718cf3b01aa05fed2/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#JungKLLC13</url>
		<author>Sung-Min Jung</author>
		<author>Nam-Uk Kim</author>
		<author>Seung-Hyun Lee</author>
		<author>Dong-Young Lee</author>
		<author>Tai-Myoung Chung</author>
		<authors>
			<first>Sung-Min</first>
		</authors>
		<authors>
			<last>Jung</last>
		</authors>
		<authors>
			<first>Nam-Uk</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Seung-Hyun</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Dong-Young</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Tai-Myoung</first>
		</authors>
		<authors>
			<last>Chung</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Tai-Myoung</first>
		</editors>
		<editors>
			<last>Chung</last>
		</editors>
		<editors>
			<first>Tai-Myoung</first>
		</editors>
		<editors>
			<last>Chung</last>
		</editors>
		<editors>
			<first>Tai-Myoung</first>
		</editors>
		<editors>
			<last>Chung</last>
		</editors>
		<editors>
			<first>Tai-Myoung</first>
		</editors>
		<editors>
			<last>Chung</last>
		</editors>
		<volume>274</volume>
		<pages>111-116</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Applying Different Cryptographic Algorithms for Mobile Cloud Computing.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/201056f9fadfdf61ac08704b0df8e7703/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#YehHL13</url>
		<author>Ching-Hung Yeh</author>
		<author>Meng-Yen Hsieh</author>
		<author>Kuan-Ching Li</author>
		<authors>
			<first>Ching-Hung</first>
		</authors>
		<authors>
			<last>Yeh</last>
		</authors>
		<authors>
			<first>Meng-Yen</first>
		</authors>
		<authors>
			<last>Hsieh</last>
		</authors>
		<authors>
			<first>Kuan-Ching</first>
		</authors>
		<authors>
			<last>Li</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Kuan-Ching</first>
		</editors>
		<editors>
			<last>Li</last>
		</editors>
		<editors>
			<first>Kuan-Ching</first>
		</editors>
		<editors>
			<last>Li</last>
		</editors>
		<editors>
			<first>Kuan-Ching</first>
		</editors>
		<editors>
			<last>Li</last>
		</editors>
		<editors>
			<first>Kuan-Ching</first>
		</editors>
		<editors>
			<last>Li</last>
		</editors>
		<volume>274</volume>
		<pages>563-568</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>An Anonymous Communication Scheme with Non-reputation for Vehicular Ad Hoc Networks.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2de0a65380c0fb58f0a0b2bfa7bfc6829/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#AnCZ13</url>
		<author>Xin An</author>
		<author>Jiancheng Chen</author>
		<author>Yuan Zhang</author>
		<authors>
			<first>Xin</first>
		</authors>
		<authors>
			<last>An</last>
		</authors>
		<authors>
			<first>Jiancheng</first>
		</authors>
		<authors>
			<last>Chen</last>
		</authors>
		<authors>
			<first>Yuan</first>
		</authors>
		<authors>
			<last>Zhang</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Yuan</first>
		</editors>
		<editors>
			<last>Zhang</last>
		</editors>
		<editors>
			<first>Yuan</first>
		</editors>
		<editors>
			<last>Zhang</last>
		</editors>
		<editors>
			<first>Yuan</first>
		</editors>
		<editors>
			<last>Zhang</last>
		</editors>
		<editors>
			<first>Yuan</first>
		</editors>
		<editors>
			<last>Zhang</last>
		</editors>
		<volume>274</volume>
		<pages>253-260</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Risk Aversion Parameter Estimation for First-Price Auction with Nonparametric Method.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23d5d9c7a7e1aeac827f960f0c5e702d1/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#LeePB13</url>
		<author>Hyungkyu Lee</author>
		<author>Namje Park</author>
		<author>Hyo-Chan Bang</author>
		<authors>
			<first>Hyungkyu</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Namje</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<authors>
			<first>Hyo-Chan</first>
		</authors>
		<authors>
			<last>Bang</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Hyo-Chan</first>
		</editors>
		<editors>
			<last>Bang</last>
		</editors>
		<editors>
			<first>Hyo-Chan</first>
		</editors>
		<editors>
			<last>Bang</last>
		</editors>
		<editors>
			<first>Hyo-Chan</first>
		</editors>
		<editors>
			<last>Bang</last>
		</editors>
		<editors>
			<first>Hyo-Chan</first>
		</editors>
		<editors>
			<last>Bang</last>
		</editors>
		<volume>274</volume>
		<pages>457-462</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>The Architecture Design of Semantic Based Open USN Service Platform Model.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22f2e031e973d7d0e05bd322d9de793a0/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#KimHJLC13</url>
		<author>Myoungjin Kim</author>
		<author>Seungho Han</author>
		<author>Jong jin Jung</author>
		<author>Hanku Lee</author>
		<author>Okkyung Choi</author>
		<authors>
			<first>Myoungjin</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Seungho</first>
		</authors>
		<authors>
			<last>Han</last>
		</authors>
		<authors>
			<first>Jong</first>
		</authors>
		<authors>
			<last>jin Jung</last>
		</authors>
		<authors>
			<first>Hanku</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Okkyung</first>
		</authors>
		<authors>
			<last>Choi</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Okkyung</first>
		</editors>
		<editors>
			<last>Choi</last>
		</editors>
		<editors>
			<first>Okkyung</first>
		</editors>
		<editors>
			<last>Choi</last>
		</editors>
		<editors>
			<first>Okkyung</first>
		</editors>
		<editors>
			<last>Choi</last>
		</editors>
		<editors>
			<first>Okkyung</first>
		</editors>
		<editors>
			<last>Choi</last>
		</editors>
		<volume>274</volume>
		<pages>365-370</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>A Robust Cloud-Based Service Architecture for Multimedia Streaming Using Hadoop.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/299e05bc4ec268e74b97311c893876fdc/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#KimK13</url>
		<author>Eunhye Kim</author>
		<author>Sehun Kim</author>
		<authors>
			<first>Eunhye</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Sehun</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Sehun</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Sehun</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Sehun</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Sehun</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<volume>274</volume>
		<pages>279-286</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>A Novel Anomaly Detection System Based on HFR-MLR Method.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e426d7d39520220b1871630ced0e3680/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#KangKK13</url>
		<author>Myeongsu Kang</author>
		<author>Cheol Hong Kim</author>
		<author>Jong-Myon Kim</author>
		<authors>
			<first>Myeongsu</first>
		</authors>
		<authors>
			<last>Kang</last>
		</authors>
		<authors>
			<first>Cheol Hong</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Jong-Myon</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Jong-Myon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Jong-Myon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Jong-Myon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Jong-Myon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<volume>274</volume>
		<pages>599-607</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>High-Performance Sound Engine of Guitar on Optimal Many-Core Processors.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f71b7f743253e314875a6c32b7991e03/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#KangLPBP13a</url>
		<author>Taekyeong Kang</author>
		<author>Hyungkyu Lee</author>
		<author>Dong-Hwan Park</author>
		<author>Hyo-Chan Bang</author>
		<author>Namje Park</author>
		<authors>
			<first>Taekyeong</first>
		</authors>
		<authors>
			<last>Kang</last>
		</authors>
		<authors>
			<first>Hyungkyu</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Dong-Hwan</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<authors>
			<first>Hyo-Chan</first>
		</authors>
		<authors>
			<last>Bang</last>
		</authors>
		<authors>
			<first>Namje</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Namje</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Namje</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Namje</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Namje</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<volume>274</volume>
		<pages>125-130</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Creation Mechanism for Access Group Based on User Privacy Policy-Based Protection.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f402078d4e7ddd091e4560f7746b9cd6/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#JungSLJ13</url>
		<author>Sung-Jae Jung</author>
		<author>Dongmin Seo</author>
		<author>Seungwoo Lee</author>
		<author>Hanmin Jung</author>
		<authors>
			<first>Sung-Jae</first>
		</authors>
		<authors>
			<last>Jung</last>
		</authors>
		<authors>
			<first>Dongmin</first>
		</authors>
		<authors>
			<last>Seo</last>
		</authors>
		<authors>
			<first>Seungwoo</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Hanmin</first>
		</authors>
		<authors>
			<last>Jung</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Hanmin</first>
		</editors>
		<editors>
			<last>Jung</last>
		</editors>
		<editors>
			<first>Hanmin</first>
		</editors>
		<editors>
			<last>Jung</last>
		</editors>
		<editors>
			<first>Hanmin</first>
		</editors>
		<editors>
			<last>Jung</last>
		</editors>
		<editors>
			<first>Hanmin</first>
		</editors>
		<editors>
			<last>Jung</last>
		</editors>
		<volume>274</volume>
		<pages>247-251</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Scalable RDF Path Query Processing Based on Runtime Class Path Lookup Scheme.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26ba765cb79aed305c24623f14de70a05/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#WonK13</url>
		<author>Jongjin Won</author>
		<author>Moonhyun Kim</author>
		<authors>
			<first>Jongjin</first>
		</authors>
		<authors>
			<last>Won</last>
		</authors>
		<authors>
			<first>Moonhyun</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Moonhyun</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Moonhyun</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Moonhyun</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Moonhyun</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<volume>274</volume>
		<pages>469-474</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>A Neural Network Based Simple Weak Learner for Improving Generalization Ability for AdaBoost.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2772444dab8226d3c38c532a9a770e903/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#KimPAK13a</url>
		<author>Jong-Ho Kim</author>
		<author>Jun-Jae Park</author>
		<author>Sang-Ho Ahn</author>
		<author>Sang-Kyoon Kim</author>
		<authors>
			<first>Jong-Ho</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Jun-Jae</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<authors>
			<first>Sang-Ho</first>
		</authors>
		<authors>
			<last>Ahn</last>
		</authors>
		<authors>
			<first>Sang-Kyoon</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Sang-Kyoon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Sang-Kyoon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Sang-Kyoon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Sang-Kyoon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<volume>274</volume>
		<pages>549-556</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>A Small Target Detection System Based on Morphology and Modified Gaussian Distance Function.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/204eff9c4e020371ba43fa159e9c66850/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#TsengCY13</url>
		<author>Shih-Pang Tseng</author>
		<author>Ming-Chao Chiang</author>
		<author>Chu-Sing Yang</author>
		<authors>
			<first>Shih-Pang</first>
		</authors>
		<authors>
			<last>Tseng</last>
		</authors>
		<authors>
			<first>Ming-Chao</first>
		</authors>
		<authors>
			<last>Chiang</last>
		</authors>
		<authors>
			<first>Chu-Sing</first>
		</authors>
		<authors>
			<last>Yang</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Chu-Sing</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<editors>
			<first>Chu-Sing</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<editors>
			<first>Chu-Sing</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<editors>
			<first>Chu-Sing</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<volume>274</volume>
		<pages>615-620</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>An Improved ACO by Neighborhood Strategy for Color Image Segmentation.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/218c3894a61567f5aa1a20e1a771a120f/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#ChangL13</url>
		<author>Jed Kao-Tung Chang</author>
		<author>Chen Liu</author>
		<authors>
			<first>Jed Kao-Tung</first>
		</authors>
		<authors>
			<last>Chang</last>
		</authors>
		<authors>
			<first>Chen</first>
		</authors>
		<authors>
			<last>Liu</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Chen</first>
		</editors>
		<editors>
			<last>Liu</last>
		</editors>
		<editors>
			<first>Chen</first>
		</editors>
		<editors>
			<last>Liu</last>
		</editors>
		<editors>
			<first>Chen</first>
		</editors>
		<editors>
			<last>Liu</last>
		</editors>
		<editors>
			<first>Chen</first>
		</editors>
		<editors>
			<last>Liu</last>
		</editors>
		<volume>274</volume>
		<pages>557-562</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Using Hardware Acceleration to Improve the Security of Wi-Fi Client Devices.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a4ad95de9c40a7e93e9c54e729aaead7/ks-plugin-devel</id>
		<tags>Informatik</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2006</date>
		<count>2</count>
		<journal>Computer Music Modeling and Retrieval</journal>
		<year>2006</year>
		<url>http://dx.doi.org/10.1007/11751069_3</url>
		<author>Leonello Tarabella</author>
		<authors>
			<first>Leonello</first>
		</authors>
		<authors>
			<last>Tarabella</last>
		</authors>
		<pages>34--44</pages>
		<abstract>The pureCMusic (pCM++) framework gives the possibility to write a piece of music in terms of an algorithmic-composition-based program -also controlled by data streaming from external devices for giving expressiveness in electro-acoustic music performances- and of synthesis algorithms. Everything is written following the C language syntax and compiled into machine code that runs at CPU speed. The framework provides a number of predefined functions for sound processing, for generating complex events and for managing external data coming from standard Midi controllers and/or other special gesture interfaces. I'm going to propose pCM++ as open-source code.</abstract>
		<doi>10.1007/11751069_3</doi>
		<title>The pureCMusic (pCM++) Framework as Open-Source Music Language</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/275caaf2a8880bb8726ffd5f77a7fda95/ks-plugin-devel</id>
		<tags>Informatik</tags>
		<tags>Musiktheorie</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2009</date>
		<count>2</count>
		<journal>Computer Music Modeling and Retrieval. Sense of Sounds</journal>
		<booktitle>The Usability of Music Theory Software: The Analysis of Twelve-Tone Music as a Case Study</booktitle>
		<series>Lecture Notes in Computer Science</series>
		<publisher>Springer</publisher>
		<address>Berlin, Heidelberg</address>
		<year>2009</year>
		<url></url>
		<editor>Richard Kronland-Martinet</editor>
		<editor>Sølvi Ystad</editor>
		<editor>Kristoffer Jensen</editor>
		<editors>
			<first>Leonello</first>
		</editors>
		<editors>
			<last>Tarabella</last>
		</editors>
		<editors>
			<first>Leonello</first>
		</editors>
		<editors>
			<last>Tarabella</last>
		</editors>
		<editors>
			<first>Leonello</first>
		</editors>
		<editors>
			<last>Tarabella</last>
		</editors>
		<volume>4969</volume>
		<issn>1611-3349</issn>
		<doi>10.1007/978-3-540-85035-9</doi>
		<title>The Usability of Music Theory Software: The Analysis of Twelve-Tone Music as a Case Study</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c60fd4b18b3f98cf43f3f35722bff1d9/mandrean</id>
		<tags>electroacoustic</tags>
		<description></description>
		<date>2015-12-19 00:53:42</date>
		<count>1</count>
		<journal>Journal of New Music Research</journal>
		<year>1995</year>
		<url>/brokenurl#         http://www.tandfonline.com/doi/abs/10.1080/09298219508570691    </url>
		<author>Agostino Di Scipio</author>
		<authors>
			<first>Agostino Di</first>
		</authors>
		<authors>
			<last>Scipio</last>
		</authors>
		<volume>24</volume>
		<number>4</number>
		<pages>369-383</pages>
		<abstract>Abstract Electroacoustic music poses methodological problems to purely hermeneutic and historical musicology — problems of epistemological nature betrayed by a lack of ethnomusicological awareness (in a sense to be defined). What is missing is a methodology capable of characterising the technical processes and the designing tools that make up the compositional environment, models, representations, and knowledge‐level strategies, which is understood as traces of cognitive and aesthetic paradigms specific to the medium. It is also shown that an analysis of such kind — drawing on the téchne of the making of music — is indispensable in order to shed light on the renewed relation of sound materials to musical form in electroacoustic music. Finally, the problem of whether the theoretical approach outlined here would lend itself to an outlook on normative aesthetics in the context of the presumed a‐normativity of postmodern intellectual endeavours is discussed.</abstract>
		<eprint>http://www.tandfonline.com/doi/pdf/10.1080/09298219508570691</eprint>
		<doi>10.1080/09298219508570691</doi>
		<title>Centrality of Téchne for an Aesthetic approach on electroacoustic music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28b9f59a921f1f4302a30285094dcb4e2/mandrean</id>
		<tags>electroacoustic</tags>
		<description></description>
		<date>2015-12-19 09:37:30</date>
		<count>1</count>
		<journal>Contemporary Music Review</journal>
		<year>1994</year>
		<url>/brokenurl#         http://dx.doi.org/10.1080/07494469400640351    </url>
		<author>Mike Vaughan</author>
		<authors>
			<first>Mike</first>
		</authors>
		<authors>
			<last>Vaughan</last>
		</authors>
		<volume>10</volume>
		<number>2</number>
		<pages>111-127</pages>
		<abstract>The hardware and software used to realize an electroacoustic music composition exerts a major influence on the creative processes, the selection of effective composing strategies and the quality of the sounds produced. This paper examines some aspects of the relationship between the concepts and procedures embodied in a work and their realization via the human-machine interface in general. It also explores the degree to which certain compositional procedures may arise directly from the architecture of devices in the production chain in contrast to either compositional planning or aural necessity.</abstract>
		<eprint>http://dx.doi.org/10.1080/07494469400640351</eprint>
		<doi>10.1080/07494469400640351</doi>
		<title>The human-machine interface in electroacoustic music composition</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21d12e121e43f0f32202ab3710d17eeb1/mandrean</id>
		<tags>listening</tags>
		<tags>ECLARKE</tags>
		<tags>musical-meaning</tags>
		<description></description>
		<date>2015-12-20 18:20:37</date>
		<count>2</count>
		<publisher>Oxford University Press</publisher>
		<address>Oxford; New York</address>
		<year>2005</year>
		<url>http://www.worldcat.org/search?qt=worldcat_org_all&q=9780195151947</url>
		<author>Eric Clarke</author>
		<authors>
			<first>Eric</first>
		</authors>
		<authors>
			<last>Clarke</last>
		</authors>
		<abstract>In Ways of Listening, musicologist Eric Clarke explores musical meaning, music's critical function in human lives, and the relationship between listening and musical material. Clarke outlines an "ecological approach" to understanding the perception of music, arguing that the way we hear and understand music is not simply a function of our brain structure or of the musical "codes" given to us by culture, but must be considered within the physical and social contexts of listening.</abstract>
		<isbn>0195151941 9780195151947</isbn>
		<title>Ways of listening : an ecological approach to the perception of musical meaning</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25acf7485f81174a7490940a72a951cbe/alexarje</id>
		<tags>Embodied</tags>
		<tags>Music</tags>
		<tags>Cognition</tags>
		<description></description>
		<date>2016-02-26 20:58:13</date>
		<count>2</count>
		<publisher>MIT press Cambridge, Massachusetts</publisher>
		<year>2007</year>
		<url></url>
		<author>Marc Leman</author>
		<authors>
			<first>Marc</first>
		</authors>
		<authors>
			<last>Leman</last>
		</authors>
		<title>Embodied Music Cognition and Mediation Technology</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ab55f447b9dcda2302306e0e90d23045/alexarje</id>
		<tags>imported</tags>
		<description></description>
		<date>2016-02-26 20:58:13</date>
		<count>2</count>
		<booktitle>Proceedings of the International Computer Music Conference</booktitle>
		<publisher>L'Escola Superior de Música de Catalunya</publisher>
		<address>Barcelona, Spain</address>
		<year>2005</year>
		<url></url>
		<author>R. Benjamin Knapp</author>
		<author>Perry R. Cook</author>
		<authors>
			<first>R. Benjamin</first>
		</authors>
		<authors>
			<last>Knapp</last>
		</authors>
		<authors>
			<first>Perry R.</first>
		</authors>
		<authors>
			<last>Cook</last>
		</authors>
		<pages>4--9</pages>
		<title>The Integral Music Controller: Introducing a direct emotio nal interface to gestural control of sound synthesis</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ee0bbb74f182e26015172598c345e3e1/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-11-10 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#NasridinovIJP13</url>
		<author>Aziz Nasridinov</author>
		<author>Sun-Young Ihm</author>
		<author>Young-Sik Jeong</author>
		<author>Young-Ho Park</author>
		<authors>
			<first>Aziz</first>
		</authors>
		<authors>
			<last>Nasridinov</last>
		</authors>
		<authors>
			<first>Sun-Young</first>
		</authors>
		<authors>
			<last>Ihm</last>
		</authors>
		<authors>
			<first>Young-Sik</first>
		</authors>
		<authors>
			<last>Jeong</last>
		</authors>
		<authors>
			<first>Young-Ho</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Young-Ho</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Young-Ho</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Young-Ho</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Young-Ho</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<volume>274</volume>
		<pages>585-590</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Event Detection in Wireless Sensor Networks: Survey and Challenges.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26fc125567da1edba35d74bbf5fd41c17/vitelot</id>
		<tags>myown</tags>
		<tags>innovations</tags>
		<tags>music</tags>
		<tags>metrics</tags>
		<description></description>
		<date>2017-11-06 13:43:07</date>
		<count>2</count>
		<journal>Royal Society Open Science</journal>
		<publisher>The Royal Society</publisher>
		<year>2017</year>
		<url>http://rsos.royalsocietypublishing.org/content/4/7/170433</url>
		<author>Bernardo Monechi</author>
		<author>Pietro Gravino</author>
		<author>Vito D. P. Servedio</author>
		<author>Francesca Tria</author>
		<author>Vittorio Loreto</author>
		<authors>
			<first>Bernardo</first>
		</authors>
		<authors>
			<last>Monechi</last>
		</authors>
		<authors>
			<first>Pietro</first>
		</authors>
		<authors>
			<last>Gravino</last>
		</authors>
		<authors>
			<first>Vito D. P.</first>
		</authors>
		<authors>
			<last>Servedio</last>
		</authors>
		<authors>
			<first>Francesca</first>
		</authors>
		<authors>
			<last>Tria</last>
		</authors>
		<authors>
			<first>Vittorio</first>
		</authors>
		<authors>
			<last>Loreto</last>
		</authors>
		<volume>4</volume>
		<number>7</number>
		<abstract>Creative industries constantly strive for fame and popularity. Though highly desirable, popularity is not the only achievement artistic creations might ever acquire. Leaving a longstanding mark in the global production and influencing future works is an even more important achievement, usually acknowledged by experts and scholars. ‘Significant’ or ‘influential’ works are not always well known to the public or have sometimes been long forgotten by the vast majority. In this paper, we focus on the duality between what is successful and what is significant in the musical context. To this end, we consider a user-generated set of tags collected through an online music platform, whose evolving co-occurrence network mirrors the growing conceptual space underlying music production. We define a set of general metrics aiming at characterizing music albums throughout history, and their relationships with the overall musical production. We show how these metrics allow to classify albums according to their current popularity or their belonging to expert-made lists of important albums. In this way, we provide the scientific community and the public at large with quantitative tools to tell apart popular albums from culturally or aesthetically relevant artworks. The generality of the methodology presented here lends itself to be used in all those fields where innovation and creativity are in play.</abstract>
		<eprint>http://rsos.royalsocietypublishing.org/content/4/7/170433.full.pdf</eprint>
		<doi>10.1098/rsos.170433</doi>
		<title>Significance and popularity in music production</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2cee5b74d979e2c19de2511cc0a10bdf5/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2018-04-12 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#XiaWZZ13</url>
		<author>Xiao Xia</author>
		<author>Xiaodong Wang</author>
		<author>Xingming Zhou</author>
		<author>Tao Zhu</author>
		<authors>
			<first>Xiao</first>
		</authors>
		<authors>
			<last>Xia</last>
		</authors>
		<authors>
			<first>Xiaodong</first>
		</authors>
		<authors>
			<last>Wang</last>
		</authors>
		<authors>
			<first>Xingming</first>
		</authors>
		<authors>
			<last>Zhou</last>
		</authors>
		<authors>
			<first>Tao</first>
		</authors>
		<authors>
			<last>Zhu</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Tao</first>
		</editors>
		<editors>
			<last>Zhu</last>
		</editors>
		<editors>
			<first>Tao</first>
		</editors>
		<editors>
			<last>Zhu</last>
		</editors>
		<editors>
			<first>Tao</first>
		</editors>
		<editors>
			<last>Zhu</last>
		</editors>
		<editors>
			<first>Tao</first>
		</editors>
		<editors>
			<last>Zhu</last>
		</editors>
		<volume>274</volume>
		<pages>405-412</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Collaborative Recommendation of Mobile Apps: A Swarm Intelligence Method.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25425861486b947aaf10f87296df5db08/ven7u</id>
		<tags>neural-network</tags>
		<tags>music-generation</tags>
		<description>Talking Drums: Generating drum grooves with neural networks</description>
		<date>2017-12-18 12:45:49</date>
		<count>2</count>
		<year>2017</year>
		<url>http://arxiv.org/abs/1706.09558</url>
		<author>P. Hutchings</author>
		<authors>
			<first>P.</first>
		</authors>
		<authors>
			<last>Hutchings</last>
		</authors>
		<abstract>Presented is a method of generating a full drum kit part for a provided
kick-drum sequence. A sequence to sequence neural network model used in natural
language translation was adopted to encode multiple musical styles and an
online survey was developed to test different techniques for sampling the
output of the softmax function. The strongest results were found using a
sampling technique that drew from the three most probable outputs at each
subdivision of the drum pattern but the consistency of output was found to be
heavily dependent on style.</abstract>
		<title>Talking Drums: Generating drum grooves with neural networks</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28479e7d2b4c5a8663faf4d6939316564/lepsky</id>
		<tags>musik</tags>
		<tags>inhaltserschliessung</tags>
		<description></description>
		<date>2018-11-04 17:02:36</date>
		<count>1</count>
		<journal>Cataloging & Classification Quarterly</journal>
		<year>2018</year>
		<url>https://doi.org/10.1080/01639374.2017.1422582</url>
		<author>Shuheng Wu</author>
		<author>Yun Fan</author>
		<authors>
			<first>Shuheng</first>
		</authors>
		<authors>
			<last>Wu</last>
		</authors>
		<authors>
			<first>Yun</first>
		</authors>
		<authors>
			<last>Fan</last>
		</authors>
		<volume>56</volume>
		<number>4</number>
		<pages>330--353</pages>
		<abstract>This study examined the characteristics of users' free-text queries submitted to RILM Abstracts of Music Literature (a music literature database), and compared those queries with the controlled vocabularies used by RILM. Search-log analysis identified 11 categories of user-created search terms, and mapped each user-created search term to RILM's index terms, assessing whether it was a perfect match, a partial match, or no match. Only 30.04\% of the user-created search terms did not match RILM's index terms. Most of the partial-matching and non-matching user-created search terms were personal names, work titles, and topical terms. Suggestions are offered to enhance RILM's controlled vocabularies.</abstract>
		<shorttitle>Music Literature Indexing</shorttitle>
		<issn>0163-9374</issn>
		<doi>10.1080/01639374.2017.1422582</doi>
		<title>Music literature indexing : comparing users' free-text queries and controlled vocabularies</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29d1e95f743a8b2421edf83c097e89b18/brusilovsky</id>
		<tags>individual-differences</tags>
		<tags>personal-traits</tags>
		<tags>visual-recommender</tags>
		<tags>recommender</tags>
		<description>Effects of Individual Traits on Diversity-Aware Music Recommender User Interfaces</description>
		<date>2019-02-19 15:45:22</date>
		<count>2</count>
		<booktitle>Proceedings of the 26th Conference on User Modeling, Adaptation and Personalization</booktitle>
		<series>UMAP '18</series>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2018</year>
		<url>http://doi.acm.org/10.1145/3209219.3209225</url>
		<author>Yucheng Jin</author>
		<author>Nava Tintarev</author>
		<author>Katrien Verbert</author>
		<authors>
			<first>Yucheng</first>
		</authors>
		<authors>
			<last>Jin</last>
		</authors>
		<authors>
			<first>Nava</first>
		</authors>
		<authors>
			<last>Tintarev</last>
		</authors>
		<authors>
			<first>Katrien</first>
		</authors>
		<authors>
			<last>Verbert</last>
		</authors>
		<pages>291--299</pages>
		<abstract>When recommendations become increasingly personalized, users are often presented with a narrower range of content. To mitigate this issue, diversity-enhanced user interfaces for recommender systems have in the past found to be effective in increasing overall user satisfaction with recommendations. However, users may have different requirements for diversity, and consequently different visualization requirements. In this paper, we evaluate two visual user interfaces, SimBub and ComBub, to present the diversity of a music recommender system from different perspectives. SimBub is a baseline bubble chart that shows music genres and popularity by color and size, respectively. In addition, ComBub visualizes selected audio features along the X and Y axis in a more advanced and complex visualization. Our goal is to investigate how individual traits such as musical sophistication (MS) and visual memory (VM) influence the satisfaction of the visualization for perceived music diversity, overall usability, and support to identify blind-spots. We hypothesize that music experts, or people with better visual memory, will perceive higher diversity in ComBub than SimBub. A within-subjects user study (N=83) is conducted to compare these two visualizations. Results of our study show that participants with high MS and VM tend to perceive significantly higher diversity from ComBub compared to SimBub. In contrast, participants with low MS perceived significantly higher diversity from SimBub than ComBub; however, no significant result is found for the participants with low VM. Our research findings show the necessity of considering individual traits while designing diversity-aware interfaces.</abstract>
		<isbn>978-1-4503-5589-6</isbn>
		<doi>10.1145/3209219.3209225</doi>
		<title>Effects of Individual Traits on Diversity-Aware Music Recommender User Interfaces</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a42d6328a8479ae9dee097a8f19cbe6f/meneteqel</id>
		<tags>young_workers</tags>
		<tags>jazz_musicians</tags>
		<tags>autonomy</tags>
		<tags>artistic_work</tags>
		<tags>jazz</tags>
		<tags>musicians</tags>
		<tags>networks</tags>
		<tags>creative_work</tags>
		<tags>creative_labour</tags>
		<description></description>
		<date>2019-04-29 10:35:45</date>
		<count>1</count>
		<journal>Work, Employment and Society</journal>
		<publisher>SAGE Publications</publisher>
		<year>2014</year>
		<url>https://journals.sagepub.com/doi/10.1177/0950017013491452</url>
		<author>Charles Umney</author>
		<author>Lefteris Kretsos</author>
		<authors>
			<first>Charles</first>
		</authors>
		<authors>
			<last>Umney</last>
		</authors>
		<authors>
			<first>Lefteris</first>
		</authors>
		<authors>
			<last>Kretsos</last>
		</authors>
		<volume>28</volume>
		<number>4</number>
		<pages>571--588</pages>
		<abstract>This article explores the types of work undertaken by jazz musicians in London, categorizing their activities using two axes derived from debates over ‘creative labour’. Firstly, the extent to which different jobs offer scope for creative autonomy and, secondly, the extent to which they involve collective as opposed to individualized working relationships. It focuses on the process of becoming established on the London ‘scene’, presenting qualitative interview data primarily with young workers seeking to build their careers. Musicians may make conscious decisions to pursue types of work which enable greater creative autonomy, but in doing so they may exacerbate fatalism about poor working conditions and undermine professional solidarity. The article also explores how pressures towards ‘entrepreneurialism’ in other forms of music work constitute further barriers to collective contestation of working conditions. Finally, it points towards types of music work where notions of professional economic interest have more traction.</abstract>
		<language>eng</language>
		<doi>10.1177/0950017013491452</doi>
		<title>Creative labour and collective interaction: the working lives of young jazz musicians in London</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23c4b95962bf08bec9e1a8f44712b432f/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<year>2001</year>
		<url>http://recherche.ircam.fr/equipes/analyse-synthese/wanderle/Gestes/Externe/index.html{\%}5Cnhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.137.9460{\&}rep=rep1{\&}type=pdf</url>
		<author>Marcelo Mortensen Wanderley</author>
		<authors>
			<first>Marcelo Mortensen</first>
		</authors>
		<authors>
			<last>Wanderley</last>
		</authors>
		<pages>14</pages>
		<abstract>Digital musical instruments do not depend on physical constraints
	faced by their acoustic counterparts, such as characteristics of
	tubes, membranes, strings, etc. This fact permits a huge diversity
	of possi- bilities regarding sound production, but on the other hand
	strategies to design and perform these new instruments need to be
	devised in order to provide the same level of control subtlety available
	in acous- tic instruments. In this paper I review various topics
	related to gestural control of music using digital musical instruments
	and identify possible trends in this domain.</abstract>
		<journaltitle>Supervision and Control in Engineering and Music</journaltitle>
		<title>Gestural control of music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2194a74ce290f850c6aed05b783004615/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<publisher>University of California Press 2000 Center St., Ste. 303, Berkeley, CA 94704-1223 USA journals@ ucpress. edu</publisher>
		<year>2006</year>
		<url></url>
		<author>Emilios Cambouropoulos</author>
		<authors>
			<first>Emilios</first>
		</authors>
		<authors>
			<last>Cambouropoulos</last>
		</authors>
		<volume>23</volume>
		<number>3</number>
		<pages>249--268</pages>
		<abstract>Despite the consideration that musical parallelism is an important
	factor for musical segmentation, there have been relatively few systematic
	attempts to describe exactly how it affects grouping processes. The
	main problem is that musical parallelism itself is difficult to formalize.
	In this study, a computational model that extracts melodic patterns
	from a given melodic surface is presented. Following the assumption
	that the beginning and ending points of “significant” repeating musical
	patterns influence the segmentation of a musical surface, the discovered
	patterns are used as a means to determine probable segmentation points
	of the melody. “Significant” patterns are defined primarily in terms
	of frequency of occurrence and length of pattern. The special status
	of nonoverlapping, immediately repeating patterns is examined. All
	the discovered patterns merge into a single “pattern” segmentation
	profile that signifies points in the surface most likely to be perceived
	as points of segmentation. The effectiveness of the proposed melodic
	representations and algorithms is tested against a series of melodic
	surfaces illustrating both strengths and weaknesses of the approach.</abstract>
		<journaltitle>Music Perception</journaltitle>
		<doi>10.1525/mp.2006.23.3.249</doi>
		<title>Musical Parallelism and Melodic Segmentation</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28dae67ed344470cedc6f7c16a0de240e/sapo</id>
		<tags>Internet_of_Things</tags>
		<tags>IoT</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>Dmrn+12: Digital Music Research Network One-Day Workshop 2017</booktitle>
		<year>2017</year>
		<url>https://www.researchgate.net/publication/321950946</url>
		<author>Luca Turchet</author>
		<author>Mathieu Barthet</author>
		<authors>
			<first>Luca</first>
		</authors>
		<authors>
			<last>Turchet</last>
		</authors>
		<authors>
			<first>Mathieu</first>
		</authors>
		<authors>
			<last>Barthet</last>
		</authors>
		<abstract>This paper presents an architecture supporting novel forms of tactile
	interactions between live music performers and audience members.
	Such interactions are enabled by the multidirectional communication
	between Smart Musical Instruments and Smart Musical Haptic Wearables.</abstract>
		<title>An Internet of Musical Things architecture for performers-audience tactile interactions</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29b21b9de68e03f362ccadf6e88c8e68f/sapo</id>
		<tags>Real-time</tags>
		<tags>Score-to-audio_alignment</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>2</count>
		<booktitle>Advances in Music Information Retrieval</booktitle>
		<publisher>Springer Berlin Heidelberg</publisher>
		<year>2010</year>
		<url></url>
		<author>Riccardo Miotto</author>
		<author>Nicola Montecchio</author>
		<author>Nicola Orio</author>
		<authors>
			<first>Riccardo</first>
		</authors>
		<authors>
			<last>Miotto</last>
		</authors>
		<authors>
			<first>Nicola</first>
		</authors>
		<authors>
			<last>Montecchio</last>
		</authors>
		<authors>
			<first>Nicola</first>
		</authors>
		<authors>
			<last>Orio</last>
		</authors>
		<editor>Zbigniew W. Raś</editor>
		<editor>Alicja A. Wieczorkowska</editor>
		<editors>
			<first>Nicola</first>
		</editors>
		<editors>
			<last>Orio</last>
		</editors>
		<editors>
			<first>Nicola</first>
		</editors>
		<editors>
			<last>Orio</last>
		</editors>
		<pages>187--212</pages>
		<abstract>This paper describes a methodology for the statistical modeling of music works. Starting from either the representation of the symbolic score or the audio recording of a performance, a hidden Markov model is built to represent the corresponding music work. The model can be used to identify unknown recordings and to align them with the corresponding score. Experimental evaluation using a collection of classical music recordings showed that this approach is effective in terms of both identification and alignment. The methodology can be exploited as the core component for a set of tools aimed at accessing and actively listening to a music collection.</abstract>
		<isbn>978-3-642-11674-2</isbn>
		<doi>10.1007/978-3-642-11674-2_9</doi>
		<title>Statistical Music Modeling Aimed at Identification and Alignment</title>
		<pubtype>inbook</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/268a3fbc35a128ea2eb4a59e82bd8c5bb/sapo</id>
		<tags>Real-time</tags>
		<tags>Score-to-audio_alignment</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>Vienna Talk on Music Acoustics, (2015),</booktitle>
		<year>2015</year>
		<url></url>
		<author>Andreas Arzt</author>
		<author>Werner Goebl</author>
		<author>Gerhard Widmer</author>
		<authors>
			<first>Andreas</first>
		</authors>
		<authors>
			<last>Arzt</last>
		</authors>
		<authors>
			<first>Werner</first>
		</authors>
		<authors>
			<last>Goebl</last>
		</authors>
		<authors>
			<first>Gerhard</first>
		</authors>
		<authors>
			<last>Widmer</last>
		</authors>
		<abstract>In our talk we will present a piano music companion that is able to follow and understand (at least to some extent) a live piano performance. Within a few seconds the system is able to identify the piece that is being played, and the position within the piece. It then tracks the performance over time via a robust score following algorithm. Furthermore, the system continuously re-evaluates its current position hypotheses within a database of scores and is capable of detecting arbitrary ‘jumps’ by the performer. The system can be of use in multiple ways, e.g. for piano rehearsal, for live visualisation of music, and for automatic page turning. At the conference, we will demonstrate this system live on stage. If possible, we would also like to encourage (hobby-)pianists in the audience to try the companion themselves. Additionally, we will give an outlook on our efforts to extend this approach to classical music in general, including heavily polyphonic orchestral music.</abstract>
		<language>EN</language>
		<title>Flexible Score Following: The Piano Music Companion and Beyond</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/268d1758cdfa2cad7d593dc557b491558/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<publisher>Taylor & Francis</publisher>
		<year>2005</year>
		<url></url>
		<author>Alan Marsden</author>
		<authors>
			<first>Alan</first>
		</authors>
		<authors>
			<last>Marsden</last>
		</authors>
		<volume>34</volume>
		<number>4</number>
		<pages>409--428</pages>
		<abstract>The usefulness and desirability of representation schemes which explicitly
	show musical structure has often been commented upon. A particular
	aim of music theory and analysis has been to describe and derive
	musical structure, and this article discusses computational systems
	based on this work. Six desirable properties of a structural representation
	are described: that it should be constructive, derivable, meaningful,
	decomposable, hierarchical, and generative. Previous computational
	work based on the generative and reductional theories of Schenker
	and of Lerdahl and Jackendoff is examined in the light of these properties.
	Proposals are made for a representational framework which promises
	the desirable properties. The framework shares characteristics with
	earlier work but does not use pure trees as a representational structure,
	instead allowing joining of branches in limited circumstances to
	make directed acyclic graphs. Important issues in developing a representation
	scheme within this framework are discussed, especially concerning
	the representation of polyphonic music, of rhythmic patterns, and
	of up-beats. An example is given of two alternative representations
	within this framework of the same segment of music used to exemplify
	earlier work: the opening of the theme of Mozart's piano sonata in
	A major, K.331.</abstract>
		<journaltitle>Journal of New Music Research</journaltitle>
		<doi>10.1080/09298210600578295</doi>
		<title>Generative structural representation of tonal music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2701f31f2d4836e39172d49f1fb21c2f7/sapo</id>
		<tags>music_analysis</tags>
		<tags>Digital_Musicology</tags>
		<tags>content-based_music_resources</tags>
		<tags>music_encoding</tags>
		<tags>music_scores_corpora</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>2</count>
		<publisher>Frontiers</publisher>
		<year>2015</year>
		<url></url>
		<author>Laurent Pugin</author>
		<authors>
			<first>Laurent</first>
		</authors>
		<authors>
			<last>Pugin</last>
		</authors>
		<volume>2</volume>
		<pages>4</pages>
		<abstract>Most of our work in the humanities is increasingly driven by digital
	technology. Musicology is no exception and the field is undergoing
	the same revolution as all disciplines in the humanities. There are
	at least two key areas in which digital technology is transforming
	research: access and scale. Technology, and the internet in particular,
	has radically changed how we can access data, but also how we can
	make research results accessible to others. Correlatively, the scope
	of projects can be broadened to a completely new extent. What does
	this mean for musicology? Scholars in musicology base their work
	on a wide range of materials. Since most of the music that forms
	our heritage in Western culture has been preserved in a text-based
	form, this is by far, the most widely used type of material for musicological
	studies. Handwritten and printed sources constitute the core data,
	but historical studies also rely on various types of textual and
	archival material, be they letter writings, libretti, or inventories
	of diverse kind. These are essential for understanding the socio-economic
	context in which the music sources were written or produced and for
	better understanding of specific aspects, such as performance practice
	of the time. Performance practice study itself may also be based
	on sound recordings when focusing on relatively recent history, as
	it is often the case for studies in ethno-musicology or in folk-songs
	(Cook, 2010). Obtaining access to the sources has always been a struggle
	for musicologists. Only a few years ago, studying a particular source
	meant first locating the relevant sources using printed bibliographies,
	writing to the holding library, and then waiting for a microfilm
	to be prepared and sent out. The process could take months and be
	unpredictably expensive, with no guarantee of success. Such an obstacle
	seriously reduced the breadth of research musicologists could reasonably
	envisage, with a consequent inclination toward close-reading approaches
	on a restricted set of sources. With the coming of the digital world,
	the situation changed. Many resources are now available online, including
	the bibliographic finding aids, which makes locating sources significantly
	easier. Collections are being digitized and made accessible online,
	which greatly facilitates access to them for musicologists. This
	is also the case for secondary sources. Some projects are composer-specific,
	such as the Digital Archive of the Beethoven-Haus, others are repertoire-oriented,
	such as the digital image archive for medieval manuscripts (DIAMM)
	or based on a particular library collection, such as the Julliard
	Manuscript Collection, to cite only three examples. In the archives,
	digital cameras are often allowed and can be used to capture sources
	quickly. It is now straightforward for scholars to store thousands
	of images on their personal computer, in the cloud, or even share
	them on community websites, although this in its turn raises new
	copyright concerns. What other issues need to be addressed? Digital
	access in musicology is still overwhelmingly linked to images. Several
	important digital musicology research projects, such as the OCVE
	and the Edirom projects, focusing mostly on philological issues have
	been very successful in relying extensively on digital image resources
	(Bradley and Vetch, 2007; Bohl et al., 2011). However, digital musicology
	projects that address a wide range of other issues, such as music
	analysis or music searching, require access to the music itself in
	digital form, are referred to as content-based resources. Musicology
	has never been behind other disciplines for experimenting with computational
	approaches in these domains, quite on the Frontiers in Digital Humanities
	| www.frontiersin.org August 2015 | Volume 2 | Article 4 1 Pugin
	Data in digital musicology contrary. However, obtaining or accessing
	high quality datasets remains a serious hurdle, especially on a large
	scale, in a similar way to accessing sources a couple of decades
	ago. It is a major barrier that needs to be removed if digital musicology
	research is to be taken to the next level. Several initiatives have
	laid down the basis for large-scale content-based resources. First
	and foremost, the CCARH with its KernScores repository 1 , which
	represents years of careful data creation and curation is made available
	for research and is an invaluable contribution. The Josquin Research
	Project (JRP 2) at Stanford is a groundbreaking project that is currently
	building a considerable dataset of pieces of Josquin des Prez and
	of other composers of the time (1400--1500). Another is the Electronic
	Locator of Vertical Interval Successions project at McGill Uni-versity
	(ELVIS 3). These two projects pursue similar goals and follow more
	or less comparable strategies: respectively creating or collecting
	a large collection of data and making it accessi-ble and analyzable
	by integrating state-of-the-art analysis tools Humdrum and Music21.
	Their output in terms of counterpoint analysis is a breakthrough
	and opens new perspectives for style analysis and composition attribution.
	The use of the harmonic and melodic intervals in ELVIS illuminates
	areas in which inno-vative research might be needed to address the
	question of how to represent music appropriately for such corpus-based
	analysis undertakings. These are undoubtedly models to follow, but
	they also illustrate how much still needs to be done. They hold a
	few thousand pieces 4</abstract>
		<journaltitle>Frontiers in Digital Humanities</journaltitle>
		<doi>10.3389/fdigh.2015.00004</doi>
		<title>The Challenge of Data in Digital Musicology</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d96758408c1aa25ec7cf2b40e52a8dcb/sapo</id>
		<tags>information_visualization</tags>
		<tags>music_information_retrieval</tags>
		<tags>classical_music</tags>
		<tags>recommender_systems</tags>
		<tags>personalization</tags>
		<tags>audio_processing</tags>
		<tags>creative_industries</tags>
		<tags>video_analysis</tags>
		<tags>concert</tags>
		<tags>human-computer_interaction</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>2015 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2015</booktitle>
		<publisher>IEEE</publisher>
		<year>2015</year>
		<url>http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7169835</url>
		<author>Cynthia C. S. Liem</author>
		<author>Emilia Gomez</author>
		<author>Markus Schedl</author>
		<authors>
			<first>Cynthia C. S.</first>
		</authors>
		<authors>
			<last>Liem</last>
		</authors>
		<authors>
			<first>Emilia</first>
		</authors>
		<authors>
			<last>Gomez</last>
		</authors>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<pages>1--4</pages>
		<abstract>PHENICX ('Performances as Highly Enriched aNd Interactive Concert
	eXperiences') is an EU FP7 project that lasts from February 2013
	to January 2016. It focuses on creating novel digital concert experiences,
	improving the accessibility of classical music concert performances
	by enhancing and enriching them in novel multimodal ways. This requires
	a usercentered approach throughout the project. After introducing
	the project, we discuss its goals, the technological challenges it
	offers, and current scientific and technological outcomes. Subsequently,
	we discuss how integrated prototypes combine several technological
	advances in the project into coherent user-ready interfaces, offering
	novel ways to experience the timeline of a concert, and rediscover
	and re-experience it afterwards. Finally, we discuss how PHENICX
	outcomes have been demonstrated live in concert halls. ©
	2015 IEEE.</abstract>
		<doi>10.1109/ICMEW.2015.7169835</doi>
		<title>PHENICX: Innovating the classical music experience</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d8606c6cbb5cd9a951ddfee610faf103/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<publisher>Routledge</publisher>
		<year>2007</year>
		<url></url>
		<author>Nicola Bernardini</author>
		<author>Giovanni De Poli</author>
		<authors>
			<first>Nicola</first>
		</authors>
		<authors>
			<last>Bernardini</last>
		</authors>
		<authors>
			<first>Giovanni</first>
		</authors>
		<authors>
			<last>De Poli</last>
		</authors>
		<volume>36</volume>
		<number>3</number>
		<pages>143--148</pages>
		<abstract>This paper is a general introduction for the theme of this special
	issue. It attempts to give a definition of the Sound and Music Computing
	research field stemming from its methodologies, aims and approaches.
	A brief account of the disciplines involved along with their academic
	organization follows, along with a short description of the areas
	of application involved. Since Sound and Music Computing has recently
	enjoyed a deep world-wide reflection upon its own goals, visions
	and perspectives which has resulted in several roadmapping exercises,
	the last part of this article provides a summary of these exercises
	by introducing them in the context in which they were created.</abstract>
		<journaltitle>Journal of New Music Research</journaltitle>
		<doi>10.1080/09298210701862432</doi>
		<title>The sound and music computing field: Present and future</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/205614dbbe115a6ba6c7eec55e6ede321/sapo</id>
		<tags>music_information_retrieval</tags>
		<tags>music_sequences</tags>
		<tags>tonality</tags>
		<tags>applications_in_multimedia</tags>
		<tags>2_tree_representation_for</tags>
		<tags>cognitive_modeling</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>24th IASTED Multi-Conference: Artificial Intelligence and Applications</booktitle>
		<series>Artificial Intelligence and Applications</series>
		<year>2006</year>
		<url>http://www.actapress.com/PDFViewer.aspx?paperId=23204</url>
		<author>David Rizo</author>
		<author>J. M. I. Quereda</author>
		<author>P. J. P. de León</author>
		<authors>
			<first>David</first>
		</authors>
		<authors>
			<last>Rizo</last>
		</authors>
		<authors>
			<first>J. M. I.</first>
		</authors>
		<authors>
			<last>Quereda</last>
		</authors>
		<authors>
			<first>P. J. P.</first>
		</authors>
		<authors>
			<last>de León</last>
		</authors>
		<volume>2006</volume>
		<number>level 1</number>
		<pages>299--304</pages>
		<abstract>Most of the western tonal music is based on the concept of tonality
	or key. It is often desirable to know the tonality of a song stored
	in a symbolic format (digital scores), both for content based management
	and musicological studies to name just two applications. The majority
	of the freely available symbolic music is coded in MIDI format. But,
	unfortunately many MIDI sequences do not contain the proper key meta-event
	that should be manually inserted at the beginning of the song. In
	this work, a polyphonic symbolic music representation that uses a
	tree model for tonality guessing is proposed. It has been compared
	to other previous methods available obtaining better success rates
	and lower performance times.</abstract>
		<title>Tree Model of Symbolic Music for Tonality Guessing</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24233f9c5b1aca029b6433b45ebf51ae2/sapo</id>
		<tags>music</tags>
		<tags>recommender_systems</tags>
		<tags>deep_learning</tags>
		<tags>semantics</tags>
		<tags>multimodal</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>3</count>
		<booktitle>Proceedings of the 2Nd Workshop on Deep Learning for Recommender Systems</booktitle>
		<series>DLRS 2017</series>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2017</year>
		<url></url>
		<author>Sergio Oramas</author>
		<author>Oriol Nieto</author>
		<author>Mohamed Sordo</author>
		<author>Xavier Serra</author>
		<authors>
			<first>Sergio</first>
		</authors>
		<authors>
			<last>Oramas</last>
		</authors>
		<authors>
			<first>Oriol</first>
		</authors>
		<authors>
			<last>Nieto</last>
		</authors>
		<authors>
			<first>Mohamed</first>
		</authors>
		<authors>
			<last>Sordo</last>
		</authors>
		<authors>
			<first>Xavier</first>
		</authors>
		<authors>
			<last>Serra</last>
		</authors>
		<pages>32--37</pages>
		<abstract>An increasing amount of digital music is being published daily. Music
	streaming services often ingest all available music, but this poses
	a challenge: how to recommend new artists for which prior knowledge
	is scarce? In this work we aim to address this so-called cold-start
	problem by combining text and audio information with user feedback
	data using deep network architectures. Our method is divided into
	three steps. First, artist embeddings are learned from biographies
	by combining semantics, text features, and aggregated usage data.
	Second, track embeddings are learned from the audio signal and available
	feedback data. Finally, artist and track embeddings are combined
	in a multimodal network. Results suggest that both splitting the
	recommendation problem between feature levels (i.e., artist metadata
	and audio track), and merging feature embeddings in a multimodal
	approach improve the accuracy of the recommendations.</abstract>
		<isbn>978-1-4503-5353-3</isbn>
		<doi>10.1145/3125486.3125492</doi>
		<title>A Deep Multimodal Approach for Cold-start Music Recommendation</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f6b67ae396f243bd5a41098c3d4b50f8/sapo</id>
		<tags>performance_analysis</tags>
		<tags>automatic_transcription</tags>
		<description>Multipitch Estimation of Piano Music by Exemplar-Based Sparse Representation - IEEE Journals & Magazine</description>
		<date>2019-09-20 10:03:23</date>
		<count>2</count>
		<journal>IEEE Trans. Multimedia</journal>
		<year>2012</year>
		<url>https://ieeexplore.ieee.org/abstract/document/6172242</url>
		<author>Cheng-Te Lee</author>
		<author>Yi-Hsuan Yang</author>
		<author>Homer H. Chen</author>
		<authors>
			<first>Cheng-Te</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Yi-Hsuan</first>
		</authors>
		<authors>
			<last>Yang</last>
		</authors>
		<authors>
			<first>Homer H.</first>
		</authors>
		<authors>
			<last>Chen</last>
		</authors>
		<volume>14</volume>
		<number>3-1</number>
		<pages>608-618</pages>
		<abstract>Pitch, together with other midlevel music features such as rhythm and timbre, holds the promise of bridging the semantic gap between low-level features and high-level semantics for music understanding. This paper investigates the pitch estimation of a piano music signal by exemplar-based sparse representation. A note exemplar is a segment of a piano note, stored in the dictionary. We first describe how to represent a segment of the piano music signal as a linear combination of a small number of note exemplars from a large note exemplar dictionary and then show how the sparse representation problem can be solved by -regularized minimization. The proposed approach incorporates tuning factor estimation, note candidate selection, and hidden-Markov-model-based smoothing into the estimation process to improve accuracy. Unlike previous approaches, the proposed approach does not require retraining for a new piano. Instead, only a dozen notes of the new piano are needed. This feature is computationally attractive and avoids intense manual labeling. The system performance is evaluated using 70 classical music recordings of two real pianos under different recording conditions. The results show that the proposed system outperforms four state-of-the-art systems.</abstract>
		<doi>10.1109/TMM.2012.2191398</doi>
		<title>Multipitch Estimation of Piano Music by Exemplar-Based Sparse Representation.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a9e281f088e8f58fbdc5ee6fd2fde48b/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2018-11-14 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#AhsanIK12</url>
		<author>Md. Rezwanul Ahsan</author>
		<author>Muhammad Ibn Ibrahimy</author>
		<author>Othman Omran Khalifa</author>
		<authors>
			<first>Md. Rezwanul</first>
		</authors>
		<authors>
			<last>Ahsan</last>
		</authors>
		<authors>
			<first>Muhammad Ibn</first>
		</authors>
		<authors>
			<last>Ibrahimy</last>
		</authors>
		<authors>
			<first>Othman Omran</first>
		</authors>
		<authors>
			<last>Khalifa</last>
		</authors>
		<pages>225-229</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>The Use of Artificial Neural Network in the Classification of EMG Signals.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/258750ffaf4a4efce790492f381101d57/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2018-11-14 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#BaisM12</url>
		<author>Abdul Bais</author>
		<author>Yasser L. Morgan</author>
		<authors>
			<first>Abdul</first>
		</authors>
		<authors>
			<last>Bais</last>
		</authors>
		<authors>
			<first>Yasser L.</first>
		</authors>
		<authors>
			<last>Morgan</last>
		</authors>
		<pages>201-206</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>Evaluation of Base Station Placement Scenarios for Mobile Node Localization.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/295184acb96fbda090c654f1aab19df90/gdmcbain</id>
		<tags>97m80-arts-music-language-architecture</tags>
		<tags>01a60-20th-century</tags>
		<description>Australian music and modernism, 1960-1975 (Livre, 2019) [WorldCat.org]</description>
		<date>2019-12-21 02:56:39</date>
		<count>1</count>
		<year>2019</year>
		<url>https://www.worldcat.org/title/australian-music-and-modernism-1960-1975/oclc/1128850252&referer=brief_results</url>
		<author>Michael Hooper</author>
		<authors>
			<first>Michael</first>
		</authors>
		<authors>
			<last>Hooper</last>
		</authors>
		<isbn>9781501348181 1501348183</isbn>
		<title>Australian music and modernism, 1960-1975</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b9cf31eff09f4240fce974d2fa307919/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2020-04-30 00:00:00</date>
		<count>1</count>
		<booktitle>The Oxford Handbook of Algorithmic Music</booktitle>
		<publisher>Oxford University Press</publisher>
		<year>2018</year>
		<url>http://dblp.uni-trier.de/db/books/collections/MD2018.html#Simoni18</url>
		<author>Mary Simoni</author>
		<authors>
			<first>Mary</first>
		</authors>
		<authors>
			<last>Simoni</last>
		</authors>
		<editor>Alex McLean</editor>
		<editor>Roger T. Dean</editor>
		<editors>
			<first>Mary</first>
		</editors>
		<editors>
			<last>Simoni</last>
		</editors>
		<editors>
			<first>Mary</first>
		</editors>
		<editors>
			<last>Simoni</last>
		</editors>
		<isbn>978-0-19-022699-2</isbn>
		<title>The Audience Reception of Algorithmic Music.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fda76ef0c5a43167d0f9b386637d887d/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2020-04-30 00:00:00</date>
		<count>1</count>
		<booktitle>The Oxford Handbook of Algorithmic Music</booktitle>
		<publisher>Oxford University Press</publisher>
		<year>2018</year>
		<url>http://dblp.uni-trier.de/db/books/collections/MD2018.html#Haworth18</url>
		<author>Christopher Haworth</author>
		<authors>
			<first>Christopher</first>
		</authors>
		<authors>
			<last>Haworth</last>
		</authors>
		<editor>Alex McLean</editor>
		<editor>Roger T. Dean</editor>
		<editors>
			<first>Christopher</first>
		</editors>
		<editors>
			<last>Haworth</last>
		</editors>
		<editors>
			<first>Christopher</first>
		</editors>
		<editors>
			<last>Haworth</last>
		</editors>
		<isbn>978-0-19-022699-2</isbn>
		<title>Technology, Creativity and The Social in Algorithmic Music.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27fb1429988cd56cd03010860dd6b7d12/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2020-04-30 00:00:00</date>
		<count>1</count>
		<booktitle>The Oxford Handbook of Algorithmic Music</booktitle>
		<publisher>Oxford University Press</publisher>
		<year>2018</year>
		<url>http://dblp.uni-trier.de/db/books/collections/MD2018.html#Brown18</url>
		<author>Andrew Brown</author>
		<authors>
			<first>Andrew</first>
		</authors>
		<authors>
			<last>Brown</last>
		</authors>
		<editor>Alex McLean</editor>
		<editor>Roger T. Dean</editor>
		<editors>
			<first>Andrew</first>
		</editors>
		<editors>
			<last>Brown</last>
		</editors>
		<editors>
			<first>Andrew</first>
		</editors>
		<editors>
			<last>Brown</last>
		</editors>
		<isbn>978-0-19-022699-2</isbn>
		<title>Algorithms and Computation in Music Education.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/284c04cef1336a06b74b6f38d6176c38d/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2020-04-30 00:00:00</date>
		<count>1</count>
		<booktitle>The Oxford Handbook of Algorithmic Music</booktitle>
		<publisher>Oxford University Press</publisher>
		<year>2018</year>
		<url>http://dblp.uni-trier.de/db/books/collections/MD2018.html#Rohrhuber18</url>
		<author>Julian Rohrhuber</author>
		<authors>
			<first>Julian</first>
		</authors>
		<authors>
			<last>Rohrhuber</last>
		</authors>
		<editor>Alex McLean</editor>
		<editor>Roger T. Dean</editor>
		<editors>
			<first>Julian</first>
		</editors>
		<editors>
			<last>Rohrhuber</last>
		</editors>
		<editors>
			<first>Julian</first>
		</editors>
		<editors>
			<last>Rohrhuber</last>
		</editors>
		<isbn>978-0-19-022699-2</isbn>
		<title>Algorithmic Music and the Philosophy of Time.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/280b739a2292aff5296cef89a2421b875/sapo</id>
		<description>End-to-end learning for music audio - IEEE Conference Publication</description>
		<date>2020-04-29 17:29:35</date>
		<count>2</count>
		<booktitle>ICASSP</booktitle>
		<year>2014</year>
		<url>https://ieeexplore.ieee.org/abstract/document/6854950</url>
		<author>Sander Dieleman</author>
		<author>Benjamin Schrauwen</author>
		<authors>
			<first>Sander</first>
		</authors>
		<authors>
			<last>Dieleman</last>
		</authors>
		<authors>
			<first>Benjamin</first>
		</authors>
		<authors>
			<last>Schrauwen</last>
		</authors>
		<pages>6964-6968</pages>
		<abstract>Content-based music information retrieval tasks have traditionally been solved using engineered features and shallow processing architectures. In recent years, there has been increasing interest in using feature learning and deep architectures instead, thus reducing the required engineering effort and the need for prior knowledge. However, this new approach typically still relies on mid-level representations of music audio, e.g. spectrograms, instead of raw audio signals. In this paper, we investigate whether it is possible to apply feature learning directly to raw audio signals. We train convolutional neural networks using both approaches and compare their performance on an automatic tagging task. Although they do not outperform a spectrogram-based approach, the networks are able to autonomously discover frequency decompositions from raw audio, as well as phase-and translation-invariant feature representations.</abstract>
		<issn>2379-190X</issn>
		<doi>10.1109/ICASSP.2014.6854950</doi>
		<title>End-to-end learning for music audio.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/283b9f70a82b7c2442c605f02fafe4596/sapo</id>
		<tags>automatic_transcription</tags>
		<description>A Holistic Approach to Polyphonic Music Transcription with Neural Networks | Zenodo</description>
		<date>2019-11-14 12:54:36</date>
		<count>1</count>
		<booktitle>Proceedings of the 20th International Society for                    Music Information Retrieval Conference</booktitle>
		<publisher>ISMIR</publisher>
		<address>Delft, The Netherlands</address>
		<year>2019</year>
		<url>https://doi.org/10.5281/zenodo.3527914</url>
		<author>Miguel Roman</author>
		<author>Antonio Pertusa</author>
		<author>Jorge Calvo-Zaragoza</author>
		<authors>
			<first>Miguel</first>
		</authors>
		<authors>
			<last>Roman</last>
		</authors>
		<authors>
			<first>Antonio</first>
		</authors>
		<authors>
			<last>Pertusa</last>
		</authors>
		<authors>
			<first>Jorge</first>
		</authors>
		<authors>
			<last>Calvo-Zaragoza</last>
		</authors>
		<pages>731-737</pages>
		<doi>10.5281/zenodo.3527914</doi>
		<title>A Holistic Approach to Polyphonic Music Transcription with Neural Networks</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a5948c2119f75622c6bd3e1213a577df/sapo</id>
		<tags>writer_recognition</tags>
		<tags>optical_music_recognition_systems</tags>
		<description>Writer Identification in Old Music Manuscripts Using Contour-Hinge Feature and Dimensionality Reduction with an Autoencoder | SpringerLink</description>
		<date>2020-07-01 13:20:20</date>
		<count>2</count>
		<booktitle>Computer Analysis of Images and Patterns</booktitle>
		<publisher>Springer Berlin Heidelberg</publisher>
		<address>Berlin, Heidelberg</address>
		<year>2013</year>
		<url>https://link.springer.com/chapter/10.1007/978-3-642-40246-3_69</url>
		<author>Masahiro Niitsuma</author>
		<author>Lambert Schomaker</author>
		<author>Jean-Paul van Oosten</author>
		<author>Yo Tomita</author>
		<authors>
			<first>Masahiro</first>
		</authors>
		<authors>
			<last>Niitsuma</last>
		</authors>
		<authors>
			<first>Lambert</first>
		</authors>
		<authors>
			<last>Schomaker</last>
		</authors>
		<authors>
			<first>Jean-Paul</first>
		</authors>
		<authors>
			<last>van Oosten</last>
		</authors>
		<authors>
			<first>Yo</first>
		</authors>
		<authors>
			<last>Tomita</last>
		</authors>
		<editor>Richard Wilson</editor>
		<editor>Edwin Hancock</editor>
		<editor>Adrian Bors</editor>
		<editor>William Smith</editor>
		<editors>
			<first>Yo</first>
		</editors>
		<editors>
			<last>Tomita</last>
		</editors>
		<editors>
			<first>Yo</first>
		</editors>
		<editors>
			<last>Tomita</last>
		</editors>
		<editors>
			<first>Yo</first>
		</editors>
		<editors>
			<last>Tomita</last>
		</editors>
		<editors>
			<first>Yo</first>
		</editors>
		<editors>
			<last>Tomita</last>
		</editors>
		<pages>555--562</pages>
		<abstract>Although most of the previous studies in writer identification in music scores assumed successful prior staff-line removal, this assumption does not hold when the music scores suffer from a certain level of degradation or deformation. The impact of staff-line removal on the result of writer identification in such documents is rather vague. In this study, we propose a novel writer identification method that requires no staff-line removal and no segmentation. Staff-line removal is virtually achieved without image processing, by dimensionality reduction with an autoencoder in Contour-Hinge feature space. The experimental result with a wide range of music manuscripts shows the proposed method can achieve favourable results without prior staff-line removal.</abstract>
		<isbn>978-3-642-40246-3</isbn>
		<title>Writer Identification in Old Music Manuscripts Using Contour-Hinge Feature and Dimensionality Reduction with an Autoencoder</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22f3b881f6605dae2e96989e79c2ff819/sapo</id>
		<tags>writer_recognition</tags>
		<tags>optical_music_recognition_systems</tags>
		<description>HMM-based writer identification in music score documents without staff-line removal - ScienceDirect</description>
		<date>2020-07-01 13:16:48</date>
		<count>3</count>
		<journal>Expert Systems with Applications</journal>
		<year>2017</year>
		<url>http://www.sciencedirect.com/science/article/pii/S0957417417305080</url>
		<author>Partha Pratim Roy</author>
		<author>Ayan Kumar Bhunia</author>
		<author>Umapada Pal</author>
		<authors>
			<first>Partha Pratim</first>
		</authors>
		<authors>
			<last>Roy</last>
		</authors>
		<authors>
			<first>Ayan Kumar</first>
		</authors>
		<authors>
			<last>Bhunia</last>
		</authors>
		<authors>
			<first>Umapada</first>
		</authors>
		<authors>
			<last>Pal</last>
		</authors>
		<volume>89</volume>
		<pages>222 - 240</pages>
		<abstract>Writer identification from musical score documents is a challenging task due to its inherent problem of overlapping of musical symbols with staff-lines. Most of the existing works in the literature of writer identification in musical score documents were performed after a pre-processing stage of staff-lines removal. In this paper we propose a novel writer identification framework in musical score documents without removing staff-lines from the documents. In our approach, Hidden Markov Model (HMM) has been used to model the writing style of the writers without removing staff-lines. The sliding window features are extracted from musical score-lines and they are used to build writer specific HMM models. Given a query musical sheet, writer specific confidence for each musical line is returned by each writer specific model using a log-likelihood score. Next, a log-likelihood score in page level is computed by weighted combination of these scores from the corresponding line images of the page. A novel Factor Analysis-based feature selection technique is applied in sliding window features to reduce the noise appearing from staff-lines which proves efficiency in writer identification performance. In our framework we have also proposed a novel score-line detection approach in musical sheet using HMM. The experiment has been performed in CVC-MUSCIMA data set and the results obtained show that the proposed approach is efficient for score-line detection and writer identification without removing staff-lines. To get the idea of computation time of our method, detail analysis of execution time is also provided.</abstract>
		<issn>0957-4174</issn>
		<doi>https://doi.org/10.1016/j.eswa.2017.07.031</doi>
		<title>HMM-based writer identification in music score documents without staff-line removal</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/236b52297e236f4eecd60b93a8fd8ecbf/sapo</id>
		<tags>perceptual_evaluation</tags>
		<tags>excerpt_duration</tags>
		<description></description>
		<date>2020-03-24 12:26:51</date>
		<count>2</count>
		<journal>Research Perspectives in Music Education</journal>
		<publisher>Florida Music Educators' Association, Inc.</publisher>
		<year>2016</year>
		<url>https://www.ingentaconnect.com/content/fmea/rpme/2016/00000018/00000001/art00002</url>
		<author>Matthew Williams</author>
		<authors>
			<first>Matthew</first>
		</authors>
		<authors>
			<last>Williams</last>
		</authors>
		<volume>18</volume>
		<number>1</number>
		<pages>16--25</pages>
		<abstract>The purpose of this study was to explore the effect of excerpt duration and performance level on evaluator ratings. Participants (N = 70) were undergraduate music education majors at a large university in the Southeastern United States. Participants listened to 15 excerpts and rated each on a 5-point scale. A factorial ANOVa with one between-subjects factor (order) and two within-subjects factors (excerpt duration and performance level) showed a significant interaction between performance level and duration. The mean ratings for the 'superior' performance level remained 'superior' as participants heard longer excerpts, whereas mean ratings of the 'poor' and 'good' performances decreased as excerpt duration increased.</abstract>
		<title>Effect of Excerpt Duration on Adjudicator Ratings of Middle School Band Performances</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2cdf3171e8b0d7e4d54f3fe4225065398/brusilovsky</id>
		<tags>cognitive-style</tags>
		<tags>individual-differences</tags>
		<tags>transparency</tags>
		<tags>umap2020</tags>
		<tags>recommender</tags>
		<description>Cogito ergo quid? The Effect of Cognitive Style in a Transparent Mobile Music Recommender System | Proceedings of the 28th ACM Conference on User Modeling, Adaptation and Personalization</description>
		<date>2020-07-16 22:10:20</date>
		<count>2</count>
		<booktitle>Proceedings of the 28th ACM Conference on User Modeling, Adaptation and Personalization</booktitle>
		<publisher>ACM</publisher>
		<year>2020</year>
		<url>https://doi.org/10.1145%2F3340631.3394871</url>
		<author>Martijn Millecamp</author>
		<author>Robin Haveneers</author>
		<author>Katrien Verbert</author>
		<authors>
			<first>Martijn</first>
		</authors>
		<authors>
			<last>Millecamp</last>
		</authors>
		<authors>
			<first>Robin</first>
		</authors>
		<authors>
			<last>Haveneers</last>
		</authors>
		<authors>
			<first>Katrien</first>
		</authors>
		<authors>
			<last>Verbert</last>
		</authors>
		<pages>323-327</pages>
		<abstract>An increasing body of research indicates that transparency in recommender systems affects trust of users. Additionally, a vast amount of studies already showed that personality impacts the way users perceive a recommender system. However, only recently, research has begun to investigate the effects of cognitive style on the perception of recommender systems. Furthermore, it is still unclear whether this cognitive style also affects the interaction strategies of users, and whether the reason why and when users want transparency is affected by this cognitive style. Additionally, despite the ubiquitous presence of recommender systems on mobile environments, no study has investigated the effect of transparency for mobile music recommender systems. In this paper, we report the results of a within-subject study (N=25) on a mobile music recommender system where we investigated the effect of cognitive styles on three different aspects: the interaction strategies with the different applications, the reasons why and when users want transparency and the effect of transparency on the trust of users. The results show that users with a rational thinking style put more effort in seeking the best recommendations and that they want scrutable explanations to adjust the recommendation. In contrast, intuitive thinkers only need explanations when they search for a very specific kind of music.</abstract>
		<doi>10.1145/3340631.3394871</doi>
		<title>Cogito ergo quid? The Effect of Cognitive Style in a Transparent Mobile Music Recommender System</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29af6081d62492eab3080f8ff93f81ccd/brusilovsky</id>
		<tags>user-control</tags>
		<tags>transparency</tags>
		<tags>umap2020</tags>
		<tags>information-visualization</tags>
		<tags>recommender</tags>
		<description>What's in a User? Towards Personalising Transparency for Music Recommender Interfaces | Proceedings of the 28th ACM Conference on User Modeling, Adaptation and Personalization</description>
		<date>2020-07-15 19:00:45</date>
		<count>2</count>
		<booktitle>Proceedings of the 28th ACM Conference on User Modeling, Adaptation and Personalization</booktitle>
		<publisher>ACM</publisher>
		<year>2020</year>
		<url>https://doi.org/10.1145%2F3340631.3394844</url>
		<author>Martijn Millecamp</author>
		<author>Nyi Nyi Htun</author>
		<author>Cristina Conati</author>
		<author>Katrien Verbert</author>
		<authors>
			<first>Martijn</first>
		</authors>
		<authors>
			<last>Millecamp</last>
		</authors>
		<authors>
			<first>Nyi Nyi</first>
		</authors>
		<authors>
			<last>Htun</last>
		</authors>
		<authors>
			<first>Cristina</first>
		</authors>
		<authors>
			<last>Conati</last>
		</authors>
		<authors>
			<first>Katrien</first>
		</authors>
		<authors>
			<last>Verbert</last>
		</authors>
		<abstract>We have become increasingly reliant on recommender systems to help us make decisions in our daily live. As such, it is becoming essential to explain to users how these systems reason to enable them to correct system assumptions and to trust the system. The advantages of explaining the recommendation process has been shown by a vast amount of research. Additionally, previous studies showed that personality affects users' attitudes, tastes and information processing. However, it is still unclear whether personality has an impact on the way users process and perceive explanations. In this paper, we report the results of a study that investigated differences between personal characteristics of the perception and the gaze pattern of a music recommender interface in the presence and absence of explanations. We investigated the differences between Need For Cognition, Musical Sophistication and the Big Five personality traits. Results show empirical evidence of the differences between Musical Sophistication and Openness on both perception and gaze pattern. We found that users with a high Musical Sophistication and a low Openness score benefit the most from explanations.</abstract>
		<doi>10.1145/3340631.3394844</doi>
		<title>What's in a User? Towards Personalising Transparency for Music Recommender Interfaces</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fff655457d0724ddf78f0bb830e47fa9/yihong-liu</id>
		<tags>#music</tags>
		<tags>#dl</tags>
		<description>POP909: A Pop-song Dataset for Music Arrangement Generation</description>
		<date>2020-08-18 05:42:32</date>
		<count>2</count>
		<year>2020</year>
		<url>http://arxiv.org/abs/2008.07142</url>
		<author>Ziyu Wang</author>
		<author>Ke Chen</author>
		<author>Junyan Jiang</author>
		<author>Yiyi Zhang</author>
		<author>Maoran Xu</author>
		<author>Shuqi Dai</author>
		<author>Xianbin Gu</author>
		<author>Gus Xia</author>
		<authors>
			<first>Ziyu</first>
		</authors>
		<authors>
			<last>Wang</last>
		</authors>
		<authors>
			<first>Ke</first>
		</authors>
		<authors>
			<last>Chen</last>
		</authors>
		<authors>
			<first>Junyan</first>
		</authors>
		<authors>
			<last>Jiang</last>
		</authors>
		<authors>
			<first>Yiyi</first>
		</authors>
		<authors>
			<last>Zhang</last>
		</authors>
		<authors>
			<first>Maoran</first>
		</authors>
		<authors>
			<last>Xu</last>
		</authors>
		<authors>
			<first>Shuqi</first>
		</authors>
		<authors>
			<last>Dai</last>
		</authors>
		<authors>
			<first>Xianbin</first>
		</authors>
		<authors>
			<last>Gu</last>
		</authors>
		<authors>
			<first>Gus</first>
		</authors>
		<authors>
			<last>Xia</last>
		</authors>
		<abstract>Music arrangement generation is a subtask of automatic music generation,
which involves reconstructing and re-conceptualizing a piece with new
compositional techniques. Such a generation process inevitably requires
reference from the original melody, chord progression, or other structural
information. Despite some promising models for arrangement, they lack more
refined data to achieve better evaluations and more practical results. In this
paper, we propose POP909, a dataset which contains multiple versions of the
piano arrangements of 909 popular songs created by professional musicians. The
main body of the dataset contains the vocal melody, the lead instrument melody,
and the piano accompaniment for each song in MIDI format, which are aligned to
the original audio files. Furthermore, we provide the annotations of tempo,
beat, key, and chords, where the tempo curves are hand-labeled and others are
done by MIR algorithms. Finally, we conduct several baseline experiments with
this dataset using standard deep music generation algorithms.</abstract>
		<title>POP909: A Pop-song Dataset for Music Arrangement Generation</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2389186c8c3640c9944ec2fea717fc170/rikbose</id>
		<tags>vocal</tags>
		<description>Deep Karaoke: Extracting Vocals from Musical Mixtures Using a Convolutional Deep Neural Network</description>
		<date>2020-11-08 06:25:54</date>
		<count>3</count>
		<year>2015</year>
		<url>http://arxiv.org/abs/1504.04658</url>
		<author>Andrew J. R. Simpson</author>
		<author>Gerard Roma</author>
		<author>Mark D. Plumbley</author>
		<authors>
			<first>Andrew J. R.</first>
		</authors>
		<authors>
			<last>Simpson</last>
		</authors>
		<authors>
			<first>Gerard</first>
		</authors>
		<authors>
			<last>Roma</last>
		</authors>
		<authors>
			<first>Mark D.</first>
		</authors>
		<authors>
			<last>Plumbley</last>
		</authors>
		<abstract>Identification and extraction of singing voice from within musical mixtures
is a key challenge in source separation and machine audition. Recently, deep
neural networks (DNN) have been used to estimate 'ideal' binary masks for
carefully controlled cocktail party speech separation problems. However, it is
not yet known whether these methods are capable of generalizing to the
discrimination of voice and non-voice in the context of musical mixtures. Here,
we trained a convolutional DNN (of around a billion parameters) to provide
probabilistic estimates of the ideal binary mask for separation of vocal sounds
from real-world musical mixtures. We contrast our DNN results with more
traditional linear methods. Our approach may be useful for automatic removal of
vocal sounds from musical mixtures for 'karaoke' type applications.</abstract>
		<title>Deep Karaoke: Extracting Vocals from Musical Mixtures Using a
  Convolutional Deep Neural Network</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d5a3da47106bcae11560129e8ebaf0c6/sapo</id>
		<tags>source_separation</tags>
		<description>Score-Informed Source Separation for Musical Audio Recordings: An overview - IEEE Journals & Magazine</description>
		<date>2020-11-12 16:21:31</date>
		<count>1</count>
		<journal>IEEE Signal Processing Magazine</journal>
		<year>2014</year>
		<url>https://ieeexplore.ieee.org/abstract/document/6784086</url>
		<author>S. Ewert</author>
		<author>B. Pardo</author>
		<author>M. Muller</author>
		<author>M. D. Plumbley</author>
		<authors>
			<first>S.</first>
		</authors>
		<authors>
			<last>Ewert</last>
		</authors>
		<authors>
			<first>B.</first>
		</authors>
		<authors>
			<last>Pardo</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Muller</last>
		</authors>
		<authors>
			<first>M. D.</first>
		</authors>
		<authors>
			<last>Plumbley</last>
		</authors>
		<volume>31</volume>
		<number>3</number>
		<pages>116--124</pages>
		<abstract>In recent years, source separation has been a central research topic in music signal processing, with applications in stereo-to-surround up-mixing, remixing tools for disc jockeys or producers, instrument-wise equalizing, karaoke systems, and preprocessing in music analysis tasks. Musical sound sources, however, are often strongly correlated in time and frequency, and without additional knowledge about the sources, a decomposition of a musical recording is often infeasible. To simplify this complex task, various methods have recently been proposed that exploit the availability of a musical score. The additional instrumentation and note information provided by the score guides the separation process, leading to significant improvements in terms of separation quality and robustness. A major challenge in utilizing this rich source of information is to bridge the gap between high-level musical events specified by the score and their corresponding acoustic realizations in an audio recording. In this article, we review recent developments in score-informed source separation and discuss various strategies for integrating the prior knowledge encoded by the score.</abstract>
		<issn>1558-0792</issn>
		<doi>10.1109/MSP.2013.2296076</doi>
		<title>Score-informed Source Separation for Musical Audio Recordings: An Overview</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25e8ff4f61e4d4b0f7c8f187651005300/sapo</id>
		<tags>audio_restoration</tags>
		<description>GACELA: A Generative Adversarial Context Encoder for Long Audio Inpainting of Music | IEEE Journals & Magazine | IEEE Xplore https://ieeexplore.ieee.org/abstract/document/9257074</description>
		<date>2021-11-13 17:25:51</date>
		<count>2</count>
		<journal>IEEE Journal of Selected Topics in Signal Processing</journal>
		<year>2021</year>
		<url>https://ieeexplore.ieee.org/abstract/document/9257074</url>
		<author>Andrés Marafioti</author>
		<author>Piotr Majdak</author>
		<author>Nicki Holighaus</author>
		<author>Nathanaël Perraudin</author>
		<authors>
			<first>Andrés</first>
		</authors>
		<authors>
			<last>Marafioti</last>
		</authors>
		<authors>
			<first>Piotr</first>
		</authors>
		<authors>
			<last>Majdak</last>
		</authors>
		<authors>
			<first>Nicki</first>
		</authors>
		<authors>
			<last>Holighaus</last>
		</authors>
		<authors>
			<first>Nathanaël</first>
		</authors>
		<authors>
			<last>Perraudin</last>
		</authors>
		<volume>15</volume>
		<number>1</number>
		<pages>120-131</pages>
		<abstract>In this article, we introduce GACELA, a conditional generative adversarial network (cGAN) designed to restore missing audio data with durations ranging between hundreds of milliseconds and a few seconds, i.e., to perform long-gap audio inpainting. While previous work either addressed shorter gaps or relied on exemplars by copying available information from other signal parts, GACELA addresses the inpainting of long gaps in two aspects. First, it considers various time scales of audio information by relying on five parallel discriminators with increasing resolution of receptive fields. Second, it is conditioned not only on the available information surrounding the gap, i.e., the context, but also on the latent variable of the cGAN. This addresses the inherent multi-modality of audio inpainting for such long gaps while providing the user with different inpainting options. GACELA was evaluated in listening tests on music signals of varying complexity and varying gap durations from 375 to 1500 ms. Under laboratory conditions, our subjects were often able to detect the inpainting. However, the severity of the inpainted artifacts was rated between not disturbing and mildly disturbing. GACELA represents a framework capable of integrating future improvements such as processing of more auditory-related features or explicit musical features. Our software and trained models, complemented by instructive examples, are available online.</abstract>
		<issn>1941-0484</issn>
		<doi>10.1109/JSTSP.2020.3037506</doi>
		<title>GACELA: A Generative Adversarial Context Encoder for Long Audio Inpainting of Music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21b18e98b48746c93a7a16d91690c6e7c/brusilovsky</id>
		<tags>conversational-recommender</tags>
		<tags>critique</tags>
		<tags>interactive-recommender</tags>
		<tags>iui2021</tags>
		<tags>information-exploration</tags>
		<description>Critiquing for Music Exploration in Conversational Recommender Systems | 26th International Conference on Intelligent User Interfaces</description>
		<date>2022-01-16 21:07:14</date>
		<count>1</count>
		<booktitle>26th International Conference on Intelligent User Interfaces</booktitle>
		<publisher>ACM</publisher>
		<year>2021</year>
		<url>https://doi.org/10.1145%2F3397481.3450657</url>
		<author>Wanling Cai</author>
		<author>Yucheng Jin</author>
		<author>Li Chen</author>
		<authors>
			<first>Wanling</first>
		</authors>
		<authors>
			<last>Cai</last>
		</authors>
		<authors>
			<first>Yucheng</first>
		</authors>
		<authors>
			<last>Jin</last>
		</authors>
		<authors>
			<first>Li</first>
		</authors>
		<authors>
			<last>Chen</last>
		</authors>
		<pages>480-490</pages>
		<abstract>Dialogue-based conversational recommender systems allow users to give language-based feedback on the recommended item, which has great potential for supporting users to explore the space of recommendations through conversation. In this work, we consider incorporating critiquing techniques into conversational systems to facilitate users’ exploration of music recommendations. Thus, we have developed a music chatbot with three system variants, which are respectively featured with three different critiquing techniques, i.e., user-initiated critiquing (UC), progressive system-suggested critiquing (Progressive SC), and cascading system-suggested critiquing (Cascading SC). We conducted a between-subject study (N=107) to compare these three types of systems with regards to music exploration in terms of user perception and user interaction. Results show that both UC and SC are useful for music exploration, while users perceive higher diversity of recommendations with the system that offers Cascading SC and perceive more serendipitous with the system that offers Progressive SC. In addition, we find that the critiquing techniques significantly moderate the relationships between some interaction metrics (e.g., number of listened songs, number of dialogue turns) and users’ perceived helpfulness and serendipity during music exploration.</abstract>
		<doi>10.1145/3397481.3450657</doi>
		<title>Critiquing for Music Exploration in Conversational Recommender Systems</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/266624a4122208c664f3819cd7f2362dd/choko</id>
		<tags>television,TiVo,advertising,affective,affective</tags>
		<tags>Interactive</tags>
		<tags>character,entertainment,human-computer</tags>
		<tags>video,set-top</tags>
		<tags>interaction,multimedia,music</tags>
		<tags>computing,user</tags>
		<tags>usability,animated</tags>
		<tags>box,ubiquitous</tags>
		<tags>interface</tags>
		<description></description>
		<date>2013-12-30 18:09:17</date>
		<count>3</count>
		<journal>Comput. Entertain.</journal>
		<publisher>ACM Press</publisher>
		<year>2004</year>
		<url>http://portal.acm.org/citation.cfm?id=1027177</url>
		<author>Konstantinos Chorianopoulos</author>
		<author>Diomidis Spinellis</author>
		<authors>
			<first>Konstantinos</first>
		</authors>
		<authors>
			<last>Chorianopoulos</last>
		</authors>
		<authors>
			<first>Diomidis</first>
		</authors>
		<authors>
			<last>Spinellis</last>
		</authors>
		<volume>2</volume>
		<number>3</number>
		<pages>14--14</pages>
		<abstract>Computer-mediated television brings new requirements for user interface design and evaluation, since interactive television applications are deployed in a relaxed domestic setting and aim to gratify the need for entertainment. Digital video recorders, the generation of custom computer graphics on each digital set-top box, and the introduction of new advertising formats are important issues for research and practice. We explore the employment of an animated character and the dynamic insertion of advertising in the design of an intuitive user interface for interactive music-video television. We found that the animated character and the skippable videoclip feature seamlessly enhanced consumer satisfaction, as shown by affective usability questionnaires.</abstract>
		<issn>1544-3574</issn>
		<doi>10.1145/1027154.1027177</doi>
		<title>Affective usability evaluation for an interactive music television channel</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a678c5bd47e88a99833c2f2428a6809a/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#ChoM13</url>
		<author>Young-Sung Cho</author>
		<author>Song Chul Moon</author>
		<authors>
			<first>Young-Sung</first>
		</authors>
		<authors>
			<last>Cho</last>
		</authors>
		<authors>
			<first>Song Chul</first>
		</authors>
		<authors>
			<last>Moon</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Song Chul</first>
		</editors>
		<editors>
			<last>Moon</last>
		</editors>
		<editors>
			<first>Song Chul</first>
		</editors>
		<editors>
			<last>Moon</last>
		</editors>
		<editors>
			<first>Song Chul</first>
		</editors>
		<editors>
			<last>Moon</last>
		</editors>
		<editors>
			<first>Song Chul</first>
		</editors>
		<editors>
			<last>Moon</last>
		</editors>
		<volume>274</volume>
		<pages>441-450</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Weighted Mining Frequent Itemsets Using FP-Tree Based on RFM for Personalized u-Commerce Recommendation System.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c31eba6fa45a905b37c9f5bd1b6a670f/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#KimCKM13</url>
		<author>Sung Ki Kim</author>
		<author>Jae-Yeong Choi</author>
		<author>Byung-Gyu Kim</author>
		<author>Byoung-Joon Min</author>
		<authors>
			<first>Sung Ki</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Jae-Yeong</first>
		</authors>
		<authors>
			<last>Choi</last>
		</authors>
		<authors>
			<first>Byung-Gyu</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Byoung-Joon</first>
		</authors>
		<authors>
			<last>Min</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Byoung-Joon</first>
		</editors>
		<editors>
			<last>Min</last>
		</editors>
		<editors>
			<first>Byoung-Joon</first>
		</editors>
		<editors>
			<last>Min</last>
		</editors>
		<editors>
			<first>Byoung-Joon</first>
		</editors>
		<editors>
			<last>Min</last>
		</editors>
		<editors>
			<first>Byoung-Joon</first>
		</editors>
		<editors>
			<last>Min</last>
		</editors>
		<volume>274</volume>
		<pages>117-124</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Intrusion-Tolerant Jini Service Architecture for Ensuring Survivability of U-Services Based on WSN.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22948dbaa61f75d8b9637c6b34dfee1ab/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#ChoiCCKLKY13</url>
		<author>Jinsung Choi</author>
		<author>Okkyung Choi</author>
		<author>Yun Cui</author>
		<author>Myoungjin Kim</author>
		<author>Hanku Lee</author>
		<author>Kangseok Kim</author>
		<author>Hongjin Yeh</author>
		<authors>
			<first>Jinsung</first>
		</authors>
		<authors>
			<last>Choi</last>
		</authors>
		<authors>
			<first>Okkyung</first>
		</authors>
		<authors>
			<last>Choi</last>
		</authors>
		<authors>
			<first>Yun</first>
		</authors>
		<authors>
			<last>Cui</last>
		</authors>
		<authors>
			<first>Myoungjin</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Hanku</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Kangseok</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Hongjin</first>
		</authors>
		<authors>
			<last>Yeh</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Hongjin</first>
		</editors>
		<editors>
			<last>Yeh</last>
		</editors>
		<editors>
			<first>Hongjin</first>
		</editors>
		<editors>
			<last>Yeh</last>
		</editors>
		<editors>
			<first>Hongjin</first>
		</editors>
		<editors>
			<last>Yeh</last>
		</editors>
		<editors>
			<first>Hongjin</first>
		</editors>
		<editors>
			<last>Yeh</last>
		</editors>
		<volume>274</volume>
		<pages>391-395</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Authority Delegation for Safe Social Media Services in Mobile NFC Environment.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20bc9c20ede97a0d646bd7fabdff8557d/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#ChoiSK13</url>
		<author>Maengsik Choi</author>
		<author>Junsoo Shin</author>
		<author>Harksoo Kim</author>
		<authors>
			<first>Maengsik</first>
		</authors>
		<authors>
			<last>Choi</last>
		</authors>
		<authors>
			<first>Junsoo</first>
		</authors>
		<authors>
			<last>Shin</last>
		</authors>
		<authors>
			<first>Harksoo</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Harksoo</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Harksoo</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Harksoo</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Harksoo</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<volume>274</volume>
		<pages>225-229</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Lexical Feature Extraction Method for Classification of Erroneous Online Customer Reviews Based on Pattern Matching.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23cba7f5b216e6786e3c344ca7311b114/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#HsiehLHHHL13</url>
		<author>Ting-An Hsieh</author>
		<author>Kuan-Ching Li</author>
		<author>Kuo-Chan Huang</author>
		<author>Kuo-Hsun Hsu</author>
		<author>Ching-Hsien Hsu</author>
		<author>Kuan-Chou Lai</author>
		<authors>
			<first>Ting-An</first>
		</authors>
		<authors>
			<last>Hsieh</last>
		</authors>
		<authors>
			<first>Kuan-Ching</first>
		</authors>
		<authors>
			<last>Li</last>
		</authors>
		<authors>
			<first>Kuo-Chan</first>
		</authors>
		<authors>
			<last>Huang</last>
		</authors>
		<authors>
			<first>Kuo-Hsun</first>
		</authors>
		<authors>
			<last>Hsu</last>
		</authors>
		<authors>
			<first>Ching-Hsien</first>
		</authors>
		<authors>
			<last>Hsu</last>
		</authors>
		<authors>
			<first>Kuan-Chou</first>
		</authors>
		<authors>
			<last>Lai</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Kuan-Chou</first>
		</editors>
		<editors>
			<last>Lai</last>
		</editors>
		<editors>
			<first>Kuan-Chou</first>
		</editors>
		<editors>
			<last>Lai</last>
		</editors>
		<editors>
			<first>Kuan-Chou</first>
		</editors>
		<editors>
			<last>Lai</last>
		</editors>
		<editors>
			<first>Kuan-Chou</first>
		</editors>
		<editors>
			<last>Lai</last>
		</editors>
		<volume>274</volume>
		<pages>609-614</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Community Identification in Multiple Relationship Social Networks.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22feea1a1c6f9661d9d15924cf8e315c7/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#Min13</url>
		<author>Jun-Ki Min</author>
		<authors>
			<first>Jun-Ki</first>
		</authors>
		<authors>
			<last>Min</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Jun-Ki</first>
		</editors>
		<editors>
			<last>Min</last>
		</editors>
		<editors>
			<first>Jun-Ki</first>
		</editors>
		<editors>
			<last>Min</last>
		</editors>
		<editors>
			<first>Jun-Ki</first>
		</editors>
		<editors>
			<last>Min</last>
		</editors>
		<editors>
			<first>Jun-Ki</first>
		</editors>
		<editors>
			<last>Min</last>
		</editors>
		<volume>274</volume>
		<pages>151-156</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Efficient Data Monitoring in Sensor Networks Using Spatial Correlation.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ecbc709395f623c9626865a4215c5b0e/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#JungKGK13</url>
		<author>HaRim Jung</author>
		<author>Seongkyu Kim</author>
		<author>Joon-Min Gil</author>
		<author>Ung-Mo Kim</author>
		<authors>
			<first>HaRim</first>
		</authors>
		<authors>
			<last>Jung</last>
		</authors>
		<authors>
			<first>Seongkyu</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Joon-Min</first>
		</authors>
		<authors>
			<last>Gil</last>
		</authors>
		<authors>
			<first>Ung-Mo</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Ung-Mo</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Ung-Mo</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Ung-Mo</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Ung-Mo</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<volume>274</volume>
		<pages>31-38</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Processing Continuous Range Queries with Non-spatial Selections.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26e25d0b2f03ba91135761417cbb25d2c/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#InKK13</url>
		<author>Kwanho In</author>
		<author>Seongkyu Kim</author>
		<author>Ung-Mo Kim</author>
		<authors>
			<first>Kwanho</first>
		</authors>
		<authors>
			<last>In</last>
		</authors>
		<authors>
			<first>Seongkyu</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Ung-Mo</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Ung-Mo</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Ung-Mo</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Ung-Mo</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Ung-Mo</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<volume>274</volume>
		<pages>39-46</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>DSPI: An Efficient Index for Processing Range Queries on Wireless Broadcast Stream.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/246a43c21657100ff36497be81fa97254/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#KangLPBP13</url>
		<author>Taekyeong Kang</author>
		<author>Hyungkyu Lee</author>
		<author>Dong-Hwan Park</author>
		<author>Hyo-Chan Bang</author>
		<author>Namje Park</author>
		<authors>
			<first>Taekyeong</first>
		</authors>
		<authors>
			<last>Kang</last>
		</authors>
		<authors>
			<first>Hyungkyu</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Dong-Hwan</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<authors>
			<first>Hyo-Chan</first>
		</authors>
		<authors>
			<last>Bang</last>
		</authors>
		<authors>
			<first>Namje</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Namje</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Namje</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Namje</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Namje</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<volume>274</volume>
		<pages>87-92</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Implementation of Load Management Application System in Energy Management Service.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26a010f85e3d93c54c858d833c0ddc189/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#KoYP13</url>
		<author>Gunhwan Ko</author>
		<author>Jongcheol Yoon</author>
		<author>Kyongseok Park</author>
		<authors>
			<first>Gunhwan</first>
		</authors>
		<authors>
			<last>Ko</last>
		</authors>
		<authors>
			<first>Jongcheol</first>
		</authors>
		<authors>
			<last>Yoon</last>
		</authors>
		<authors>
			<first>Kyongseok</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Kyongseok</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Kyongseok</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Kyongseok</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Kyongseok</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<volume>274</volume>
		<pages>321-327</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>HASV: Hadoop-Based NGS Analyzer for Predicting Genomic Structure Variations.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24b2534297fe494814bb0aa9a9ee5b154/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#JoP13</url>
		<author>Hoon Jo</author>
		<author>Soon cheol Park</author>
		<authors>
			<first>Hoon</first>
		</authors>
		<authors>
			<last>Jo</last>
		</authors>
		<authors>
			<first>Soon</first>
		</authors>
		<authors>
			<last>cheol Park</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Soon</first>
		</editors>
		<editors>
			<last>cheol Park</last>
		</editors>
		<editors>
			<first>Soon</first>
		</editors>
		<editors>
			<last>cheol Park</last>
		</editors>
		<editors>
			<first>Soon</first>
		</editors>
		<editors>
			<last>cheol Park</last>
		</editors>
		<editors>
			<first>Soon</first>
		</editors>
		<editors>
			<last>cheol Park</last>
		</editors>
		<volume>274</volume>
		<pages>75-80</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>BK-means Algorithm with Minimal Performance Degradation Caused by Improper Initial Centroid.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e8ee5eb8edd34268ed15e601c464fa68/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#LimL13</url>
		<author>Mingyu Lim</author>
		<author>Yunjin Lee</author>
		<authors>
			<first>Mingyu</first>
		</authors>
		<authors>
			<last>Lim</last>
		</authors>
		<authors>
			<first>Yunjin</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Yunjin</first>
		</editors>
		<editors>
			<last>Lee</last>
		</editors>
		<editors>
			<first>Yunjin</first>
		</editors>
		<editors>
			<last>Lee</last>
		</editors>
		<editors>
			<first>Yunjin</first>
		</editors>
		<editors>
			<last>Lee</last>
		</editors>
		<editors>
			<first>Yunjin</first>
		</editors>
		<editors>
			<last>Lee</last>
		</editors>
		<volume>274</volume>
		<pages>385-389</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Distributed 2D Contents Stylization for Low-End Devices.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/299332f774b560f883e107dd3c97eec28/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#OhBL13</url>
		<author>Jeong Seok Oh</author>
		<author>Hyo Jung Bng</author>
		<author>Si-Hyung Lim</author>
		<authors>
			<first>Jeong Seok</first>
		</authors>
		<authors>
			<last>Oh</last>
		</authors>
		<authors>
			<first>Hyo Jung</first>
		</authors>
		<authors>
			<last>Bng</last>
		</authors>
		<authors>
			<first>Si-Hyung</first>
		</authors>
		<authors>
			<last>Lim</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Si-Hyung</first>
		</editors>
		<editors>
			<last>Lim</last>
		</editors>
		<editors>
			<first>Si-Hyung</first>
		</editors>
		<editors>
			<last>Lim</last>
		</editors>
		<editors>
			<first>Si-Hyung</first>
		</editors>
		<editors>
			<last>Lim</last>
		</editors>
		<editors>
			<first>Si-Hyung</first>
		</editors>
		<editors>
			<last>Lim</last>
		</editors>
		<volume>274</volume>
		<pages>451-456</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>The System of Stress Estimation for the Exposed Gas Pipeline Using the Wireless Tilt Sensor.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/226e63bc76e335d064f6d0012e3a27bae/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#Zhang13</url>
		<author>Yunliang Zhang</author>
		<authors>
			<first>Yunliang</first>
		</authors>
		<authors>
			<last>Zhang</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Yunliang</first>
		</editors>
		<editors>
			<last>Zhang</last>
		</editors>
		<editors>
			<first>Yunliang</first>
		</editors>
		<editors>
			<last>Zhang</last>
		</editors>
		<editors>
			<first>Yunliang</first>
		</editors>
		<editors>
			<last>Zhang</last>
		</editors>
		<editors>
			<first>Yunliang</first>
		</editors>
		<editors>
			<last>Zhang</last>
		</editors>
		<volume>274</volume>
		<pages>205-209</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>The Study on Semantic Self-sufficiency in Factual Knowledge Extraction.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26c4200dbe2a75b8151ac5fd18f15abfc/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#LeeLW13</url>
		<author>Hyun-Jung Lee</author>
		<author>Youngsook Lee</author>
		<author>Dongho Won</author>
		<authors>
			<first>Hyun-Jung</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Youngsook</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Dongho</first>
		</authors>
		<authors>
			<last>Won</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Dongho</first>
		</editors>
		<editors>
			<last>Won</last>
		</editors>
		<editors>
			<first>Dongho</first>
		</editors>
		<editors>
			<last>Won</last>
		</editors>
		<editors>
			<first>Dongho</first>
		</editors>
		<editors>
			<last>Won</last>
		</editors>
		<editors>
			<first>Dongho</first>
		</editors>
		<editors>
			<last>Won</last>
		</editors>
		<volume>274</volume>
		<pages>495-500</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Protection Profile for PoS (Point of Sale) System.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2edde6cc7761fb4a67cb5c0dc0cd4881f/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#KimPW13</url>
		<author>Mijin Kim</author>
		<author>Namje Park</author>
		<author>Dongho Won</author>
		<authors>
			<first>Mijin</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Namje</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<authors>
			<first>Dongho</first>
		</authors>
		<authors>
			<last>Won</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Dongho</first>
		</editors>
		<editors>
			<last>Won</last>
		</editors>
		<editors>
			<first>Dongho</first>
		</editors>
		<editors>
			<last>Won</last>
		</editors>
		<editors>
			<first>Dongho</first>
		</editors>
		<editors>
			<last>Won</last>
		</editors>
		<editors>
			<first>Dongho</first>
		</editors>
		<editors>
			<last>Won</last>
		</editors>
		<volume>274</volume>
		<pages>483-488</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Security Analysis on a Group Key Transfer Protocol Based on Secret Sharing.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2414227eb930a8767d3575b8a98cb9885/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#KimHJSGJXZ13</url>
		<author>Jinhyung Kim</author>
		<author>Myunggwon Hwang</author>
		<author>Do-Heon Jeong</author>
		<author>Sa-Kwang Song</author>
		<author>Jangwon Gim</author>
		<author>Hanmin Jung</author>
		<author>Shuo Xu</author>
		<author>Lijun Zhu</author>
		<authors>
			<first>Jinhyung</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Myunggwon</first>
		</authors>
		<authors>
			<last>Hwang</last>
		</authors>
		<authors>
			<first>Do-Heon</first>
		</authors>
		<authors>
			<last>Jeong</last>
		</authors>
		<authors>
			<first>Sa-Kwang</first>
		</authors>
		<authors>
			<last>Song</last>
		</authors>
		<authors>
			<first>Jangwon</first>
		</authors>
		<authors>
			<last>Gim</last>
		</authors>
		<authors>
			<first>Hanmin</first>
		</authors>
		<authors>
			<last>Jung</last>
		</authors>
		<authors>
			<first>Shuo</first>
		</authors>
		<authors>
			<last>Xu</last>
		</authors>
		<authors>
			<first>Lijun</first>
		</authors>
		<authors>
			<last>Zhu</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Lijun</first>
		</editors>
		<editors>
			<last>Zhu</last>
		</editors>
		<editors>
			<first>Lijun</first>
		</editors>
		<editors>
			<last>Zhu</last>
		</editors>
		<editors>
			<first>Lijun</first>
		</editors>
		<editors>
			<last>Zhu</last>
		</editors>
		<editors>
			<first>Lijun</first>
		</editors>
		<editors>
			<last>Zhu</last>
		</editors>
		<volume>274</volume>
		<pages>261-266</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Diverse Heterogeneous Information Source-Based Researcher Evaluation Model for Research Performance Measurement.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e536e85a539365af86fe6080eacf19a0/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#PyunY13</url>
		<author>Gwangbum Pyun</author>
		<author>Unil Yun</author>
		<authors>
			<first>Gwangbum</first>
		</authors>
		<authors>
			<last>Pyun</last>
		</authors>
		<authors>
			<first>Unil</first>
		</authors>
		<authors>
			<last>Yun</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Unil</first>
		</editors>
		<editors>
			<last>Yun</last>
		</editors>
		<editors>
			<first>Unil</first>
		</editors>
		<editors>
			<last>Yun</last>
		</editors>
		<editors>
			<first>Unil</first>
		</editors>
		<editors>
			<last>Yun</last>
		</editors>
		<editors>
			<first>Unil</first>
		</editors>
		<editors>
			<last>Yun</last>
		</editors>
		<volume>274</volume>
		<pages>1-5</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>A Novel Ranking Technique Based on Page Queries.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23cd95cd10705ea406abe90cc5f2ee476/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#ChoKCJ13</url>
		<author>Chang-Woo Cho</author>
		<author>Ki-Hyun Kim</author>
		<author>Ki-Young Choi</author>
		<author>Chang-Sung Jeong</author>
		<authors>
			<first>Chang-Woo</first>
		</authors>
		<authors>
			<last>Cho</last>
		</authors>
		<authors>
			<first>Ki-Hyun</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Ki-Young</first>
		</authors>
		<authors>
			<last>Choi</last>
		</authors>
		<authors>
			<first>Chang-Sung</first>
		</authors>
		<authors>
			<last>Jeong</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Chang-Sung</first>
		</editors>
		<editors>
			<last>Jeong</last>
		</editors>
		<editors>
			<first>Chang-Sung</first>
		</editors>
		<editors>
			<last>Jeong</last>
		</editors>
		<editors>
			<first>Chang-Sung</first>
		</editors>
		<editors>
			<last>Jeong</last>
		</editors>
		<editors>
			<first>Chang-Sung</first>
		</editors>
		<editors>
			<last>Jeong</last>
		</editors>
		<volume>274</volume>
		<pages>335-339</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Fast Shear Skew Warp Volume Rendering Using GPGPU for Cloud 3D Visualization.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2536abed85bb83042b807bb47336d5934/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#JungHJ13</url>
		<author>In-Yong Jung</author>
		<author>Byong-John Han</author>
		<author>Chang-Sung Jeong</author>
		<authors>
			<first>In-Yong</first>
		</authors>
		<authors>
			<last>Jung</last>
		</authors>
		<authors>
			<first>Byong-John</first>
		</authors>
		<authors>
			<last>Han</last>
		</authors>
		<authors>
			<first>Chang-Sung</first>
		</authors>
		<authors>
			<last>Jeong</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Chang-Sung</first>
		</editors>
		<editors>
			<last>Jeong</last>
		</editors>
		<editors>
			<first>Chang-Sung</first>
		</editors>
		<editors>
			<last>Jeong</last>
		</editors>
		<editors>
			<first>Chang-Sung</first>
		</editors>
		<editors>
			<last>Jeong</last>
		</editors>
		<editors>
			<first>Chang-Sung</first>
		</editors>
		<editors>
			<last>Jeong</last>
		</editors>
		<volume>274</volume>
		<pages>329-334</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Provisioning On-Demand HLA/RTI Simulation Environment on Cloud for Distributed-Parallel Computer Simulations.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21c8c832e7cbd0c14969d1f1895c713fe/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#RyangY13</url>
		<author>Heungmo Ryang</author>
		<author>Unil Yun</author>
		<authors>
			<first>Heungmo</first>
		</authors>
		<authors>
			<last>Ryang</last>
		</authors>
		<authors>
			<first>Unil</first>
		</authors>
		<authors>
			<last>Yun</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Unil</first>
		</editors>
		<editors>
			<last>Yun</last>
		</editors>
		<editors>
			<first>Unil</first>
		</editors>
		<editors>
			<last>Yun</last>
		</editors>
		<editors>
			<first>Unil</first>
		</editors>
		<editors>
			<last>Yun</last>
		</editors>
		<editors>
			<first>Unil</first>
		</editors>
		<editors>
			<last>Yun</last>
		</editors>
		<volume>274</volume>
		<pages>7-11</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Ranking Book Reviews Based on User Discussion.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28044505429b0fa75f6b17a8246cfd8fc/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#ChungCLW13</url>
		<author>Youngseok Chung</author>
		<author>Seokjin Choi</author>
		<author>Youngsook Lee</author>
		<author>Dongho Won</author>
		<authors>
			<first>Youngseok</first>
		</authors>
		<authors>
			<last>Chung</last>
		</authors>
		<authors>
			<first>Seokjin</first>
		</authors>
		<authors>
			<last>Choi</last>
		</authors>
		<authors>
			<first>Youngsook</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Dongho</first>
		</authors>
		<authors>
			<last>Won</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Dongho</first>
		</editors>
		<editors>
			<last>Won</last>
		</editors>
		<editors>
			<first>Dongho</first>
		</editors>
		<editors>
			<last>Won</last>
		</editors>
		<editors>
			<first>Dongho</first>
		</editors>
		<editors>
			<last>Won</last>
		</editors>
		<editors>
			<first>Dongho</first>
		</editors>
		<editors>
			<last>Won</last>
		</editors>
		<volume>274</volume>
		<pages>535-540</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Security Enhanced Unlinkable Authentication Scheme with Anonymity for Global Mobility Networks.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2afb60a19725f097965c0c13fc3795bc8/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#LuLLC13</url>
		<author>Ssu-Hsuan Lu</author>
		<author>Kuan-Ching Li</author>
		<author>Kuan-Chou Lai</author>
		<author>Yeh-Ching Chung</author>
		<authors>
			<first>Ssu-Hsuan</first>
		</authors>
		<authors>
			<last>Lu</last>
		</authors>
		<authors>
			<first>Kuan-Ching</first>
		</authors>
		<authors>
			<last>Li</last>
		</authors>
		<authors>
			<first>Kuan-Chou</first>
		</authors>
		<authors>
			<last>Lai</last>
		</authors>
		<authors>
			<first>Yeh-Ching</first>
		</authors>
		<authors>
			<last>Chung</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Yeh-Ching</first>
		</editors>
		<editors>
			<last>Chung</last>
		</editors>
		<editors>
			<first>Yeh-Ching</first>
		</editors>
		<editors>
			<last>Chung</last>
		</editors>
		<editors>
			<first>Yeh-Ching</first>
		</editors>
		<editors>
			<last>Chung</last>
		</editors>
		<editors>
			<first>Yeh-Ching</first>
		</editors>
		<editors>
			<last>Chung</last>
		</editors>
		<volume>274</volume>
		<pages>577-583</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>An Overlay Network Based on Arrangement Graph with Fault Tolerance.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bef2921e8a071693e56a43ee1df8ec7c/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#XuSQZJLC13</url>
		<author>Shuo Xu</author>
		<author>Qingwei Shi</author>
		<author>Xiaodong Qiao</author>
		<author>Lijun Zhu</author>
		<author>Hanmin Jung</author>
		<author>Seungwoo Lee</author>
		<author>Sung-Pil Choi</author>
		<authors>
			<first>Shuo</first>
		</authors>
		<authors>
			<last>Xu</last>
		</authors>
		<authors>
			<first>Qingwei</first>
		</authors>
		<authors>
			<last>Shi</last>
		</authors>
		<authors>
			<first>Xiaodong</first>
		</authors>
		<authors>
			<last>Qiao</last>
		</authors>
		<authors>
			<first>Lijun</first>
		</authors>
		<authors>
			<last>Zhu</last>
		</authors>
		<authors>
			<first>Hanmin</first>
		</authors>
		<authors>
			<last>Jung</last>
		</authors>
		<authors>
			<first>Seungwoo</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Sung-Pil</first>
		</authors>
		<authors>
			<last>Choi</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Sung-Pil</first>
		</editors>
		<editors>
			<last>Choi</last>
		</editors>
		<editors>
			<first>Sung-Pil</first>
		</editors>
		<editors>
			<last>Choi</last>
		</editors>
		<editors>
			<first>Sung-Pil</first>
		</editors>
		<editors>
			<last>Choi</last>
		</editors>
		<editors>
			<first>Sung-Pil</first>
		</editors>
		<editors>
			<last>Choi</last>
		</editors>
		<volume>274</volume>
		<pages>239-245</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Author-Topic over Time (AToT): A Dynamic Users' Interest Model.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b132ea5fa178a2d6c8cead618a37adb1/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>International Review of the Aesthetics and Sociology of Music</journal>
		<year>1997</year>
		<url>http://www.jstor.org/stable/3108435</url>
		<author>Ian Inglis</author>
		<authors>
			<first>Ian</first>
		</authors>
		<authors>
			<last>Inglis</last>
		</authors>
		<volume>28</volume>
		<number>1</number>
		<pages>37--62</pages>
		<abstract>Explorations and expressions of love have dominated the lyrical content of popular music for decades, to the extent that the love song can be said to be the archetypal pop song. A detailed division of "lovestyles" has been proposed by the psychologist John Alan Lee, who suggests that there are six distinct styles of loving. Through a consideration of the application of his typology to the lyrics of popular songs, it can be seen that the categories he has identified have relevance musically, as well as socially and emotionally. When these insights are employed in the analysis of songs written and performed by the Beatles, significant differences are seen in the approach to love between the group's earlier and later material. It is argued that these are not random variations, but indications of the ways in which their personal experiences and professional evolution were reflected in the nature of their music.</abstract>
		<issn>03515796</issn>
		<shorttitle>Variations on a Theme</shorttitle>
		<title>Variations on a Theme: The Love Songs of the Beatles</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/290ece0820c8c3861e105ff8357874960/bfields</id>
		<tags>Evaluation_systems</tags>
		<tags>Taxonomy</tags>
		<tags>music_classification</tags>
		<tags>ground-truth</tags>
		<description>initial imports</description>
		<date>2010-01-28 11:42:45</date>
		<count>2</count>
		<booktitle>Proc. of Int. Symposium on Music Information Retrieval</booktitle>
		<year>2007</year>
		<url></url>
		<author>Jeremy Reed</author>
		<author>Chin-Hui Lee</author>
		<authors>
			<first>Jeremy</first>
		</authors>
		<authors>
			<last>Reed</last>
		</authors>
		<authors>
			<first>Chin-Hui</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<title>A Study On Attribute-Based Taxonomy For Music Information Retrieval</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b7ea448afe7e74b8736e55a25ef983a8/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>The Journal of Musicology</journal>
		<year>2005</year>
		<url>http://www.jstor.org/stable/4138358</url>
		<author>John Platoff</author>
		<authors>
			<first>John</first>
		</authors>
		<authors>
			<last>Platoff</last>
		</authors>
		<volume>22</volume>
		<number>2</number>
		<pages>241--267</pages>
		<abstract>The Beatles recorded two starkly different musical settings of John Lennon's controversial 1968 song "Revolution": One was released as a single, the other appeared on the White Album (as "Revolution 1"). Lennon's lyrics express deep skepticism about political radicalism, and the single, with its lines "But when you talk about destruction/... you can count me out," incited rage among critics and activists on the Left. Lennon appears less opposed to violent protest in "Revolution 1" - recorded first, though released later-where he sang "you can count me out-in." The reception of "Revolution" reflected a focus on the words and their apparent political meanings, largely ignoring the musical differences between the two recordings of the song. Moreover, the response to "Revolution" had much to do with public perceptions of the Beatles. Their rivals the Rolling Stones, seen as a more radical alternative voice, released the equally political "Street Fighting Man" at virtually the same moment in 1968. The much more favorable public reaction to the latter had at least as much to do with the way the bands themselves were perceived as with differences between the songs.</abstract>
		<issn>02779269</issn>
		<title>John Lennon, "Revolution," and the Politics of Musical Reception</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f81fada852f8f2a780c62388130fc554/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<series>Cambridge companions to music</series>
		<publisher>Cambridge University Press</publisher>
		<address>Cambridge, England</address>
		<year>2002</year>
		<url></url>
		<editor>Allan F. ed Moore</editor>
		<editors>
			<first>John</first>
		</editors>
		<editors>
			<last>Platoff</last>
		</editors>
		<abstract>The following contributions are cited separately in RILM: Matt BACKER, "The guitar" (2002-8270); Graeme M. BOONE, "Twelve key recordings" (2002-8267); Don CUSIC, "The development of gospel music" (2002-8266); David EVANS, "The development of the blues" (2002-8265); Dave HEADLAM, Äppropriations of blues and gospel in popular music" (2002-8273); Barb JUNGR, "Vocal expression in blues and gospel" (2002-8269); Allan F. MOORE, "Surveying the field: Our knowledge of blues and gospel music" (2002-8262); Guido van RIJN, "Imagery in the lyrics: An initial approach" (2002-8272); Jeff Todd TITON, "Labels: Identifying categories of blues and gospel" (2002-8264); Steve TRACY, "Black twice: Performance conditions for blues and gospel artists" (2002-8268); Adrian YORK, "Keyboard techniques" (2002-8271).</abstract>
		<title>The Cambridge companion to blues and gospel music</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ac9c1a56fc4d9944725f63085eaced5d/bfields</id>
		<tags>music_similarity</tags>
		<description>initial imports</description>
		<date>2010-01-28 11:42:45</date>
		<count>2</count>
		<booktitle>Proc. of Int. Symposium on Music Information Retrieval</booktitle>
		<year>2002</year>
		<url></url>
		<author>M. Cooper</author>
		<author>J. Foote</author>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Cooper</last>
		</authors>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Foote</last>
		</authors>
		<pages>81 - 85</pages>
		<title>Automatic music summarization via similarity analysis</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b72a421668e954784a94dad0a585124d/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>Music analysis. Vol. 22</journal>
		<year>2003</year>
		<url></url>
		<author>Adam author Krims</author>
		<authors>
			<first>Adam</first>
		</authors>
		<authors>
			<last>author Krims</last>
		</authors>
		<volume>22</volume>
		<number>1-2</number>
		<pages>181--209</pages>
		<abstract>Unedited Largely a review of four publications examining aspects of popular music. Roy Shuker’s dictionary "Key concepts in popular music" (London, 1998; RILM 1999-26276) is a solid and informed introduction to the central ideas of the field, although not without possible improvements. Keith Negus has a similarly eloquent exposition of recent debates in the field in his "Popular music in theory" (Cambridge, 1996; RILM 1997-5754), although the tacit assumption that the subject of musicology is "the music itself" is perhaps short-sighted in the light of recent semiotic discussions, and the careful organisation into single-word chapter headings causes a certain amount of repetition of material. Allan Moore’s "The Beatles: Sgt. Pepper’s Lonely Hearts Club Band" (Cambridge, 1997; RILM 1997-5755) has an excellent description of the historio-cultural context of the album, but its small format makes it impossible for some passages and assertions to be adequately expanded upon. Finally, John Covach and Graeme Boone’s collection Ünderstanding rock" (New York and Oxford, 1997; RILM 1997-8775) is unlikely to be popular with popular music scholars, with its heavy reliance on analytical techniques. Taken as a set, these publications belie a pervasive underlying split between "internal-analytical" and "external-poietic" ways still awaiting unification.</abstract>
		<issn>0262-5245</issn>
		<title>What does it mean to analyze popular music?</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28e121312aa9284c1f5e91de2a0ea57c6/bfields</id>
		<tags>music_networks</tags>
		<description>initial imports</description>
		<date>2010-01-28 11:42:45</date>
		<count>4</count>
		<booktitle>Proc. of Int. Symposium on Music Information Retrieval</booktitle>
		<year>2007</year>
		<url></url>
		<author>Amélie Anglade</author>
		<author>Marco Tiemann</author>
		<author>Fabio Vignoli</author>
		<authors>
			<first>Amélie</first>
		</authors>
		<authors>
			<last>Anglade</last>
		</authors>
		<authors>
			<first>Marco</first>
		</authors>
		<authors>
			<last>Tiemann</last>
		</authors>
		<authors>
			<first>Fabio</first>
		</authors>
		<authors>
			<last>Vignoli</last>
		</authors>
		<title>Virtual communities for creating shared music channels</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26fc4d22dd7e757542195c5d51d55efd0/lachenma</id>
		<tags>Xenakis,art,composition,music,tangible</tags>
		<tags>interface</tags>
		<description></description>
		<date>2009-12-10 10:41:28</date>
		<count>1</count>
		<journal>Tangible and embedded interaction</journal>
		<year>2008</year>
		<url>http://portal.acm.org/citation.cfm?id=1347390.1347416</url>
		<author>Markus Bischof</author>
		<author>Bettina Conradi</author>
		<author>Peter Lachenmaier</author>
		<author>Kai Linde</author>
		<author>Max Meier</author>
		<author>Philipp Pötzl</author>
		<author>Elisabeth André</author>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Bischof</last>
		</authors>
		<authors>
			<first>Bettina</first>
		</authors>
		<authors>
			<last>Conradi</last>
		</authors>
		<authors>
			<first>Peter</first>
		</authors>
		<authors>
			<last>Lachenmaier</last>
		</authors>
		<authors>
			<first>Kai</first>
		</authors>
		<authors>
			<last>Linde</last>
		</authors>
		<authors>
			<first>Max</first>
		</authors>
		<authors>
			<last>Meier</last>
		</authors>
		<authors>
			<first>Philipp</first>
		</authors>
		<authors>
			<last>Pötzl</last>
		</authors>
		<authors>
			<first>Elisabeth</first>
		</authors>
		<authors>
			<last>André</last>
		</authors>
		<pages>3</pages>
		<abstract>In this paper we present the table-based tangible interface application Xenakis which uses probability models in order to compose music in a way that can be strongly influenced by the user. Our musical sequencing application is based on a framework for tangible interfaces with an architecture that is strongly inspired by the model-view-controller pattern. In addition, we developed a hardware setup for tangible interfaces and used MatraX for tracking markers. The sequencer is the first implementation based on this framework. It allows users to create music simply by moving tangibles on the table. The graphics engine Horde3D is used to visualize the user-interaction and to show the relationships between the tangible objects on the table, creating an appealing audio-visual experience. An evaluation with 37 first time users was conducted in order to discover the strong and the weak points of such tangible user interfaces, especially in the context of our application.</abstract>
		<title>Xenakis: combining tangible interaction with probability-based musical composition</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2139dd38bb0c072d880b0b35183f7d695/syslogd</id>
		<tags>Retrieval,</tags>
		<tags>web-mining</tags>
		<tags>Visualization,</tags>
		<tags>Music</tags>
		<tags>Information</tags>
		<description></description>
		<date>2010-01-07 13:57:09</date>
		<count>1</count>
		<address>Linz, Austria</address>
		<year>2008</year>
		<url>http://www.cp.jku.at/people/schedl/Research/Publications/pdf/phd_schedl_2008.pdf</url>
		<author>Markus Schedl</author>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<abstract>In the context of this PhD thesis, methods for automatically extracting music-related information from 
the World Wide Web have been elaborated, implemented, and analyzed. Such information is becoming 
more and more important in times of digital music distribution via the Internet as users of online music 
stores nowadays expect to be offered additional music-related information beyond the pure digital music 
file. Novel techniques have been developed as well as existing ones refined in order to gather information 
about music artists and bands from the Web. These techniques are related to the research fields of 
music information retrieval, Web mining, and information visualization. More precisely, on sets of Web 
pages that are related to a music artist or band, Web content mining techniques are applied to address 
the following categories of information: 
* similarities between music artists or bands 
* prototypicality of an artist or a band for a genre 
* descriptive properties of an artist or a band 
* band members and instrumentation 
* images of album cover artwork 
Different approaches to retrieve the corresponding pieces of information for each of these categories 
have been elaborated and evaluated thoroughly on a considerable variety of music repositories. The 
results and main findings of these assessments are reported. Moreover, visualization methods and user 
interaction models for prototypical and similar artists as well as for descriptive terms have evolved from 
this work. 
Based on the insights gained by the various experiments and evaluations conducted, the core applica- 
tion of this thesis, the Automatically Generated Music Information System (AGMIS) was build. AGMIS 
demonstrates the applicability of the elaborated techniques on a large collection of more than 600,000 
artists by providing a Web-based user interface to access a database that has been populated automat- 
ically with the extracted information. Although AGMIS does not always give perfectly accurate results, 
the automatic approaches to information retrieval have some advantages in comparison with those 
employed in existing music information systems, which are either based on labor-intensive information 
processing by music experts or on community knowledge that is vulnerable to distortion of information.</abstract>
		<title>Automatically Extracting, Analyzing, and Visualizing Information on Music Artists from the World Wide Web</title>
		<pubtype>phdthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23a177fa8b84e3fe0b2d8f08d5ad63de5/syslogd</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-07 13:57:09</date>
		<count>1</count>
		<year>2008</year>
		<url>http://www.dfki.de/web/forschung/km/kompetenz/forschung/music-information-retrieval</url>
		<author> Deutsches Forschungszentrum für künstliche Intelligenz</author>
		<authors>
			<first></first>
		</authors>
		<authors>
			<last>Deutsches Forschungszentrum für künstliche Intelligenz</last>
		</authors>
		<title>Music Information Retrieval</title>
		<pubtype>www</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/221c759fa77ca33b14fb4ca92fdd1c12d/penkib</id>
		<tags>TP</tags>
		<tags>UR</tags>
		<tags>arxiv</tags>
		<description>[0909.3976] The Information Theory of Emotions of Musical Chords</description>
		<date>2009-10-21 04:57:14</date>
		<count>2</count>
		<year>2009</year>
		<url>http://arxiv.org/abs/0909.3976</url>
		<author>Vadim R. Madgazin</author>
		<authors>
			<first>Vadim R.</first>
		</authors>
		<authors>
			<last>Madgazin</last>
		</authors>
		<abstract>The paper offers a solution to the centuries-old puzzle - why the majorchords are perceived as happy and the minor chords as sad - based on theinformation theory of emotions. A theory and a formula of musical emotions werecreated. They define the sign and the amplitude of the utilitarian emotionalcoloration of separate major and minor chords through relative pitches ofconstituent sounds. Keywords: chord, major, minor, the formula of musicalemotions, the information theory of emotions.</abstract>
		<title>The Information Theory of Emotions of Musical Chords</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/266f5257645ff77dba84b2d0293ef22d6/joaakive</id>
		<tags>music</tags>
		<tags>interface</tags>
		<description></description>
		<date>2014-04-27 09:46:10</date>
		<count>2</count>
		<booktitle>BCS HCI</booktitle>
		<publisher>ACM</publisher>
		<year>2010</year>
		<url>http://dblp.uni-trier.de/db/conf/bcshci/bcshci2010.html#Hoadley10</url>
		<author>Richard Hoadley</author>
		<authors>
			<first>Richard</first>
		</authors>
		<authors>
			<last>Hoadley</last>
		</authors>
		<editor>Tom McEwan</editor>
		<editor>Lachlan McKinnon</editor>
		<editors>
			<first>Richard</first>
		</editors>
		<editors>
			<last>Hoadley</last>
		</editors>
		<editors>
			<first>Richard</first>
		</editors>
		<editors>
			<last>Hoadley</last>
		</editors>
		<pages>479-483</pages>
		<abstract>This paper presents observations on the creation of digital music controllers and the music that they
generate from the perspectives of the designer and the artist. In the case of musical instruments,
what is the role of the form (the hardware) where it concerns the function (the production of musically
interesting sounds)? Specific projects are presented, and a set of operational principles is supported
from those examples. The associated encounter session will allow delegates to experiment with the
interfaces exhibited, further informing these principles.</abstract>
		<title>Form and function: examples of music interface design.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bbf5ff0829143c8bd18a2d4a778f1c49/ar0berts</id>
		<tags>Mental</tags>
		<tags>Child;</tags>
		<tags>Humans;</tags>
		<tags>Motor</tags>
		<tags>Cerebral</tags>
		<tags>Therapy;</tags>
		<tags>Projects</tags>
		<tags>(Psychology);</tags>
		<tags>Pilot</tags>
		<tags>Activity;</tags>
		<tags>Retardation;</tags>
		<tags>Palsy;</tags>
		<tags>Music</tags>
		<tags>Biofeedback</tags>
		<tags>Head;</tags>
		<description></description>
		<date>2014-07-19 21:53:56</date>
		<count>1</count>
		<journal>Dev Med Child Neurol</journal>
		<year>1981</year>
		<url></url>
		<author>R. P. Walmsley</author>
		<author>L. Crichton</author>
		<author>D. Droog</author>
		<authors>
			<first>R. P.</first>
		</authors>
		<authors>
			<last>Walmsley</last>
		</authors>
		<authors>
			<first>L.</first>
		</authors>
		<authors>
			<last>Crichton</last>
		</authors>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Droog</last>
		</authors>
		<volume>23</volume>
		<number>6</number>
		<pages>739--746</pages>
		<abstract>Five profoundly mentally retarded cerebral-palsied children were studied in order to determine the effectiveness of music as a biofeedback mechanism in the training of head control. The method used a Head Position Trainer and Time Event Counter, developed at the Ontario Crippled Children's Centre in Toronto. Improvement was obtained in three of the five children in their ability to control their head movements when music was used as the biofeedback stimulus. However, these results should be treated cautiously because the sample was small and the training period was brief.</abstract>
		<title>Music as a feedback mechanism for teaching head control to severely handicapped children: a pilot study.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2cf32189a28d8c76995a5b4fa42563e3b/sauli</id>
		<tags>design</tags>
		<tags>gradu</tags>
		<description></description>
		<date>2014-11-10 08:43:13</date>
		<count>1</count>
		<booktitle>Improvise: The Australasian Computer Music Conference 2009</booktitle>
		<year>2009</year>
		<url></url>
		<author>Andrew R Brown</author>
		<author>Thorin Kerr</author>
		<authors>
			<first>Andrew R</first>
		</authors>
		<authors>
			<last>Brown</last>
		</authors>
		<authors>
			<first>Thorin</first>
		</authors>
		<authors>
			<last>Kerr</last>
		</authors>
		<pages>26--31</pages>
		<title>Adaptive music techniques</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/255c20657b7f2031786f76ea51d12050c/ks-plugin-devel</id>
		<tags>Notensatz</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2013-02-02 14:43:20</date>
		<count>2</count>
		<journal>Computing in Musicology</journal>
		<year>2001</year>
		<url></url>
		<author>Holger H. Hoos</author>
		<author>Keith A. Hamel</author>
		<author>Kai Renz</author>
		<author>Jürgen Kilian</author>
		<authors>
			<first>Holger H.</first>
		</authors>
		<authors>
			<last>Hoos</last>
		</authors>
		<authors>
			<first>Keith A.</first>
		</authors>
		<authors>
			<last>Hamel</last>
		</authors>
		<authors>
			<first>Kai</first>
		</authors>
		<authors>
			<last>Renz</last>
		</authors>
		<authors>
			<first>Jürgen</first>
		</authors>
		<authors>
			<last>Kilian</last>
		</authors>
		<volume>12</volume>
		<abstract>GUIDO Music Notation is a novel approach for adequately representing score-level music. Based on a simple, yet powerful and easily extensible formalism, GUIDO is realized as a plain-text, human-readable and platform independent format. The key feature of the underlying design is representational adequacy: simple musical concepts can be expressed in a simple way, while complex musical notions may require more complex representations. GUIDO Music Notation can be used for a broad range of applications, including notation software, composition and analysis systems and tools, music databases, and music on the WWW. In this article, we discuss the motivation for developing GUIDO Music Notation, give an overview of its design and features, and describe its current applications.</abstract>
		<title>Representing Score-Level Music Using the GUIDO Music-Notation Format</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25365c4635774a126f310b84d080aaf2c/ks-plugin-devel</id>
		<tags>imported</tags>
		<description>Mathematics and Music</description>
		<date>2013-02-02 14:42:57</date>
		<count>2</count>
		<booktitle>Mathematics and music : a Diderot Mathematical Forum</booktitle>
		<publisher>Springer</publisher>
		<address>Berlin; New York</address>
		<year>2002</year>
		<url>http://www.worldcat.org/search?qt=worldcat_org_all&q=9783540437277</url>
		<editor>Gerard Assayag</editor>
		<editor>Feichtinger Hans G.</editor>
		<editor>José Francisco Rodrigues</editor>
		<editors>
			<first>Jürgen</first>
		</editors>
		<editors>
			<last>Kilian</last>
		</editors>
		<editors>
			<first>Jürgen</first>
		</editors>
		<editors>
			<last>Kilian</last>
		</editors>
		<editors>
			<first>Jürgen</first>
		</editors>
		<editors>
			<last>Kilian</last>
		</editors>
		<pages>288</pages>
		<isbn>3540437274 9783540437277</isbn>
		<title>Mathematics and music : a Diderot Mathematical Forum</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e3ffdf5c3b6c9ec1eee823acf6d67042/keinstein</id>
		<tags>zitiert_wille</tags>
		<tags>MaMu</tags>
		<description></description>
		<date>2012-12-14 13:30:48</date>
		<count>2</count>
		<journal>Acta Musicologica</journal>
		<publisher>International Musicological Society</publisher>
		<year>1986</year>
		<url>http://www.jstor.org/stable/932818</url>
		<author>Albrecht Schneider</author>
		<author>Uwe Seifert</author>
		<authors>
			<first>Albrecht</first>
		</authors>
		<authors>
			<last>Schneider</last>
		</authors>
		<authors>
			<first>Uwe</first>
		</authors>
		<authors>
			<last>Seifert</last>
		</authors>
		<volume>58</volume>
		<number>2</number>
		<pages>pp. 305-338</pages>
		<issn>00016241</issn>
		<language>English</language>
		<copyright>Copyright © 1986 International Musicological Society</copyright>
		<title>Zu einigen Ansätzen und Verfahren in neueren musiktheoretischen Konzepten</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25365c4635774a126f310b84d080aaf2c/keinstein</id>
		<tags>imported</tags>
		<description>Mathematics and Music</description>
		<date>2010-03-22 14:50:39</date>
		<count>2</count>
		<booktitle>Mathematics and music : a Diderot Mathematical Forum</booktitle>
		<publisher>Springer</publisher>
		<address>Berlin; New York</address>
		<year>2002</year>
		<url>http://www.worldcat.org/search?qt=worldcat_org_all&q=9783540437277</url>
		<editor>Gerard Assayag</editor>
		<editor>Feichtinger Hans G.</editor>
		<editor>José Francisco Rodrigues</editor>
		<editors>
			<first>Uwe</first>
		</editors>
		<editors>
			<last>Seifert</last>
		</editors>
		<editors>
			<first>Uwe</first>
		</editors>
		<editors>
			<last>Seifert</last>
		</editors>
		<editors>
			<first>Uwe</first>
		</editors>
		<editors>
			<last>Seifert</last>
		</editors>
		<pages>288</pages>
		<isbn>3540437274 9783540437277</isbn>
		<title>Mathematics and music : a Diderot Mathematical Forum</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bba3643842bbae988b17ca77cdb6f11a/mediadigits</id>
		<tags>based</tags>
		<tags>retrieval</tags>
		<tags>music</tags>
		<tags>humming</tags>
		<tags>qbh</tags>
		<tags>query</tags>
		<tags>content</tags>
		<description></description>
		<date>2011-01-14 15:32:22</date>
		<count>3</count>
		<journal>Proceedings of the IEEE</journal>
		<year>2008</year>
		<url></url>
		<author>M.A. Casey</author>
		<author>R. Veltkamp</author>
		<author>M. Goto</author>
		<author>M. Leman</author>
		<author>C. Rhodes</author>
		<author>M. Slaney</author>
		<authors>
			<first>M.A.</first>
		</authors>
		<authors>
			<last>Casey</last>
		</authors>
		<authors>
			<first>R.</first>
		</authors>
		<authors>
			<last>Veltkamp</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Goto</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Leman</last>
		</authors>
		<authors>
			<first>C.</first>
		</authors>
		<authors>
			<last>Rhodes</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Slaney</last>
		</authors>
		<volume>96</volume>
		<number>4</number>
		<pages>668 -696</pages>
		<abstract>The steep rise in music downloading over CD sales has created a major shift in the music industry away from physical media formats and towards online products and services. Music is one of the most popular types of online information and there are now hundreds of music streaming and download services operating on the World-Wide Web. Some of the music collections available are approaching the scale of ten million tracks and this has posed a major challenge for searching, retrieving, and organizing music content. Research efforts in music information retrieval have involved experts from music perception, cognition, musicology, engineering, and computer science engaged in truly interdisciplinary activity that has resulted in many proposed algorithmic and methodological solutions to music search using content-based methods. This paper outlines the problems of content-based music information retrieval and explores the state-of-the-art methods using audio cues (e.g., query by humming, audio fingerprinting, content-based music retrieval) and other cues (e.g., music notation and symbolic representation), and identifies some of the major challenges for the coming years.</abstract>
		<issn>0018-9219</issn>
		<doi>10.1109/JPROC.2008.916370</doi>
		<title>Content-Based Music Information Retrieval: Current Directions and Future Challenges</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f496fa4e87915619a2c7dd733ff21c26/smatthiesen</id>
		<tags>music</tags>
		<tags>primates</tags>
		<tags>language</tags>
		<tags>evolution</tags>
		<tags>tamarins</tags>
		<description></description>
		<date>2010-11-30 22:39:03</date>
		<count>1</count>
		<journal>Biology Letters</journal>
		<year>2010</year>
		<url>http://dx.doi.org/10.1098/rsbl.2009.0593</url>
		<author>Charles T. Snowdon</author>
		<author>David Teie</author>
		<authors>
			<first>Charles T.</first>
		</authors>
		<authors>
			<last>Snowdon</last>
		</authors>
		<authors>
			<first>David</first>
		</authors>
		<authors>
			<last>Teie</last>
		</authors>
		<volume>6</volume>
		<number>1</number>
		<pages>30--32</pages>
		<abstract>10.1098/rsbl.2009.0593 Theories of music evolution agree that human music has an affective influence on listeners. Tests of non-humans provided little evidence of preferences for human music. However, prosodic features of speech ('motherese') influence affective behaviour of non-verbal infants as well as domestic animals, suggesting that features of music can influence the behaviour of non-human species. We incorporated acoustical characteristics of tamarin affiliation vocalizations and tamarin threat vocalizations into corresponding pieces of music. We compared music composed for tamarins with that composed for humans. Tamarins were generally indifferent to playbacks of human music, but responded with increased arousal to tamarin threat vocalization based music, and with decreased activity and increased calm behaviour to tamarin affective vocalization based music. Affective components in human music may have evolutionary origins in the structure of calls of non-human animals. In addition, animal signals may have evolved to manage the behaviour of listeners by influencing their affective state.</abstract>
		<issn>1744-957X</issn>
		<doi>10.1098/rsbl.2009.0593</doi>
		<title>Affective responses in tamarins elicited by species-specific music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2080b99a7ef22cdc7396c7458e4cf99f2/andymilne</id>
		<tags>imported</tags>
		<description></description>
		<date>2011-12-08 13:36:28</date>
		<count>1</count>
		<booktitle>Proceedings of the 11th International Conference on Music Perception and Cognition</booktitle>
		<address>University of Washinton, Seattle, USA</address>
		<year>2010</year>
		<url>http://oro.open.ac.uk/21509/1/Tonal_Music_Theory_-_Full_Paper.pdf</url>
		<author>Andrew J. Milne</author>
		<authors>
			<first>Andrew J.</first>
		</authors>
		<authors>
			<last>Milne</last>
		</authors>
		<editor>S. M. Demorest</editor>
		<editor>S. J. Morrison</editor>
		<editor>P. S. Campbell</editor>
		<editors>
			<first>Andrew J.</first>
		</editors>
		<editors>
			<last>Milne</last>
		</editors>
		<editors>
			<first>Andrew J.</first>
		</editors>
		<editors>
			<last>Milne</last>
		</editors>
		<editors>
			<first>Andrew J.</first>
		</editors>
		<editors>
			<last>Milne</last>
		</editors>
		<pages>597--600</pages>
		<abstract>Models of the perceived distance between pairs of pitch collections are a core component of broader models of the perception of tonality as a whole. Numerous different distance measures have been proposed, including voice-leading, psychoacoustic, and pitch and interval class distances; but, so far, there has been no attempt to bind these different measures into a single mathematical framework, nor to incorporate the uncertain or probabilistic nature of pitch perception (whereby tones with similar frequencies may, or may not, be heard as having the same pitch).
To achieve these aims, we embed pitch collections in novel multi-way expectation arrays, and show how metrics between such arrays can model the perceived dissimilarity of the pitch collections they embed. By modeling the uncertainties of human pitch perception, expectation arrays indicate the expected number of tones, ordered pairs of tones, ordered triples of tones and so forth, that are heard as having any given pitch, dyad of pitches, triad of pitches, and so forth. The pitches can be either absolute or relative (in which case the arrays are invariant with respect to transposition).
We provide a number of examples that show how the metrics accord well with musical intuition, and suggest some ways in which this work may be developed.</abstract>
		<isbn>1 876346 62 0</isbn>
		<title>Tonal music theory: A psychoacoustic explanation?</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c3c10ced463981a5792bc995d4e53f48/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-10-01 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#YoonKCJC13</url>
		<author>Min Yoon</author>
		<author>Hyeong-Il Kim</author>
		<author>Dong Hoon Choi</author>
		<author>Heeseung Jo</author>
		<author>Jae-Woo Chang</author>
		<authors>
			<first>Min</first>
		</authors>
		<authors>
			<last>Yoon</last>
		</authors>
		<authors>
			<first>Hyeong-Il</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Dong Hoon</first>
		</authors>
		<authors>
			<last>Choi</last>
		</authors>
		<authors>
			<first>Heeseung</first>
		</authors>
		<authors>
			<last>Jo</last>
		</authors>
		<authors>
			<first>Jae-Woo</first>
		</authors>
		<authors>
			<last>Chang</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Jae-Woo</first>
		</editors>
		<editors>
			<last>Chang</last>
		</editors>
		<editors>
			<first>Jae-Woo</first>
		</editors>
		<editors>
			<last>Chang</last>
		</editors>
		<editors>
			<first>Jae-Woo</first>
		</editors>
		<editors>
			<last>Chang</last>
		</editors>
		<editors>
			<first>Jae-Woo</first>
		</editors>
		<editors>
			<last>Chang</last>
		</editors>
		<volume>274</volume>
		<pages>293-299</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Performance Analysis of MapReduce-Based Distributed Systems for Iterative Data Processing Applications.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/210dd3e9679b541e414f3a9d4b8ed4805/tb2332</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>1</count>
		<year>2004</year>
		<url></url>
		<author>D. Eck</author>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Eck</last>
		</authors>
		<abstract>Slides and musical examples available on request.</abstract>
		<source>OwnPublication</source>
		<title>Bridging Long Timelags in Music</title>
		<pubtype>unpublished</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/294bd871995c4b7966519817185cbf9bc/andre@ismll</id>
		<tags>music</tags>
		<tags>record</tags>
		<tags>managing</tags>
		<tags>manual_playlisting</tags>
		<description></description>
		<date>2010-05-06 08:38:42</date>
		<count>2</count>
		<journal>Qualitative Sociology</journal>
		<year>2002</year>
		<url>http://dx.doi.org/10.1023/A:1015494716804</url>
		<author>Jarl A. Ahlkvist</author>
		<author>Robert Faulkner</author>
		<authors>
			<first>Jarl A.</first>
		</authors>
		<authors>
			<last>Ahlkvist</last>
		</authors>
		<authors>
			<first>Robert</first>
		</authors>
		<authors>
			<last>Faulkner</last>
		</authors>
		<volume>25</volume>
		<number>2</number>
		<pages>189--215</pages>
		<abstract>How do radio stations decide what music to play on the air? Previous studies offer a single answer to this question. In contrast, this study examines the variety of ways that radio programmers answer this question by conceptualizing them as mediators between record companies and radio audiences. From interviews with programmers at commercial radio stations in the United States we identify key programming practices that programmers use to manage their stations' music formats and present a typology of repertoires of such practices. We then discuss the implications of each programming repertoire for stations' music formats and the structural conditions that promote the use of each programming repertoire. In conclusion, we consider the study's implications for understanding culture production in the commercial radio industry.
ER  -</abstract>
		<title>“Will This Record Work for Us?”: Managing Music Formats in Commercial Radio</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2db1172e52824faab19e89c2babae4222/mediadigits</id>
		<tags>emotion</tags>
		<tags>music</tags>
		<description></description>
		<date>2010-10-01 10:29:40</date>
		<count>1</count>
		<booktitle>11th International Society for Music Information and Retrieval Conference</booktitle>
		<year>2010</year>
		<url></url>
		<author>Youngmoo E. Kim</author>
		<author>Erik M. Schmidt</author>
		<author>Raymond Migneco</author>
		<author>Brandon G. Morton</author>
		<author>Patrick Richardson</author>
		<author>Jeffrey Scott</author>
		<author>Jacquelin A. Speck</author>
		<author>Douglas Urnbull</author>
		<authors>
			<first>Youngmoo E.</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Erik M.</first>
		</authors>
		<authors>
			<last>Schmidt</last>
		</authors>
		<authors>
			<first>Raymond</first>
		</authors>
		<authors>
			<last>Migneco</last>
		</authors>
		<authors>
			<first>Brandon G.</first>
		</authors>
		<authors>
			<last>Morton</last>
		</authors>
		<authors>
			<first>Patrick</first>
		</authors>
		<authors>
			<last>Richardson</last>
		</authors>
		<authors>
			<first>Jeffrey</first>
		</authors>
		<authors>
			<last>Scott</last>
		</authors>
		<authors>
			<first>Jacquelin A.</first>
		</authors>
		<authors>
			<last>Speck</last>
		</authors>
		<authors>
			<first>Douglas</first>
		</authors>
		<authors>
			<last>Urnbull</last>
		</authors>
		<editor>J. Stephen Downie</editor>
		<editor>Remco C. Veltkamp</editor>
		<editors>
			<first>Douglas</first>
		</editors>
		<editors>
			<last>Urnbull</last>
		</editors>
		<editors>
			<first>Douglas</first>
		</editors>
		<editors>
			<last>Urnbull</last>
		</editors>
		<title>Music Emotion Recognition: a State of the Art Review</title>
		<pubtype>conference</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/254d952d325c5f26bececd3c3fc87b59c/lwa2010</id>
		<tags>music</tags>
		<tags>system</tags>
		<tags>room:0446</tags>
		<tags>playlist</tags>
		<tags>collaborative</tags>
		<tags>recommendation</tags>
		<tags>recommender</tags>
		<tags>workshop:kdml</tags>
		<tags>session:kdml2</tags>
		<tags>markov-chain</tags>
		<tags>filtering</tags>
		<description></description>
		<date>2010-10-05 14:15:12</date>
		<count>1</count>
		<booktitle>Proceedings of LWA2010 - Workshop-Woche: Lernen, Wissen & Adaptivitaet</booktitle>
		<address>Kassel, Germany</address>
		<year>2010</year>
		<url>http://www.kde.cs.uni-kassel.de/conf/lwa10/papers/kdml7.pdf</url>
		<author>Andre Busche</author>
		<author>Artus Krohn-Grimberghe</author>
		<author>Lars Schmidt-Thieme.</author>
		<authors>
			<first>Andre</first>
		</authors>
		<authors>
			<last>Busche</last>
		</authors>
		<authors>
			<first>Artus</first>
		</authors>
		<authors>
			<last>Krohn-Grimberghe</last>
		</authors>
		<authors>
			<first>Lars</first>
		</authors>
		<authors>
			<last>Schmidt-Thieme.</last>
		</authors>
		<editor>Martin Atzmüller</editor>
		<editor>Dominik Benz</editor>
		<editor>Andreas Hotho</editor>
		<editor>Gerd Stumme</editor>
		<editors>
			<first>Lars</first>
		</editors>
		<editors>
			<last>Schmidt-Thieme.</last>
		</editors>
		<editors>
			<first>Lars</first>
		</editors>
		<editors>
			<last>Schmidt-Thieme.</last>
		</editors>
		<editors>
			<first>Lars</first>
		</editors>
		<editors>
			<last>Schmidt-Thieme.</last>
		</editors>
		<editors>
			<first>Lars</first>
		</editors>
		<editors>
			<last>Schmidt-Thieme.</last>
		</editors>
		<abstract>Recommender systems are popular social web tools, because they can address information overload and provide personalization of results rec05. This paper presents a collaborative approach to music playlist recommendation in a large scale scenario. We show that a simple markov-chain based algorithm improves performance compared to baseline models when neither content, nor user metadata is available. The lack of content-based features makes this task particularly hard.</abstract>
		<presentation_start>2010-10-05 16:45:00</presentation_start>
		<session>kdml2</session>
		<track>kdml</track>
		<presentation_end>2010-10-05 16:55:00</presentation_end>
		<room>0446</room>
		<title>Mining Music Playlogs for Next Song Recommendations</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23adfe412519d7ada72b3d495120d2244/svrist</id>
		<tags>line</tags>
		<tags>optical</tags>
		<tags>music</tags>
		<tags>recognition;</tags>
		<tags>restoration;</tags>
		<tags>staff</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>1</count>
		<journal>Seventh International Conference on Image Processing and Its Applications (Conf. Publ. No.465)</journal>
		<year>1999</year>
		<url></url>
		<author>K. Wijaya</author>
		<author>D. Bainbridge</author>
		<authors>
			<first>K.</first>
		</authors>
		<authors>
			<last>Wijaya</last>
		</authors>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Bainbridge</last>
		</authors>
		<volume>2</volume>
		<pages>760-4</pages>
		<abstract>Optical music recognition (OMR), the conversion of scanned pages of music into a musical database, has reached an exciting level of maturity. Like optical character recognition, it has now reached the point where the returns in accuracy from increasingly sophisticated pattern recognition algorithms appears saturated and more significant gains are being made from the application of structured a priori knowledge. This paper describes one such technique for improved staff line processing-the detection and subsequent correction of bowing in the staff lines, which is an important category given the significant source of music in book form. Two versions of the algorithm are tested: the first, based on mathematical morphology, has the added benefit of automatically fusing small breaks in staff lines, common for example in older works; the second, based on a flood-fill algorithm, requires a minor modification if fragmented staff lines are to be repaired. The correct detection and processing of staff lines is fundamental to OMR. Without adequate knowledge of staff line location, notation superimposed on the staves cannot be correctly separated, classified and processed (5 Refs.) recognition</abstract>
		<title>Staff line restoration</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/258c7f24c99c37e07e2ab41e6016ec5fa/svrist</id>
		<tags>imported</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>1</count>
		<journal>ISMIR 2007 - Proceedings of the 8th International Conference on Music Information Retrieval, Vienna, Austria, September 23-27</journal>
		<year>2007</year>
		<url></url>
		<author>Donald Knopke</author>
		<authors>
			<first>Donald</first>
		</authors>
		<authors>
			<last>Knopke</last>
		</authors>
		<pages>123-126</pages>
		<isbn>978-3-85403-218</isbn>
		<title>Towards Musicdiff: A Foundation for Improved Optical Music Recognition Using Multiple Recognizers</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28a2ff9ee8f3c6740c3b5de70848b352b/svrist</id>
		<tags>imported</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>1</count>
		<journal>Computing in Musicology</journal>
		<year>1994</year>
		<url></url>
		<author>N. P. Carter</author>
		<authors>
			<first>N. P.</first>
		</authors>
		<authors>
			<last>Carter</last>
		</authors>
		<volume>9</volume>
		<pages>152-8</pages>
		<title>Music score recognition: Problems and prospects</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2cab77db594fe241393128631de242bf3/aucelum</id>
		<tags>music</tags>
		<tags>mir</tags>
		<tags>mem</tags>
		<tags>xml</tags>
		<description></description>
		<date>2010-02-02 17:03:01</date>
		<count>2</count>
		<journal>ACM Transactions on Information Systems</journal>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2007</year>
		<url></url>
		<author>Alberto Pinto</author>
		<author>Goffredo Haus</author>
		<authors>
			<first>Alberto</first>
		</authors>
		<authors>
			<last>Pinto</last>
		</authors>
		<authors>
			<first>Goffredo</first>
		</authors>
		<authors>
			<last>Haus</last>
		</authors>
		<volume>25</volume>
		<number>4</number>
		<pages>19</pages>
		<abstract>The increasing diffusion of XML languages for the encoding of domain-specific multimedia information raises the need for new information retrieval models that can fully exploit structural information. An XML language specifically designed for music like MX allows queries to be made directly on the thematic material. The main advantage of such a system is that it can handle symbolic, notational, and audio objects at the same time through a multilayered structure. On the model side, common music information retrieval methods do not take into account the inner structure of melodic themes and the metric relationships between notes.

In this article we deal with two main topics: a novel architecture based on a new XML language for music and a new model of melodic themes based on graph theory.

This model takes advantage of particular graph invariants that can be linked to melodic themes as metadata in order to characterize all their possible modifications through specific transformations and that can be exploited in filtering algorithms. We provide a similarity function and show through an evaluation stage how it improves existing methods, particularly in the case of same-structured themes.</abstract>
		<issn>1046-8188</issn>
		<doi>http://doi.acm.org.proxy-remote.galib.uga.edu:2048/10.1145/1281485.1281490</doi>
		<title>A novel XML music information retrieval method using graph invariants</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27b2d3076dc4f378771765cc7b9ba26f4/kurtjx</id>
		<tags>music</tags>
		<tags>music_recommendation</tags>
		<tags>networks</tags>
		<description>Oscar Celma's PhD thesis</description>
		<date>2009-01-14 18:23:02</date>
		<count>2</count>
		<address>Barcelona, Spain</address>
		<year>2008</year>
		<url>http://mtg.upf.edu/~ocelma/PhD/doc/ocelma-thesis.pdf</url>
		<author>O. Celma</author>
		<authors>
			<first>O.</first>
		</authors>
		<authors>
			<last>Celma</last>
		</authors>
		<abstract>Music consumption is biased towards a few popular artists. For instance,
	in 2007 only 1% of all digital tracks accounted for 80% of all sales.
	Similarly, 1,000 albums accounted for 50% of all album sales, and
	80% of all albums sold were purchased less than 100 times. There
	is a need to assist people to filter, discover, personalise and recommend
	from the huge amount of music content available along the Long Tail.
	
	Current music recommendation algorithms try to accurately predict
	what people demand to listen to. However, quite often these algorithms
	tend to recommend popular -or well-known to the user- music, decreasing
	the effectiveness of the recommendations. These approaches focus
	on improving the accuracy of the recommendations. That is, try to
	make accurate predictions about what a user could listen to, or buy
	next, independently of how useful to the user could be the provided
	recommendations.
	
	In this Thesis we stress the importance of the user's perceived quality
	of the recommendations. We model the Long Tail curve of artist popularity
	to predict -potentially-interesting and unknown music, hidden in
	the tail of the popularity curve. Effective recommendation systems
	should promote novel and relevant material (non-obvious recommendations),
	taken primarily from the tail of a popularity distribution.
	
	The main contributions of this Thesis are: <i>(i)</i> a novel network-based
	approach for recommender systems, based on the analysis of the item
	(or user) similarity graph, and the popularity of the items, <i>(ii)</i>
	a user-centric evaluation that measures the user's relevance and
	novelty of the recommendations, and <i>(iii)</i> two prototype systems
	that implement the ideas derived from the theoretical work. Our findings
	have significant implications for recommender systems that assist
	users to explore the Long Tail, digging for content they might like.</abstract>
		<title>Music Recommendation and Discovery in the Long Tail</title>
		<pubtype>phdthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a2fbf9823867f0f11863b0941a18be12/mediadigits</id>
		<tags>music</tags>
		<tags>subjective</tags>
		<tags>similarity</tags>
		<tags>acoustic</tags>
		<description></description>
		<date>2009-02-05 10:19:19</date>
		<count>5</count>
		<journal>Computer Music Journal</journal>
		<publisher>MIT Press</publisher>
		<address>Cambridge, MA, USA</address>
		<year>2004</year>
		<url></url>
		<author>Adam Berenzweig</author>
		<author>Beth Logan</author>
		<author>Daniel P. W. Ellis</author>
		<author>Brian P. W. Whitman</author>
		<authors>
			<first>Adam</first>
		</authors>
		<authors>
			<last>Berenzweig</last>
		</authors>
		<authors>
			<first>Beth</first>
		</authors>
		<authors>
			<last>Logan</last>
		</authors>
		<authors>
			<first>Daniel P. W.</first>
		</authors>
		<authors>
			<last>Ellis</last>
		</authors>
		<authors>
			<first>Brian P. W.</first>
		</authors>
		<authors>
			<last>Whitman</last>
		</authors>
		<volume>28</volume>
		<number>2</number>
		<pages>63-76</pages>
		<issn>0148-9267</issn>
		<doi>http://dx.doi.org/10.1162/014892604323112257</doi>
		<title>A Large-Scale Evaluation of Acoustic and Subjective Music-Similarity Measures</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a66ad49809c6c844af8a16a2ff79338e/ged</id>
		<tags>imported</tags>
		<tags>structural</tags>
		<tags>plasticity</tags>
		<description>Musical Training Shapes Structural Brain Development -- Hyde et al. 29 (10): 3019 -- Journal of Neuroscience</description>
		<date>2009-08-09 01:40:45</date>
		<count>2</count>
		<journal>J. Neurosci.</journal>
		<year>2009</year>
		<url>http://www.jneurosci.org/cgi/content/abstract/29/10/3019</url>
		<author>Krista L. Hyde</author>
		<author>Jason Lerch</author>
		<author>Andrea Norton</author>
		<author>Marie Forgeard</author>
		<author>Ellen Winner</author>
		<author>Alan C. Evans</author>
		<author>Gottfried Schlaug</author>
		<authors>
			<first>Krista L.</first>
		</authors>
		<authors>
			<last>Hyde</last>
		</authors>
		<authors>
			<first>Jason</first>
		</authors>
		<authors>
			<last>Lerch</last>
		</authors>
		<authors>
			<first>Andrea</first>
		</authors>
		<authors>
			<last>Norton</last>
		</authors>
		<authors>
			<first>Marie</first>
		</authors>
		<authors>
			<last>Forgeard</last>
		</authors>
		<authors>
			<first>Ellen</first>
		</authors>
		<authors>
			<last>Winner</last>
		</authors>
		<authors>
			<first>Alan C.</first>
		</authors>
		<authors>
			<last>Evans</last>
		</authors>
		<authors>
			<first>Gottfried</first>
		</authors>
		<authors>
			<last>Schlaug</last>
		</authors>
		<volume>29</volume>
		<number>10</number>
		<pages>3019--3025</pages>
		<abstract>The human brain has the remarkable capacity to alter in response to environmental demands. Training-induced structural brain changes have been demonstrated in the healthy adult human brain. However, no study has yet directly related structural brain changes to behavioral changes in the developing brain, addressing the question of whether structural brain differences seen in adults (comparing experts with matched controls) are a product of "nature" (via biological brain predispositions) or "nurture" (via early training). Long-term instrumental music training is an intense, multisensory, and motor experience and offers an ideal opportunity to study structural brain plasticity in the developing brain in correlation with behavioral changes induced by training. Here we demonstrate structural brain changes after only 15 months of musical training in early childhood, which were correlated with improvements in musically relevant motor and auditory skills. These findings shed light on brain plasticity and suggest that structural brain differences in adult experts (whether musicians or experts in other areas) are likely due to training-induced brain plasticity.</abstract>
		<doi>10.1523/JNEUROSCI.5118-08.2009</doi>
		<eprint>http://www.jneurosci.org/cgi/reprint/29/10/3019.pdf</eprint>
		<title>Musical Training Shapes Structural Brain Development</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24cd7c573c9490dab2d9ab2b45a897d1a/kurtjx</id>
		<tags>Playlist,</tags>
		<tags>music</tags>
		<tags>similarity</tags>
		<description>main bib file</description>
		<date>2008-03-17 17:08:40</date>
		<count>3</count>
		<booktitle>Proc. 8th ACM international workshop on Multimedia information retrieval</booktitle>
		<year>2006</year>
		<url></url>
		<author>P. Knees</author>
		<author>T. Pohle</author>
		<author>M. Schedl</author>
		<author>G. Widmer</author>
		<authors>
			<first>P.</first>
		</authors>
		<authors>
			<last>Knees</last>
		</authors>
		<authors>
			<first>T.</first>
		</authors>
		<authors>
			<last>Pohle</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<authors>
			<first>G.</first>
		</authors>
		<authors>
			<last>Widmer</last>
		</authors>
		<pages>147 - 154</pages>
		<title>Combining Audio-based Similarity with Web-based Data to Accelerate Automatic Music Playlist Generation</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28e121312aa9284c1f5e91de2a0ea57c6/kurtjx</id>
		<tags>music</tags>
		<tags>networks</tags>
		<description>main bib file</description>
		<date>2008-03-17 17:08:40</date>
		<count>4</count>
		<booktitle>Proc. of Int. Symposium on Music Information Retrieval</booktitle>
		<year>2007</year>
		<url></url>
		<author>Amélie Anglade</author>
		<author>Marco Tiemann</author>
		<author>Fabio Vignoli</author>
		<authors>
			<first>Amélie</first>
		</authors>
		<authors>
			<last>Anglade</last>
		</authors>
		<authors>
			<first>Marco</first>
		</authors>
		<authors>
			<last>Tiemann</last>
		</authors>
		<authors>
			<first>Fabio</first>
		</authors>
		<authors>
			<last>Vignoli</last>
		</authors>
		<title>Virtual communities for creating shared music channels</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e35a25c1b01d0dff343bb9c5c54e7e0f/brazovayeye</id>
		<tags>genetic</tags>
		<tags>algorithms,</tags>
		<tags>programming</tags>
		<description></description>
		<date>2008-06-19 17:35:00</date>
		<count>2</count>
		<year>1998</year>
		<url>http://graphics.stanford.edu/~bjohanso/gp-music/tech-report</url>
		<author>Bradley E Johanson</author>
		<author>Riccardo Poli</author>
		<authors>
			<first>Bradley E</first>
		</authors>
		<authors>
			<last>Johanson</last>
		</authors>
		<authors>
			<first>Riccardo</first>
		</authors>
		<authors>
			<last>Poli</last>
		</authors>
		<number>CSRP-98-13</number>
		<abstract>In this paper we present the GP-Music System, an
                 interactive system which allows users to evolve short
                 musical sequences using interactive genetic
                 programming, and its extensions aimed at making the
                 system fully automated. The basic GP system works by
                 using a genetic programming algorithm, a small set of
                 functions for creating musical sequences, and a user
                 interface which allows the user to rate individual
                 sequences. With this user interactive technique it was
                 possible to generate pleasant tunes over runs of 20
                 individuals over 10 generations. As the user is the
                 bottleneck in interactive systems, the system takes
                 rating data from a users run and uses it to train a
                 neural network based automatic rater, or äuto
                 rater", which can replace the user in bigger runs.
                 Using this auto rater we were able to make runs of up
                 to 50 generations with 500 individuals per generation.
                 The best of run pieces generated by the auto raters
                 were pleasant but were not, in general, as nice as
                 those generated in user interactive runs.</abstract>
		<email>bjohanso@stanford.edu, R.Poli@cs.bham.ac.uk</email>
		<title>GP-Music: An Interactive Genetic Programming System
                 for Music Generation with Automated Fitness Raters</title>
		<pubtype>techreport</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21ce13ff7f85386768d0e52376e90f54d/kurtjx</id>
		<tags>soundbite</tags>
		<description></description>
		<date>2008-07-23 13:46:52</date>
		<count>1</count>
		<booktitle>AMCMM '06: Proceedings of the 1st ACM workshop on Audio and music computing multimedia</booktitle>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2006</year>
		<url>http://portal.acm.org/citation.cfm?id=1178723.1178728</url>
		<author>Mark Levy</author>
		<author>Mark Sandler</author>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Levy</last>
		</authors>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Sandler</last>
		</authors>
		<pages>27--36</pages>
		<abstract>Timbral similarity measures basedon Mel-Frequency Cepstral Coefficients have been widely reported as the basis for a possible general music similarity function, which would have wide application to searching, browsing and recommendation. Many of the reported methods, however, have computational requirements that make them impractical for searching realistic collections using current hardware. We compare lightweight measures that appear to perform equally well, and introduce a simplification that reduces memory requirements and execution time by a further order of magnitude. This yields a similarity measure that will scale easily to large commercial collections. We give comparative results over two contrasting music collections, one of which has been widely studied, allowing direct comparison with previous work.</abstract>
		<isbn>1-59593-501-0</isbn>
		<doi>http://doi.acm.org/10.1145/1178723.1178728</doi>
		<title>Lightweight measures for timbral similarity of musical audio</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b7a9bfc91fbe1044a6dd7b0565133c70/reader1066</id>
		<tags>preservation</tags>
		<tags>sound</tags>
		<description></description>
		<date>2007-06-20 23:49:02</date>
		<count>1</count>
		<journal>Fontes Artis Musicae</journal>
		<year>20011001</year>
		<url>http://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=9059826&site=ehost-live</url>
		<author>Mary Russell Bucknum</author>
		<authors>
			<first>Mary Russell</first>
		</authors>
		<authors>
			<last>Bucknum</last>
		</authors>
		<volume>48</volume>
		<number>4</number>
		<pages>p381 -</pages>
		<abstract>United States music sound archives, like sound archives throughout the world, continue to collect and preserve recent sound recordings while simultaneously coping with a century's cumulation of deteriorating physical formats and the ever-present challenges to the ideal of providing the fullest possible access to the recordings. This article provides a brief historical overview of music sound archives in the United States, followed by an examination of some of the more pressing issues involving the acquisition, preservation, and cataloging of sound recordings. The author discusses recent developments in sound archiving, how some United States archives have reacted to these developments, and possible directions for the future of sound archiving. ABSTRACT FROM AUTHOR Copyright of Fontes Artis Musicae is the property of International Association of Music Libraries and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express</abstract>
		<issn>00156191</issn>
		<title>MUSIC SOUND ARCHIVES IN THE UNITED STATES.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20eaca24c343fad6c84b6884a1e781e4b/roos</id>
		<tags>music</tags>
		<tags>metadata</tags>
		<tags>IR</tags>
		<description></description>
		<date>2007-07-19 22:35:37</date>
		<count>2</count>
		<booktitle>Proceedings of the 7th International Conference on Music Information Retrieval</booktitle>
		<publisher>University of Victoria</publisher>
		<address>Victoria, BC, Canada</address>
		<year>2006</year>
		<url></url>
		<author>Jenn Riley</author>
		<author>Constance A. Mayer</author>
		<authors>
			<first>Jenn</first>
		</authors>
		<authors>
			<last>Riley</last>
		</authors>
		<authors>
			<first>Constance A.</first>
		</authors>
		<authors>
			<last>Mayer</last>
		</authors>
		<editor>Roger Dannenberg</editor>
		<editor>Kjell Lemstrom</editor>
		<editor>Adam Tindale</editor>
		<editors>
			<first>Constance A.</first>
		</editors>
		<editors>
			<last>Mayer</last>
		</editors>
		<editors>
			<first>Constance A.</first>
		</editors>
		<editors>
			<last>Mayer</last>
		</editors>
		<editors>
			<first>Constance A.</first>
		</editors>
		<editors>
			<last>Mayer</last>
		</editors>
		<pages>13--18</pages>
		<isbn>1-55058-349-2</isbn>
		<title>Ask a Librarian: The Role of Librarians in the Music Information Retrieval Community</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bcf722766f63261d71f19db059747868/yaxu</id>
		<tags>toplap</tags>
		<tags>livecoding</tags>
		<description></description>
		<date>2007-03-12 23:12:41</date>
		<count>2</count>
		<journal>Contemporary Music Review</journal>
		<year>2003</year>
		<url>http://www.ingentaconnect.com/content/routledg/gcmr/2003/00000022/00000004/art00009</url>
		<author>Nick Collins</author>
		<authors>
			<first>Nick</first>
		</authors>
		<authors>
			<last>Collins</last>
		</authors>
		<volume>22</volume>
		<pages>67-79(13)</pages>
		<abstract>Live computer music is the perfect medium for generative music systems, for non-linear compositional constructions and for interactive manipulation of sound processing. Unfortunately, much of the complexity of these real-time systems is lost on a potential audience, excepting those few connoisseurs who sneak round the back to check the laptop screen. An artist using powerful software like SuperCollider or PD cannot be readily distinguished from someone checking their e-mail whilst DJ-ing with iTunes. Without a culture of understanding of both the laptop performer and current generation graphical and text-programming languages for audio, audiences tend to respond most to often gimmicky controllers, or to the tools they have had more exposure to - the (yawn) superstar DJs and their decks. This article attempts to convey the exciting things that are being explored with algorithmic composition and interactive synthesis techniques in live performance. The reasons for building generative music systems and the forms of control attainable over algorithmic processes are investigated. Direct manual control is set against the use of autonomous software agents. In line with this, four techniques for software control during live performance are introduced, namely presets, previewing, autopilot, and the powerful method of live coding. Finally, audio-visual collaboration is discussed.</abstract>
		<doi>doi:10.1080/0749446032000156919</doi>
		<title>Generative Music and Laptop Performance</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f17c154bbc519d8a6c03afea475559fc/gresch</id>
		<tags>music</tags>
		<tags>software</tags>
		<tags>musik</tags>
		<tags>analysis</tags>
		<description></description>
		<date>2010-12-11 23:06:20</date>
		<count>6</count>
		<journal>Science</journal>
		<year>2006</year>
		<url>http://www.sciencemag.org/cgi/content/abstract/313/5783/72</url>
		<author>Dmitri Tymoczko</author>
		<authors>
			<first>Dmitri</first>
		</authors>
		<authors>
			<last>Tymoczko</last>
		</authors>
		<volume>313</volume>
		<number>5783</number>
		<pages>72-74</pages>
		<abstract>A musical chord can be represented as a point in a geometrical space called an orbifold. Line segments represent mappings from the notes of one chord to those of another. Composers in a wide range of styles have exploited the non-Euclidean geometry of these spaces, typically by using short line segments between structurally similar chords. Such line segments exist only when chords are nearly symmetrical under translation, reflection, or permutation. Paradigmatically consonant and dissonant chords possess different near-symmetries and suggest different musical uses.</abstract>
		<doi>10.1126/science.1126287</doi>
		<eprint>http://www.sciencemag.org/cgi/reprint/313/5783/72.pdf</eprint>
		<title>The Geometry of Musical Chords</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2df6b0cc29a926561f05a7cbeba90f9f4/jamie.bullock</id>
		<tags>features</tags>
		<tags>myown</tags>
		<tags>music</tags>
		<tags>audio</tags>
		<tags>live</tags>
		<description></description>
		<date>2011-03-30 14:04:50</date>
		<count>1</count>
		<year>2008</year>
		<url></url>
		<author>Jamie Bullock</author>
		<authors>
			<first>Jamie</first>
		</authors>
		<authors>
			<last>Bullock</last>
		</authors>
		<abstract>Music with live electronics involves capturing an acoustic input, converting it to an electrical signal, processing it electronically and converting it back to an acoustic waveform through a loudspeaker. The electronic processing is usually controlled during performance through human interaction with potentiometers, switches, sensors and other tactile controllers. These tan- gible interfaces, when operated by a technical assistant or dedicated elec- tronics performer can be effective for controlling multiple processing pa- rameters. However, when a composer wishes to delegate control over the electronics to an (acoustic) instrumental performer, physical interfaces can sometimes be problematic. Performers who are unfamiliar with electron- ics technology, must learn to operate and interact effectively with the inter- faces provided. The operation of the technology is sometimes unintuitive, and fits uncomfortably with the performer’s learned approach to her instru- ment, creating uncertainty for both performer and audience. The presence of switches or sensors on and around the instrumental performer begs the questions: how should I interact with this and is it working correctly?
In this thesis I propose an alternative to the physical control paradigm, whereby features derived from the sound produced by the acoustic instru- ment itself are used as a control source. This approach removes the potential for performer anxiety posed by tangible interfaces and allows the performer to focus on instrumental sound production and the effect this has on the electronic processing. A number of experiments will be conducted through a reciprocal process of composition, performance and software develop- ment in order to evaluate a range of methods for instrumental interaction with electronics through sonic change. The focus will be on the use of ‘low level’ audio features including, but not limited to, fundamental frequency,
amplitude, brightness and noise content. To facilitate these experiments, a number of pieces of software for audio feature extraction and visualisa- tion will be developed and tested, the final aim being that this software will be publically released for download and usable in a range of audio feature extraction contexts.
In the conclusion, I will propose a new approach to working with audio feature extraction in the context of live electronic music. This approach will combine the audio feature extraction and visualisation techniques dis- cussed and evaluated in previous chapters. A new piece of software will be presented in the form of a graphical user interface for perfomers to work interactively using sound as an expressive control source. Conclusions will also be drawn about the methdology employed during this research, with particular focus on the relationship between composition, ‘do-it-yourself ’ live electronics and software development as research process.</abstract>
		<title>Implementing audio feature extraction in live electronic music</title>
		<pubtype>phdthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c10c14e487fef6f9fa1c9f801e890c40/yevb0</id>
		<tags>music,musicality,neuro,syntax</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Psychophysiology</journal>
		<year>1987</year>
		<url>http://doi.wiley.com/10.1111/j.1469-8986.1987.tb01853.x</url>
		<author>Mireille Besson</author>
		<author>F. Macar</author>
		<authors>
			<first>Mireille</first>
		</authors>
		<authors>
			<last>Besson</last>
		</authors>
		<authors>
			<first>F.</first>
		</authors>
		<authors>
			<last>Macar</last>
		</authors>
		<volume>24</volume>
		<number>1</number>
		<pages>14--25</pages>
		<issn>0048-5772</issn>
		<title>An event-related potential analysis of incongruity in music and other
	non-linguistic contexts</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a0efd036d94f2ac78c0b86be185b25e5/yevb0</id>
		<tags>Auditory</tags>
		<tags>Perception,Auditory</tags>
		<tags>Perception:</tags>
		<tags>physiology,Cognition,Emotions,Humans,Learning,Music,Music:</tags>
		<tags>psychology,Pitch</tags>
		<tags>Perception,Psychoacoustics</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Cognition</journal>
		<year>2006</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/16412412</url>
		<author>E Bigand</author>
		<author>B Poulin-Charronnat</author>
		<authors>
			<first>E</first>
		</authors>
		<authors>
			<last>Bigand</last>
		</authors>
		<authors>
			<first>B</first>
		</authors>
		<authors>
			<last>Poulin-Charronnat</last>
		</authors>
		<volume>100</volume>
		<number>1</number>
		<pages>100--30</pages>
		<abstract>The present paper reviews a set of studies designed to investigate
	different aspects of the capacity for processing Western music. This
	includes perceiving the relationships between a theme and its variations,
	perceiving musical tensions and relaxations, generating musical expectancies,
	integrating local structures in large-scale structures, learning
	new compositional systems and responding to music in an emotional
	(affective) way. The main focus of these studies was to evaluate
	the influence of intensive musical training on these capacities.
	The overall set of data highlights that some musical capacities are
	acquired through exposure to music without the help of explicit training.
	These capacities reach such a degree of sophistication that they
	enable untrained listeners to respond to music as "musically experienced
	listeners" do.</abstract>
		<issn>0010-0277</issn>
		<doi>10.1016/j.cognition.2005.11.007</doi>
		<title>Are we ``experienced listeners''? A review of the musical capacities
	that do not depend on formal musical training</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/261be558a6b23b6717b1dbea4ee01f4a2/yevb0</id>
		<tags>Stimulation:</tags>
		<tags>Stimulation,Acoustic</tags>
		<tags>Acoustic</tags>
		<tags>anatomy</tags>
		<tags>Discrimination:</tags>
		<tags>Perception,Pitch</tags>
		<tags>Discrimination,Pitch</tags>
		<tags>\&</tags>
		<tags>physiology,Pitch</tags>
		<tags>Perception,Auditory</tags>
		<tags>Perception:</tags>
		<tags>histology,Brain:</tags>
		<tags>physiology,Psychoacoustics,Sound,music,musicality,perception,pitch</tags>
		<tags>methods,Animals,Auditory</tags>
		<tags>physiology,Humans,Music,Pitch</tags>
		<tags>physiology,Brain,Brain:</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>2</count>
		<journal>Current Opinion in Neurobiology</journal>
		<year>2008</year>
		<url>http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2629434\&tool=pmcentrez\&rendertype=abstract</url>
		<author>Josh H McDermott</author>
		<author>Andrew J Oxenham</author>
		<authors>
			<first>Josh H</first>
		</authors>
		<authors>
			<last>McDermott</last>
		</authors>
		<authors>
			<first>Andrew J</first>
		</authors>
		<authors>
			<last>Oxenham</last>
		</authors>
		<volume>18</volume>
		<number>4</number>
		<pages>452--63</pages>
		<abstract>The perception of music depends on many culture-specific factors,
	but is also constrained by properties of the auditory system. This
	has been best characterized for those aspects of music that involve
	pitch. Pitch sequences are heard in terms of relative as well as
	absolute pitch. Pitch combinations give rise to emergent properties
	not present in the component notes. In this review we discuss the
	basic auditory mechanisms contributing to these and other perceptual
	effects in music.</abstract>
		<issn>0959-4388</issn>
		<doi>10.1016/j.conb.2008.09.005</doi>
		<title>Music perception, pitch, and the auditory system</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bc6e44f48bdb8eb590f1dbbab3ee4f06/yevb0</id>
		<tags>L1,M1,acquisition,language,music,musicality</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Music Perception</journal>
		<year>2010</year>
		<url></url>
		<author>Erin McMullen</author>
		<author>Jenny R Saffran</author>
		<authors>
			<first>Erin</first>
		</authors>
		<authors>
			<last>McMullen</last>
		</authors>
		<authors>
			<first>Jenny R</first>
		</authors>
		<authors>
			<last>Saffran</last>
		</authors>
		<volume>21</volume>
		<number>3</number>
		<pages>289--311</pages>
		<title>Music and language: A developmental comparison</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28a8ec9bd3408beadd50ab5b90829d58a/yevb0</id>
		<tags>Development,Hearing,Hearing</tags>
		<tags>physiology,Humans,Language,Music,Pitch</tags>
		<tags>psychology,acquisition,language,music,musicality,perception,speech</tags>
		<tags>Discrimination,Students,Students:</tags>
		<tags>Adult,Aptitude,Child,Child</tags>
		<tags>Tests,Hearing:</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Brain Research</journal>
		<year>2007</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/17509539</url>
		<author>Riia Milovanov</author>
		<author>Mari Tervaniemi</author>
		<author>Fiia Takio</author>
		<author>Heikki Hämäläinen</author>
		<authors>
			<first>Riia</first>
		</authors>
		<authors>
			<last>Milovanov</last>
		</authors>
		<authors>
			<first>Mari</first>
		</authors>
		<authors>
			<last>Tervaniemi</last>
		</authors>
		<authors>
			<first>Fiia</first>
		</authors>
		<authors>
			<last>Takio</last>
		</authors>
		<authors>
			<first>Heikki</first>
		</authors>
		<authors>
			<last>Hämäläinen</last>
		</authors>
		<volume>1156</volume>
		<number>Dl</number>
		<pages>168--73</pages>
		<abstract>To increase our understanding of the phonemic processing skills of
	musical and non-musical subjects, the Dichotic Listening task was
	performed in children and adults with varying degrees of musical
	aptitude. The roles of maturation and musical training were also
	investigated. The results showed superior left ear monitoring skills
	among the adults who practised music regularly. This may indicate
	altered hemispheric functioning. Other musically talented subjects
	did not have the ability to control left ear functioning in an equal
	manner, for instance, the performance of musical children and their
	non-musical controls in the forced-left / left ear condition did
	not differ. Thus, regular music practice may have a modulatory effect
	on the brain's linguistic organization and therefore, the beneficial
	effects of music on other cognitive skills should not be underestimated.</abstract>
		<issn>0006-8993</issn>
		<doi>10.1016/j.brainres.2007.04.048</doi>
		<title>Modification of dichotic listening (DL) performance by musico-linguistic
	abilities and age</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fd8e961c451d74de66ef6a5582e23168/yevb0</id>
		<tags>attention,music,musicality,perception</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>11th International Conference on Music Perception and Cognition</booktitle>
		<address>Seattle, WA</address>
		<year>2010</year>
		<url></url>
		<author>Dana L Strait</author>
		<author>Nina Kraus</author>
		<author>Victor Abecassis</author>
		<author>Richard Ashley</author>
		<authors>
			<first>Dana L</first>
		</authors>
		<authors>
			<last>Strait</last>
		</authors>
		<authors>
			<first>Nina</first>
		</authors>
		<authors>
			<last>Kraus</last>
		</authors>
		<authors>
			<first>Victor</first>
		</authors>
		<authors>
			<last>Abecassis</last>
		</authors>
		<authors>
			<first>Richard</first>
		</authors>
		<authors>
			<last>Ashley</last>
		</authors>
		<title>Musical training's impact on neural mechanisms of selective auditory
	attention</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/216828f6028678583cdc42ad0a68f685d/yevb0</id>
		<tags>Style,Male,Music,Occupations,Pitch</tags>
		<tags>Threshold:</tags>
		<tags>Stimulation,Adult,Audiometry,Auditory</tags>
		<tags>Threshold,Auditory</tags>
		<tags>Acoustic</tags>
		<tags>Discrimination:</tags>
		<tags>Discrimination,Pitch</tags>
		<tags>Threshold,Differential</tags>
		<tags>physiology,Differential</tags>
		<tags>physiology,Pure-Tone,music,musicality,perception,pitch</tags>
		<tags>physiology,Education,Humans,Life</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Journal of Basic and Clinical Physiology and Pharmacology</journal>
		<year>2001</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/11605682</url>
		<author>Liat Kishon-Rabin</author>
		<author>Ofer Amir</author>
		<author>Y Vexler</author>
		<author>Y Zaltz</author>
		<authors>
			<first>Liat</first>
		</authors>
		<authors>
			<last>Kishon-Rabin</last>
		</authors>
		<authors>
			<first>Ofer</first>
		</authors>
		<authors>
			<last>Amir</last>
		</authors>
		<authors>
			<first>Y</first>
		</authors>
		<authors>
			<last>Vexler</last>
		</authors>
		<authors>
			<first>Y</first>
		</authors>
		<authors>
			<last>Zaltz</last>
		</authors>
		<volume>12</volume>
		<number>2 Suppl</number>
		<pages>125--43</pages>
		<abstract>Musicians are typically considered to exhibit exceptional auditory
	skills. Only few studies, however, have substantiated this in basic
	psychoacoustic tasks. The purpose of the present investigation was
	to expand our knowledge on basic auditory abilities of musicians
	compared to non-musicians. Specific goals were: (1) to compare frequency
	discrimination thresholds (difference limen for frequency DLF)
	of non-musical pure tones in controlled groups of professional musicians
	and non-musicians; (2) to relate DLF performance to musical background;
	and (3) to compare DLF thresholds obtained with two threshold estimation
	procedures: 2- and 3- interval forced choice procedures (2IFC and
	3IFC). Subjects were 16 professional musicians and 14 non-musicians.
	DLFs were obtained for three frequencies (0.25, 1 and 1.5 kHz) using
	the 3IFC adaptive procedure, and for one frequency (1 kHz) also using
	the 2IFC. Three threshold estimates were obtained for each frequency,
	procedure and subject. The results of the present study support five
	major findings: (a) mean DLFs for musicians were approximately half
	the values of the non-musicians; (b) significant learning for both
	groups during the three threshold estimations; (c) classical musicians
	performed better than those with contemporary musical background;
	(d) performance was influenced by years of musical experience; and
	(e) both groups showed better DLF in a 2IFC paradigm compared to
	the 3IFC. These data highlight the importance of short-term training
	on an auditory task, auditory memory and factors related to musical
	background (such as musical genre and years of experience) on auditory
	performance.</abstract>
		<issn>0792-6855</issn>
		<title>Pitch discrimination: are professional musicians better than non-musicians?</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f5072d93a283216d3aafd3a4198219fd/yevb0</id>
		<tags>Imaging,Male,Music,Perception,Perception:</tags>
		<tags>Laterality:</tags>
		<tags>anatomy</tags>
		<tags>histology,Auditory</tags>
		<tags>Cortex,Auditory</tags>
		<tags>Perception,Pitch</tags>
		<tags>physiology,Humans,Magnetic</tags>
		<tags>\&</tags>
		<tags>physiology,Pitch</tags>
		<tags>physiology,Questionnaires,music,musicality,neuro,perception</tags>
		<tags>Cortex:</tags>
		<tags>Factors,Auditory</tags>
		<tags>Perception:</tags>
		<tags>physiology,Female,Functional</tags>
		<tags>Resonance</tags>
		<tags>Adult,Age</tags>
		<tags>Laterality,Functional</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Cerebral Cortex</journal>
		<year>2001</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/11459765</url>
		<author>T Ohnishi</author>
		<author>H Matsuda</author>
		<author>T Asada</author>
		<author>M Aruga</author>
		<author>M Hirakata</author>
		<author>M Nishikawa</author>
		<author>A Katoh</author>
		<author>E Imabayashi</author>
		<authors>
			<first>T</first>
		</authors>
		<authors>
			<last>Ohnishi</last>
		</authors>
		<authors>
			<first>H</first>
		</authors>
		<authors>
			<last>Matsuda</last>
		</authors>
		<authors>
			<first>T</first>
		</authors>
		<authors>
			<last>Asada</last>
		</authors>
		<authors>
			<first>M</first>
		</authors>
		<authors>
			<last>Aruga</last>
		</authors>
		<authors>
			<first>M</first>
		</authors>
		<authors>
			<last>Hirakata</last>
		</authors>
		<authors>
			<first>M</first>
		</authors>
		<authors>
			<last>Nishikawa</last>
		</authors>
		<authors>
			<first>A</first>
		</authors>
		<authors>
			<last>Katoh</last>
		</authors>
		<authors>
			<first>E</first>
		</authors>
		<authors>
			<last>Imabayashi</last>
		</authors>
		<volume>11</volume>
		<number>8</number>
		<pages>754--60</pages>
		<abstract>The present study used functional magnetic resonance to examine the
	cerebral activity pattern associated with musical perception in musicians
	and non-musicians. Musicians showed left dominant secondary auditory
	areas in the temporal cortex and the left posterior dorsolateral
	prefrontal cortex during a passive music listening task, whereas
	non-musicians demonstrated right dominant secondary auditory areas
	during the same task. A significant difference in the degree of activation
	between musicians and non-musicians was noted in the bilateral planum
	temporale and the left posterior dorsolateral prefrontal cortex.
	The degree of activation of the left planum temporale correlated
	well with the age at which the person had begun musical training.
	Furthermore, the degree of activation in the left posterior dorsolateral
	prefrontal cortex and the left planum temporale correlated significantly
	with absolute pitch ability. The results indicated distinct neural
	activity in the auditory association areas and the prefrontal cortex
	of trained musicians. We suggest that such activity is associated
	with absolute pitch ability and the use-dependent functional reorganization
	produced by the early commencement of long-term training.</abstract>
		<issn>1047-3211</issn>
		<title>Functional anatomy of musical perception in musicians</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29792b9fd239a3fe136f9bea2ef169185/yevb0</id>
		<tags>Cortex:</tags>
		<tags>Stimulation:</tags>
		<tags>physiology,Auditory:</tags>
		<tags>Stimulation,Acoustic</tags>
		<tags>Acoustic</tags>
		<tags>Cortex,Auditory</tags>
		<tags>Potentials,Female,Humans,Magnetoencephalography,Male,Music,music,musicality,neuro,timbre</tags>
		<tags>of</tags>
		<tags>physiology,Evoked</tags>
		<tags>Variance,Auditory,Auditory</tags>
		<tags>methods,Adult,Analysis</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Neuroreport</journal>
		<year>2001</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/11201080</url>
		<author>Christo Pantev</author>
		<author>Larry E Roberts</author>
		<author>Matthias Schulz</author>
		<author>A Engelien</author>
		<author>Bernhard Ross</author>
		<authors>
			<first>Christo</first>
		</authors>
		<authors>
			<last>Pantev</last>
		</authors>
		<authors>
			<first>Larry E</first>
		</authors>
		<authors>
			<last>Roberts</last>
		</authors>
		<authors>
			<first>Matthias</first>
		</authors>
		<authors>
			<last>Schulz</last>
		</authors>
		<authors>
			<first>A</first>
		</authors>
		<authors>
			<last>Engelien</last>
		</authors>
		<authors>
			<first>Bernhard</first>
		</authors>
		<authors>
			<last>Ross</last>
		</authors>
		<volume>12</volume>
		<number>1</number>
		<pages>169--74</pages>
		<abstract>Neural imaging studies have shown that the brains of skilled musicians
	respond differently to musical stimuli than do the brains of non-musicians,
	particularly for musicians who commenced practice at an early age.
	Whether brain attributes related to musical skill are attributable
	to musical practice or are hereditary traits that influence the decision
	to train musically is a subject of controversy, owing to its pedagogic
	implications. Here we report that auditory cortical representations
	measured neuromagnetically for tones of different timbre (violin
	and trumpet) are enhanced compared to sine tones in violinists and
	trumpeters, preferentially for timbres of the instrument of training.
	Timbre specificity is predicted by a principle of use-dependent plasticity
	and imposes new requirements on nativistic accounts of brain attributes
	associated with musical skill.</abstract>
		<issn>0959-4965</issn>
		<title>Timbre-specific enhancement of auditory cortical representations
	in musicians</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2501873b888e4eaaa653c6ad68790f930/yevb0</id>
		<tags>music,musicality,neuro,perception,plasticity</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Annals of the New York Academy of Sciences</journal>
		<year>2003</year>
		<url></url>
		<author>Christo Pantev</author>
		<author>Bernhard Ross</author>
		<author>Takako Fujioka</author>
		<author>Laurel J. Trainor</author>
		<author>Michael Schulte</author>
		<author>Matthias Schulz</author>
		<authors>
			<first>Christo</first>
		</authors>
		<authors>
			<last>Pantev</last>
		</authors>
		<authors>
			<first>Bernhard</first>
		</authors>
		<authors>
			<last>Ross</last>
		</authors>
		<authors>
			<first>Takako</first>
		</authors>
		<authors>
			<last>Fujioka</last>
		</authors>
		<authors>
			<first>Laurel J.</first>
		</authors>
		<authors>
			<last>Trainor</last>
		</authors>
		<authors>
			<first>Michael</first>
		</authors>
		<authors>
			<last>Schulte</last>
		</authors>
		<authors>
			<first>Matthias</first>
		</authors>
		<authors>
			<last>Schulz</last>
		</authors>
		<volume>999</volume>
		<pages>438--450</pages>
		<doi>10.1196/annals.1284.054</doi>
		<title>Music and learning-induced cortical plasticity</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a1466292405358c8173b5d2967fba22e/yevb0</id>
		<tags>music,uganda</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>The Journal of the Royal Anthropological Institute of Great Britain
	and Ireland</journal>
		<year>1953</year>
		<url>http://www.jstor.org/stable/2844153?origin=crossref</url>
		<author>K. P. Wachsmann</author>
		<authors>
			<first>K. P.</first>
		</authors>
		<authors>
			<last>Wachsmann</last>
		</authors>
		<volume>83</volume>
		<number>1</number>
		<pages>50--57</pages>
		<issn>03073114</issn>
		<doi>10.2307/2844153</doi>
		<title>Musicology in Uganda</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2289d58e3eec3326c040a25e5db823526/yevb0</id>
		<tags>attention,language,memory,music,musicality,perception</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>11th International Conference on Music Perception and Cognition</booktitle>
		<address>Seattle, WA</address>
		<year>2010</year>
		<url></url>
		<author>Vijayachandra Ramachandra</author>
		<author>Colleen Meighan</author>
		<author>Jillian Gradzki</author>
		<authors>
			<first>Vijayachandra</first>
		</authors>
		<authors>
			<last>Ramachandra</last>
		</authors>
		<authors>
			<first>Colleen</first>
		</authors>
		<authors>
			<last>Meighan</last>
		</authors>
		<authors>
			<first>Jillian</first>
		</authors>
		<authors>
			<last>Gradzki</last>
		</authors>
		<title>The influence of musical training on the phonological loop and the
	central executive</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e5315b729144e145b3abb76c8543067f/yevb0</id>
		<tags>Uganda,amadinda,music,scale</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>African Music</journal>
		<year>1969</year>
		<url>http://www.jstor.org/stable/30249659</url>
		<author>Gerhard Kubik</author>
		<authors>
			<first>Gerhard</first>
		</authors>
		<authors>
			<last>Kubik</last>
		</authors>
		<volume>4</volume>
		<number>3</number>
		<pages>22--72</pages>
		<title>Composition techniques in Kiganda xylophone music: With an introduction
	into some Kiganda musical concepts</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25b22591971726370737dae582368855b/yevb0</id>
		<tags>language,linguistics,music\_cognition,neuroscience,syntax</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Nature Neuroscience</journal>
		<year>2003</year>
		<url></url>
		<author>A. D. Patel</author>
		<authors>
			<first>A. D.</first>
		</authors>
		<authors>
			<last>Patel</last>
		</authors>
		<volume>6</volume>
		<number>7</number>
		<pages>674--681</pages>
		<abstract>The comparative study of music and language is drawing an increasing
	amount of research interest. Like language, music is a human universal
	involving perceptually discrete elements organized into hierarchically
	structured sequences. Music and language can thus serve as foils
	for each other in the study of brain mechanisms underlying complex
	sound processing, and comparative research can provide novel insights
	into the functional and neural architecture of both domains. This
	review focuses on syntax, using recent neuroimaging data and cognitive
	theory to propose a specific point of convergence between syntactic
	processing in language and music. This leads to testable predictions,
	including the prediction that that syntactic comprehension problems
	in Broca's aphasia are not selective to language but influence music
	perception as well.</abstract>
		<doi>http://dx.doi.org/10.1038/nn1082</doi>
		<title>Language, Music, Syntax and the Brain.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25b8d079eecf7dbd724dd2a84aeb9cd29/yevb0</id>
		<tags>physiology,Auditory:</tags>
		<tags>Association</tags>
		<tags>Perception,Auditory</tags>
		<tags>Perception:</tags>
		<tags>physiology,Auditory,Auditory</tags>
		<tags>physiology,Evoked</tags>
		<tags>Learning:</tags>
		<tags>Learning,Association</tags>
		<tags>Perception,Speech</tags>
		<tags>physiology,language,music,musicality,speech</tags>
		<tags>Potentials,Humans,Language,Music,Phonetics,Speech</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Trends in Cognitive Sciences</journal>
		<year>2007</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/17698406</url>
		<author>Aniruddh D. Patel</author>
		<author>John R. Iversen</author>
		<authors>
			<first>Aniruddh D.</first>
		</authors>
		<authors>
			<last>Patel</last>
		</authors>
		<authors>
			<first>John R.</first>
		</authors>
		<authors>
			<last>Iversen</last>
		</authors>
		<volume>11</volume>
		<number>9</number>
		<pages>369--72</pages>
		<abstract>Growing evidence points to a link between musical abilities and certain
	phonetic and prosodic skills in language. However, the mechanisms
	that underlie these relations are not well understood. A recent study
	by Wong et al. suggests that musical training sharpens the subcortical
	encoding of linguistic pitch patterns. We consider the implications
	of their methods and findings for establishing a link between musical
	training and phonetic abilities more generally.</abstract>
		<issn>1364-6613</issn>
		<doi>10.1016/j.tics.2007.08.003</doi>
		<title>The linguistic benefits of musical abilities</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ae787b387dbf2b60bbd72ab9e8ddc698/yevb0</id>
		<tags>physiology,Behavior,Behavior:</tags>
		<tags>Auditory</tags>
		<tags>Perception,Auditory</tags>
		<tags>Perception:</tags>
		<tags>physiology,Behavioral,Emotions,Genetics,Humans,Instinct,Language</tags>
		<tags>physiology,Psychophysiology,music,neuro,perception</tags>
		<tags>Development,Music,Music:</tags>
		<tags>Perception,Pitch</tags>
		<tags>psychology,Pitch</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>2</count>
		<journal>Cognition</journal>
		<year>2006</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/16487953</url>
		<author>Isabelle Peretz</author>
		<authors>
			<first>Isabelle</first>
		</authors>
		<authors>
			<last>Peretz</last>
		</authors>
		<volume>100</volume>
		<number>1</number>
		<pages>1--32</pages>
		<abstract>Music, as language, is a universal human trait. Throughout human history
	and across all cultures, people have produced and enjoyed music.
	Despite its ubiquity, the musical capacity is rarely studied as a
	biological function. Music is typically viewed as a cultural invention.
	In this paper, the evidence bearing on the biological perspective
	of the musical capacity is reviewed. Related issues, such as domain-specificity,
	innateness, and brain localization, are addressed in an attempt to
	offer a unified conceptual basis for the study of music processing.
	This scheme should facilitate the study of the biological foundations
	of music by bringing together the fields of genetics, developmental
	and comparative research, neurosciences, and musicology.</abstract>
		<issn>0010-0277</issn>
		<doi>10.1016/j.cognition.2005.11.004</doi>
		<title>The nature of music from a biological perspective</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f71bb34d6e6b76c61ce539d5871ff599/yevb0</id>
		<tags>audition,evolution,hearing,language,linguistics,music\_cognition,perception,processing,speech</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Proceedings of the National Academy of Sciences of the United States
	of America</journal>
		<year>2007</year>
		<url></url>
		<author>Deborah Ross</author>
		<author>Jonathan Choi</author>
		<author>Dale Purves</author>
		<authors>
			<first>Deborah</first>
		</authors>
		<authors>
			<last>Ross</last>
		</authors>
		<authors>
			<first>Jonathan</first>
		</authors>
		<authors>
			<last>Choi</last>
		</authors>
		<authors>
			<first>Dale</first>
		</authors>
		<authors>
			<last>Purves</last>
		</authors>
		<volume>104</volume>
		<number>23</number>
		<pages>9852--9857</pages>
		<abstract>Throughout history and across cultures, humans have created music
	using pitch intervals that divide octaves into the 12 tones of the
	chromatic scale. Why these specific intervals in music are preferred,
	however, is not known. In the present study, we analyzed a database
	of individually spoken English vowel phones to examine the hypothesis
	that musical intervals arise from the relationships of the formants
	in speech spectra that determine the perceptions of distinct vowels.
	Expressed as ratios, the frequency relationships of the first two
	formants in vowel phones represent all 12 intervals of the chromatic
	scale. Were the formants to fall outside the ranges found in the
	human voice, their relationships would generate either a less complete
	or a more dilute representation of these specific intervals. These
	results imply that human preference for the intervals of the chromatic
	scale arises from experience with the way speech formants modulate
	laryngeal harmonics to create different phonemes.</abstract>
		<doi>10.1073/pnas.0703140104</doi>
		<title>Musical Intervals in Speech.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22e93b57ae1c42bc26b3fad2b3c775001/yevb0</id>
		<tags>imported</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>Musicophilia: Tales of music and the brain</booktitle>
		<publisher>Vintage Books</publisher>
		<address>New York</address>
		<year>2008</year>
		<url></url>
		<author>Oliver Sacks</author>
		<authors>
			<first>Oliver</first>
		</authors>
		<authors>
			<last>Sacks</last>
		</authors>
		<title>Musicophilia: Tales of music and the brain</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2504400930b0037da0638a3493eca6448/yevb0</id>
		<tags>Performance:</tags>
		<tags>Laterality:</tags>
		<tags>Adult,Brain</tags>
		<tags>physiology,Speech</tags>
		<tags>Perception,Pitch</tags>
		<tags>physiology,Temporal</tags>
		<tags>Perception,Speech</tags>
		<tags>Lobe,Temporal</tags>
		<tags>psychology,Pitch</tags>
		<tags>physiology,Male,Music,Music:</tags>
		<tags>Lobe:</tags>
		<tags>physiology,Humans,Learning,Learning:</tags>
		<tags>physiology,language,music,musicality,perception,pitch,speech</tags>
		<tags>physiology,Psychomotor</tags>
		<tags>Perception:</tags>
		<tags>Mapping,Electroencephalography,Female,Functional</tags>
		<tags>Time,Reaction</tags>
		<tags>Time:</tags>
		<tags>Performance,Psychomotor</tags>
		<tags>Laterality,Functional</tags>
		<tags>physiology,Reaction</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Psychophysiology</journal>
		<year>2004</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/15102118</url>
		<author>Daniele Schön</author>
		<author>Cyrille Magne</author>
		<author>Mireille Besson</author>
		<authors>
			<first>Daniele</first>
		</authors>
		<authors>
			<last>Schön</last>
		</authors>
		<authors>
			<first>Cyrille</first>
		</authors>
		<authors>
			<last>Magne</last>
		</authors>
		<authors>
			<first>Mireille</first>
		</authors>
		<authors>
			<last>Besson</last>
		</authors>
		<volume>41</volume>
		<number>3</number>
		<pages>341--9</pages>
		<abstract>The main aim of the present experiment was to determine whether extensive
	musical training facilitates pitch contour processing not only in
	music but also in language. We used a parametric manipulation of
	final notes' or words' fundamental frequency (F0), and we recorded
	behavioral and electrophysiological data to examine the precise time
	course of pitch processing. We compared professional musicians and
	nonmusicians. Results revealed that within both domains, musicians
	detected weak F0 manipulations better than nonmusicians. Moreover,
	F0 manipulations within both music and language elicited similar
	variations in brain electrical potentials, with overall shorter onset
	latency for musicians than for nonmusicians. Finally, the scalp distribution
	of an early negativity in the linguistic task varied with musical
	expertise, being largest over temporal sites bilaterally for musicians
	and largest centrally and over left temporal sites for nonmusicians.
	These results are taken as evidence that extensive musical training
	influences the perception of pitch contour in spoken language.</abstract>
		<issn>0048-5772</issn>
		<doi>10.1111/1469-8986.00172.x</doi>
		<title>The music of speech: music training facilitates pitch processing
	in both music and language</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26c140a7c4ed5c0cf02566e24d49c27af/mandrean</id>
		<tags>timbre</tags>
		<description></description>
		<date>2015-12-19 09:34:14</date>
		<count>1</count>
		<journal>Contemporary Music Review</journal>
		<year>1994</year>
		<url>/brokenurl#         http://dx.doi.org/10.1080/07494469400640261    </url>
		<author>Pascal Decroupet</author>
		<authors>
			<first>Pascal</first>
		</authors>
		<authors>
			<last>Decroupet</last>
		</authors>
		<volume>10</volume>
		<number>2</number>
		<pages>13-23</pages>
		<abstract>Since their early works, serial composers have attempted to vary their compositions and to strengthen the articulation of form through selections within the range of variability of different dimensions. In electronic music, this strategy was applied to the combination of timbres. Boulez's Second Study of musique concrète, Stockhausen's Gesang der Jünglinge and Koenig's Klangfiguren II provide three different examples of how this general idea could be realised. In addition spatialization became an important tool in clarifying the structuring of serial music. The analyses presented in this paper are based on the composers' sketches and supported by graphic notations of their electronic music.</abstract>
		<eprint>http://dx.doi.org/10.1080/07494469400640261</eprint>
		<doi>10.1080/07494469400640261</doi>
		<title>Timbre diversification in serial tape music and its consequence on form</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fbd096ca51c8e175b291b75b3beeeb25/angrrr</id>
		<tags>social-information-access</tags>
		<tags>implicit-feedback</tags>
		<tags>social-recommendation</tags>
		<description>Context-aware music recommendation based on latenttopic sequential patterns</description>
		<date>2017-02-01 16:45:31</date>
		<count>6</count>
		<booktitle>Proceedings of the Sixth ACM Conference on Recommender Systems</booktitle>
		<series>RecSys '12</series>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2012</year>
		<url>http://doi.acm.org/10.1145/2365952.2365979</url>
		<author>Negar Hariri</author>
		<author>Bamshad Mobasher</author>
		<author>Robin Burke</author>
		<authors>
			<first>Negar</first>
		</authors>
		<authors>
			<last>Hariri</last>
		</authors>
		<authors>
			<first>Bamshad</first>
		</authors>
		<authors>
			<last>Mobasher</last>
		</authors>
		<authors>
			<first>Robin</first>
		</authors>
		<authors>
			<last>Burke</last>
		</authors>
		<pages>131--138</pages>
		<abstract>Contextual factors can greatly influence the users' preferences in listening to music. Although it is hard to capture these factors directly, it is possible to see their effects on the sequence of songs liked by the user in his/her current interaction with the system. In this paper, we present a context-aware music recommender system which infers contextual information based on the most recent sequence of songs liked by the user. Our approach mines the top frequent tags for songs from social tagging Web sites and uses topic modeling to determine a set of latent topics for each song, representing different contexts. Using a database of human-compiled playlists, each playlist is mapped into a sequence of topics and frequent sequential patterns are discovered among these topics. These patterns represent frequent sequences of transitions between the latent topics representing contexts. Given a sequence of songs in a user's current interaction, the discovered patterns are used to predict the next topic in the playlist. The predicted topics are then used to post-filter the initial ranking produced by a traditional recommendation algorithm. Our experimental evaluation suggests that our system can help produce better recommendations in comparison to a conventional recommender system based on collaborative or content-based filtering. Furthermore, the topic modeling approach proposed here is also useful in providing better insight into the underlying reasons for song selection and in applications such as playlist construction and context prediction.</abstract>
		<isbn>978-1-4503-1270-7</isbn>
		<doi>10.1145/2365952.2365979</doi>
		<title>Context-aware Music Recommendation Based on Latenttopic Sequential Patterns</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/299f3f79516fe2605be6405244154c6aa/sapo</id>
		<tags>automatic_transcription</tags>
		<description>Data representations for audio-to-score monophonic music transcription - ScienceDirect</description>
		<date>2020-08-19 13:33:51</date>
		<count>2</count>
		<journal>Expert Systems with Applications</journal>
		<year>2020</year>
		<url>http://www.sciencedirect.com/science/article/pii/S0957417420305935</url>
		<author>Miguel A. Román</author>
		<author>Antonio Pertusa</author>
		<author>Jorge Calvo-Zaragoza</author>
		<authors>
			<first>Miguel A.</first>
		</authors>
		<authors>
			<last>Román</last>
		</authors>
		<authors>
			<first>Antonio</first>
		</authors>
		<authors>
			<last>Pertusa</last>
		</authors>
		<authors>
			<first>Jorge</first>
		</authors>
		<authors>
			<last>Calvo-Zaragoza</last>
		</authors>
		<volume>162</volume>
		<pages>113769</pages>
		<abstract>This work presents an end-to-end method based on deep neural networks for audio-to-score music transcription of monophonic excerpts. Unlike existing music transcription methods, which normally perform pitch estimation, the proposed approach is formulated as an end-to-end task that outputs a notation-level music score. Using an audio file as input, modeled as a sequence of frames, a deep neural network is trained to provide a sequence of music symbols encoding a score, including key and time signatures, barlines, notes (with their pitch spelling and duration) and rests. Our framework is based on a Convolutional Recurrent Neural Network (CRNN) with Connectionist Temporal Classification (CTC) loss function trained in an end-to-end fashion, without requiring to align the input frames with the output symbols. A total of 246,870 incipits from the Répertoire International des Sources Musicales online catalog were synthesized using different timbres and tempos to build the training data. Alternative input representations (raw audio, Short-Time Fourier Transform (STFT), log-spaced STFT and Constant-Q transform) were evaluated for this task, as well as different output representations (Plaine & Easie Code, Kern, and a purpose-designed output). Results show that it is feasible to directly infer score representations from audio files and most errors come from music notation ambiguities and metering (time signatures and barlines).</abstract>
		<issn>0957-4174</issn>
		<doi>https://doi.org/10.1016/j.eswa.2020.113769</doi>
		<title>Data representations for audio-to-score monophonic music transcription</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29ec10973485497e6052bcbc479ad6c38/bauerc</id>
		<tags>myown</tags>
		<tags>music</tags>
		<tags>chi</tags>
		<tags>recsys</tags>
		<tags>musicplatform</tags>
		<description></description>
		<date>2019-06-19 17:01:18</date>
		<count>2</count>
		<booktitle>ACM CHI 2019 Workshop on Computational Modeling in Human-Computer Interaction</booktitle>
		<series>CompHCI 2019</series>
		<year>2019</year>
		<url></url>
		<author>Christine Bauer</author>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<abstract>The task of a music recommender system is to predict what music item a particular user would like to listen to next. This position paper discusses the main challenges of the music preference prediction task: the lack of information on the many contextual factors influencing a user’s music preferences in existing open datasets, the lack of clarity of what the right choice of music is and whether a right choice exists at all; the multitude of criteria (beyond accuracy) that have to be met for a “good” music item recommendation; and the need for explanations on relationships to identify (and potentially counteract) unwanted biases in recommendation approaches.
The paper substantiates the position that the confluence of theoretical modeling (which seeks to explain behaviors) and algorithmic modeling (which seeks to predict behaviors) seems to be an effective avenue to take in computational modeling for music recommender systems.</abstract>
		<language>English</language>
		<title>The potential of the confluence of theoretical and algorithmic modeling in music recommendation</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23f795c8d1824d0a1b6c08c3be3a6d455/brusilovsky</id>
		<tags>individual-differences</tags>
		<tags>user-control</tags>
		<tags>visual-recommender</tags>
		<tags>information-visualization</tags>
		<tags>recommender</tags>
		<tags>explanation</tags>
		<description>Effects of personal characteristics on music recommender systems with different levels of controllability</description>
		<date>2019-02-19 15:42:50</date>
		<count>2</count>
		<booktitle>Proceedings of the 12th ACM Conference on Recommender Systems</booktitle>
		<series>RecSys '18</series>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2018</year>
		<url>http://doi.acm.org/10.1145/3240323.3240358</url>
		<author>Yucheng Jin</author>
		<author>Nava Tintarev</author>
		<author>Katrien Verbert</author>
		<authors>
			<first>Yucheng</first>
		</authors>
		<authors>
			<last>Jin</last>
		</authors>
		<authors>
			<first>Nava</first>
		</authors>
		<authors>
			<last>Tintarev</last>
		</authors>
		<authors>
			<first>Katrien</first>
		</authors>
		<authors>
			<last>Verbert</last>
		</authors>
		<pages>13--21</pages>
		<abstract>Previous research has found that enabling users to control the recommendation process increases user satisfaction. However, providing additional controls also increases cognitive load, and different users have different needs for control. Therefore, in this study, we investigate the effect of two personal characteristics: musical sophistication and visual memory capacity. We designed a visual user interface, on top of a commercial music recommender, with different controls: interactions with recommendations (i.e., the output of a recommender system), the user profile (i.e., the top listened songs), and algorithm parameters (i.e., weights in an algorithm). We created eight experimental settings with combinations of these three user controls and conducted a between-subjects study (N=240), to explore the effect on cognitive load and recommendation acceptance for different personal characteristics. We found that controlling recommendations is the most favorable single control element. In addition, controlling user profile and algorithm parameters was the most beneficial setting with multiple controls. Moreover, the participants with high musical sophistication perceived recommendations to be of higher quality, which in turn lead to higher recommendation acceptance. However, we found no effect of visual working memory on either cognitive load or recommendation acceptance. This work contributes an understanding of how to design control that hits the sweet spot between the perceived quality of recommendations and acceptable cognitive load.</abstract>
		<isbn>978-1-4503-5901-6</isbn>
		<doi>10.1145/3240323.3240358</doi>
		<title>Effects of Personal Characteristics on Music Recommender Systems with Different Levels of Controllability</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e2379dfd776470742472ef4df020ff5c/sapo</id>
		<tags>datasets</tags>
		<description>mtg.upf.edu/biblio/export/bibtex/2374</description>
		<date>2019-07-01 12:36:59</date>
		<count>1</count>
		<booktitle>12th International Society for Music Information Retrieval Conference (ISMIR 2011)</booktitle>
		<address>Miami, USA</address>
		<year>2011</year>
		<url>http://mtg.upf.edu/system/files/publications/RepoVizz_ISMIR_2011_v5.pdf</url>
		<author>O. Mayor</author>
		<author>J. Llop</author>
		<author>E. Maestre</author>
		<authors>
			<first>O.</first>
		</authors>
		<authors>
			<last>Mayor</last>
		</authors>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Llop</last>
		</authors>
		<authors>
			<first>E.</first>
		</authors>
		<authors>
			<last>Maestre</last>
		</authors>
		<abstract>RepoVizz is a data repository and visualization tool for structured storage and user-friendly browsing of music performance multi-modal recordings. The primary purpose of RepoVizz is to offer means for the scientific community to gain on-line access to a music performance multi-modal database shared among researchers. RepoVizz is designed to hold synchronized streams of heterogeneous data (audio, video, motion capture, physiological measures, extracted descriptors, etc.), annotations, and musical scores. Data streams are stored as single-channel PCM files. The motion-sequence format MJPEG is chosen for video. Annotations are stored as text files. MusicXML is used for musical scores. Data is structured by customizable XML skeleton files enabling meaningful retrieval. Skeleton files are first created during data gathering, and provide means to (re-)organize acquired data in any desired hierarchical structure, as they only hold pointers to stored data files. Once a dataset is created and uploaded to the server, each skeleton file defines a view. Multi-track data visualization is done remotely via a powerful HTML5-based environment that enables web-driven editing (add annotations, extract descriptors) and downloading of datasets. A first instance of RepoVizz has been created within the EU FET-Open Project SIEMPRE, devoted to studying social interaction in ensemble performance.</abstract>
		<title>RepoVizz: A multimodal on-line database and browsing tool for music performance research</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/248b37ce75d71f2e7428d9a7c63df4000/sapo</id>
		<tags>music_notation</tags>
		<tags>mei</tags>
		<tags>dtd</tags>
		<tags>document_type_definition</tags>
		<tags>design</tags>
		<tags>tei</tags>
		<tags>extensible_markup_language</tags>
		<tags>music_representation</tags>
		<tags>xml</tags>
		<tags>text_encoding_initiative</tags>
		<tags>music_encoding_initiative</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<year>2018</year>
		<url>http://music-encoding.org/home</url>
		<author>Perry Roland</author>
		<authors>
			<first>Perry</first>
		</authors>
		<authors>
			<last>Roland</last>
		</authors>
		<title>Music Encoding Initiative</title>
		<pubtype>other</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29408ab48865176ab2397eb0fd3817ed4/sapo</id>
		<tags>M_Music</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<year>2004</year>
		<url>http://recherche.ircam.fr/equipes/repmus/SMC04/</url>
		<author>Alan Marsden</author>
		<authors>
			<first>Alan</first>
		</authors>
		<authors>
			<last>Marsden</last>
		</authors>
		<abstract>A system of representing melodies as a network of elaborations has
	been developed, and used as the basis for software which generates
	melodies in response to the movements of a dancer. This paper examines
	the issues of extending this representation system to polyphonic
	music, and of deriving a structural representation of this kind from
	a musical score. The theories of Heinrich Schenker and of Species
	Counterpoint are proposed as potentially fruitful bases.</abstract>
		<journaltitle>Sound and Music Computing Paris</journaltitle>
		<title>Extending a Network-of-Elaborations Representation to Polyphonic Music: Schenker and Species Counterpoint.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24dbbc9f13a22b20b4cc16a38920cc33c/sapo</id>
		<tags>Music_information_retrieval</tags>
		<tags>Melody_extraction</tags>
		<tags>Pitch_histogram</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>2</count>
		<booktitle>Proceedings - Seventh IEEE International Symposium on Multimedia, ISM 2005</booktitle>
		<series>Multimedia, Seventh IEEE International Symposium on</series>
		<publisher>Ieee</publisher>
		<year>2005</year>
		<url></url>
		<author>Giyasettin Ozcan</author>
		<author>Cihan Isikhan</author>
		<author>Adil Alpkocak</author>
		<authors>
			<first>Giyasettin</first>
		</authors>
		<authors>
			<last>Ozcan</last>
		</authors>
		<authors>
			<first>Cihan</first>
		</authors>
		<authors>
			<last>Isikhan</last>
		</authors>
		<authors>
			<first>Adil</first>
		</authors>
		<authors>
			<last>Alpkocak</last>
		</authors>
		<volume>2005</volume>
		<pages>414--421</pages>
		<abstract>In this study, we propose a new approach to extract monophonic melody
	from MIDI files and provide a comparison of existing methods. Our
	approach is based on the elimination of MIDI channels those do not
	contain melodic information. First, MIDI channels are clustered depending
	on pitch histogram. Afterwards, a channel is selected from each cluster
	as representative and remaining channels and their notes are removed.
	Finally, skyline algorithm is applied on the modified MIDI set to
	ensure accuracy of monophonic melody. We evaluated our approach within
	a test bed of MIDI files, composed of variable music styles. Both
	our approach and the results from experiments are presented in detail.</abstract>
		<doi>10.1109/ISM.2005.77</doi>
		<title>Melody extraction on MIDI music files</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/214ee7aa58f8ff6b00cf9bd60a1a73408/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>Extended abstracts for the Late-Breaking Demo Session of the 16th International Society for Music Information Retrieval Conference</booktitle>
		<series>Demo at the 16th International Society for Music Information Retrieval Conference (ISMIR 2015), Malaga, Spain</series>
		<year>2015</year>
		<url></url>
		<author>Nicholas Harley</author>
		<author>Geraint A. Wiggins</author>
		<authors>
			<first>Nicholas</first>
		</authors>
		<authors>
			<last>Harley</last>
		</authors>
		<authors>
			<first>Geraint A.</first>
		</authors>
		<authors>
			<last>Wiggins</last>
		</authors>
		<title>An Ontology for Abstract, Hierarchical Music Representation</title>
		<pubtype>proceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f09a5d4b7995f15cceaea785b8a2590a/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<publisher>Taylor & Francis</publisher>
		<year>2003</year>
		<url></url>
		<author>George Tzanetakis</author>
		<author>Andrey Ermolinskyi</author>
		<author>Perry Cook</author>
		<authors>
			<first>George</first>
		</authors>
		<authors>
			<last>Tzanetakis</last>
		</authors>
		<authors>
			<first>Andrey</first>
		</authors>
		<authors>
			<last>Ermolinskyi</last>
		</authors>
		<authors>
			<first>Perry</first>
		</authors>
		<authors>
			<last>Cook</last>
		</authors>
		<volume>32</volume>
		<number>2</number>
		<pages>143--152</pages>
		<abstract>In order to represent musical content, pitch and timing information
	is utilized in the majority of existing work in Symbolic Music Information
	Retrieval (MIR). Symbolic representations such as MIDI allow the
	easy calculation of such information and its manipulation. In contrast,
	most of the existing work in Audio MIR uses timbral and beat information,
	which can be calculated using automatic computer audition techniques.
	In this paper, Pitch Histograms are defined and proposed as a way
	to represent the pitch content of music signals both in symbolic
	and audio form. This representation is evaluated in the context of
	automatic musical genre classification. A multiple-pitch detection
	algorithm for polyphonic signals is used to calculate Pitch Histograms
	for audio signals. In order to evaluate the extent and significance
	of errors resulting from the automatic multiple-pitch detection,
	automatic musical genre classification results from symbolic and
	audio data are compared. The comparison indicates that Pitch Histograms
	provide valuable information for musical genre classification. The
	results obtained for both symbolic and audio cases indicate that
	although pitch errors degrade classification performance for the
	audio case, Pitch Histograms can be effectively used for classification
	in both cases.</abstract>
		<journaltitle>Journal of New Music Research</journaltitle>
		<doi>10.1076/jnmr.32.2.143.16743</doi>
		<title>Pitch Histograms in Audio and Symbolic Music Information Retrieval</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fbacbc9478d8bf8d6887f6ab0622ff58/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>2</count>
		<booktitle>Proceedings of the International Computer Music Conference</booktitle>
		<series>ICMC</series>
		<year>2002</year>
		<url>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.198.10{\&}rep=rep1{\&}type=pdf{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC146749/</url>
		<author>Belinda Thom</author>
		<author>Christian Spevak</author>
		<author>Karin Höthker</author>
		<authors>
			<first>Belinda</first>
		</authors>
		<authors>
			<last>Thom</last>
		</authors>
		<authors>
			<first>Christian</first>
		</authors>
		<authors>
			<last>Spevak</last>
		</authors>
		<authors>
			<first>Karin</first>
		</authors>
		<authors>
			<last>Höthker</last>
		</authors>
		<volume>2002</volume>
		<number>12</number>
		<pages>65--72</pages>
		<abstract>We review several segmentation algorithms, qualitatively highlighting
	their strengths and weaknesses. We also provide a detailed quantitative
	evaluation of two existing approaches, Temperleys Grouper and Cambouropoulos
	Local Boundary Detection Model. In order to facilitate the comparison
	of an algorithms performance with human behavior, we compiled a corpus
	of melodic excerpts in different musical styles and collected individual
	segmentations from 19 musicians. We then empirically assessed the
	algorithms performance by observing how well they can predict both
	the musicians segmentations and data taken from the Essen folk song
	collection.</abstract>
		<title>Melodic segmentation: Evaluating the performance of algorithms and musical experts</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/247203c589ebee577f648478fb5bd1b72/bauerc</id>
		<tags>imported</tags>
		<tags>myown</tags>
		<tags>music</tags>
		<tags>events</tags>
		<description></description>
		<date>2019-04-27 18:22:14</date>
		<count>2</count>
		<booktitle>WWW '18 Companion: The 2018 Web Conference Companion (WWW 2018)</booktitle>
		<series>WWW '18 Companion</series>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2018</year>
		<url>http://dblp.uni-trier.de/db/conf/www/www2018c.html#SchedlW018</url>
		<author>Markus Schedl</author>
		<author>Eelco Wiechert</author>
		<author>Christine Bauer</author>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<authors>
			<first>Eelco</first>
		</authors>
		<authors>
			<last>Wiechert</last>
		</authors>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<editor>Pierre-Antoine Champin</editor>
		<editor>Fabien L. Gandon</editor>
		<editor>Mounia Lalmas</editor>
		<editor>Panagiotis G. Ipeirotis</editor>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<pages>75-76</pages>
		<abstract>We approach the research question whether real-world events, such as sport events or product launches, influence music consumption behavior. To this end, we consider events of different categories from Google Trends and model listening events as time series using Last.fm data. Performing an auto-regressive integrated moving average analysis to decompose the signal and subsequently an intervention time series analysis, we find significant signal discontinuities, in particular for the Google news category. We found that news and events are likely to increase the number of songs listened to per person per day by about 2%, while tech events commonly cause 1% less music being consumed.</abstract>
		<isbn>978-1-4503-5640-4</isbn>
		<language>English</language>
		<doi>10.1145/3184558.3186936</doi>
		<title>The effects of real-world events on music listening behavior: an intervention time series analysis</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d1f30ed06c9ebd9943dfefe9142df30e/bauerc</id>
		<tags>imported</tags>
		<tags>myown</tags>
		<tags>music</tags>
		<tags>generated</tags>
		<tags>jogging</tags>
		<description></description>
		<date>2019-04-27 18:22:14</date>
		<count>2</count>
		<booktitle>CHI 2013 Extended Abstracts on Human Factors in Computing Systems</booktitle>
		<series>CHI EA 2013</series>
		<publisher>ACM</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/chi/chi2013a.html#BauerW13</url>
		<author>Christine Bauer</author>
		<author>Florian Waldner</author>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<authors>
			<first>Florian</first>
		</authors>
		<authors>
			<last>Waldner</last>
		</authors>
		<editor>Wendy E. Mackay</editor>
		<editor>Stephen A. Brewster</editor>
		<editor>Susanne Bødker</editor>
		<editors>
			<first>Florian</first>
		</editors>
		<editors>
			<last>Waldner</last>
		</editors>
		<editors>
			<first>Florian</first>
		</editors>
		<editors>
			<last>Waldner</last>
		</editors>
		<editors>
			<first>Florian</first>
		</editors>
		<editors>
			<last>Waldner</last>
		</editors>
		<pages>739-744</pages>
		<abstract>It is a natural predisposition of humans to respond to the rhythmical qualities of music. Now, we turn the setting around: The music responds to the user’s behavior. So-called ‘reactive music’ is a non-linear format of music that is able to react to the listener and her or his environment in real-time. Giant Steps is an iPhone application that implements such reactive music in correspondence to a jogger’s movements and the sounds in her or his environment. We hope that our approach contributes to a better understanding of ‘machine to user’ adaption, and to mobile sports applications in particular.</abstract>
		<language>English</language>
		<doi>10.1145/2468356.2468488</doi>
		<title>Reactive Music: When User Behavior affects Sounds in Real-Time</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fba37306a05e895677cb5bda0cea2a27/bauerc</id>
		<tags>imported</tags>
		<tags>myown</tags>
		<tags>country</tags>
		<tags>music</tags>
		<tags>culture</tags>
		<tags>recsys</tags>
		<description></description>
		<date>2019-04-27 18:20:03</date>
		<count>2</count>
		<booktitle>15th International Conference on Advances in Mobile Computing & Multimedia (MoMM2017)</booktitle>
		<series>MoMM 2017</series>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2017</year>
		<url>http://dblp.uni-trier.de/db/conf/momm/momm2017.html#Schedl017</url>
		<author>Markus Schedl</author>
		<author>Christine Bauer</author>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<editor>Eric Pardede</editor>
		<editor>Pari Delir Haghighi</editor>
		<editor>Ivan Luiz Salvadori</editor>
		<editor>Matthias Steinbauer</editor>
		<editor>Ismail Khalil</editor>
		<editor>Gabriele Anderst-Kotsis</editor>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<pages>74-81</pages>
		<abstract>The music mainstreaminess of a user reflects how strong a user's listening preferences correspond to those of the larger population. Considering that music mainstream may be defined from different perspectives and on various levels, e.g.,~geographical (charts of a country), genre (``Indie charts"), or distribution channel (radio charts vs.~download charts), we study how the user's music mainstreaminess influences the quality of music recommendations. The paper's contribution is three-fold.
First, we propose 11 novel mainstreaminess measures characterizing music listeners, considering both a global and a country-specific basis. To this end, we model preference profiles (as a vector over artists) for users, countries, and globally, incorporating artist frequency, listener frequency, and a newly proposed TF-IDF-inspired weighting function, which we call artist frequency--inverse listener frequency (AF-ILF). The resulting preference profile for each user $u$ is then related to the respective country-specific and global preference profile using fraction-based approaches, symmetrized Kullback-Leibler divergence, and Kendall's $\tau$ rank correlation, in order to quantify $u$'s mainstreaminess.
Second, we demonstrate country-specific peculiarities of these mainstreaminess definitions.
Third, we show that incorporating the proposed global and country-specific mainstreaminess measures into the music recommendation process can notably improve accuracy of rating prediction.</abstract>
		<isbn>978-1-4503-5300-7</isbn>
		<language>English</language>
		<doi>10.1145/3151848.3151849</doi>
		<title>Introducing global and regional mainstreaminess for improving personalized music recommendation</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29e13f741f1c91c1be5184a95e8d05ae8/bauerc</id>
		<tags>imported</tags>
		<tags>myown</tags>
		<tags>music</tags>
		<tags>recsys</tags>
		<tags>kids</tags>
		<description></description>
		<date>2019-04-27 18:20:03</date>
		<count>1</count>
		<booktitle>1st International Workshop on Children and Recommender Systems, in conjunction with 11th ACM Conference on Recommender Systems (RecSys 2017)</booktitle>
		<series>KidRec 2017</series>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2017</year>
		<url>https://arxiv.org/abs/1912.11564</url>
		<author>Markus Schedl</author>
		<author>Christine Bauer</author>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<abstract>In this paper, we analyze a large dataset of user-generated music listening events from Last.fm, focusing on users aged 6 to 18 years. Our contribution is two-fold. First, we study the music genre preferences of this young user group and analyze these preferences for homogeneity within more fine-grained age groups and with respect to gender and countries. Second, we investigate the performance of a collaborative filtering recommender when tailoring music recommendations to different age groups. We find that doing so improves performance for all user groups up to 18 years, but decreases performance for adult users aged 19 years and older.</abstract>
		<language>English</language>
		<title>Online Music Listening Culture of Kids and Adolescents: Listening Analysis and Music Recommendation Tailored to the Young</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27dbe51e80803b1237b3430131687f11c/sapo</id>
		<tags>performance_analysis</tags>
		<tags>read_list</tags>
		<description>Online/offline score informed music signal decomposition: application to minus one | EURASIP Journal on Audio, Speech, and Music Processing | Full Text</description>
		<date>2020-09-22 11:41:58</date>
		<count>1</count>
		<journal>EURASIP Journal on Audio, Speech, and Music Processing</journal>
		<publisher>Springer Science and Business Media LLC</publisher>
		<year>2019</year>
		<url>https://doi.org/10.1186%2Fs13636-019-0168-6</url>
		<author>Antonio Jesús Munoz-Montoro</author>
		<author>Julio José Carabias-Orti</author>
		<author>Pedro Vera-Candeas</author>
		<author>Francisco Jesús Canadas-Quesada</author>
		<author>Nicolás Ruiz-Reyes</author>
		<authors>
			<first>Antonio Jesús</first>
		</authors>
		<authors>
			<last>Munoz-Montoro</last>
		</authors>
		<authors>
			<first>Julio José</first>
		</authors>
		<authors>
			<last>Carabias-Orti</last>
		</authors>
		<authors>
			<first>Pedro</first>
		</authors>
		<authors>
			<last>Vera-Candeas</last>
		</authors>
		<authors>
			<first>Francisco Jesús</first>
		</authors>
		<authors>
			<last>Canadas-Quesada</last>
		</authors>
		<authors>
			<first>Nicolás</first>
		</authors>
		<authors>
			<last>Ruiz-Reyes</last>
		</authors>
		<volume>2019</volume>
		<number>1</number>
		<doi>10.1186/s13636-019-0168-6</doi>
		<title>Online/offline score informed music signal decomposition: application to minus one</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24ae12111d4049dea80d3e0385655392f/rikbose</id>
		<tags>music</tags>
		<description>Modeling the Music Genre Perception across Language-Bound Cultures - ACL Anthology</description>
		<date>2020-11-09 19:22:18</date>
		<count>3</count>
		<booktitle>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</booktitle>
		<publisher>Association for Computational Linguistics</publisher>
		<address>Online</address>
		<year>2020</year>
		<url>https://www.aclweb.org/anthology/2020.emnlp-main.386</url>
		<author>Elena V. Epure</author>
		<author>Guillaume Salha</author>
		<author>Manuel Moussallam</author>
		<author>Romain Hennequin</author>
		<authors>
			<first>Elena V.</first>
		</authors>
		<authors>
			<last>Epure</last>
		</authors>
		<authors>
			<first>Guillaume</first>
		</authors>
		<authors>
			<last>Salha</last>
		</authors>
		<authors>
			<first>Manuel</first>
		</authors>
		<authors>
			<last>Moussallam</last>
		</authors>
		<authors>
			<first>Romain</first>
		</authors>
		<authors>
			<last>Hennequin</last>
		</authors>
		<pages>4765--4779</pages>
		<title>Modeling the Music Genre Perception across Language-Bound Cultures</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23652121f469e9409090013f14156a638/sapo</id>
		<tags>score-to-audio_alignment</tags>
		<description>AudioLabs - Using Weakly Aligned Score—Audio Pairs to Train Deep Chroma Models for Cross-Modal Music Retrieval</description>
		<date>2020-11-18 16:03:30</date>
		<count>1</count>
		<booktitle>Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)</booktitle>
		<address>Montréal, Canada</address>
		<year>2020</year>
		<url></url>
		<author>Frank Zalkow</author>
		<author>Meinard Müller</author>
		<authors>
			<first>Frank</first>
		</authors>
		<authors>
			<last>Zalkow</last>
		</authors>
		<authors>
			<first>Meinard</first>
		</authors>
		<authors>
			<last>Müller</last>
		</authors>
		<pages>184--191</pages>
		<url-details>https://www.audiolabs-erlangen.de/resources/MIR/2020-ISMIR-chroma-ctc</url-details>
		<url-pdf>2020_ZalkowM_CTC_ISMIR.pdf</url-pdf>
		<title>Using Weakly Aligned Score--Audio Pairs to Train Deep Chroma Models for Cross-Modal Music Retrieval</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2daa65f6b5eabb7085ec9b1b1701bfc98/hadi_saadat</id>
		<tags>#music</tags>
		<tags>#uncovr</tags>
		<description>Contrastive Learning of Musical Representations</description>
		<date>2021-06-07 17:02:19</date>
		<count>1</count>
		<year>2021</year>
		<url>http://arxiv.org/abs/2103.09410</url>
		<author>Janne Spijkervet</author>
		<author>John Ashley Burgoyne</author>
		<authors>
			<first>Janne</first>
		</authors>
		<authors>
			<last>Spijkervet</last>
		</authors>
		<authors>
			<first>John Ashley</first>
		</authors>
		<authors>
			<last>Burgoyne</last>
		</authors>
		<abstract>While supervised learning has enabled great advances in many areas of music,
labeled music datasets remain especially hard, expensive and time-consuming to
create. In this work, we introduce SimCLR to the music domain and contribute a
large chain of audio data augmentations, to form a simple framework for
self-supervised learning of raw waveforms of music: CLMR. This approach
requires no manual labeling and no preprocessing of music to learn useful
representations. We evaluate CLMR in the downstream task of music
classification on the MagnaTagATune and Million Song datasets. A linear
classifier fine-tuned on representations from a pre-trained CLMR model achieves
an average precision of 35.4% on the MagnaTagATune dataset, superseding fully
supervised models that currently achieve a score of 34.9%. Moreover, we show
that CLMR's representations are transferable using out-of-domain datasets,
indicating that they capture important musical knowledge. Lastly, we show that
self-supervised pre-training allows us to learn efficiently on smaller labeled
datasets: we still achieve a score of 33.1% despite using only 259 labeled
songs during fine-tuning. To foster reproducibility and future research on
self-supervised learning in music, we publicly release the pre-trained models
and the source code of all experiments of this paper on GitHub.</abstract>
		<title>Contrastive Learning of Musical Representations</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d49179491ec1c718beb2ceeab4b51c25/sapo</id>
		<tags>automatic_transcription</tags>
		<description>Automatic music transcription: challenges and future directions | SpringerLink https://link.springer.com/article/10.1007%2Fs10844-013-0258-3</description>
		<date>2021-05-26 23:58:34</date>
		<count>3</count>
		<journal>Journal of Intelligent Information Systems</journal>
		<year>2013</year>
		<url>https://doi.org/10.1007/s10844-013-0258-3</url>
		<author>Emmanouil Benetos</author>
		<author>Simon Dixon</author>
		<author>Dimitrios Giannoulis</author>
		<author>Holger Kirchhoff</author>
		<author>Anssi Klapuri</author>
		<authors>
			<first>Emmanouil</first>
		</authors>
		<authors>
			<last>Benetos</last>
		</authors>
		<authors>
			<first>Simon</first>
		</authors>
		<authors>
			<last>Dixon</last>
		</authors>
		<authors>
			<first>Dimitrios</first>
		</authors>
		<authors>
			<last>Giannoulis</last>
		</authors>
		<authors>
			<first>Holger</first>
		</authors>
		<authors>
			<last>Kirchhoff</last>
		</authors>
		<authors>
			<first>Anssi</first>
		</authors>
		<authors>
			<last>Klapuri</last>
		</authors>
		<volume>41</volume>
		<number>3</number>
		<pages>407--434</pages>
		<abstract>Automatic music transcription is considered by many to be a key enabling technology in music signal processing. However, the performance of transcription systems is still significantly below that of a human expert, and accuracies reported in recent years seem to have reached a limit, although the field is still very active. In this paper we analyse limitations of current methods and identify promising directions for future research. Current transcription methods use general purpose models which are unable to capture the rich diversity found in music signals. One way to overcome the limited performance of transcription systems is to tailor algorithms to specific use-cases. Semi-automatic approaches are another way of achieving a more reliable transcription. Also, the wealth of musical scores and corresponding audio data now available are a rich potential source of training data, via forced alignment of audio to scores, but large scale utilisation of such data has yet to be attempted. Other promising approaches include the integration of information from multiple algorithms and different musical aspects.</abstract>
		<issn>1573-7675</issn>
		<doi>10.1007/s10844-013-0258-3</doi>
		<title>Automatic music transcription: challenges and future directions</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2399634b8132d281744efbb120fe70ed3/sapo</id>
		<tags>recommender_systems</tags>
		<description>Current challenges and visions in music recommender systems research | SpringerLink</description>
		<date>2020-07-31 18:30:14</date>
		<count>2</count>
		<journal>International Journal of Multimedia Information Retrieval</journal>
		<year>2018</year>
		<url>https://doi.org/10.1007/s13735-018-0154-2</url>
		<author>Markus Schedl</author>
		<author>Hamed Zamani</author>
		<author>Ching-Wei Chen</author>
		<author>Yashar Deldjoo</author>
		<author>Mehdi Elahi</author>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<authors>
			<first>Hamed</first>
		</authors>
		<authors>
			<last>Zamani</last>
		</authors>
		<authors>
			<first>Ching-Wei</first>
		</authors>
		<authors>
			<last>Chen</last>
		</authors>
		<authors>
			<first>Yashar</first>
		</authors>
		<authors>
			<last>Deldjoo</last>
		</authors>
		<authors>
			<first>Mehdi</first>
		</authors>
		<authors>
			<last>Elahi</last>
		</authors>
		<volume>7</volume>
		<number>2</number>
		<pages>95--116</pages>
		<abstract>Music recommender systems (MRSs) have experienced a boom in recent years, thanks to the emergence and success of online streaming services, which nowadays make available almost all music in the world at the user's fingertip. While today's MRSs considerably help users to find interesting music in these huge catalogs, MRS research is still facing substantial challenges. In particular when it comes to build, incorporate, and evaluate recommendation strategies that integrate information beyond simple user--item interactions or content-based descriptors, but dig deep into the very essence of listener needs, preferences, and intentions, MRS research becomes a big endeavor and related publications quite sparse. The purpose of this trends and survey article is twofold. We first identify and shed light on what we believe are the most pressing challenges MRS research is facing, from both academic and industry perspectives. We review the state of the art toward solving these challenges and discuss its limitations. Second, we detail possible future directions and visions we contemplate for the further evolution of the field. The article should therefore serve two purposes: giving the interested reader an overview of current challenges in MRS research and providing guidance for young researchers by identifying interesting, yet under-researched, directions in the field.</abstract>
		<issn>2192-662X</issn>
		<doi>10.1007/s13735-018-0154-2</doi>
		<title>Current challenges and visions in music recommender systems research.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21afb8b24d18515d29a79ac8530c13967/sapo</id>
		<tags>perceptual_evaluation</tags>
		<tags>excerpt_duration</tags>
		<description>Effects of Excerpt Tempo and Duration on Musicians' Ratings of High-Level Piano Performances - Joel Wapnick, Charlene Ryan, Louise Campbell, Patricia Deek, Renata Lemire, Alice-Ann Darrow, 2005</description>
		<date>2020-03-24 12:49:23</date>
		<count>1</count>
		<journal>Journal of Research in Music Education</journal>
		<publisher>SAGE Publications</publisher>
		<year>2005</year>
		<url>https://doi.org/10.1177%2F002242940505300206</url>
		<author>Joel Wapnick</author>
		<author>Charlene Ryan</author>
		<author>Louise Campbell</author>
		<author>Patricia Deek</author>
		<author>Renata Lemire</author>
		<author>Alice-Ann Darrow</author>
		<authors>
			<first>Joel</first>
		</authors>
		<authors>
			<last>Wapnick</last>
		</authors>
		<authors>
			<first>Charlene</first>
		</authors>
		<authors>
			<last>Ryan</last>
		</authors>
		<authors>
			<first>Louise</first>
		</authors>
		<authors>
			<last>Campbell</last>
		</authors>
		<authors>
			<first>Patricia</first>
		</authors>
		<authors>
			<last>Deek</last>
		</authors>
		<authors>
			<first>Renata</first>
		</authors>
		<authors>
			<last>Lemire</last>
		</authors>
		<authors>
			<first>Alice-Ann</first>
		</authors>
		<authors>
			<last>Darrow</last>
		</authors>
		<volume>53</volume>
		<number>2</number>
		<pages>162--176</pages>
		<doi>10.1177/002242940505300206</doi>
		<title>Effects of Excerpt Tempo and Duration on MusiciansRatings of High-Level Piano Performances</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2378d42e72863e813bbb24a01c829836d/jpooley</id>
		<tags>music</tags>
		<tags>schutz</tags>
		<tags>sociology</tags>
		<tags>phenomenology</tags>
		<tags>becker</tags>
		<description></description>
		<date>2020-01-26 00:55:05</date>
		<count>1</count>
		<journal>Journal of Classical Sociology</journal>
		<year>2020</year>
		<url>https://doi.org/10.1177/1468795X18807592</url>
		<author>Sandro Segre</author>
		<authors>
			<first>Sandro</first>
		</authors>
		<authors>
			<last>Segre</last>
		</authors>
		<volume>20</volume>
		<number>1</number>
		<pages>64-79</pages>
		<abstract>This article compares the sociological writings of Schutz and Becker on making music together. Schutz laid stress on the We-relationship resulting from face-to-face performance of music, in which every listener participates with the other listeners, the performers, and the composers. Becker emphasized the existence of taken-for-granted conventions on which cooperative relations between performers are based, in conjunction with other members of the music subcommunity with whom they are engaged in a communication flow. The involved actors may participate in several reciprocally connected worlds of shared experiences, which if jointly considered constitute the art world. The theoretical and conceptual differences between Schutz and Becker, which concern their sociological writings on music, are related to analogous diversities between Phenomenological Sociology and Symbolic Interactionism. Finally, Weber, Honigsheim, and Adorno’s contributions to the sociology of music have been here considered for comparative purposes.</abstract>
		<eprint>https://doi.org/10.1177/1468795X18807592</eprint>
		<doi>10.1177/1468795X18807592</doi>
		<title>Schutz and Becker on Making Music Together: A Note on Their Respective Contributions to the Sociology of Music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20c5a7d03981197c1ccc7f8b6075ab2cd/gdmcbain</id>
		<tags>music</tags>
		<tags>the-scientists</tags>
		<tags>01a60-20th-century</tags>
		<tags>Kim-salmon</tags>
		<description>READ: This Is Why The Scientists Thought The Angels' Fans Would Kill Them At A 1983 Show | theMusic.com.au | Australia’s Premier Music News &amp; Reviews Website</description>
		<date>2019-11-15 06:15:51</date>
		<count>1</count>
		<journal>The Music</journal>
		<year>2019</year>
		<url>https://themusic.com.au/features/kim-salmon-nine-parts-water-one-part-sand-kim-salmon-and-the-formula-for-grunge-douglas-galbraith/wz3f19bZ2Ns</url>
		<author>Kim Salmon</author>
		<authors>
			<first>Kim</first>
		</authors>
		<authors>
			<last>Salmon</last>
		</authors>
		<title>READ: This Is Why The Scientists Thought The Angels' Fans Would Kill Them At A 1983 Show</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21a282c7e0d90026217c9c30c5446a6c6/meneteqel</id>
		<tags>live_music</tags>
		<tags>marketization</tags>
		<tags>hbs-2017-516-2</tags>
		<tags>platform_economy</tags>
		<tags>musicians</tags>
		<tags>music_industry</tags>
		<tags>plattformarbeit</tags>
		<tags>digitalization</tags>
		<tags>creative_work</tags>
		<tags>platform_work</tags>
		<description></description>
		<date>2019-08-16 09:49:26</date>
		<count>1</count>
		<series>Forschungsförderung Working Paper</series>
		<address>Düsseldorf</address>
		<year>2019</year>
		<url>https://www.boeckler.de/64509.htm?produkt=HBS-007242</url>
		<author>Dario Azzellini</author>
		<author>Ian Greer</author>
		<author>Charles Umney</author>
		<authors>
			<first>Dario</first>
		</authors>
		<authors>
			<last>Azzellini</last>
		</authors>
		<authors>
			<first>Ian</first>
		</authors>
		<authors>
			<last>Greer</last>
		</authors>
		<authors>
			<first>Charles</first>
		</authors>
		<authors>
			<last>Umney</last>
		</authors>
		<number>154</number>
		<abstract>Online platforms have disrupted parts of the capitalist economy, with allegedly severe consequences in the world of work. This study examines live music in Germany and the UK, where online platforms do not dominate, despite considerable digitalization of market intermediaries. The analysis shows that, as the degree of digitalization increases, matching services tend to work less as a workers representative  which is traditionally the case for live music agents  and more as a force of marketization that disciplines workers by orchestrating price-based competition.</abstract>
		<language>ger</language>
		<title>Limits of the platform economy: Digitalization and marketization in live music</title>
		<pubtype>techreport</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://hal.telecom-paris.fr/hal-03356164, hal-03356164, https://hal.telecom-paris.fr/hal-03356164/document</id>
		<tags>identification</tags>
		<tags>cover</tags>
		<tags>music</tags>
		<tags>uncovr</tags>
		<tags>mir</tags>
		<tags>similarity</tags>
		<tags>audio</tags>
		<tags>duplicate</tags>
		<tags>lyrics</tags>
		<description></description>
		<date>2021-10-15 08:18:13</date>
		<count>1</count>
		<booktitle>Proceedings of the 22nd International Society for Music Information Retrieval Conferenc</booktitle>
		<year>2021</year>
		<url>https://hal.archives-ouvertes.fr/hal-03356164/</url>
		<author>Andrea Vaglio</author>
		<author>Romain Hennequin</author>
		<author>Manuel Moussallam</author>
		<author>Gael Richard</author>
		<authors>
			<first>Andrea</first>
		</authors>
		<authors>
			<last>Vaglio</last>
		</authors>
		<authors>
			<first>Romain</first>
		</authors>
		<authors>
			<last>Hennequin</last>
		</authors>
		<authors>
			<first>Manuel</first>
		</authors>
		<authors>
			<last>Moussallam</last>
		</authors>
		<authors>
			<first>Gael</first>
		</authors>
		<authors>
			<last>Richard</last>
		</authors>
		<abstract>Cover detection has gained sustained interest in the scientific community and has recently made significant progress both in terms of scalability and accuracy. However, most approaches are based on the estimation of harmonic and melodic features and neglect lyrics information although it is an important invariant across covers. In this work, we propose a novel approach leveraging lyrics without requiring access to full texts though the use of lyrics recognition on audio. Our approach relies on the fusion of a singing voice recognition framework and a more classic tonal-based cover detection method. To the best of our knowledge, this is the first time that lyrics estimation from audio has been explicitly used for cover detection. Furthermore, we exploit efficient string matching and an approximated nearest neighbors search algorithm which lead to a scalable system which is able to operate on very large databases. Extensive experiments on the largest publicly available cover detection dataset demonstrate the validity of using lyrics information for this task.</abstract>
		<title>The Words Remain the Same: Cover Detection with Lyrics Transcription</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e757ff635f36e98251430014f62baaca/sapo</id>
		<tags>imported</tags>
		<tags>phdthesis</tags>
		<description></description>
		<date>2021-11-22 11:44:52</date>
		<count>3</count>
		<booktitle>Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010, Utrecht, Netherlands, August 9-13, 2010</booktitle>
		<publisher>International Society for Music Information Retrieval</publisher>
		<year>2010</year>
		<url></url>
		<author>Youngmoo E. Kim</author>
		<author>Erik M. Schmidt</author>
		<author>Raymond Migneco</author>
		<author>Brandon G. Morton</author>
		<author>Patrick Richardson</author>
		<author>Jeffrey J. Scott</author>
		<author>Jacquelin A. Speck</author>
		<author>Douglas Turnbull</author>
		<authors>
			<first>Youngmoo E.</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Erik M.</first>
		</authors>
		<authors>
			<last>Schmidt</last>
		</authors>
		<authors>
			<first>Raymond</first>
		</authors>
		<authors>
			<last>Migneco</last>
		</authors>
		<authors>
			<first>Brandon G.</first>
		</authors>
		<authors>
			<last>Morton</last>
		</authors>
		<authors>
			<first>Patrick</first>
		</authors>
		<authors>
			<last>Richardson</last>
		</authors>
		<authors>
			<first>Jeffrey J.</first>
		</authors>
		<authors>
			<last>Scott</last>
		</authors>
		<authors>
			<first>Jacquelin A.</first>
		</authors>
		<authors>
			<last>Speck</last>
		</authors>
		<authors>
			<first>Douglas</first>
		</authors>
		<authors>
			<last>Turnbull</last>
		</authors>
		<editor>J. Stephen Downie</editor>
		<editor>Remco C. Veltkamp</editor>
		<editors>
			<first>Douglas</first>
		</editors>
		<editors>
			<last>Turnbull</last>
		</editors>
		<editors>
			<first>Douglas</first>
		</editors>
		<editors>
			<last>Turnbull</last>
		</editors>
		<pages>255--266</pages>
		<title>State of the Art Report: Music Emotion Recognition: A State of the Art Review</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2529d45746af9695ce92dd516e5445236/jpooley</id>
		<tags>popular-music</tags>
		<tags>journal</tags>
		<tags>music</tags>
		<description></description>
		<date>2021-12-26 18:29:09</date>
		<count>1</count>
		<journal>Popular Music and Society</journal>
		<publisher>Routledge</publisher>
		<year>2021</year>
		<url>https://doi.org/10.1080/03007766.2021.2000101</url>
		<author>Gary Burns</author>
		<authors>
			<first>Gary</first>
		</authors>
		<authors>
			<last>Burns</last>
		</authors>
		<volume>44</volume>
		<number>5</number>
		<pages>491-503</pages>
		<eprint>https://doi.org/10.1080/03007766.2021.2000101</eprint>
		<doi>10.1080/03007766.2021.2000101</doi>
		<title>Fifty Years of Popular Music and Society: Workin’ on a Groovy Thing</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://hal.archives-ouvertes.fr/hal-03208235, hal-03208235</id>
		<tags>myown</tags>
		<tags>journal</tags>
		<tags>phdthesis</tags>
		<tags>mia</tags>
		<description>A Perceptual Measure for Evaluating the Resynthesis of Automatic Music Transcriptions - Archive ouverte HAL https://hal.archives-ouvertes.fr/hal-03208235/</description>
		<date>2021-12-01 20:36:18</date>
		<count>1</count>
		<journal>Multimedia Tools and Applications</journal>
		<year>2022</year>
		<url>https://hal.archives-ouvertes.fr/hal-03208235/</url>
		<author>Federico Simonetta</author>
		<author>Federico Avanzini</author>
		<author>Stavros Ntalampiras</author>
		<authors>
			<first>Federico</first>
		</authors>
		<authors>
			<last>Simonetta</last>
		</authors>
		<authors>
			<first>Federico</first>
		</authors>
		<authors>
			<last>Avanzini</last>
		</authors>
		<authors>
			<first>Stavros</first>
		</authors>
		<authors>
			<last>Ntalampiras</last>
		</authors>
		<abstract>This study focuses on the perception of music performances when contextual factors, such as room acoustics and instrument, change. We propose to distinguish the concept of &quot;performance&quot; from the one of &quot;interpretation&quot;, which expresses the &quot;artistic intention&quot;. Towards assessing this distinction, we carried out an experimental evaluation where subjects were invited to listen to various audio recordings created by resynthesizing MIDI data obtained through Automatic Music Transcription (AMT) systems and a sensorized acoustic piano. During the resynthesis, we simulated different contexts and asked listeners to evaluate how much the interpretation changes when the context changes. Results show that: (1) MIDI format alone is not able to completely grasp the artistic intention of a music performance; (2) usual objective evaluation measures based on MIDI data present low correlations with the average subjective evaluation. To bridge this gap, we propose a novel measure which is meaningfully correlated with the outcome of the tests. In addition, we investigate multimodal machine learning by providing a new score-informed AMT method and propose an approximation algorithm for the p-dispersion problem.</abstract>
		<title>A Perceptual Measure for Evaluating the Resynthesis of Automatic Music Transcriptions</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2201f8a754fce353526b6b2e29825b2f7/anupriya</id>
		<tags>tangible</tags>
		<tags>2007</tags>
		<tags>social</tags>
		<tags>iui</tags>
		<tags>radio</tags>
		<description>IUI: IUI '07, Social radio: a music-based ...</description>
		<date>2007-11-08 22:58:37</date>
		<count>1</count>
		<booktitle>IUI '07: Proceedings of the 12th international conference on Intelligent user interfaces</booktitle>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2007</year>
		<url>http://portal.acm.org/citation.cfm?id=1216295.1216348&coll=ACM&dl=ACM&type=series&idx=SERIES823&part=series&WantType=Proceedings&title=IUI&CFID=424762&CFTOKEN=45660172</url>
		<author>Carsten Röcker</author>
		<author>Richard Etter</author>
		<authors>
			<first>Carsten</first>
		</authors>
		<authors>
			<last>Röcker</last>
		</authors>
		<authors>
			<first>Richard</first>
		</authors>
		<authors>
			<last>Etter</last>
		</authors>
		<pages>286--289</pages>
		<abstract>This paper presents a novel approach for mediating awareness in small intimate groups. Instead of traditional communication media, music is used to inform users about the presence and mood of multiple remote peers. Based on this conceptual idea, an awareness system called 'Social Radio' was developed. The system consists of several smart artifacts and an underlying multi-user communication infrastructure.</abstract>
		<doi>http://doi.acm.org/10.1145/1216295.1216348</doi>
		<isbn>1-59593-481-2</isbn>
		<title>Social radio: a music-based approach to emotional awareness mediation</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b170ab92de4041cf2d84133d7e70bd90/stefano</id>
		<tags>music</tags>
		<tags>creativity</tags>
		<tags>performance</tags>
		<tags>livecoding</tags>
		<description>The Programming Language as a Musical Instrument (ResearchIndex)</description>
		<date>2007-03-15 11:01:54</date>
		<count>4</count>
		<journal>Psychology of Programming Interest Group</journal>
		<year>2005</year>
		<url>http://citeseer.ist.psu.edu/blackwell05programming.html</url>
		<author>Alan Blackwell</author>
		<author>Nick Collins</author>
		<authors>
			<first>Alan</first>
		</authors>
		<authors>
			<last>Blackwell</last>
		</authors>
		<authors>
			<first>Nick</first>
		</authors>
		<authors>
			<last>Collins</last>
		</authors>
		<title>The Programming Language as a Musical Instrument</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2835651c0ebafe7b525a78aba1ecc0e43/yaxu</id>
		<tags>glitch</tags>
		<tags>cascone</tags>
		<description>The Aesthetics of Failure: "Post-Digital" Tendencies in Contemporary Computer Music</description>
		<date>2007-03-24 00:36:09</date>
		<count>3</count>
		<journal>Computer Music Journal</journal>
		<publisher>MIT Press</publisher>
		<address>Cambridge, MA, USA</address>
		<year>2000</year>
		<url></url>
		<author>Kim Cascone</author>
		<authors>
			<first>Kim</first>
		</authors>
		<authors>
			<last>Cascone</last>
		</authors>
		<volume>24</volume>
		<number>4</number>
		<pages>12--18</pages>
		<issn>0148-9267</issn>
		<doi>http://dx.doi.org/10.1162/014892600559489</doi>
		<title>The Aesthetics of Failure: "Post-Digital" Tendencies in Contemporary Computer Music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2418c9b6b6cd72c96f109ac5ab7e15598/ichun</id>
		<tags>music</tags>
		<tags>computer</tags>
		<description></description>
		<date>2007-01-16 01:47:06</date>
		<count>1</count>
		<year>1989</year>
		<url></url>
		<author>Curtis Road</author>
		<authors>
			<first>Curtis</first>
		</authors>
		<authors>
			<last>Road</last>
		</authors>
		<title>The Music Machine</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b4817b453f0b8ad4303a0d070269b6ea/svrist</id>
		<tags>imported</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>1</count>
		<journal>Proceedings of the Computers in Music Research Conference</journal>
		<year>1988</year>
		<url></url>
		<author>A. T. Clarke</author>
		<author>B. M. Brown</author>
		<author>M. P. Thorne</author>
		<authors>
			<first>A. T.</first>
		</authors>
		<authors>
			<last>Clarke</last>
		</authors>
		<authors>
			<first>B. M.</first>
		</authors>
		<authors>
			<last>Brown</last>
		</authors>
		<authors>
			<first>M. P.</first>
		</authors>
		<authors>
			<last>Thorne</last>
		</authors>
		<pages>84-7</pages>
		<title>Inexpensive optical character recognition of music notation: A new alternative for publishers</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22d761286f0a9d4ef315eaa360dece2e6/svrist</id>
		<tags>recognition;</tags>
		<tags>primitives</tags>
		<tags>reconstruction;</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>2</count>
		<journal>Image and Vision Computing</journal>
		<year>1996</year>
		<url>http://www.sciencedirect.com/science/article/B6V09-3VVCMM1-1S/2/1924f7a858dbfb89b48f14e5000c674b</url>
		<author>K. C. Ng</author>
		<author>R. D. Boyle</author>
		<authors>
			<first>K. C.</first>
		</authors>
		<authors>
			<last>Ng</last>
		</authors>
		<authors>
			<first>R. D.</first>
		</authors>
		<authors>
			<last>Boyle</last>
		</authors>
		<volume>14</volume>
		<number>1</number>
		<pages>39-46</pages>
		<abstract>Music recognition bears similarities and differences to OCR. In this paper we identify some of the problems peculiar to musical scores, and propose an approach which succeeds in a wide range of non-trivial cases. The composer customarily proceeds by writing notes, then stems, beams, ties and slurs-we have inverted this approach by segmenting and then subsegmenting scores to recapture the component parts of symbols. In this paper, we concentrate on the strategy of recognizing sub-segmented primitives, and the reassembly process which reconstructs low level graphical primitives back to musical symbols. The sub-segmentation process proves to be worthwhile, since many primitives complement each other and high level musical theory can be employed to enhance the recognition process (12 Refs.)</abstract>
		<doi>10.1016/0262-8856(95)01038-6</doi>
		<title>Recognition and reconstruction of primitives in music scores</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bc686469708574e68bab66ca80eb3acb/svrist</id>
		<tags>imported</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>1</count>
		<year>2005-2007</year>
		<url>http://music-staves.sourceforge.net</url>
		<author>Christoph Dalitz</author>
		<author>Michael Droettboom</author>
		<author>Bastian Czerwinski</author>
		<author>Ichiro Fujigana</author>
		<authors>
			<first>Christoph</first>
		</authors>
		<authors>
			<last>Dalitz</last>
		</authors>
		<authors>
			<first>Michael</first>
		</authors>
		<authors>
			<last>Droettboom</last>
		</authors>
		<authors>
			<first>Bastian</first>
		</authors>
		<authors>
			<last>Czerwinski</last>
		</authors>
		<authors>
			<first>Ichiro</first>
		</authors>
		<authors>
			<last>Fujigana</last>
		</authors>
		<title>Staff Removal Toolkit for Gamera</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23d1b6add094eb00d0989e649d6ee178f/svrist</id>
		<tags>imported</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>1</count>
		<journal>Perspectives of New Music</journal>
		<year>1972</year>
		<url></url>
		<author>M. Kassler</author>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Kassler</last>
		</authors>
		<volume>11</volume>
		<pages>250-4</pages>
		<title>Optical character recognition of printed music: A review of two
    dissertations</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2dae7ad94763184a8646f52decf90aa24/wiljami74</id>
		<tags>expectation</tags>
		<tags>musical_meaning</tags>
		<tags>suspension</tags>
		<description>emotion and meaning in music (expectations, suspension)</description>
		<date>2007-06-05 10:03:01</date>
		<count>1</count>
		<publisher>The University of Chicago Press</publisher>
		<address>Chicago</address>
		<year>1956</year>
		<url></url>
		<author>Leonard B. Meyer</author>
		<authors>
			<first>Leonard B.</first>
		</authors>
		<authors>
			<last>Meyer</last>
		</authors>
		<title>Emotion and Meaning in Music</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2437899313ae25c2ee4ae4ead00915656/cwillems</id>
		<tags>cinearchiv</tags>
		<tags>Music</tags>
		<tags>Genre</tags>
		<tags>Recognition</tags>
		<description></description>
		<date>2008-10-12 22:17:07</date>
		<count>2</count>
		<journal>In Proc. Int. Computer Music Conf. (ICMC)</journal>
		<address>Thessaloniki, Greece</address>
		<year>1997</year>
		<url></url>
		<author>R. B. Dannenberg</author>
		<author>B. Thom</author>
		<author>D. Watson</author>
		<authors>
			<first>R. B.</first>
		</authors>
		<authors>
			<last>Dannenberg</last>
		</authors>
		<authors>
			<first>B.</first>
		</authors>
		<authors>
			<last>Thom</last>
		</authors>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Watson</last>
		</authors>
		<pages>pages 344--347</pages>
		<title>A machine learning approach to musical style recognition</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2146e68c8b0e0fdd370bc0a2a35d347e3/ocelma</id>
		<tags>Last.fm,</tags>
		<tags>agreement,</tags>
		<tags>community,</tags>
		<tags>tags,</tags>
		<tags>experts,</tags>
		<tags>distance,</tags>
		<tags>cosine,</tags>
		<tags>components,</tags>
		<tags>genre,</tags>
		<tags>wisdom</tags>
		<tags>crowds</tags>
		<tags>mp3.com,</tags>
		<tags>similarity,</tags>
		<tags>LSA,</tags>
		<tags>folksonomy,</tags>
		<tags>of</tags>
		<tags>taxonomy,</tags>
		<tags>experiments,</tags>
		<tags>classification,</tags>
		<tags>PhD,</tags>
		<description>PhD</description>
		<date>2009-01-08 17:06:55</date>
		<count>3</count>
		<journal>Proceedings of the 9th International Conference on Music Information Retrieval</journal>
		<address>Philadelphia, USA</address>
		<year>2008</year>
		<url></url>
		<author>M. Sordo</author>
		<author>O. Celma</author>
		<author>M. Blech</author>
		<author>E. Guaus</author>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Sordo</last>
		</authors>
		<authors>
			<first>O.</first>
		</authors>
		<authors>
			<last>Celma</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Blech</last>
		</authors>
		<authors>
			<first>E.</first>
		</authors>
		<authors>
			<last>Guaus</last>
		</authors>
		<abstract>This paper presents some findings around musical genres. The main
	goal is to analyse whether there is any agreement between a group
	of experts and a community, when defining a set of genres and their
	relationships. For this purpose, three different experiments are
	conducted using two datasets: the MP3.com expert taxonomy, and last.fm
	tags at artist level. The experimental results show a clear agreement
	for some components of the taxonomy (Blues, Hip-Hop), whilst in other
	cases (e.g. Rock) there is no correlations. Interestingly enough,
	the same results are found in the MIREX2007 results for audio genre
	classification task. Therefore, a multi–faceted approach for musical
	genre using expert based classifications, dynamic associations derived
	from the wisdom of crowds, and content–based analysis can improve
	genre classification, as well as other relevant MIR tasks such as
	music similarity or music recommendation.</abstract>
		<title>The Quest for Musical Genres: Do the Experts and the Wisdom of Crowds
	Agree?</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28f74c00a74d55bf20e6943387e38a728/syslogd</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-07 13:57:09</date>
		<count>2</count>
		<journal>Journal of New Music Research</journal>
		<publisher>Routledge</publisher>
		<year>2005</year>
		<url></url>
		<author>Stephan Baumann</author>
		<author>Oliver Hummel</author>
		<authors>
			<first>Stephan</first>
		</authors>
		<authors>
			<last>Baumann</last>
		</authors>
		<authors>
			<first>Oliver</first>
		</authors>
		<authors>
			<last>Hummel</last>
		</authors>
		<volume>34</volume>
		<number>2</number>
		<pages>161--172</pages>
		<abstract>In today's online commercial music marketplaces, a common requirement is to generate lists of artists that are ``similar'' to a given chosen artist. However, this is by no means a trivial task. A recent trend has been to tackle this challenge using sociocultural connotations rather than the traditional content-based audio or lyrics analysis. This article describes an enhancement to this approach that relies on the acquisition, filtering and condensing of unstructured, text-based information that can be found on the World Wide Web to recognize what the music community regards as ``similar'' artists. The beauty of this approach lies in its ability to access so-called ``cultural metadata'' (i.e., textual data about musical content) which is the aggregation of several independent - originally subjective - perspectives about a piece of music. The major focus of this work is the evaluation and enhancement of existing approaches in this area using filtering methods to increase their precision. A meaningful evaluation of the results is provided by a comparison with ground truth data.</abstract>
		<title>Enhancing Music Recommendation Algorithms Using Cultural Metadata</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b53ac784ff50273d1d26b52330630a23/syslogd</id>
		<tags>Retrieval</tags>
		<tags>Music</tags>
		<tags>Information</tags>
		<description></description>
		<date>2010-01-07 13:57:09</date>
		<count>2</count>
		<year>2005</year>
		<url>http://dspace.mit.edu/bitstream/1721.1/32500/1/61896668.pdf</url>
		<author>Brian A. Whitman</author>
		<authors>
			<first>Brian A.</first>
		</authors>
		<authors>
			<last>Whitman</last>
		</authors>
		<abstract>Expression as complex and personal as music is not adequately represented by the sig- 
nal alone. We define and model meaning in music as the mapping between the acoustic 
signal and its contextual interpretation - the 'community metadata' based on popular- 
ity, description and personal reaction, collected from reviews, usage, and discussion. In 
this thesis we present a framework for capturing community metadata from free text 
sources, audio representations general enough to work across domains of music, and 
a machine learning framework for learning the relationship between the music signals 
and the contextual reaction iteratively at a large scale. 
Our work is evaluated and applied as semantic basis 
functions - meaning classifiers that 
are used to maximize semantic content in a perceptual signal. This process improves 
upon statistical methods of rank reduction as it aims to model a community's reaction 
to perception instead of relationships found in the signal alone. We show increased 
accuracy of common music retrieval tasks with audio projected through semantic ba- 
sis functions. We also evaluate our models in a 'query-by-description' task for music, 
where we predict description and community interpretation of audio. These unbi- 
ased learning approaches show superior accuracy in music and multimedia intelligence 
tasks such as similarity, classification and recommendation.</abstract>
		<title>Learning the Meaning of Music</title>
		<pubtype>phdthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ee0d79590da2c718c93735ef08e7feb5/aucelum</id>
		<tags>music</tags>
		<tags>mem</tags>
		<tags>analysis</tags>
		<description></description>
		<date>2010-02-02 17:03:01</date>
		<count>2</count>
		<journal>Computer Music Journal</journal>
		<publisher>The MIT Press</publisher>
		<year>1999</year>
		<url></url>
		<author>Heinrich Taube</author>
		<authors>
			<first>Heinrich</first>
		</authors>
		<authors>
			<last>Taube</last>
		</authors>
		<volume>23</volume>
		<number>4</number>
		<pages>18--32</pages>
		<issn>01489267</issn>
		<copyright>Copyright © 1999 The MIT Press</copyright>
		<title>Automatic Tonal Analysis: Toward the Implementation of a Music Theory Workbench</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24413cb5c355fcad7acaefd50e1e00d4e/aucelum</id>
		<tags>music</tags>
		<tags>mir</tags>
		<tags>mem</tags>
		<description></description>
		<date>2010-02-02 17:03:01</date>
		<count>1</count>
		<journal>Foundations and Trends® in Information Retrieval</journal>
		<year>2006</year>
		<url>http://www.nowpublishers.com/product.aspx?product=INR&doi=1500000002</url>
		<author>Nicolo Oria</author>
		<authors>
			<first>Nicolo</first>
		</authors>
		<authors>
			<last>Oria</last>
		</authors>
		<editor>Now Publishers</editor>
		<editors>
			<first>Nicolo</first>
		</editors>
		<editors>
			<last>Oria</last>
		</editors>
		<volume>1</volume>
		<number>1</number>
		<abstract>The increasing availability of music in digital format needs to be matched by the development of tools for music accessing, filtering, classification, and retrieval. The research area of Music Information Retrieval (MIR) covers many of these aspects. The aim of this paper is to present an overview of this vast and new field. A number of issues, which are peculiar to the music language, are described--including forms, formats, and dimensions of music--together with the typologies of users and their information needs. To fulfil these needs a number of approaches are discussed, from direct search to information filtering and clustering of music documents. An overview of the techniques for music processing, which are commonly exploited in many approaches, is also presented. Evaluation and comparisons of the approaches on a common benchmark are other important issues. To this end, a description of the initial efforts and evaluation campaigns for MIR is provided.</abstract>
		<title>Music Retrieval: A Tutorial and Review</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21f08437b4ef5a304c0e34fccba5ea885/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>Berkeley</journal>
		<year>2000</year>
		<url></url>
		<author>Richard Middleton</author>
		<authors>
			<first>Richard</first>
		</authors>
		<authors>
			<last>Middleton</last>
		</authors>
		<pages>59--85</pages>
		<abstract>The combinations of high and low musical languages in Mozart's "Die Zauberflöte" and Gershwin's "Porgy and Bess" are explored, and the ways that Duke Ellington's "jungle music" and South African popular music represent otherness are discussed.</abstract>
		<shorttitle>Musical belongings</shorttitle>
		<title>Musical belongings: Western music and its low-other</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24cd7c573c9490dab2d9ab2b45a897d1a/bfields</id>
		<tags>Playlist</tags>
		<tags>music_similarity</tags>
		<description>initial imports</description>
		<date>2010-01-28 11:42:45</date>
		<count>3</count>
		<booktitle>Proc. 8th ACM international workshop on Multimedia information retrieval</booktitle>
		<year>2006</year>
		<url></url>
		<author>P. Knees</author>
		<author>T. Pohle</author>
		<author>M. Schedl</author>
		<author>G. Widmer</author>
		<authors>
			<first>P.</first>
		</authors>
		<authors>
			<last>Knees</last>
		</authors>
		<authors>
			<first>T.</first>
		</authors>
		<authors>
			<last>Pohle</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<authors>
			<first>G.</first>
		</authors>
		<authors>
			<last>Widmer</last>
		</authors>
		<pages>147 - 154</pages>
		<title>Combining Audio-based Similarity with Web-based Data to Accelerate Automatic Music Playlist Generation</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22dc5e0eda726805b8af9ca8f396002cb/bfields</id>
		<tags>Music_recommendation</tags>
		<tags>sociology</tags>
		<tags>social_context</tags>
		<description>initial imports</description>
		<date>2010-01-28 11:42:45</date>
		<count>3</count>
		<booktitle>Proc. of Int. Symposium on Music Information Retrieval</booktitle>
		<year>2007</year>
		<url></url>
		<author>Daniel McEnnis</author>
		<author>Sally Jo Cunningham</author>
		<authors>
			<first>Daniel</first>
		</authors>
		<authors>
			<last>McEnnis</last>
		</authors>
		<authors>
			<first>Sally Jo</first>
		</authors>
		<authors>
			<last>Cunningham</last>
		</authors>
		<title>Sociology and Music Recommendation Systems</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b2fb0e41911aa28eb1db00ce5ccc0725/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<publisher>Greenwood Press</publisher>
		<address>Westport, Conn</address>
		<year>1985</year>
		<url></url>
		<author>Irene V. Jackson</author>
		<author>Howard University. Center for Ethnic Music</author>
		<authors>
			<first>Irene V.</first>
		</authors>
		<authors>
			<last>Jackson</last>
		</authors>
		<authors>
			<first>Howard University. Center</first>
		</authors>
		<authors>
			<last>for Ethnic Music</last>
		</authors>
		<pages>281</pages>
		<abstract>Loretta Burns, "The structure of blues lyrics"</abstract>
		<isbn>0313245541 (lib. bdg.)</isbn>
		<title>More than dancing : essays on Afro-American music and musicians</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c6bfa2280eb051c802f0a913ac4704cd/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>Black Music Research Journal</journal>
		<year>2002</year>
		<url>http://www.jstor.org.ezproxy.library.yorku.ca/stable/1519948</url>
		<author>Peter Narvaez</author>
		<authors>
			<first>Peter</first>
		</authors>
		<authors>
			<last>Narvaez</last>
		</authors>
		<volume>22</volume>
		<pages>175--196</pages>
		<issn>02763605</issn>
		<title>The Influences of Hispanic Music Cultures on African-American Blues Musicians</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25a1d24a1303b0d8cc7278331e0a4413e/bfields</id>
		<tags>acoustic_measures</tags>
		<tags>Evaluation_systems</tags>
		<tags>music_similarity</tags>
		<tags>ground-truth</tags>
		<tags>MIREX</tags>
		<description>initial imports</description>
		<date>2010-01-28 11:42:45</date>
		<count>3</count>
		<booktitle>Proc. of Int. Symposium on Music Information Retrieval</booktitle>
		<year>2003</year>
		<url></url>
		<author>Adam Berenzweig</author>
		<author>Beth Logan</author>
		<author>Daniel Ellis</author>
		<author>Brian Whitman</author>
		<authors>
			<first>Adam</first>
		</authors>
		<authors>
			<last>Berenzweig</last>
		</authors>
		<authors>
			<first>Beth</first>
		</authors>
		<authors>
			<last>Logan</last>
		</authors>
		<authors>
			<first>Daniel</first>
		</authors>
		<authors>
			<last>Ellis</last>
		</authors>
		<authors>
			<first>Brian</first>
		</authors>
		<authors>
			<last>Whitman</last>
		</authors>
		<title>A Large-Scale Evaluation of Acoustic and Subjective Music Similarity Measures</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22bae24569ec71b53142d59c2dca06664/kurtjx</id>
		<tags>music</tags>
		<tags>networks</tags>
		<description></description>
		<date>2009-02-17 12:19:11</date>
		<count>2</count>
		<year>2005</year>
		<url>http://arxiv.org/abs/physics/0508233</url>
		<author>R. Lambiotte</author>
		<author>M. Ausloos</author>
		<authors>
			<first>R.</first>
		</authors>
		<authors>
			<last>Lambiotte</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Ausloos</last>
		</authors>
		<abstract>In this paper, we analyze web-downloaded data on people sharing their music
library, that we use as their individual musical signatures (IMS). The system
is represented by a bipartite network, nodes being the music groups and the
listeners. Music groups audience size behaves like a power law, but the
individual music library size is an exponential with deviations at small
values. In order to extract structures from the network, we focus on
correlation matrices, that we filter by removing the least correlated links.
This percolation idea-based method reveals the emergence of social communities
and music genres, that are visualised by a branching representation. Evidence
of collective listening habits that do not fit the neat usual genres defined by
the music industry indicates an alternative way of classifying listeners/music
groups. The structure of the network is also studied by a more refined method,
based upon a random walk exploration of its properties. Finally, a personal
identification - community imitation model (PICI) for growing bipartite
networks is outlined, following Potts ingredients. Simulation results do
reproduce quite well the empirical data.</abstract>
		<title>Uncovering collective listening habits and music genres in bipartite
  networks</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21d227cd63e95402966b0297fd484cebd/abromwell</id>
		<tags>Richard;PostWorldWarIEra;Nationalism;Modernism;Composers;GermanOpera;Sociomusicology</tags>
		<tags>Paul;Hindemith</tags>
		<tags>ResearchandAnalysis;Opera;Theory/Analysis/Composition;Pfitzner</tags>
		<tags>Hans;Bekker</tags>
		<tags>Paul;Wagner</tags>
		<description></description>
		<date>2010-12-22 20:03:59</date>
		<count>1</count>
		<journal>Journal of Musicology - A Quarterly Review of Music History, Criticism, Analysis, and Performance Practice</journal>
		<year>2008</year>
		<url>http://gateway.proquest.com/openurl?url_ver=Z39.88-2004&res_dat=xri:iimp:&rft_dat=xri:iimp:article:citation:iimp00661601</url>
		<author>Joel Haney</author>
		<authors>
			<first>Joel</first>
		</authors>
		<authors>
			<last>Haney</last>
		</authors>
		<volume>25</volume>
		<number>4</number>
		<pages>339-393</pages>
		<abstract>Post-World War I Germany's music and musical discourse focused on issues of national identity, nationalism, and the legacy of Richard Wagner. One musical statement that attracted much notice was Paul Hindemith's 1921 burlesque opera "Das Nusch-Nuschi." It was based on a Burmese marionette play that scandalously satirized Wagner's "Tristan und Isolde." Hindemith's opera resonates strongly with a position then being voiced by Paul Bekker, who spoke out against Hans Pfitzner's Wagnerian hypernationalism and called for a decisive internationalist turn in postwar German composition. It sharpens its ironic, anti-Wangerian tone by reaching beyond German modernism to embrace the Russian "neonationalism" of Igor Stravinsky.</abstract>
		<isbn>0277-9269</isbn>
		<title>Slaying the Wagnerian Monster: Hindemith, "Das Nusch-Nuschi," and Musical Germanness After the Great War</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c8f5d8699cd9025ea698faa50be992d5/kurtjx</id>
		<tags>music</tags>
		<tags>semantic_web</tags>
		<description></description>
		<date>2009-01-28 15:20:50</date>
		<count>3</count>
		<journal>ACM Trans. Multimedia Comput. Commun. Appl.</journal>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>t 06</year>
		<url>http://portal.acm.org/citation.cfm?id=1152151</url>
		<author>Alfio Ferrara</author>
		<author>Luca A. Ludovico</author>
		<author>Stefano Montanelli</author>
		<author>Silvana Castano</author>
		<author>Goffredo Haus</author>
		<authors>
			<first>Alfio</first>
		</authors>
		<authors>
			<last>Ferrara</last>
		</authors>
		<authors>
			<first>Luca A.</first>
		</authors>
		<authors>
			<last>Ludovico</last>
		</authors>
		<authors>
			<first>Stefano</first>
		</authors>
		<authors>
			<last>Montanelli</last>
		</authors>
		<authors>
			<first>Silvana</first>
		</authors>
		<authors>
			<last>Castano</last>
		</authors>
		<authors>
			<first>Goffredo</first>
		</authors>
		<authors>
			<last>Haus</last>
		</authors>
		<volume>2</volume>
		<number>3</number>
		<pages>177--198</pages>
		<abstract>In this article, we describe the MX-Onto ontology for providing a Semantic Web compatible representation of music resources based on their context. The context representation is realized by means of an OWL ontology that describes music information and that defines rules and classes for a flexible genre classification. By flexible classification we mean that the proposed approach enables capturing the subjective interpretation of music genres by defining multiple membership relations between a music resource and the corresponding music genres, thus supporting context-based and proximity-based search of music resources.</abstract>
		<issn>1551-6857</issn>
		<doi>http://doi.acm.org/10.1145/1152149.1152151</doi>
		<title>A Semantic Web ontology for context-based classification and retrieval of music resources</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bf082668567a66f253b1bffb3442b040/tfalk</id>
		<tags>imported</tags>
		<description>ISMIR 2007 - 8th International Conference on Music Information Retrieval</description>
		<date>2009-02-28 21:01:39</date>
		<count>6</count>
		<booktitle>8th International Conference on Music Information Retrieval (ISMIR
	2007)</booktitle>
		<year>2007</year>
		<url>http://ismir2007.ismir.net/proceedings/ISMIR2007_p411_levy.pdf</url>
		<author>Mark Levy</author>
		<author>Mark Sandler</author>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Levy</last>
		</authors>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Sandler</last>
		</authors>
		<title>A Semantic Space for Music Derived from Social Tags</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/249db2e198ba1c20debd67a6917f92b7d/acka47</id>
		<tags>similarity</tags>
		<tags>culturegraph.org</tags>
		<tags>ontology</tags>
		<description>An Ecosystem for Transparent Music Similarity in an Open World</description>
		<date>2010-12-08 07:36:06</date>
		<count>1</count>
		<year>2010</year>
		<url>http://docs.kurtisrandom.com/ismir2009-tth/</url>
		<author>Kurt Jacobson</author>
		<author>Yves Raimond</author>
		<author>Mark Sandler</author>
		<authors>
			<first>Kurt</first>
		</authors>
		<authors>
			<last>Jacobson</last>
		</authors>
		<authors>
			<first>Yves</first>
		</authors>
		<authors>
			<last>Raimond</last>
		</authors>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Sandler</last>
		</authors>
		<abstract>There exist many methods for deriving music similarity associations and additional variations are likely to be seen in the future. In this work we introduce the Similarity Ontology for describing associations between items. Using a combination of RDF/OWL and N3, our ontology allows for transparency and provenance tracking in a distributed and open system. We describe a similarity ecosystem where agents assert and aggregate similarity statements on the Web of Data allowing a client application to make queries for recommendation, playlisting, or other tasks. In this ecosystem any number of similarity derivation methods can exist side-by-side, specifying similarity relationships as well as the processes used to derive these statements. The data consumer can then select which similarity statements to trust based on knowledge of the similarity derivation processes or a list of trusted assertion agents.</abstract>
		<title>An Ecosystem for Transparent Music Similarity in an Open World</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21708a4b46ae195d42ade52955a1e6e8b/smatthiesen</id>
		<tags>music</tags>
		<description></description>
		<date>2010-11-30 22:39:03</date>
		<count>1</count>
		<journal>NeuroImage</journal>
		<year>2009</year>
		<url>http://dx.doi.org/10.1016/j.neuroimage.2009.12.019</url>
		<author>Marcus T. Pearce</author>
		<author>Mar\'ıa H. Ruiz</author>
		<author>Selina Kapasi</author>
		<author>Geraint A. Wiggins</author>
		<author>Joydeep Bhattacharya</author>
		<authors>
			<first>Marcus T.</first>
		</authors>
		<authors>
			<last>Pearce</last>
		</authors>
		<authors>
			<first>Mar\'ıa H.</first>
		</authors>
		<authors>
			<last>Ruiz</last>
		</authors>
		<authors>
			<first>Selina</first>
		</authors>
		<authors>
			<last>Kapasi</last>
		</authors>
		<authors>
			<first>Geraint A.</first>
		</authors>
		<authors>
			<last>Wiggins</last>
		</authors>
		<authors>
			<first>Joydeep</first>
		</authors>
		<authors>
			<last>Bhattacharya</last>
		</authors>
		<abstract>The ability to anticipate forthcoming events has clear evolutionary advantages, and predictive successes or failures often entail significant psychological and physiological consequences. In music perception, the confirmation and violation of expectations are critical to the communication of emotion and aesthetic effects of a composition. Neuroscientific research on musical expectations has focused on harmony. Although harmony is important in Western tonal styles, other musical traditions, emphasizing pitch and melody, have been rather neglected. In this study, we investigated melodic pitch expectations elicited by ecologically valid musical stimuli by drawing together computational, behavioural, and electrophysiological evidence. Unlike rule-based models, our computational model acquires knowledge through unsupervised statistical learning of sequential structure in music and uses this knowledge to estimate the conditional probability (and information content) of musical notes. Unlike previous behavioural paradigms that interrupt a stimulus, we devised a new paradigm for studying auditory expectation without compromising ecological validity. A strong negative correlation was found between the probability of notes predicted by our model and the subjectively perceived degree of expectedness. Our electrophysiological results showed that low-probability notes, as compared to high-probability notes, elicited a larger (i) negative ERP component at a late time period (400–450 ms), (ii) beta band (14–30 Hz) oscillation over the parietal lobe, and (iii) long-range phase synchronization between multiple brain regions. Altogether, the study demonstrated that statistical learning produces information-theoretic descriptions of musical notes that are proportional to their perceived expectedness and are associated with characteristic patterns of neural activity.</abstract>
		<issn>10538119</issn>
		<doi>10.1016/j.neuroimage.2009.12.019</doi>
		<title>Unsupervised statistical learning underpins computational, behavioural, and neural manifestations of musical expectation</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2da84c6833294dbe6f641f06b980d3ee5/mediadigits</id>
		<tags>retrieval</tags>
		<tags>semantic</tags>
		<tags>music</tags>
		<tags>evaluation</tags>
		<tags>similarity</tags>
		<description>PhD</description>
		<date>2009-09-29 09:47:34</date>
		<count>3</count>
		<booktitle>Proceedings of the 6th International Conference on Music Information Retrieval</booktitle>
		<address>London, UK</address>
		<year>2005</year>
		<url></url>
		<author>Fabio Vignoli</author>
		<author>Steffen Pauws</author>
		<authors>
			<first>Fabio</first>
		</authors>
		<authors>
			<last>Vignoli</last>
		</authors>
		<authors>
			<first>Steffen</first>
		</authors>
		<authors>
			<last>Pauws</last>
		</authors>
		<pages>272-279</pages>
		<title>A Music Retrieval System Based on User Driven Similarity and Its Evaluation</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2179759e00f65aeef1c3ccc92cb93a037/mediadigits</id>
		<tags>semantic</tags>
		<tags>music</tags>
		<tags>mood</tags>
		<description></description>
		<date>2011-03-10 18:36:20</date>
		<count>2</count>
		<journal>EURASIP Journal on Audio, Speech, and Music Processing</journal>
		<publisher>Hindawi Publishing Corp.</publisher>
		<year>2010</year>
		<url></url>
		<author>B. Schuller</author>
		<author>J. Dorfner</author>
		<author>G. Rigoll</author>
		<authors>
			<first>B.</first>
		</authors>
		<authors>
			<last>Schuller</last>
		</authors>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Dorfner</last>
		</authors>
		<authors>
			<first>G.</first>
		</authors>
		<authors>
			<last>Rigoll</last>
		</authors>
		<volume>2010</volume>
		<pages>5</pages>
		<issn>1687-4714</issn>
		<title>Determination of nonprototypical valence and arousal in popular music: features and performances</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2eea386ed89367dc9e8d317fac24ebe03/mediadigits</id>
		<tags>framework</tags>
		<tags>retrieval</tags>
		<tags>music</tags>
		<tags>information</tags>
		<description>The Bibliography of my Belegarbeit</description>
		<date>2010-10-04 10:22:53</date>
		<count>2</count>
		<year>2006</year>
		<url></url>
		<author>Micheline Lesaffre</author>
		<authors>
			<first>Micheline</first>
		</authors>
		<authors>
			<last>Lesaffre</last>
		</authors>
		<title>Music Information Retrieval - Conceptual Framework, Annotation and User Behaviour</title>
		<pubtype>phdthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/243954ebffaa434705c92b3454f4879aa/mediadigits</id>
		<tags>music</tags>
		<tags>taxonomy</tags>
		<description></description>
		<date>2011-03-10 15:57:32</date>
		<count>9</count>
		<journal>Journal of New Music Research</journal>
		<year>2003</year>
		<url></url>
		<author>J.J. Aucouturier</author>
		<author>F. Pachet</author>
		<authors>
			<first>J.J.</first>
		</authors>
		<authors>
			<last>Aucouturier</last>
		</authors>
		<authors>
			<first>F.</first>
		</authors>
		<authors>
			<last>Pachet</last>
		</authors>
		<volume>32</volume>
		<number>1</number>
		<pages>83-93</pages>
		<title>Representing Musical Genre: A State of the Art</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2852ae9f3218c3da3130f258c0c13257d/yevb0</id>
		<tags>Affect,Auditory</tags>
		<tags>Perception,Auditory</tags>
		<tags>Perception:</tags>
		<tags>physiology,Cognition,Humans,Music,Music:</tags>
		<tags>psychology,Pitch</tags>
		<tags>Perception,Psychoacoustics,Psycholinguistics,Psychological</tags>
		<tags>Theory,acquisition,modularity,music</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Cognition</journal>
		<year>2006</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/16384553</url>
		<author>Ray Jackendoff</author>
		<author>Fred Lerdahl</author>
		<authors>
			<first>Ray</first>
		</authors>
		<authors>
			<last>Jackendoff</last>
		</authors>
		<authors>
			<first>Fred</first>
		</authors>
		<authors>
			<last>Lerdahl</last>
		</authors>
		<volume>100</volume>
		<number>1</number>
		<pages>33--72</pages>
		<abstract>We explore the capacity for music in terms of five questions: (1)
	What cognitive structures are invoked by music? (2) What are the
	principles that create these structures? (3) How do listeners acquire
	these principles? (4) What pre-existing resources make such acquisition
	possible? (5) Which aspects of these resources are specific to music,
	and which are more general? We examine these issues by looking at
	the major components of musical organization: rhythm (an interaction
	of grouping and meter), tonal organization (the structure of melody
	and harmony), and affect (the interaction of music with emotion).
	Each domain reveals a combination of cognitively general phenomena,
	such as gestalt grouping principles, harmonic roughness, and stream
	segregation, with phenomena that appear special to music and language,
	such as metrical organization. These are subtly interwoven with a
	residue of components that are devoted specifically to music, such
	as the structure of tonal systems and the contours of melodic tension
	and relaxation that depend on tonality. In the domain of affect,
	these components are especially tangled, involving the interaction
	of such varied factors as general-purpose aesthetic framing, communication
	of affect by tone of voice, and the musically specific way that tonal
	pitch contours evoke patterns of posture and gesture.</abstract>
		<issn>0010-0277</issn>
		<doi>10.1016/j.cognition.2005.11.005</doi>
		<title>The capacity for music: what is it, and what's special about it?</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f5b0120bd6ff92f70adfee5a146936b9/yevb0</id>
		<tags>language,modularity,music,neuro,speech</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>Language and music as cognitive systems</booktitle>
		<publisher>Oxford University Press</publisher>
		<address>Oxford</address>
		<year>2009</year>
		<url></url>
		<author>Aniruddh D. Patel</author>
		<authors>
			<first>Aniruddh D.</first>
		</authors>
		<authors>
			<last>Patel</last>
		</authors>
		<editor>P Rebuschat</editor>
		<editor>M Rohrmeier</editor>
		<editor>J Hawkins</editor>
		<editor>Ian Cross</editor>
		<editors>
			<first>Aniruddh D.</first>
		</editors>
		<editors>
			<last>Patel</last>
		</editors>
		<editors>
			<first>Aniruddh D.</first>
		</editors>
		<editors>
			<last>Patel</last>
		</editors>
		<editors>
			<first>Aniruddh D.</first>
		</editors>
		<editors>
			<last>Patel</last>
		</editors>
		<editors>
			<first>Aniruddh D.</first>
		</editors>
		<editors>
			<last>Patel</last>
		</editors>
		<title>Language, music, and the brain: a resource-sharing framework</title>
		<pubtype>inbook</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b69642338ff1ae668e423e9865977f81/yevb0</id>
		<tags>Masking,Pitch</tags>
		<tags>Acoustic</tags>
		<tags>Thresholds,Short-Term,Sound</tags>
		<tags>(Psychology),Reference</tags>
		<tags>Perception,Speech</tags>
		<tags>Reception</tags>
		<tags>Spectrography,Speech</tags>
		<tags>Stimulation,Adult,Female,Humans,Male,Memory,Music,Perceptual</tags>
		<tags>Values,Sensory</tags>
		<tags>Adult,language,music,musicality,perception,speech</tags>
		<tags>Discrimination,Practice</tags>
		<tags>Threshold</tags>
		<tags>Test,Young</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>2</count>
		<journal>Ear and Hearing</journal>
		<year>2009</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/19734788</url>
		<author>Alexandra Parbery-Clark</author>
		<author>Erika Skoe</author>
		<author>Carrie Lam</author>
		<author>Nina Kraus</author>
		<authors>
			<first>Alexandra</first>
		</authors>
		<authors>
			<last>Parbery-Clark</last>
		</authors>
		<authors>
			<first>Erika</first>
		</authors>
		<authors>
			<last>Skoe</last>
		</authors>
		<authors>
			<first>Carrie</first>
		</authors>
		<authors>
			<last>Lam</last>
		</authors>
		<authors>
			<first>Nina</first>
		</authors>
		<authors>
			<last>Kraus</last>
		</authors>
		<volume>30</volume>
		<number>6</number>
		<pages>653--61</pages>
		<abstract>OBJECTIVE: To investigate the effect of musical training on speech-in-noise
	(SIN) performance, a complex task requiring the integration of working
	memory and stream segregation as well as the detection of time-varying
	perceptual cues. Previous research has indicated that, in combination
	with lifelong experience with musical stream segregation, musicians
	have better auditory perceptual skills and working memory. It was
	hypothesized that musicians would benefit from these factors and
	perform better on speech perception in noise than age-matched nonmusician
	controls. DESIGN: The performance of 16 musicians and 15 nonmusicians
	was compared on clinical measures of speech perception in noise-QuickSIN
	and Hearing-In-Noise Test (HINT). Working memory capacity and frequency
	discrimination were also assessed. All participants had normal hearing
	and were between the ages of 19 and 31 yr. To be categorized as a
	musician, participants needed to have started musical training before
	the age of 7 yr, have 10 or more years of consistent musical experience,
	and have practiced more than three times weekly within the 3 yr before
	study enrollment. Nonmusicians were categorized by the failure to
	meet the musician criteria, along with not having received musical
	training within the 7 yr before the study. RESULTS: Musicians outperformed
	the nonmusicians on both QuickSIN and HINT, in addition to having
	more fine-grained frequency discrimination and better working memory.
	Years of consistent musical practice correlated positively with QuickSIN,
	working memory, and frequency discrimination but not HINT. The results
	also indicate that working memory and frequency discrimination are
	more important for QuickSIN than for HINT. CONCLUSIONS: Musical experience
	appears to enhance the ability to hear speech in challenging listening
	environments. Large group differences were found for QuickSIN, and
	the results also suggest that this enhancement is derived in part
	from musicians' enhanced working memory and frequency discrimination.
	For HINT, in which performance was not linked to frequency discrimination
	ability and was only moderately linked to working memory, musicians
	still performed significantly better than the nonmusicians. The group
	differences for HINT were evident in the most difficult condition
	in which the speech and noise were presented from the same location
	and not spatially segregated. Understanding which cognitive and psychoacoustic
	factors as well as which lifelong experiences contribute to SIN may
	lead to more effective remediation programs for clinical populations
	for whom SIN poses a particular perceptual challenge. These results
	provide further evidence for musical training transferring to nonmusical
	domains and highlight the importance of taking musical training into
	consideration when evaluating a person's SIN ability in a clinical
	setting.</abstract>
		<issn>1538-4667</issn>
		<doi>10.1097/AUD.0b013e3181b412e9</doi>
		<title>Musician enhancement for speech-in-noise</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c77293c658685b395b86f8a4532cf896/yevb0</id>
		<tags>physiology,Auditory</tags>
		<tags>Perception,Visual</tags>
		<tags>Pathways,Auditory</tags>
		<tags>Pathways:</tags>
		<tags>Perception,Speech</tags>
		<tags>Stem,Brain</tags>
		<tags>Acoustics,Auditory</tags>
		<tags>physiology,language,music,neuro,perception,speech,vision</tags>
		<tags>Perception,Auditory</tags>
		<tags>Perception:</tags>
		<tags>Stem:</tags>
		<tags>physiology,Visual</tags>
		<tags>physiology,Humans,Learning,Music,Speech,Speech</tags>
		<tags>physiology,Brain</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Proceedings of the National Academy of Sciences of the United States
	of America</journal>
		<year>2007</year>
		<url>http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2000431\&tool=pmcentrez\&rendertype=abstract</url>
		<author>Gabriella Musacchia</author>
		<author>Mikko Sams</author>
		<author>Erika Skoe</author>
		<author>Nina Kraus</author>
		<authors>
			<first>Gabriella</first>
		</authors>
		<authors>
			<last>Musacchia</last>
		</authors>
		<authors>
			<first>Mikko</first>
		</authors>
		<authors>
			<last>Sams</last>
		</authors>
		<authors>
			<first>Erika</first>
		</authors>
		<authors>
			<last>Skoe</last>
		</authors>
		<authors>
			<first>Nina</first>
		</authors>
		<authors>
			<last>Kraus</last>
		</authors>
		<volume>104</volume>
		<number>40</number>
		<pages>15894--8</pages>
		<abstract>Musical training is known to modify cortical organization. Here, we
	show that such modifications extend to subcortical sensory structures
	and generalize to processing of speech. Musicians had earlier and
	larger brainstem responses than nonmusician controls to both speech
	and music stimuli presented in auditory and audiovisual conditions,
	evident as early as 10 ms after acoustic onset. Phase-locking to
	stimulus periodicity, which likely underlies perception of pitch,
	was enhanced in musicians and strongly correlated with length of
	musical practice. In addition, viewing videos of speech (lip-reading)
	and music (instrument being played) enhanced temporal and frequency
	encoding in the auditory brainstem, particularly in musicians. These
	findings demonstrate practice-related changes in the early sensory
	encoding of auditory and audiovisual information.</abstract>
		<issn>0027-8424</issn>
		<doi>10.1073/pnas.0701498104</doi>
		<title>Musicians have enhanced subcortical auditory and audiovisual processing
	of speech and music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2eab854c99f7bbaa375e84f29f0c19d14/yevb0</id>
		<tags>music,musicality,neuro,perception</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Journal of Cognitive Neuroscience</journal>
		<year>2000</year>
		<url>http://www.mitpressjournals.org/doi/abs/10.1162/089892900562183</url>
		<author>Stefan Koelsch</author>
		<author>Tomas Gunter</author>
		<author>Angela D. Friederici</author>
		<author>Erich Schröger</author>
		<authors>
			<first>Stefan</first>
		</authors>
		<authors>
			<last>Koelsch</last>
		</authors>
		<authors>
			<first>Tomas</first>
		</authors>
		<authors>
			<last>Gunter</last>
		</authors>
		<authors>
			<first>Angela D.</first>
		</authors>
		<authors>
			<last>Friederici</last>
		</authors>
		<authors>
			<first>Erich</first>
		</authors>
		<authors>
			<last>Schröger</last>
		</authors>
		<volume>12</volume>
		<number>3</number>
		<pages>520--541</pages>
		<issn>0898-929X</issn>
		<doi>10.1162/089892900562183</doi>
		<title>Brain indices of music processing: "Nonmusicians" are musical</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bd9bdf6b99b5425b6453dbd3309b2fc2/yevb0</id>
		<tags>music,neuro,perception,scale</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Music Perception</journal>
		<publisher>University of California Press Journals Division, 2000 Center Street,
	\#303, Berkeley, CA 94704-1223 USA</publisher>
		<year>2007</year>
		<url>http://caliber.ucpress.net/doi/abs/10.1525/mp.2007.24.3.281</url>
		<author>Kaisu I. Krohn</author>
		<author>Elvira Brattico</author>
		<author>Vesa Välimäki</author>
		<author>Mari Tervaniemi</author>
		<authors>
			<first>Kaisu I.</first>
		</authors>
		<authors>
			<last>Krohn</last>
		</authors>
		<authors>
			<first>Elvira</first>
		</authors>
		<authors>
			<last>Brattico</last>
		</authors>
		<authors>
			<first>Vesa</first>
		</authors>
		<authors>
			<last>Välimäki</last>
		</authors>
		<authors>
			<first>Mari</first>
		</authors>
		<authors>
			<last>Tervaniemi</last>
		</authors>
		<volume>24</volume>
		<number>3</number>
		<pages>281--296</pages>
		<issn>0730-7829</issn>
		<title>Neural representations of the hierarchical scale pitch structure</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/205b49a3f5af0ea65b71e4d63fb60876d/yevb0</id>
		<tags>physiology,Auditory</tags>
		<tags>Potentials,Female,Humans,Male,Music,Neuronal</tags>
		<tags>Acoustic</tags>
		<tags>anatomy</tags>
		<tags>histology,Auditory</tags>
		<tags>Cortex,Auditory</tags>
		<tags>Plasticity:</tags>
		<tags>physiology,Neuropsychological</tags>
		<tags>Stimulation,Adolescent,Adult,Audiometry,Auditory,Auditory</tags>
		<tags>\&</tags>
		<tags>Cortex:</tags>
		<tags>physiology,Auditory:</tags>
		<tags>Perception,Auditory</tags>
		<tags>Perception:</tags>
		<tags>Tests,Time</tags>
		<tags>Factors,music,musicality,neuro,perception</tags>
		<tags>Plasticity,Neuronal</tags>
		<tags>physiology,Brain</tags>
		<tags>Mapping,Electroencephalography,Evoked</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Neuroscience Letters</journal>
		<year>2001</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/11445279</url>
		<author>J Rüsseler</author>
		<author>Eckart O Altenmüller</author>
		<author>W Nager</author>
		<author>C Kohlmetz</author>
		<author>Thomas F Münte</author>
		<authors>
			<first>J</first>
		</authors>
		<authors>
			<last>Rüsseler</last>
		</authors>
		<authors>
			<first>Eckart O</first>
		</authors>
		<authors>
			<last>Altenmüller</last>
		</authors>
		<authors>
			<first>W</first>
		</authors>
		<authors>
			<last>Nager</last>
		</authors>
		<authors>
			<first>C</first>
		</authors>
		<authors>
			<last>Kohlmetz</last>
		</authors>
		<authors>
			<first>Thomas F</first>
		</authors>
		<authors>
			<last>Münte</last>
		</authors>
		<volume>308</volume>
		<number>1</number>
		<pages>33--6</pages>
		<abstract>The mismatch negativity (MMN) component of the auditory event-related
	brain potential reflects the automatic detection of sound change.
	MMN to occasionally omitted sounds in a tone series can be used to
	investigate the time course of temporal integration in the acoustic
	system. We used MMN to study differences in temporal integration
	in musicians and non-musicians. In experiment 1, occasionally omitted
	'sounds' in an otherwise regular tone series evoked a reliable MMN
	at interstimulus intervals (SOAs) of 100, 120, 180 and 220 ms in
	musicians. In non-musicians, MMN was smaller/absent in the 180 and
	220 ms SOAs, respectively. In experiment 2, deviance of a tone was
	induced by presenting tones at a shorter SOA (100 or 130 ms) compared
	to the standard stimulus (150 ms). Musicians showed a reliable MMN
	for both deviant SOAs whereas non-musicians showed an MMN only for
	tones presented 50 ms prior to a standard tone (SOA 100 ms). These
	results indicate that the temporal window of integration seems to
	be longer and more precise in musicians compared to musical laypersons
	and that long-term training is reflected in changes in neural activity.</abstract>
		<issn>0304-3940</issn>
		<title>Event-related brain potentials to sound omissions differ in musicians
	and non-musicians.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/261efec10138c501210b185144730ca90/yevb0</id>
		<tags>music,scale,tonality</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>The cognitive neuroscience of music</booktitle>
		<publisher>Oxford University Press</publisher>
		<address>New York</address>
		<year>2003</year>
		<url></url>
		<author>Carol L Krumhansl</author>
		<author>Petri Toiviainen</author>
		<authors>
			<first>Carol L</first>
		</authors>
		<authors>
			<last>Krumhansl</last>
		</authors>
		<authors>
			<first>Petri</first>
		</authors>
		<authors>
			<last>Toiviainen</last>
		</authors>
		<editor>Isabelle Peretz</editor>
		<editor>Robert J. Zatorre</editor>
		<editors>
			<first>Petri</first>
		</editors>
		<editors>
			<last>Toiviainen</last>
		</editors>
		<editors>
			<first>Petri</first>
		</editors>
		<editors>
			<last>Toiviainen</last>
		</editors>
		<pages>95--108</pages>
		<title>Tonal cognition</title>
		<pubtype>inbook</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25f4f1d473a931da80a32162683dc22b1/yevb0</id>
		<tags>Stimulation:</tags>
		<tags>physiology,Time</tags>
		<tags>Stimulation,Acoustic</tags>
		<tags>methods,Evoked</tags>
		<tags>Laterality:</tags>
		<tags>Acoustic</tags>
		<tags>physiology,Humans,Language,Male,Music,Pitch</tags>
		<tags>Discrimination:</tags>
		<tags>(Psychology),Transfer</tags>
		<tags>Discrimination,Pitch</tags>
		<tags>physiology,acquisition,music,musicality,neuro,pitch</tags>
		<tags>(Psychology):</tags>
		<tags>Factors,Transfer</tags>
		<tags>Potentials,Evoked</tags>
		<tags>Mapping,Child,Electroencephalography,Electroencephalography:</tags>
		<tags>of</tags>
		<tags>Variance,Brain</tags>
		<tags>physiology,Female,Functional</tags>
		<tags>Potentials:</tags>
		<tags>Time,Reaction</tags>
		<tags>methods,Analysis</tags>
		<tags>Time:</tags>
		<tags>Laterality,Functional</tags>
		<tags>physiology,Reaction</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Journal of cognitive neuroscience</journal>
		<year>2006</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/16494681</url>
		<author>Cyrille Magne</author>
		<author>Daniele Schön</author>
		<author>Mireille Besson</author>
		<authors>
			<first>Cyrille</first>
		</authors>
		<authors>
			<last>Magne</last>
		</authors>
		<authors>
			<first>Daniele</first>
		</authors>
		<authors>
			<last>Schön</last>
		</authors>
		<authors>
			<first>Mireille</first>
		</authors>
		<authors>
			<last>Besson</last>
		</authors>
		<volume>18</volume>
		<number>2</number>
		<pages>199--211</pages>
		<abstract>The idea that extensive musical training can influence processing
	in cognitive domains other than music has received considerable attention
	from the educational system and the media. Here we analyzed behavioral
	data and recorded event-related brain potentials (ERPs) from 8-year-old
	children to test the hypothesis that musical training facilitates
	pitch processing not only in music but also in language. We used
	a parametric manipulation of pitch so that the final notes or words
	of musical phrases or sentences were congruous, weakly incongruous,
	or strongly incongruous. Musician children outperformed nonmusician
	children in the detection of the weak incongruity in both music and
	language. Moreover, the greatest differences in the ERPs of musician
	and nonmusician children were also found for the weak incongruity:
	whereas for musician children, early negative components developed
	in music and late positive components in language, no such components
	were found for nonmusician children. Finally, comparison of these
	results with previous ones from adults suggests that some aspects
	of pitch processing are in effect earlier in music than in language.
	Thus, the present results reveal positive transfer effects between
	cognitive domains and shed light on the time course and neural basis
	of the development of prosodic and melodic processing.</abstract>
		<issn>0898-929X</issn>
		<doi>10.1162/089892906775783660</doi>
		<title>Musician children detect pitch violations in both music and language
	better than nonmusician children: behavioral and electrophysiological
	approaches.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f2982ec6e1fbdc3d83f30e991d0042a3/yevb0</id>
		<tags>evolution,music</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>The cognitive neuroscience of music</booktitle>
		<publisher>Oxford University Press</publisher>
		<address>New York</address>
		<year>2003</year>
		<url></url>
		<author>Ian Cross</author>
		<authors>
			<first>Ian</first>
		</authors>
		<authors>
			<last>Cross</last>
		</authors>
		<editor>Isabelle Peretz</editor>
		<editor>Robert J. Zatorre</editor>
		<editors>
			<first>Ian</first>
		</editors>
		<editors>
			<last>Cross</last>
		</editors>
		<editors>
			<first>Ian</first>
		</editors>
		<editors>
			<last>Cross</last>
		</editors>
		<pages>42--56</pages>
		<title>Music, cognition, culture, and evolution</title>
		<pubtype>inbook</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fccda89045c7c7cb2e7bcbff7081819f/yevb0</id>
		<tags>Aged,Music,Music:</tags>
		<tags>over,Adult,Africa,Aged,Auditory</tags>
		<tags>Spectrography</tags>
		<tags>psychology,Neuropsychological</tags>
		<tags>Tests,Recognition</tags>
		<tags>(Psychology):</tags>
		<tags>Perception,Auditory</tags>
		<tags>Perception:</tags>
		<tags>physiology,Sound</tags>
		<tags>and</tags>
		<tags>80</tags>
		<tags>(Psychology),Recognition</tags>
		<tags>physiology,Culture,Emotions,Female,Humans,Male,Middle</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Current Biology</journal>
		<year>2009</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/19303300</url>
		<author>Thomas Fritz</author>
		<author>Angela D. Friederici</author>
		<author>Sebastian Jentschke</author>
		<author>Nathalie Gosselin</author>
		<author>Daniela Sammler</author>
		<author>Isabelle Peretz</author>
		<author>Robert Turner</author>
		<author>Stefan Koelsch</author>
		<authors>
			<first>Thomas</first>
		</authors>
		<authors>
			<last>Fritz</last>
		</authors>
		<authors>
			<first>Angela D.</first>
		</authors>
		<authors>
			<last>Friederici</last>
		</authors>
		<authors>
			<first>Sebastian</first>
		</authors>
		<authors>
			<last>Jentschke</last>
		</authors>
		<authors>
			<first>Nathalie</first>
		</authors>
		<authors>
			<last>Gosselin</last>
		</authors>
		<authors>
			<first>Daniela</first>
		</authors>
		<authors>
			<last>Sammler</last>
		</authors>
		<authors>
			<first>Isabelle</first>
		</authors>
		<authors>
			<last>Peretz</last>
		</authors>
		<authors>
			<first>Robert</first>
		</authors>
		<authors>
			<last>Turner</last>
		</authors>
		<authors>
			<first>Stefan</first>
		</authors>
		<authors>
			<last>Koelsch</last>
		</authors>
		<volume>19</volume>
		<number>7</number>
		<pages>573--6</pages>
		<abstract>It has long been debated which aspects of music perception are universal
	and which are developed only after exposure to a specific musical
	culture. Here, we report a crosscultural study with participants
	from a native African population (Mafa) and Western participants,
	with both groups being naive to the music of the other respective
	culture. Experiment 1 investigated the ability to recognize three
	basic emotions (happy, sad, scared/fearful) expressed in Western
	music. Results show that the Mafas recognized happy, sad, and scared/fearful
	Western music excerpts above chance, indicating that the expression
	of these basic emotions in Western music can be recognized universally.
	Experiment 2 examined how a spectral manipulation of original, naturalistic
	music affects the perceived pleasantness of music in Western as well
	as in Mafa listeners. The spectral manipulation modified, among other
	factors, the sensory dissonance of the music. The data show that
	both groups preferred original Western music and also original Mafa
	music over their spectrally manipulated versions. It is likely that
	the sensory dissonance produced by the spectral manipulation was
	at least partly responsible for this effect, suggesting that consonance
	and permanent sensory dissonance universally influence the perceived
	pleasantness of music.</abstract>
		<issn>1879-0445</issn>
		<doi>10.1016/j.cub.2009.02.058</doi>
		<title>Universal recognition of three basic emotions in music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/276e15bce3025343cddfd18a5b813dc90/yevb0</id>
		<tags>Auditory,Brain,Brain:</tags>
		<tags>Potentials,Humans,Music,music,musicality,neuro,syntax</tags>
		<tags>physiology,Electroencephalography,Evoked</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Neuroscience Letters</journal>
		<year>1994</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/8028758</url>
		<author>Mireille Besson</author>
		<author>F. Faita</author>
		<author>J. Requin</author>
		<authors>
			<first>Mireille</first>
		</authors>
		<authors>
			<last>Besson</last>
		</authors>
		<authors>
			<first>F.</first>
		</authors>
		<authors>
			<last>Faita</last>
		</authors>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Requin</last>
		</authors>
		<volume>168</volume>
		<number>1-2</number>
		<pages>101--105</pages>
		<abstract>Musicians and non-musicians were presented with short musical phrases
	that were either selected from the classical musical repertoire or
	composed for the experiment. The phrases terminated either in a congruous
	or a 'harmonically', 'melodically', or 'rhythmically' incongruous
	note. The brain waves produced by the end-notes differed greatly
	between musicians and non-musicians, and as a function of the subject's
	familiarity with the melodies and the type of incongruity. The timing
	of these brain waves revealed that musicians are faster than non-musicians
	in detecting incongruities. This study provides further neurophysiological
	evidence concerning the mechanisms underlying music perception and
	the differences between musical and linguistic processing.</abstract>
		<issn>0304-3940</issn>
		<title>Brain waves associated with musical incongruities differ for musicians
	and non-musicians</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23a3dbe9b18448c3026e9e84622074a69/yevb0</id>
		<tags>exp\'{e}rimentale,Experimental</tags>
		<tags>study,Grammaire</tags>
		<tags>experimental,Etude</tags>
		<tags>artificielle,Hombre,Homme,Human,Implicit</tags>
		<tags>implicite,Aprendizaje</tags>
		<tags>Acquisition</tags>
		<tags>learning,M2,Music,Musique,M\'{u}sica,Proceso</tags>
		<tags>acquisition,Sello,Timbre,acquisition,perception,timbre</tags>
		<tags>impl\'{\i}cito,Cognici\'{o}n,Cognition,Estudio</tags>
		<tags>adquisici\'{o}n,Processus</tags>
		<tags>process,Apprentissage</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Current Psychology of Cognition</journal>
		<publisher>Association pour la diffusion des recherches en sciences cognitives</publisher>
		<year>1998</year>
		<url>http://cat.inist.fr/?aModele=afficheN\&cpsidt=2321612</url>
		<author>Emmanuel Bigand</author>
		<author>P. Perruchet</author>
		<author>M. Boyer</author>
		<authors>
			<first>Emmanuel</first>
		</authors>
		<authors>
			<last>Bigand</last>
		</authors>
		<authors>
			<first>P.</first>
		</authors>
		<authors>
			<last>Perruchet</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Boyer</last>
		</authors>
		<volume>17</volume>
		<number>3</number>
		<pages>577--600</pages>
		<abstract>This study investigates how the artificial grammars of timbres are
	learned. In Experiment 1, the participants listened to sequences
	of timbres produced using an artificial grammar. They were then asked
	to differentiate between sequences which either did or did not violate
	the grammar. The participants in the explicit condition were informed
	of the existence of rules underlying the sequences. Those in the
	implicit condition received no such information (Reber, 1967). Experiment
	2 addressed the influence of the learning mode on the content of
	the knowledge acquired. At the end of the learning phase, the participants
	were asked to judge the grammaticality of new sequences of timbres
	(test condition) or letters (transfer condition). The results confirmed
	the advantage of the implicit condition over the explicit condition
	but suggest that the knowledge acquired pertained more to surface
	regularities than to abstract rules. The results are discussed within
	the framework of current work on implicit learning and musical cognition.</abstract>
		<issn>0249-9185</issn>
		<title>Implicit learning of an artificial grammar of musical timbres</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d3678f5329ff904c1845ab9688328f9f/yevb0</id>
		<tags>Threshold:</tags>
		<tags>Acoustic</tags>
		<tags>physiology,Dominance,Humans,Learning,Music,Pitch</tags>
		<tags>Discrimination:</tags>
		<tags>Variance,Case-Control</tags>
		<tags>of</tags>
		<tags>Studies,Cerebral,Cerebral:</tags>
		<tags>Discrimination,Pitch</tags>
		<tags>Threshold,Differential</tags>
		<tags>physiology,Differential</tags>
		<tags>physiology,Psychoacoustics,music,musicality,perception,pitch</tags>
		<tags>Stimulation,Adolescent,Adult,Analysis</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Hearing Research</journal>
		<year>2006</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/16839723</url>
		<author>Christophe Micheyl</author>
		<author>Karine Delhommeau</author>
		<author>Xavier Perrot</author>
		<author>Andrew J Oxenham</author>
		<authors>
			<first>Christophe</first>
		</authors>
		<authors>
			<last>Micheyl</last>
		</authors>
		<authors>
			<first>Karine</first>
		</authors>
		<authors>
			<last>Delhommeau</last>
		</authors>
		<authors>
			<first>Xavier</first>
		</authors>
		<authors>
			<last>Perrot</last>
		</authors>
		<authors>
			<first>Andrew J</first>
		</authors>
		<authors>
			<last>Oxenham</last>
		</authors>
		<volume>219</volume>
		<number>1-2</number>
		<pages>36--47</pages>
		<abstract>This study compared the influence of musical and psychoacoustical
	training on auditory pitch discrimination abilities. In a first experiment,
	pitch discrimination thresholds for pure and complex tones were measured
	in 30 classical musicians and 30 non-musicians, none of whom had
	prior psychoacoustical training. The non-musicians' mean thresholds
	were more than six times larger than those of the classical musicians
	initially, and still about four times larger after 2h of training
	using an adaptive two-interval forced-choice procedure; this difference
	is two to three times larger than suggested by previous studies.
	The musicians' thresholds were close to those measured in earlier
	psychoacoustical studies using highly trained listeners, and showed
	little improvement with training; this suggests that classical musical
	training can lead to optimal or nearly optimal pitch discrimination
	performance. A second experiment was performed to determine how much
	additional training was required for the non-musicians to obtain
	thresholds as low as those of the classical musicians from experiment
	1. Eight new non-musicians with no prior training practiced the frequency
	discrimination task for a total of 14 h. It took between 4 and 8h
	of training for their thresholds to become as small as those measured
	in the classical musicians from experiment 1. These findings supplement
	and qualify earlier data in the literature regarding the respective
	influence of musical and psychoacoustical training on pitch discrimination
	performance.</abstract>
		<issn>0378-5955</issn>
		<doi>10.1016/j.heares.2006.05.004</doi>
		<title>Influence of musical and psychoacoustical training on pitch discrimination</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29fce6d947ce0e74395c68b78687a6190/yevb0</id>
		<tags>imported</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>Music, thought and feeling: Understanding the psychology of music</booktitle>
		<publisher>Oxford University Press</publisher>
		<address>New York</address>
		<year>2009</year>
		<url></url>
		<author>William Forde Thompson</author>
		<authors>
			<first>William Forde</first>
		</authors>
		<authors>
			<last>Thompson</last>
		</authors>
		<title>Music, thought and feeling: Understanding the psychology of music</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/298e86c2e941811878ff00ebd1372fb21/yevb0</id>
		<tags>Stimulation:</tags>
		<tags>Stimulation,Acoustic</tags>
		<tags>Variation,Contingent</tags>
		<tags>Acoustic</tags>
		<tags>physiology,Attention,Attention:</tags>
		<tags>Potentials,Female,Humans,Language,Male,Multilingualism,Music,Verbal</tags>
		<tags>Behavior,Verbal</tags>
		<tags>physiology,Electroencephalography,Evoked</tags>
		<tags>Variation:</tags>
		<tags>physiology,Auditory:</tags>
		<tags>Perception,Auditory</tags>
		<tags>Perception:</tags>
		<tags>physiology,Auditory,Auditory</tags>
		<tags>Development,Contingent</tags>
		<tags>Negative</tags>
		<tags>physiology,acquisition,duration,language,music,musicality,neuro,perception,speech</tags>
		<tags>physiology,Child,Child</tags>
		<tags>Behavior:</tags>
		<tags>methods,Aptitude,Aptitude:</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Neuroscience Letters</journal>
		<year>2009</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/19481587</url>
		<author>Riia Milovanov</author>
		<author>Minna Huotilainen</author>
		<author>Paulo A A Esquef</author>
		<author>Paavo Alku</author>
		<author>Vesa Välimäki</author>
		<author>Mari Tervaniemi</author>
		<authors>
			<first>Riia</first>
		</authors>
		<authors>
			<last>Milovanov</last>
		</authors>
		<authors>
			<first>Minna</first>
		</authors>
		<authors>
			<last>Huotilainen</last>
		</authors>
		<authors>
			<first>Paulo A A</first>
		</authors>
		<authors>
			<last>Esquef</last>
		</authors>
		<authors>
			<first>Paavo</first>
		</authors>
		<authors>
			<last>Alku</last>
		</authors>
		<authors>
			<first>Vesa</first>
		</authors>
		<authors>
			<last>Välimäki</last>
		</authors>
		<authors>
			<first>Mari</first>
		</authors>
		<authors>
			<last>Tervaniemi</last>
		</authors>
		<volume>460</volume>
		<number>2</number>
		<pages>161--5</pages>
		<abstract>We examined 10-12-year old elementary school children's ability to
	preattentively process sound durations in music and speech stimuli.
	In total, 40 children had either advanced foreign language production
	skills and higher musical aptitude or less advanced results in both
	musicality and linguistic tests. Event-related potential (ERP) recordings
	of the mismatch negativity (MMN) show that the duration changes in
	musical sounds are more prominently and accurately processed than
	changes in speech sounds. Moreover, children with advanced pronunciation
	and musicality skills displayed enhanced MMNs to duration changes
	in both speech and musical sounds. Thus, our study provides further
	evidence for the claim that musical aptitude and linguistic skills
	are interconnected and the musical features of the stimuli could
	have a preponderant role in preattentive duration processing.</abstract>
		<issn>1872-7972</issn>
		<doi>10.1016/j.neulet.2009.05.063</doi>
		<title>The role of musical aptitude and language skills in preattentive
	duration processing in school-aged children</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26ebe7b28aac644ef574f7002963a1145/yevb0</id>
		<tags>Stimulation:</tags>
		<tags>Stimulation,Acoustic</tags>
		<tags>Acoustic</tags>
		<tags>Perception,Speech</tags>
		<tags>psychology,Psychological</tags>
		<tags>Perception:</tags>
		<tags>physiology,Female,Humans,L2,Language</tags>
		<tags>Potentials,Evoked</tags>
		<tags>Tests,Speech</tags>
		<tags>of</tags>
		<tags>Development,Male,Multilingualism,Music,Music:</tags>
		<tags>Potentials:</tags>
		<tags>methods,Analysis</tags>
		<tags>physiology,Child,Electroencephalography,Evoked</tags>
		<tags>physiology,language,music,musicality,neuro,perception</tags>
		<tags>Variance,Aptitude,Aptitude:</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>2</count>
		<journal>Brain Research</journal>
		<year>2008</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/18182165</url>
		<author>Riia Milovanov</author>
		<author>Minna Huotilainen</author>
		<author>Vesa Välimäki</author>
		<author>Paulo A A Esquef</author>
		<author>Mari Tervaniemi</author>
		<authors>
			<first>Riia</first>
		</authors>
		<authors>
			<last>Milovanov</last>
		</authors>
		<authors>
			<first>Minna</first>
		</authors>
		<authors>
			<last>Huotilainen</last>
		</authors>
		<authors>
			<first>Vesa</first>
		</authors>
		<authors>
			<last>Välimäki</last>
		</authors>
		<authors>
			<first>Paulo A A</first>
		</authors>
		<authors>
			<last>Esquef</last>
		</authors>
		<authors>
			<first>Mari</first>
		</authors>
		<authors>
			<last>Tervaniemi</last>
		</authors>
		<volume>1194</volume>
		<pages>81--9</pages>
		<abstract>The main focus of this study was to examine the relationship between
	musical aptitude and second language pronunciation skills. We investigated
	whether children with superior performance in foreign language production
	represent musical sound features more readily in the preattentive
	level of neural processing compared with children with less-advanced
	production skills. Sound processing accuracy was examined in elementary
	school children by means of event-related potential (ERP) recordings
	and behavioral measures. Children with good linguistic skills had
	better musical skills as measured by the Seashore musicality test
	than children with less accurate linguistic skills. The ERP data
	accompany the results of the behavioral tests: children with good
	linguistic skills showed more pronounced sound-change evoked activation
	with the music stimuli than children with less accurate linguistic
	skills. Taken together, the results imply that musical and linguistic
	skills could partly be based on shared neural mechanisms.</abstract>
		<issn>0006-8993</issn>
		<doi>10.1016/j.brainres.2007.11.042</doi>
		<title>Musical aptitude and second language pronunciation skills in school-aged
	children: Neural and behavioral evidence</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20fa3b577a469d38aaa088c4b0b3969c8/andymilne</id>
		<tags>imported</tags>
		<description></description>
		<date>2011-12-08 13:36:28</date>
		<count>1</count>
		<address>Centre for Music and Science, University of Cambridge, Cambridge, UK</address>
		<year>2010</year>
		<url></url>
		<author>Andrew J. Milne</author>
		<authors>
			<first>Andrew J.</first>
		</authors>
		<authors>
			<last>Milne</last>
		</authors>
		<abstract>From the seventeenth century to the present day, tonal harmonic music has had a number of invariant properties: specific chord progressions (cadences) that induce a sense of closure; the asymmetrical privileging of certain progressions; the degree of fit between pairs of successively played tones or chords; the privileging of tertial harmony and the major and minor scales.

The most widely accepted explanation (e.g., Bharucha (1987), Krumhansl (1990), Lerdahl (2001)) has been that this is due to a process of enculturation: frequently occurring musical patterns are learned by listeners, some of whom become composers and replicate the same patterns, which go on to influence the next ``generation'' of composers, and so on. Some contemporary researchers (e.g., Parncutt, Milne (2009), Large (in press)) have argued that these are circular arguments, and have proposed various psychoacoustic, or neural, processes and constraints that shape tonal harmonic music into the form it has actually taken.

In this presentation, I discuss some of the broader music theoretical implications of my recently developed psychoacoustic model of harmonic cadences (which has had encouraging experimental support (Milne, 2009)). The core of the model is two different psychoacoustically-derived measures of pitch-based distance between chords (one modelling ``fit'', the other ``voice-leading distance''), and the interaction of these two distances to model the feelings of activity, expectation, and resolution induced by certain chord progressions (notably cadences). When a played pair of chords have a poorer fit than an un-played comparison pair, that is also voice-leading-close, it is reasonable to assume the played pair is heard as an alteration of the comparison pair. This is similar to how a harmonically dissonant interval (e.g., the tritone B-F) is likely to be heard as an alteration of a voice-leading-close consonant interval (e.g., the perfect fourth B-E, or the major third C-E).

I explore the extent to which the model can predict the familiar tonal cadences described in music theory (including those containing tritone substitutions), and the asymmetries that are so characteristic of tonal harmony. I also compare and contrast the model with Riemann's functional theory, and show how it may be able to shed light upon the privileged status of the major and minor scales (over the modes), and the dependence of tonality upon triadic harmony.</abstract>
		<title>Tonal music theory: A psychoacoustical explanation?</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bca750c69ecff99514c09316d85c0af5/maxirichter</id>
		<tags>music</tags>
		<tags>metadata</tags>
		<tags>webdevelopment</tags>
		<tags>id3</tags>
		<tags>audio</tags>
		<tags>nodejs</tags>
		<tags>mp3</tags>
		<description></description>
		<date>2014-05-23 07:35:29</date>
		<count>1</count>
		<year>2014</year>
		<url>https://github.com/leetreveil/musicmetadata/</url>
		<author>Lee Treveil</author>
		<authors>
			<first>Lee</first>
		</authors>
		<authors>
			<last>Treveil</last>
		</authors>
		<title>leetreveil/musicmetadata</title>
		<pubtype>electronic</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b8420d6996fe31ade053168ffceb8023/ks-plugin-devel</id>
		<tags>Mathematik</tags>
		<tags>Musiktheorie</tags>
		<tags>MaMu</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2013-02-02 14:42:54</date>
		<count>3</count>
		<publisher>Oxford University Press</publisher>
		<address>New York</address>
		<year>2007</year>
		<url>http://www.ingentaconnect.com/content/oso/5481464/2007/00000001/00000001/art00000</url>
		<author>David Lewin</author>
		<authors>
			<first>David</first>
		</authors>
		<authors>
			<last>Lewin</last>
		</authors>
		<pages>i-258(259)</pages>
		<abstract>This book is recognized as the seminal work paving the way for current studies in mathematical and systematic approaches to music analysis. The author, one of the 20th century's most prominent figures in music theory, pushes the boundaries of the study of pitch-structure beyond its conception as a static system for classifying and inter-relating chords and sets. Known by most music theorists as &#8220;GMIT&#8221;, the book is by far the most significant contribution to the field of systematic music theory in the last half-century, generating the framework for the &#8220;transformational theory&#8221; movement. Appearing almost twenty years after GMIT's initial publication, this Oxford University Press edition features a previously unpublished preface by the author, as well as a foreword by Edward Gollin contextualizing the work's significance for the current field of music theory.</abstract>
		<isbn>9780199717842 0199717842</isbn>
		<doi>10.1093/acprof:oso/9780195317138.001.0001</doi>
		<title>Generalized Musical Intervals and Transformations</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/244b26899136055207f575931773b258f/ks-plugin-devel</id>
		<tags>MaMu</tags>
		<description></description>
		<date>2013-02-02 14:39:55</date>
		<count>2</count>
		<journal>Memoirs of the Fourth International Seminar on Mathematical Music Theory</journal>
		<year>2011</year>
		<url>http://www.smm.org.mx/smm/PEMemorias</url>
		<author>Julio Estrada</author>
		<authors>
			<first>Julio</first>
		</authors>
		<authors>
			<last>Estrada</last>
		</authors>
		<volume>4</volume>
		<pages>113–145</pages>
		<title>La teoría d1, MúSIIC-Win y algunas aplicaciones al análisis musical: Seis piezas para piano, de Arnold Schoenberg</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/230d55243304c1b35a7377822d3ce1a51/ks-plugin-devel</id>
		<tags>Tonnetz</tags>
		<tags>Musiktheorie</tags>
		<tags>MaMu</tags>
		<description></description>
		<date>2013-02-02 14:41:40</date>
		<count>2</count>
		<booktitle>Mathematics and Computation in Music</booktitle>
		<series>Communications in Computer and Information Science</series>
		<publisher>Springer Berlin Heidelberg</publisher>
		<year>2009</year>
		<url>http://dmitri.tymoczko.com/publications.html</url>
		<author>Dmitri Tymoczko</author>
		<authors>
			<first>Dmitri</first>
		</authors>
		<authors>
			<last>Tymoczko</last>
		</authors>
		<editor>Elaine Chew</editor>
		<editor>Adrian Childs</editor>
		<editor>Ching-Hua Chuan</editor>
		<editors>
			<first>Dmitri</first>
		</editors>
		<editors>
			<last>Tymoczko</last>
		</editors>
		<editors>
			<first>Dmitri</first>
		</editors>
		<editors>
			<last>Tymoczko</last>
		</editors>
		<editors>
			<first>Dmitri</first>
		</editors>
		<editors>
			<last>Tymoczko</last>
		</editors>
		<volume>38</volume>
		<pages>258-272</pages>
		<abstract>This paper considers three conceptions of musical distance (or inverse similarity ) that produce three different musico-geometrical spaces: the first, based on voice leading, yields a collection of continuous quotient spaces or orbifolds; the second, based on acoustics, gives rise to the Tonnetz and related tuning lattices ; while the third, based on the total interval content of a group of notes, generates a six-dimensional quality space first described by Ian Quinn. I will show that although these three measures are in principle quite distinct, they are in practice surprisingly interrelated. This produces the challenge of determining which model is appropriate to a given music-theoretical circumstance. Since the different models can yield comparable results, unwary theorists could potentially find themselves using one type of structure (such as a tuning lattice) to investigate properties more perspicuously represented by another (for instance, voice-leading relationships).</abstract>
		<isbn>978-3-642-02394-1</isbn>
		<doi>10.1007/978-3-642-02394-1_24</doi>
		<title>Three Conceptions of Musical Distance</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2861721ec75418992d3b389754ede5009/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#LeeKK13</url>
		<author>Seungha Lee</author>
		<author>Yangwoo Kim</author>
		<author>Woongsup Kim</author>
		<authors>
			<first>Seungha</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Yangwoo</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Woongsup</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Woongsup</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Woongsup</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Woongsup</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Woongsup</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<volume>274</volume>
		<pages>377-383</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>A Peer-to-Peer Based Job Distribution Model Using Dynamic Network Structure Transformation.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c1677621e3afc84da7347681d9624317/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#TsaiHC13a</url>
		<author>Chun-Wei Tsai</author>
		<author>Wei-Cheng Huang</author>
		<author>Ming-Chao Chiang</author>
		<authors>
			<first>Chun-Wei</first>
		</authors>
		<authors>
			<last>Tsai</last>
		</authors>
		<authors>
			<first>Wei-Cheng</first>
		</authors>
		<authors>
			<last>Huang</last>
		</authors>
		<authors>
			<first>Ming-Chao</first>
		</authors>
		<authors>
			<last>Chiang</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Ming-Chao</first>
		</editors>
		<editors>
			<last>Chiang</last>
		</editors>
		<editors>
			<first>Ming-Chao</first>
		</editors>
		<editors>
			<last>Chiang</last>
		</editors>
		<editors>
			<first>Ming-Chao</first>
		</editors>
		<editors>
			<last>Chiang</last>
		</editors>
		<editors>
			<first>Ming-Chao</first>
		</editors>
		<editors>
			<last>Chiang</last>
		</editors>
		<volume>274</volume>
		<pages>629-636</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Recent Development of Metaheuristics for Clustering.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2080a84767484a72d894360372f31fe4d/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#KangSKJC13</url>
		<author>Daehyun Kang</author>
		<author>Jongsoo Sohn</author>
		<author>Kyunglag Kwon</author>
		<author>Bok-Gyu Joo</author>
		<author>In-Jeong Chung</author>
		<authors>
			<first>Daehyun</first>
		</authors>
		<authors>
			<last>Kang</last>
		</authors>
		<authors>
			<first>Jongsoo</first>
		</authors>
		<authors>
			<last>Sohn</last>
		</authors>
		<authors>
			<first>Kyunglag</first>
		</authors>
		<authors>
			<last>Kwon</last>
		</authors>
		<authors>
			<first>Bok-Gyu</first>
		</authors>
		<authors>
			<last>Joo</last>
		</authors>
		<authors>
			<first>In-Jeong</first>
		</authors>
		<authors>
			<last>Chung</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>In-Jeong</first>
		</editors>
		<editors>
			<last>Chung</last>
		</editors>
		<editors>
			<first>In-Jeong</first>
		</editors>
		<editors>
			<last>Chung</last>
		</editors>
		<editors>
			<first>In-Jeong</first>
		</editors>
		<editors>
			<last>Chung</last>
		</editors>
		<editors>
			<first>In-Jeong</first>
		</editors>
		<editors>
			<last>Chung</last>
		</editors>
		<volume>274</volume>
		<pages>143-149</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>An Intelligent Dynamic Context-Aware System Using Fuzzy Semantic Language.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/269d1f3e8b5ae67584676b35e67ed7de6/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#HwangJKGSSJXZ13</url>
		<author>Myunggwon Hwang</author>
		<author>Do-Heon Jeong</author>
		<author>Jinhyung Kim</author>
		<author>Jangwon Gim</author>
		<author>Sa-Kwang Song</author>
		<author>Mazhar Sajjad</author>
		<author>Hanmin Jung</author>
		<author>Shuo Xu</author>
		<author>Lijun Zhu</author>
		<authors>
			<first>Myunggwon</first>
		</authors>
		<authors>
			<last>Hwang</last>
		</authors>
		<authors>
			<first>Do-Heon</first>
		</authors>
		<authors>
			<last>Jeong</last>
		</authors>
		<authors>
			<first>Jinhyung</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Jangwon</first>
		</authors>
		<authors>
			<last>Gim</last>
		</authors>
		<authors>
			<first>Sa-Kwang</first>
		</authors>
		<authors>
			<last>Song</last>
		</authors>
		<authors>
			<first>Mazhar</first>
		</authors>
		<authors>
			<last>Sajjad</last>
		</authors>
		<authors>
			<first>Hanmin</first>
		</authors>
		<authors>
			<last>Jung</last>
		</authors>
		<authors>
			<first>Shuo</first>
		</authors>
		<authors>
			<last>Xu</last>
		</authors>
		<authors>
			<first>Lijun</first>
		</authors>
		<authors>
			<last>Zhu</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Lijun</first>
		</editors>
		<editors>
			<last>Zhu</last>
		</editors>
		<editors>
			<first>Lijun</first>
		</editors>
		<editors>
			<last>Zhu</last>
		</editors>
		<editors>
			<first>Lijun</first>
		</editors>
		<editors>
			<last>Zhu</last>
		</editors>
		<editors>
			<first>Lijun</first>
		</editors>
		<editors>
			<last>Zhu</last>
		</editors>
		<volume>274</volume>
		<pages>191-195</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Application for Temporal Analysis of Scientific Technology Information.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2caee7cc54806498a057199fd17715d65/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#SongSK13</url>
		<author>Yeongkil Song</author>
		<author>Junsoo Shin</author>
		<author>Harksoo Kim</author>
		<authors>
			<first>Yeongkil</first>
		</authors>
		<authors>
			<last>Song</last>
		</authors>
		<authors>
			<first>Junsoo</first>
		</authors>
		<authors>
			<last>Shin</last>
		</authors>
		<authors>
			<first>Harksoo</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Harksoo</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Harksoo</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Harksoo</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Harksoo</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<volume>274</volume>
		<pages>219-223</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Mathematical Document Retrieval Model Using Structural Information of Equations in Pseudo-documents.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29c79116c7d02669c0f13946b1ff4aa2a/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#ZhuSG13</url>
		<author>Lijun Zhu</author>
		<author>Chen Shi</author>
		<author>Jianfeng Guo</author>
		<authors>
			<first>Lijun</first>
		</authors>
		<authors>
			<last>Zhu</last>
		</authors>
		<authors>
			<first>Chen</first>
		</authors>
		<authors>
			<last>Shi</last>
		</authors>
		<authors>
			<first>Jianfeng</first>
		</authors>
		<authors>
			<last>Guo</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Jianfeng</first>
		</editors>
		<editors>
			<last>Guo</last>
		</editors>
		<editors>
			<first>Jianfeng</first>
		</editors>
		<editors>
			<last>Guo</last>
		</editors>
		<editors>
			<first>Jianfeng</first>
		</editors>
		<editors>
			<last>Guo</last>
		</editors>
		<editors>
			<first>Jianfeng</first>
		</editors>
		<editors>
			<last>Guo</last>
		</editors>
		<volume>274</volume>
		<pages>231-237</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Unified Concept Space and Mapping Discovery Algorithm for Heterogeneous Knowledge Systems.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/241bea8ee1d988ae9ad52254d2969e73e/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-08-20 00:00:00</date>
		<count>1</count>
		<series>Cambridge Introductions to Music</series>
		<publisher>Cambridge University Press</publisher>
		<year>2013</year>
		<url></url>
		<author>Nick Collins</author>
		<author>Margaret Schedel</author>
		<author>Scott Wilson</author>
		<authors>
			<first>Nick</first>
		</authors>
		<authors>
			<last>Collins</last>
		</authors>
		<authors>
			<first>Margaret</first>
		</authors>
		<authors>
			<last>Schedel</last>
		</authors>
		<authors>
			<first>Scott</first>
		</authors>
		<authors>
			<last>Wilson</last>
		</authors>
		<pages>I-XI, 1-227</pages>
		<isbn>978-1-107-64817-3</isbn>
		<title>Electronic Music.</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/233851bd7419baf48e59ba6f43cc1318c/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#Haghighi13</url>
		<author>Mo Haghighi</author>
		<authors>
			<first>Mo</first>
		</authors>
		<authors>
			<last>Haghighi</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Mo</first>
		</editors>
		<editors>
			<last>Haghighi</last>
		</editors>
		<editors>
			<first>Mo</first>
		</editors>
		<editors>
			<last>Haghighi</last>
		</editors>
		<editors>
			<first>Mo</first>
		</editors>
		<editors>
			<last>Haghighi</last>
		</editors>
		<editors>
			<first>Mo</first>
		</editors>
		<editors>
			<last>Haghighi</last>
		</editors>
		<volume>274</volume>
		<pages>173-178</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Market-Based Resource Allocation for Energy-Efficient Execution of Multiple Concurrent Applications in Wireless Sensor Networks.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2257bf5c1a61c92311fca2974dc696dca/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#KimKPL13</url>
		<author>Yilip Kim</author>
		<author>Jeongyeun Kim</author>
		<author>Namje Park</author>
		<author>Hyungkyu Lee</author>
		<authors>
			<first>Yilip</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Jeongyeun</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Namje</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<authors>
			<first>Hyungkyu</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Hyungkyu</first>
		</editors>
		<editors>
			<last>Lee</last>
		</editors>
		<editors>
			<first>Hyungkyu</first>
		</editors>
		<editors>
			<last>Lee</last>
		</editors>
		<editors>
			<first>Hyungkyu</first>
		</editors>
		<editors>
			<last>Lee</last>
		</editors>
		<editors>
			<first>Hyungkyu</first>
		</editors>
		<editors>
			<last>Lee</last>
		</editors>
		<volume>274</volume>
		<pages>515-520</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Development of STEAM Education Program Centering on Non-traditional Energy.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/280dcb562c07e36dc1efc15fd90209d21/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#EnglmeierAMMP13</url>
		<author>Kurt Englmeier</author>
		<author>John Atkinson</author>
		<author>Josiane Mothe</author>
		<author>Fionn Murtagh</author>
		<author>Javier Pereira</author>
		<authors>
			<first>Kurt</first>
		</authors>
		<authors>
			<last>Englmeier</last>
		</authors>
		<authors>
			<first>John</first>
		</authors>
		<authors>
			<last>Atkinson</last>
		</authors>
		<authors>
			<first>Josiane</first>
		</authors>
		<authors>
			<last>Mothe</last>
		</authors>
		<authors>
			<first>Fionn</first>
		</authors>
		<authors>
			<last>Murtagh</last>
		</authors>
		<authors>
			<first>Javier</first>
		</authors>
		<authors>
			<last>Pereira</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Javier</first>
		</editors>
		<editors>
			<last>Pereira</last>
		</editors>
		<editors>
			<first>Javier</first>
		</editors>
		<editors>
			<last>Pereira</last>
		</editors>
		<editors>
			<first>Javier</first>
		</editors>
		<editors>
			<last>Pereira</last>
		</editors>
		<editors>
			<first>Javier</first>
		</editors>
		<editors>
			<last>Pereira</last>
		</editors>
		<volume>274</volume>
		<pages>421-432</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>A Context Description Language for Medical Information Systems.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/235d4f6b3ce42ec4665eb0254b8325a88/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#ChoPKMJ13</url>
		<author>Yongwon Cho</author>
		<author>Muwook Pyeon</author>
		<author>Daesung Kim</author>
		<author>Sujung Moon</author>
		<author>Illwoong Jang</author>
		<authors>
			<first>Yongwon</first>
		</authors>
		<authors>
			<last>Cho</last>
		</authors>
		<authors>
			<first>Muwook</first>
		</authors>
		<authors>
			<last>Pyeon</last>
		</authors>
		<authors>
			<first>Daesung</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Sujung</first>
		</authors>
		<authors>
			<last>Moon</last>
		</authors>
		<authors>
			<first>Illwoong</first>
		</authors>
		<authors>
			<last>Jang</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Illwoong</first>
		</editors>
		<editors>
			<last>Jang</last>
		</editors>
		<editors>
			<first>Illwoong</first>
		</editors>
		<editors>
			<last>Jang</last>
		</editors>
		<editors>
			<first>Illwoong</first>
		</editors>
		<editors>
			<last>Jang</last>
		</editors>
		<editors>
			<first>Illwoong</first>
		</editors>
		<editors>
			<last>Jang</last>
		</editors>
		<volume>274</volume>
		<pages>371-376</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Video Image Based Hyper Live Spatial Data Construction.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2353a4e9398d9c7de69d46d1c08dd0ff5/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#ShinUCJXZ13</url>
		<author>Sungho Shin</author>
		<author>Jung-Ho Um</author>
		<author>Sung-Pil Choi</author>
		<author>Hanmin Jung</author>
		<author>Shuo Xu</author>
		<author>Lijun Zhu</author>
		<authors>
			<first>Sungho</first>
		</authors>
		<authors>
			<last>Shin</last>
		</authors>
		<authors>
			<first>Jung-Ho</first>
		</authors>
		<authors>
			<last>Um</last>
		</authors>
		<authors>
			<first>Sung-Pil</first>
		</authors>
		<authors>
			<last>Choi</last>
		</authors>
		<authors>
			<first>Hanmin</first>
		</authors>
		<authors>
			<last>Jung</last>
		</authors>
		<authors>
			<first>Shuo</first>
		</authors>
		<authors>
			<last>Xu</last>
		</authors>
		<authors>
			<first>Lijun</first>
		</authors>
		<authors>
			<last>Zhu</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Lijun</first>
		</editors>
		<editors>
			<last>Zhu</last>
		</editors>
		<editors>
			<first>Lijun</first>
		</editors>
		<editors>
			<last>Zhu</last>
		</editors>
		<editors>
			<first>Lijun</first>
		</editors>
		<editors>
			<last>Zhu</last>
		</editors>
		<editors>
			<first>Lijun</first>
		</editors>
		<editors>
			<last>Zhu</last>
		</editors>
		<volume>274</volume>
		<pages>273-277</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>K-Base: Platform to Build the Knowledge Base for an Intelligent Service.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c7044c6cc4cc453f48ca35d7043f3553/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#KimCLCP13</url>
		<author>Haelyeon Kim</author>
		<author>Yeonwoo Chung</author>
		<author>Sungju Lee</author>
		<author>Yongwha Chung</author>
		<author>Daihee Park</author>
		<authors>
			<first>Haelyeon</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Yeonwoo</first>
		</authors>
		<authors>
			<last>Chung</last>
		</authors>
		<authors>
			<first>Sungju</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Yongwha</first>
		</authors>
		<authors>
			<last>Chung</last>
		</authors>
		<authors>
			<first>Daihee</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Daihee</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Daihee</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Daihee</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Daihee</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<volume>274</volume>
		<pages>105-110</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Quality-Workload Tradeoff in Pig Activity Monitoring Application.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20a9d3cb42e94698ff1c9e53d0faa312b/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#SongHPP13</url>
		<author>Yong Song</author>
		<author>Woomin Hwang</author>
		<author>Ki-Woong Park</author>
		<author>Kyu Ho Park</author>
		<authors>
			<first>Yong</first>
		</authors>
		<authors>
			<last>Song</last>
		</authors>
		<authors>
			<first>Woomin</first>
		</authors>
		<authors>
			<last>Hwang</last>
		</authors>
		<authors>
			<first>Ki-Woong</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<authors>
			<first>Kyu Ho</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Kyu Ho</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Kyu Ho</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Kyu Ho</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Kyu Ho</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<volume>274</volume>
		<pages>315-320</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Microscopic Bit-Level Wear-Leveling for NAND Flash Memory.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ed096d46ba9061a5820448930427ca2f/keinstein</id>
		<tags>Physik</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2009-10-23 11:46:41</date>
		<count>2</count>
		<journal>Science & Education</journal>
		<year>2008/04/26/</year>
		<url>http://dx.doi.org/10.1007/s11191-007-9090-x</url>
		<author>Imelda Caleon</author>
		<author>Subramaniam Ramanathan</author>
		<authors>
			<first>Imelda</first>
		</authors>
		<authors>
			<last>Caleon</last>
		</authors>
		<authors>
			<first>Subramaniam</first>
		</authors>
		<authors>
			<last>Ramanathan</last>
		</authors>
		<volume>17</volume>
		<number>4</number>
		<pages>449--456</pages>
		<abstract>Abstract~~This paper presents the early investigations about the nature of sound of the Pythagoreans, and how they started a tradition that remains valid up to present times---the use of numbers in representing natural reality. It will touch on the Pythagorean notion of musical harmony, which was extended to the notion of universal harmony. How the Pythagorean ideas have inspired many great works in physics, such as those of Galileo, Kepler and Newton, will also be presented. In exploring the legacy of Pythagoras to physics and the study of the universe, some valuable insights on the nature of science that can inspire budding physicists are extracted.</abstract>
		<ty>JOUR</ty>
		<doi>10.1007/s11191-007-9090-x</doi>
		<title>From Music to Physics: The Undervalued Legacy of Pythagoras</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25af091e48a9e19b63422574b54b1806c/keinstein</id>
		<tags>Mathematik</tags>
		<tags>Musiktheorie</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2009-10-23 11:46:41</date>
		<count>3</count>
		<journal>Science</journal>
		<year>2007</year>
		<url>http://www.sciencemag.org/cgi/content/abstract/315/5810/330b</url>
		<author>Dave Headlam</author>
		<author>Matthew Brown</author>
		<authors>
			<first>Dave</first>
		</authors>
		<authors>
			<last>Headlam</last>
		</authors>
		<authors>
			<first>Matthew</first>
		</authors>
		<authors>
			<last>Brown</last>
		</authors>
		<volume>315</volume>
		<number>5810</number>
		<pages>330b-</pages>
		<abstract>Tymoczko (Reports, 7 July 2006, p. 72) proposed that the familiar sonorities of Western tonal music cluster around the center of a multidimensional orbifold. However, this is not true for all tonal progressions. When prototypical three-voice cadential progressions by Bach converge on the tonic, the chords migrate from the center to the edge of the orbifold.</abstract>
		<doi>10.1126/science.1134013</doi>
		<eprint>http://www.sciencemag.org/cgi/reprint/315/5810/330b.pdf</eprint>
		<title>Comment on "The Geometry of Musical Chords"</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/255c20657b7f2031786f76ea51d12050c/keinstein</id>
		<tags>Notensatz</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2009-10-23 11:46:41</date>
		<count>2</count>
		<journal>Computing in Musicology</journal>
		<year>2001</year>
		<url></url>
		<author>Holger H. Hoos</author>
		<author>Keith A. Hamel</author>
		<author>Kai Renz</author>
		<author>Jürgen Kilian</author>
		<authors>
			<first>Holger H.</first>
		</authors>
		<authors>
			<last>Hoos</last>
		</authors>
		<authors>
			<first>Keith A.</first>
		</authors>
		<authors>
			<last>Hamel</last>
		</authors>
		<authors>
			<first>Kai</first>
		</authors>
		<authors>
			<last>Renz</last>
		</authors>
		<authors>
			<first>Jürgen</first>
		</authors>
		<authors>
			<last>Kilian</last>
		</authors>
		<volume>12</volume>
		<abstract>GUIDO Music Notation is a novel approach for adequately representing score-level music. Based on a simple, yet powerful and easily extensible formalism, GUIDO is realized as a plain-text, human-readable and platform independent format. The key feature of the underlying design is representational adequacy: simple musical concepts can be expressed in a simple way, while complex musical notions may require more complex representations. GUIDO Music Notation can be used for a broad range of applications, including notation software, composition and analysis systems and tools, music databases, and music on the WWW. In this article, we discuss the motivation for developing GUIDO Music Notation, give an overview of its design and features, and describe its current applications.</abstract>
		<title>Representing Score-Level Music Using the GUIDO Music-Notation Format</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b8420d6996fe31ade053168ffceb8023/keinstein</id>
		<tags>Mathematik</tags>
		<tags>Musiktheorie</tags>
		<tags>MaMu</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2010-09-14 11:15:54</date>
		<count>3</count>
		<publisher>Oxford University Press</publisher>
		<address>New York</address>
		<year>2007</year>
		<url>http://www.ingentaconnect.com/content/oso/5481464/2007/00000001/00000001/art00000</url>
		<author>David Lewin</author>
		<authors>
			<first>David</first>
		</authors>
		<authors>
			<last>Lewin</last>
		</authors>
		<pages>i-258(259)</pages>
		<abstract>This book is recognized as the seminal work paving the way for current studies in mathematical and systematic approaches to music analysis. The author, one of the 20th century's most prominent figures in music theory, pushes the boundaries of the study of pitch-structure beyond its conception as a static system for classifying and inter-relating chords and sets. Known by most music theorists as &#8220;GMIT&#8221;, the book is by far the most significant contribution to the field of systematic music theory in the last half-century, generating the framework for the &#8220;transformational theory&#8221; movement. Appearing almost twenty years after GMIT's initial publication, this Oxford University Press edition features a previously unpublished preface by the author, as well as a foreword by Edward Gollin contextualizing the work's significance for the current field of music theory.</abstract>
		<isbn>9780199717842 0199717842</isbn>
		<doi>10.1093/acprof:oso/9780195317138.001.0001</doi>
		<title>Generalized Musical Intervals and Transformations</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2773d95a278b1a90ae3b7be04d542504b/gdmcbain</id>
		<tags>97m80-arts-music-language-architecture</tags>
		<tags>00a65-mathematics-and-music</tags>
		<tags>76q05-hydro-and-aero-acoustics</tags>
		<tags>94c05-analytic-circuit-theory</tags>
		<description></description>
		<date>2017-06-29 07:13:07</date>
		<count>1</count>
		<booktitle>Proceedings of the Sound and Music Computing Conference</booktitle>
		<publisher>Logos</publisher>
		<address>Berlin</address>
		<year>2013</year>
		<url>http://smcnetwork.org/node/1852</url>
		<author>Kurijn Buys</author>
		<author>Roman Auvray</author>
		<authors>
			<first>Kurijn</first>
		</authors>
		<authors>
			<last>Buys</last>
		</authors>
		<authors>
			<first>Roman</first>
		</authors>
		<authors>
			<last>Auvray</last>
		</authors>
		<pages>569--575</pages>
		<abstract>In analogy with strings and acoustic pipes as musical har-
monic oscillators, a novice electronic oscillator is consid-
ered. The equivalent circuit of a discrete representation
of strings and pipes, which takes the form of a discrete
transmission line, is constructed with real electronic com-
ponents. The proposed model includes the  ” equivalent se-
ries resistances”, which seems to be the only relevant de-
fault for both capacitors and inductors for this application.
In an analytical approach, the complex wave number is de-
rived, allowing the study of both the wave's dispersion and
attenuation in function of frequency and resulting in rec-
ommended and critical component values. Next, compo-
nents are selected for a first eight-node prototype, which
is numerically evaluated and then practically constructed
and measured. The results prove a good match between
theory and practice, with five distinguishable modes in the
entrance impedance. A new prototype design is planned,
which is expected to have much improved quality factors.</abstract>
		<title>Towards a discrete electronic transmission line as a musical harmonic oscillator</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/254fa9894535187151dfcbfb3400ca321/pbett</id>
		<tags>music</tags>
		<tags>weathertypes</tags>
		<tags>popsci</tags>
		<description></description>
		<date>2018-06-18 21:23:34</date>
		<count>1</count>
		<journal>Weather</journal>
		<publisher>John Wiley & Sons, Ltd.</publisher>
		<year>2011</year>
		<url>http://dx.doi.org/10.1002/wea.765</url>
		<author>Karen L. Aplin</author>
		<author>Paul D. Williams</author>
		<authors>
			<first>Karen L.</first>
		</authors>
		<authors>
			<last>Aplin</last>
		</authors>
		<authors>
			<first>Paul D.</first>
		</authors>
		<authors>
			<last>Williams</last>
		</authors>
		<volume>66</volume>
		<number>11</number>
		<pages>300--306</pages>
		<abstract>Depictions of the weather are common throughout the arts. Unlike in the visual arts, however, there has been little study of meteorological inspiration in music. This article catalogues and analyzes the frequencies with which weather is depicted in a sample of classical orchestral music. The depictions vary from explicit mimicry using traditional and specialized orchestral instruments, through to subtle suggestions. It is found that composers are generally influenced by their own environment in the type of weather they choose to represent. As befits the national stereotype, British composers seem disproportionately keen to depict the UK's variable weather patterns and stormy coastline. Copyright \copyright 2011 Royal Meteorological Society</abstract>
		<doi>10.1002/wea.765</doi>
		<title>Meteorological phenomena in Western classical orchestral music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c8505da1d4b0e70b41a7c5e6e0412c57/pbett</id>
		<tags>music</tags>
		<tags>social</tags>
		<tags>history</tags>
		<description></description>
		<date>2018-06-18 21:23:34</date>
		<count>3</count>
		<journal>Royal Society Open Science</journal>
		<publisher>The Royal Society</publisher>
		<year>2015</year>
		<url>http://dx.doi.org/10.1098/rsos.150081</url>
		<author>Matthias Mauch</author>
		<author>Robert M. MacCallum</author>
		<author>Mark Levy</author>
		<author>Armand M. Leroi</author>
		<authors>
			<first>Matthias</first>
		</authors>
		<authors>
			<last>Mauch</last>
		</authors>
		<authors>
			<first>Robert M.</first>
		</authors>
		<authors>
			<last>MacCallum</last>
		</authors>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Levy</last>
		</authors>
		<authors>
			<first>Armand M.</first>
		</authors>
		<authors>
			<last>Leroi</last>
		</authors>
		<volume>2</volume>
		<number>5</number>
		<pages>150081+</pages>
		<abstract>In modern societies, cultural change seems ceaseless. The flux of fashion is especially obvious for popular music. While much has been written about the origin and evolution of pop, most claims about its history are anecdotal rather than scientific in nature. To rectify this, we investigate the US Billboard Hot 100 between 1960 and 2010. Using music information retrieval and text-mining tools, we analyse the musical properties of approximately 17 000 recordings that appeared in the charts and demonstrate quantitative trends in their harmonic and timbral properties. We then use these properties to produce an audio-based classification of musical styles and study the evolution of musical diversity and disparity, testing, and rejecting, several classical theories of cultural change. Finally, we investigate whether pop musical evolution has been gradual or punctuated. We show that, although pop music has evolved continuously, it did so with particular rapidity during three stylistic 'revolutions' around 1964, 1983 and 1991. We conclude by discussing how our study points the way to a quantitative science of cultural change.</abstract>
		<issn>2054-5703</issn>
		<doi>10.1098/rsos.150081</doi>
		<title>The evolution of popular music: USA 1960–2010</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27764b576bd1745704fcb51e34101e532/becker</id>
		<tags>song</tags>
		<tags>music</tags>
		<tags>inthesis</tags>
		<tags>playlist</tags>
		<tags>recommendation</tags>
		<tags>review</tags>
		<tags>prediction</tags>
		<tags>survey</tags>
		<tags>diss</tags>
		<tags>generation</tags>
		<description></description>
		<date>2017-01-16 14:29:07</date>
		<count>4</count>
		<journal>ACM Computer Surveys (CSUR)</journal>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2014</year>
		<url>http://doi.acm.org/10.1145/2652481</url>
		<author>Geoffray Bonnin</author>
		<author>Dietmar Jannach</author>
		<authors>
			<first>Geoffray</first>
		</authors>
		<authors>
			<last>Bonnin</last>
		</authors>
		<authors>
			<first>Dietmar</first>
		</authors>
		<authors>
			<last>Jannach</last>
		</authors>
		<volume>47</volume>
		<number>2</number>
		<pages>26:1--26:35</pages>
		<abstract>Most of the time when we listen to music on the radio or on our portable devices, the order in which the tracks are played is governed by so-called playlists. These playlists are basically sequences of tracks that traditionally are designed manually and whose organization is based on some underlying logic or theme. With the digitalization of music and the availability of various types of additional track-related information on the Web, new opportunities have emerged on how to automate the playlist creation process. Correspondingly, a number of proposals for automated playlist generation have been made in the literature during the past decade. These approaches vary both with respect to which kind of data they rely on and which types of algorithms they use. In this article, we review the literature on automated playlist generation and categorize the existing approaches. Furthermore, we discuss the evaluation designs that are used today in research to assess the quality of the generated playlists. Finally, we report the results of a comparative evaluation of typical playlist generation schemes based on historical data. Our results show that track and artist popularity can play a dominant role and that additional measures are required to better characterize and compare the quality of automatically generated playlists.</abstract>
		<issn>0360-0300</issn>
		<issue_date>January 2015</issue_date>
		<doi>10.1145/2652481</doi>
		<title>Automated Generation of Music Playlists: Survey and Experiments</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23ca5d65f886c03d01ffb43ef3e4ce02d/chwick</id>
		<tags>square_notation</tags>
		<tags>OMR</tags>
		<tags>musical_manuscript</tags>
		<tags>plainchant</tags>
		<description></description>
		<date>2018-09-05 09:41:37</date>
		<count>1</count>
		<journal>Journal of New Music Research</journal>
		<publisher>Routledge</publisher>
		<year>2014</year>
		<url>/brokenurl#         https://doi.org/10.1080/09298215.2014.931438    </url>
		<author>Carolina Ramirez</author>
		<author>Jun Ohya</author>
		<authors>
			<first>Carolina</first>
		</authors>
		<authors>
			<last>Ramirez</last>
		</authors>
		<authors>
			<first>Jun</first>
		</authors>
		<authors>
			<last>Ohya</last>
		</authors>
		<volume>43</volume>
		<number>4</number>
		<pages>390-399</pages>
		<abstract>AbstractWhile the Optical Music Recognition (OMR) of printed and handwritten music scores in modern standard notation has been broadly studied, this is not the case for early music manuscripts. This is mainly due to the high variability in the sources introduced by their severe physical degradation, the lack of notation standards and, in the case of the scanned versions, by non-homogenous image-acquisition protocols. The volume of early musical manuscripts available is considerable, and therefore we believe that computational methods can be extremely useful in helping to preserve, share and analyse this information. This paper presents an approach to recognizing handwritten square musical notation in degraded western plainchant manuscripts from the XIVth to XVIth centuries. We propose the use of image processing techniques that behave robustly under high data variability and which do not require strong hypotheses regarding the condition of the sources. The main differences from traditional OMR approaches are our avoidance of the staff line removal stage and the use of grey-level images to perform primitive segmentation and feature extraction. We used 136 images from the Digital Scriptorium repository (DS, 2007), from which we were able to extract over 90\% of the staves and over 88\% of all symbols present. For symbol classification, we used gradient-based features and SVM classifiers, obtaining over 90\% precision and recall over eight basic symbol classes.</abstract>
		<eprint>https://doi.org/10.1080/09298215.2014.931438</eprint>
		<doi>10.1080/09298215.2014.931438</doi>
		<title>Automatic Recognition of Square Notation Symbols in Western Plainchant Manuscripts</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/267e12ce3871e84a23d3c1c2f2a8ce4b5/sapo</id>
		<tags>writer_recognition</tags>
		<tags>optical_music_recognition_systems</tags>
		<description>Writer identification in handwritten musical scores with bags of notes - ScienceDirect</description>
		<date>2020-07-01 13:20:53</date>
		<count>2</count>
		<journal>Pattern Recognition</journal>
		<year>2013</year>
		<url>http://www.sciencedirect.com/science/article/pii/S0031320312004475</url>
		<author>Albert Gordo</author>
		<author>Alicia Fornés</author>
		<author>Ernest Valveny</author>
		<authors>
			<first>Albert</first>
		</authors>
		<authors>
			<last>Gordo</last>
		</authors>
		<authors>
			<first>Alicia</first>
		</authors>
		<authors>
			<last>Fornés</last>
		</authors>
		<authors>
			<first>Ernest</first>
		</authors>
		<authors>
			<last>Valveny</last>
		</authors>
		<volume>46</volume>
		<number>5</number>
		<pages>1337 - 1345</pages>
		<abstract>Writer Identification is an important task for the automatic processing of documents. However, the identification of the writer in graphical documents is still challenging. In this work, we adapt the Bag of Visual Words framework to the task of writer identification in handwritten musical scores. A vanilla implementation of this method already performs comparably to the state-of-the-art. Furthermore, we analyze the effect of two improvements of the representation: a Bhattacharyya embedding, which improves the results at virtually no extra cost, and a Fisher Vector representation that very significantly improves the results at the cost of a more complex and costly representation. Experimental evaluation shows results more than 20 points above the state-of-the-art in a new, challenging dataset.</abstract>
		<issn>0031-3203</issn>
		<doi>https://doi.org/10.1016/j.patcog.2012.10.013</doi>
		<title>Writer identification in handwritten musical scores with bags of notes</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/210233b4dae59284d36f50e68b0277d17/sapo</id>
		<tags>writer_recognition</tags>
		<tags>optical_music_recognition_systems</tags>
		<description>Wavelet-based approach for writer identification in music score - IEEE Conference Publication</description>
		<date>2020-07-01 13:19:41</date>
		<count>1</count>
		<booktitle>2014 IEEE International Advance Computing Conference (IACC)</booktitle>
		<year>2014</year>
		<url>https://ieeexplore.ieee.org/document/6779489</url>
		<author>D. Das</author>
		<author>S. Chanda</author>
		<author>P. P. Roy</author>
		<author>U. Pal</author>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Das</last>
		</authors>
		<authors>
			<first>S.</first>
		</authors>
		<authors>
			<last>Chanda</last>
		</authors>
		<authors>
			<first>P. P.</first>
		</authors>
		<authors>
			<last>Roy</last>
		</authors>
		<authors>
			<first>U.</first>
		</authors>
		<authors>
			<last>Pal</last>
		</authors>
		<pages>1152-1156</pages>
		<abstract>Writer identification is a vibrant research field, though a lot of work has been done on writer identification on normal text, writer identification in music score sheet has not been addressed in that large scale. Here we propose a method to identify writers of music score sheets using Daubchies wavelet features along with SVM classifier. We have evaluated our proposed approach in a sub-set of CVC-MUSCIMA dataset. From the experiment on 140 score sheet images from 7 writers we obtained encouraging results.</abstract>
		<doi>10.1109/IAdCC.2014.6779489</doi>
		<title>Wavelet-based approach for writer identification in music score</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b0eba5624f9b7d83adda83abee04102a/sapo</id>
		<tags>writer_recognition</tags>
		<tags>optical_music_recognition_systems</tags>
		<description>A Symbol-Dependent Writer Identification Approach in Old Handwritten Music Scores - IEEE Conference Publication</description>
		<date>2020-07-01 13:16:13</date>
		<count>2</count>
		<booktitle>2010 12th International Conference on Frontiers in Handwriting Recognition</booktitle>
		<publisher>IEEE Computer Society</publisher>
		<year>2010</year>
		<url>https://ieeexplore.ieee.org/document/5693635</url>
		<author>Alicia Fornés</author>
		<author>Josep Lladós</author>
		<authors>
			<first>Alicia</first>
		</authors>
		<authors>
			<last>Fornés</last>
		</authors>
		<authors>
			<first>Josep</first>
		</authors>
		<authors>
			<last>Lladós</last>
		</authors>
		<pages>634-639</pages>
		<abstract>Writer identification consists in determining the writer of a piece of handwriting from a set of writers. In this paper we introduce a symbol-dependent approach for identifying the writer of old music scores, which is based on two symbol recognition methods. The main idea is to use the Blurred Shape Model descriptor and a DTW-based method for detecting, recognizing and describing the music clefs and notes. The proposed approach has been evaluated in a database of old music scores, achieving very high writer identification rates.</abstract>
		<doi>10.1109/ICFHR.2010.104</doi>
		<title>A Symbol-Dependent Writer Identification Approach in Old Handwritten Music Scores</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c14f006bb30cbda962e149eef6332500/anak</id>
		<tags>musicinformationretrieval</tags>
		<tags>musicretrievalsystems</tags>
		<tags>musicretrieval</tags>
		<description></description>
		<date>2021-04-05 11:38:12</date>
		<count>2</count>
		<year>2005</year>
		<url></url>
		<author>Rainer Typke</author>
		<author>Frans Wiering</author>
		<author>Remco Veltkamp</author>
		<authors>
			<first>Rainer</first>
		</authors>
		<authors>
			<last>Typke</last>
		</authors>
		<authors>
			<first>Frans</first>
		</authors>
		<authors>
			<last>Wiering</last>
		</authors>
		<authors>
			<first>Remco</first>
		</authors>
		<authors>
			<last>Veltkamp</last>
		</authors>
		<abstract>This survey paper provides an overview of content-based
music information retrieval systems, both for audio and
for symbolic music notation. Matching algorithms and
indexing methods are briefly presented. The need for a
TREC-like comparison of matching algorithms such as
MIREX at ISMIR becomes clear from the high number
of quite different methods which so far only have been
used on different data collections. We placed the systems
on a map showing the tasks and users for which they are
suitable, and we find that existing content-based retrieval
systems fail to cover a gap between the very general and
the very specific retrieval tasks.</abstract>
		<title>A survey of music information retrieval systems</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ed231a1f94da0053b296fc9ddf0da43e/researchparks</id>
		<tags>music</tags>
		<tags>folksongs</tags>
		<tags>culture</tags>
		<tags>shashmaqom</tags>
		<description></description>
		<date>2021-03-03 06:38:39</date>
		<count>1</count>
		<journal>International Journal on Integrated Education</journal>
		<year>2020</year>
		<url>https://journals.researchparks.org/index.php/IJIE/article/view/1054</url>
		<author>Yuldasheva Aynisa</author>
		<authors>
			<first>Yuldasheva</first>
		</authors>
		<authors>
			<last>Aynisa</last>
		</authors>
		<volume>3</volume>
		<number>12</number>
		<pages>458-462</pages>
		<abstract>This article reveals the problems of studying and preserving the Uzbek traditional music culture in the fields of modern science and education. The field of education is the “intersection” of science and practice, as well as the identification and understanding of the individual’s own historical and ethnogenetic roots to the present day Yuldasheva Aynisa 2020. PRESERVATION OF UZBEK TRADITIONAL MUSIC CULTURE. International Journal on Integrated Education. 3, 12 (Dec. 2020), 458-462. DOI:https://doi.org/10.31149/ijie.v3i12.1054 Pdf Url : https://journals.researchparks.org/index.php/IJIE/article/view/1054/1002 Paper Url : https://journals.researchparks.org/index.php/IJIE/article/view/1054</abstract>
		<language>English</language>
		<issn>2620-3502</issn>
		<title>PRESERVATION OF UZBEK TRADITIONAL MUSIC CULTURE</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a39eb7aa0c8f7023807d8c8603df9143/sapo</id>
		<tags>score-to-audio_alignment</tags>
		<description>Improved score-performance alignment algorithms on polyphonic music - IEEE Conference Publication</description>
		<date>2021-03-05 17:36:56</date>
		<count>2</count>
		<booktitle>2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</booktitle>
		<year>2014</year>
		<url>https://ieeexplore.ieee.org/abstract/document/6853820</url>
		<author>C. Chen</author>
		<author>J. R. Jang</author>
		<author>W. Liou</author>
		<authors>
			<first>C.</first>
		</authors>
		<authors>
			<last>Chen</last>
		</authors>
		<authors>
			<first>J. R.</first>
		</authors>
		<authors>
			<last>Jang</last>
		</authors>
		<authors>
			<first>W.</first>
		</authors>
		<authors>
			<last>Liou</last>
		</authors>
		<pages>1365-1369</pages>
		<abstract>Automated symbolic music alignment is a challenging task due to the variation of performance by different performers. It becomes more complicated when dealing with polyphonic music because note events could occur at the same time. The goal of this study is to find an efficient algorithm for aligning two polyphonic symbolic representations (MIDI files, for instance) of the same music. To this end, we design two methods for such score-performance alignment that matches the performance with its corresponding score. The first method applies a string matching algorithm based on dynamic programming. The second method is based on the principle of "divide and conquer" that performs efficient alignment recursively. To evaluate the algorithms, we have collected a set of 21 MIDI pairs of classic piano performance with human corrected note-level mapping as ground truth. We have released the dataset as a public resource. Both the proposed algorithms achieved a precision and recall higher than 96% in our experiment, outperforming the most recently proposed method 7 in the literature. Besides, the execution time of proposed methods is much faster the method of 7.</abstract>
		<issn>2379-190X</issn>
		<doi>10.1109/ICASSP.2014.6853820</doi>
		<title>Improved score-performance alignment algorithms on polyphonic music.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2421a49231c7bfe49a1c0e247d5dd735b/simon.brown</id>
		<tags>art</tags>
		<description>GCD - Earth System Music: the methodology and reach of music generated from the United Kingdom Earth System Model</description>
		<date>2020-04-29 10:07:09</date>
		<count>1</count>
		<journal>Geoscience Communication Discussions</journal>
		<publisher>Copernicus GmbH</publisher>
		<year>2019</year>
		<url>https://www.geosci-commun-discuss.net/gc-2019-28/</url>
		<author>Lee de Mora</author>
		<author>Alistair A. Sellar</author>
		<author>Andrew Yool</author>
		<author>Julien Palmieri</author>
		<author>Robin S. Smith</author>
		<author>Till Kuhlbrodt</author>
		<author>Robert J. Parker</author>
		<author>Jeremy Walton</author>
		<author>Jeremy C. Blackford</author>
		<author>Colin G. Jones</author>
		<authors>
			<first>Lee de</first>
		</authors>
		<authors>
			<last>Mora</last>
		</authors>
		<authors>
			<first>Alistair A.</first>
		</authors>
		<authors>
			<last>Sellar</last>
		</authors>
		<authors>
			<first>Andrew</first>
		</authors>
		<authors>
			<last>Yool</last>
		</authors>
		<authors>
			<first>Julien</first>
		</authors>
		<authors>
			<last>Palmieri</last>
		</authors>
		<authors>
			<first>Robin S.</first>
		</authors>
		<authors>
			<last>Smith</last>
		</authors>
		<authors>
			<first>Till</first>
		</authors>
		<authors>
			<last>Kuhlbrodt</last>
		</authors>
		<authors>
			<first>Robert J.</first>
		</authors>
		<authors>
			<last>Parker</last>
		</authors>
		<authors>
			<first>Jeremy</first>
		</authors>
		<authors>
			<last>Walton</last>
		</authors>
		<authors>
			<first>Jeremy C.</first>
		</authors>
		<authors>
			<last>Blackford</last>
		</authors>
		<authors>
			<first>Colin G.</first>
		</authors>
		<authors>
			<last>Jones</last>
		</authors>
		<pages>1-27</pages>
		<abstract><p><strong>Abstract.</strong> Scientific data is almost always represented graphically either in figures or in videos. With the ever-growing interest from the general public towards understanding climate science, it is becoming increasingly important that we present this information in ways accessible to non-experts.</p> <p> In this pilot study, we use time series data from the first United Kingdom Earth System model (UKESM1) to create six procedurally generated musical pieces and use them to test whether we can use music to engage with the wider community. Each of these pieces is based around a unique part of UKESM1's ocean component model, either in terms of a scientific principle or a practical aspect of modelling. In addition, each piece is arranged using a different musical progression, style and tempo.</p> <p>These pieces were performed by the digital piano synthesizer, TiMidity++, and were published on the lead author's YouTube channel. The videos all show the time progression of the data in time with the music and a brief description of the methodology is posted below the video. To disseminate these works, a link to each piece was published on the lead authors personal and professional social media accounts.</p> <p>The reach of these works was analysed using YouTube's channel monitoring toolkit for content creators, YouTube studio. In the first ninety days after the first video was published, the six pieces reached at least 251 unique viewers, and have 553 total views. We found that most of the views occurred in the fourteen days immediately after each video was published. In effect, once the concept had been demonstrated to an audience, there was reduced enthusiasm from that audience to return to it immediately. This suggests that to use music effectively as an science outreach tool, the works needs to reach new audiences or new and unique content needs to be delivered to a returning audience.</p></abstract>
		<issn>2569-7102</issn>
		<doi>https://doi.org/10.5194/gc-2019-28</doi>
		<title>Earth System Music: the methodology and reach of music generated from the United Kingdom Earth System Model</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b2c94c17c949646a2b740084169c927b/meneteqel</id>
		<tags>artists</tags>
		<tags>gig_economy</tags>
		<tags>precarious_employment</tags>
		<tags>mid-sized_cities</tags>
		<tags>musicians</tags>
		<tags>creative_work</tags>
		<description></description>
		<date>2019-10-05 11:28:49</date>
		<count>1</count>
		<journal>Work, Employment and Society</journal>
		<publisher>SAGE Publications</publisher>
		<year>2019</year>
		<url>https://doi.org/10.1177%2F0950017019865877</url>
		<author>David Chafe</author>
		<author>Lisa Kaida</author>
		<authors>
			<first>David</first>
		</authors>
		<authors>
			<last>Chafe</last>
		</authors>
		<authors>
			<first>Lisa</first>
		</authors>
		<authors>
			<last>Kaida</last>
		</authors>
		<pages>095001701986587</pages>
		<abstract>Precarious employment literature has addressed a myriad of occupations increasingly characterized by employment uncertainty and reduced commitment between workers and employers due to short-term contracts and self-employment, with particular attention given to creative industries and the gig economy in recent years. The authors argue that research on creative industries also requires consideration of the role of place in the experience of employment insecurity and career commitment. This article focuses on self-employed musicians in the mid-sized city of St John’s, Canada. Interviews with 54 musicians draw attention to coping strategies for long periods of low pay and employment insecurity. These strategies include downplaying competition and conflict, acquiring higher education and changing career. It is argued that population size and location of the community where work is based have implications on such coping strategies and on career longevity.</abstract>
		<language>eng</language>
		<doi>10.1177/0950017019865877</doi>
		<title>Harmonic Dissonance: Coping with Employment Precarity among Professional Musicians in St John's, Canada</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fac463304ddd4c43edb71dde261a5d26/sapo</id>
		<tags>performance_analysis</tags>
		<tags>psychology</tags>
		<description>Predicting the perception of performed dynamics in music audio with ensemble learning: The Journal of the Acoustical Society of America: Vol 141, No 3</description>
		<date>2019-10-20 10:54:15</date>
		<count>1</count>
		<journal>The Journal of the Acoustical Society of America</journal>
		<booktitle>The Journal of the Acoustical Society of America</booktitle>
		<publisher>Acoustical Society of America</publisher>
		<year>2017</year>
		<url>https://doi.org/10.1121/1.4978245</url>
		<author>Anders Elowsson</author>
		<author>Anders Friberg</author>
		<authors>
			<first>Anders</first>
		</authors>
		<authors>
			<last>Elowsson</last>
		</authors>
		<authors>
			<first>Anders</first>
		</authors>
		<authors>
			<last>Friberg</last>
		</authors>
		<volume>141</volume>
		<number>3</number>
		<pages>2224--2242</pages>
		<abstract>By varying the dynamics in a musical performance, the musician can convey structure and different expressions. Spectral properties of most musical instruments change in a complex way with the perfo...</abstract>
		<issn>00014966</issn>
		<comment>doi: 10.1121/1.4978245</comment>
		<doi>10.1121/1.4978245</doi>
		<title>Predicting the perception of performed dynamics in music audio with ensemble learning</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28a65a06753189049d5b1048e95130b9f/bauerc</id>
		<tags>myown</tags>
		<tags>culture</tags>
		<tags>userconnections</tags>
		<tags>crosscountry</tags>
		<tags>onlinesocialnetwork</tags>
		<tags>musicplatform</tags>
		<description></description>
		<date>2019-06-19 16:48:47</date>
		<count>2</count>
		<booktitle>Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems</booktitle>
		<series>CHI EA '19</series>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2019</year>
		<url>https://doi.org/10.1145/3290607.3312831</url>
		<author>Christine Bauer</author>
		<author>Markus Schedl</author>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<pages>LBW0112:1--LBW0112:6</pages>
		<abstract>Social connections and cultural aspects play important roles in shaping an individual’s preferences. For instance, people tend to select friends with similar music preferences. Furthermore, preferences and friending are influenced by cultural aspects. Recommender systems may benefit from these phenomena by using knowledge about the nature of social ties to better tailor recommendations to an individual. Focusing on the specifities of music preferences, we study user connections on Last.fm—an online social network for music. We identify those countries whose users are mainly connected within the same country, and those countries that are characterized by cross-country user connections. Strong cross-country connection pairs are typically characterized by similar cultural, historic, or linguistic backgrounds, or geographic proximity. The United States, the United Kingdom, and Russia are identified as countries having a large relative amount of user connections from other countries. Our results contribute to understanding the complexity of social ties and how they are reflected in connection behavior, and are a promising source for advancements of personalized systems.</abstract>
		<isbn>978-1-4503-5971-9</isbn>
		<language>English</language>
		<doi>10.1145/3290607.3312831</doi>
		<title>Cross-country user connections in an online social network for music</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/201339e7341bd288f21aecab564948455/sapo</id>
		<tags>datasets</tags>
		<description>Learning Audio–Sheet Music Correspondences for Cross-Modal Retrieval and Piece Identification</description>
		<date>2019-07-01 12:28:31</date>
		<count>1</count>
		<journal>Transactions of the International Society for Music Information Retrieval</journal>
		<publisher>Ubiquity Press, Ltd.</publisher>
		<year>2018</year>
		<url>https://doi.org/10.5334%2Ftismir.12</url>
		<author>Matthias Dorfer</author>
		<author>Jan Hajic jr.</author>
		<author>Andreas Arzt</author>
		<author>Harald Frostel</author>
		<author>Gerhard Widmer</author>
		<authors>
			<first>Matthias</first>
		</authors>
		<authors>
			<last>Dorfer</last>
		</authors>
		<authors>
			<first>Jan Hajic</first>
		</authors>
		<authors>
			<last>jr.</last>
		</authors>
		<authors>
			<first>Andreas</first>
		</authors>
		<authors>
			<last>Arzt</last>
		</authors>
		<authors>
			<first>Harald</first>
		</authors>
		<authors>
			<last>Frostel</last>
		</authors>
		<authors>
			<first>Gerhard</first>
		</authors>
		<authors>
			<last>Widmer</last>
		</authors>
		<volume>1</volume>
		<number>1</number>
		<pages>22</pages>
		<doi>10.5334/tismir.12</doi>
		<title>Learning Audio–Sheet Music Correspondences for Cross-Modal Retrieval and
		                        Piece Identification</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a6df8a1dd65bafdffa35cf0f3648b18e/sapo</id>
		<tags>music_information_retrieval</tags>
		<tags>big_data</tags>
		<tags>semantic_web</tags>
		<tags>Digital_musicology</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<publisher>ACM</publisher>
		<year>2017</year>
		<url>http://dl.acm.org/citation.cfm?doid=3034773.2983918</url>
		<author>Samer Abdallah</author>
		<author>Emmanouil Benetos</author>
		<author>Nicolas Gold</author>
		<author>Steven Hargreaves</author>
		<author>Tillman Weyde</author>
		<author>Daniel Wolff</author>
		<authors>
			<first>Samer</first>
		</authors>
		<authors>
			<last>Abdallah</last>
		</authors>
		<authors>
			<first>Emmanouil</first>
		</authors>
		<authors>
			<last>Benetos</last>
		</authors>
		<authors>
			<first>Nicolas</first>
		</authors>
		<authors>
			<last>Gold</last>
		</authors>
		<authors>
			<first>Steven</first>
		</authors>
		<authors>
			<last>Hargreaves</last>
		</authors>
		<authors>
			<first>Tillman</first>
		</authors>
		<authors>
			<last>Weyde</last>
		</authors>
		<authors>
			<first>Daniel</first>
		</authors>
		<authors>
			<last>Wolff</last>
		</authors>
		<volume>10</volume>
		<number>1</number>
		<pages>1--21</pages>
		<abstract>In musicology and music research generally, the increasing availability
	of digital music, storage capacities, and computing power enable
	and require new and intelligent systems. In the transition from traditional
	to digital musicology, many techniques and tools have been developed
	for the analysis of individual pieces of music, but large-scale music
	data that are increasingly becoming available require research methods
	and systems that work on the collection-level and at scale. Although
	many relevant algorithms have been developed during the past 15 years
	of research in Music Information Retrieval, an integrated system
	that supports large-scale digital musicology research has so far
	been lacking. In the Digital Music Lab (DML) project, a collaboration
	among music librarians, musicologists, computer scientists, and human-computer
	interface specialists, the DML software system has been developed
	that fills this gap by providing intelligent large-scale music analysis
	with a user-friendly interactive interface supporting musicologists
	in their exploration and enquiry. The DML system empowers musicologists
	by addressing several challenges: distributed processing of audio
	and other music data, management of the data analysis process and
	results, remote analysis of data under copyright, logical inference
	on the extracted information and metadata, and visual web-based interfaces
	for exploring and querying the music collections. The DML system
	is scalable and based on Semantic Web technology and integrates into
	Linked Data with the vision of a distributed system that enables
	music research across archives, libraries, and other providers of
	music data. A first DML system prototype has been set up in collaboration
	with the British Library and I Like Music Ltd. This system has been
	used to analyse a diverse corpus of currently 250,000 music tracks.
	In this article, we describe the DML system requirements, design,
	architecture, components, and available data sources, explaining
	their interaction. We report use cases and applications with initial
	evaluations of the proposed system.</abstract>
		<journaltitle>Journal on Computing and Cultural Heritage</journaltitle>
		<doi>10.1145/2983918</doi>
		<title>The Digital Music Lab</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/222bf11993d28704e69d92fed40a57b6d/sapo</id>
		<tags>music_perception</tags>
		<tags>musical_expressivity</tags>
		<tags>MIR</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>2</count>
		<publisher>ACM</publisher>
		<year>2016</year>
		<url>http://arxiv.org/abs/1611.09733{\%}0Ahttp://dx.doi.org/10.1145/2899004</url>
		<author>Gerhard Widmer</author>
		<authors>
			<first>Gerhard</first>
		</authors>
		<authors>
			<last>Widmer</last>
		</authors>
		<volume>8</volume>
		<number>2</number>
		<pages>1--13</pages>
		<abstract>This text offers a personal and very subjective view on the current
	situation of Music Information Research (MIR). Motivated by the desire
	to build systems with a somewhat deeper understanding of music than
	the ones we currently have, I try to sketch a number of challenges
	for the next decade of MIR research, grouped around six simple truths
	about music that are probably generally agreed on, but often ignored
	in everyday research.</abstract>
		<eprinttype>arXiv</eprinttype>
		<eprint>1611.09733</eprint>
		<journaltitle>\ACM\ Transactions on Intelligent Systems and Technology</journaltitle>
		<doi>10.1145/2899004</doi>
		<title>Getting Closer to the Essence of Music: The Con Espressione Manifesto</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28b986349b312a430b2b05943bd04e279/sapo</id>
		<tags>Multiple_signal_classification</tags>
		<tags>novel_audio_features</tags>
		<tags>musical_acoustics</tags>
		<tags>Segmentation</tags>
		<tags>Rhythm</tags>
		<tags>tempro-related_information</tags>
		<tags>Indian_music_scenario</tags>
		<tags>Analysis</tags>
		<tags>music_recordings</tags>
		<tags>Music_information_retrieval</tags>
		<tags>Indexes</tags>
		<tags>acoustic_signal_processing</tags>
		<tags>Frequency_modulation</tags>
		<tags>Classification</tags>
		<tags>Tempo</tags>
		<tags>Salience</tags>
		<tags>capturing_tempo_salience</tags>
		<tags>Data_mining</tags>
		<tags>Music</tags>
		<tags>tempo_salience</tags>
		<tags>Audio</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</booktitle>
		<year>2015-04</year>
		<url></url>
		<author>B. Thoshkahna</author>
		<author>M. Müller</author>
		<author>V. Kulkarni</author>
		<author>N. Jiang</author>
		<authors>
			<first>B.</first>
		</authors>
		<authors>
			<last>Thoshkahna</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Müller</last>
		</authors>
		<authors>
			<first>V.</first>
		</authors>
		<authors>
			<last>Kulkarni</last>
		</authors>
		<authors>
			<first>N.</first>
		</authors>
		<authors>
			<last>Jiang</last>
		</authors>
		<pages>181--185</pages>
		<abstract>In music compositions, certain parts may be played in an improvisational style with a rather vague notion of tempo, while other parts are characterized by having a clearly perceivable tempo. Based on this observation, we introduce in this paper some novel audio features for capturing tempo-related information. Rather than measuring the specific tempo of a local section of a given recording, our objective is to capture the existence or absence of a notion of tempo, a kind of tempo salience. By a quantitative analysis within an Indian music scenario, we demonstrate that our audio features capture the aspect of tempo salience well, while being independent of continuous fluctuations and local changes in tempo.</abstract>
		<doi>10.1109/ICASSP.2015.7177956</doi>
		<title>Novel audio features for capturing tempo salience in music recordings</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ddcdcb96b1e7746cf0c4aac547b3712f/sapo</id>
		<tags>music_information</tags>
		<tags>musical_instrument_digital_interface</tags>
		<tags>musical_instruments</tags>
		<tags>IEEE_standards</tags>
		<tags>symbolic_music_representation</tags>
		<tags>music</tags>
		<tags>music_performance</tags>
		<tags>XML</tags>
		<tags>music_representation</tags>
		<tags>optical_music_recognition_systems</tags>
		<tags>score_editing</tags>
		<tags>XML_application</tags>
		<tags>musical_databases</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>IEEE Std 1599-2008</booktitle>
		<publisher>IEEE</publisher>
		<year>2008</year>
		<url>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp={\&}arnumber=4633344</url>
		<author>Standards Activities</author>
		<author>Computer Society</author>
		<authors>
			<first>Standards</first>
		</authors>
		<authors>
			<last>Activities</last>
		</authors>
		<authors>
			<first>Computer</first>
		</authors>
		<authors>
			<last>Society</last>
		</authors>
		<number>September</number>
		<pages>1--110</pages>
		<abstract>This recommended practice develops an \XML\ application defining
	a standard language for symbolic music representation. The language
	is a meta-representation of music information for describing and
	processing said music information within a multi-layered environment,
	for achieving integration among structural, score, Musical Instrument
	Digital Interface (MIDI), and digital sound levels of representation.
	Furthermore, the recommended practice integrates music representation
	with already defined and accepted common standards. This recommended
	practice will be accepted by any kind of software dealing with music
	information, e.g., score editing, optical music recognition (OMR)
	systems, music performance, musical databases, and composition and
	musicological applications.</abstract>
		<doi>10.1109/IEEESTD.2008.4633344</doi>
		<title>IEEE Recommended Practice for Defining a Commonly Acceptable Musical Application Using XML</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2de14464025a43358df0c2321ff2eacb5/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>The S2S Consortium</booktitle>
		<publisher>The S2S2 Consortium</publisher>
		<year>2007</year>
		<url>http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:A+Roadmap+for+Sound+and+Music+Computing{\#}0</url>
		<author>Xavier Serra</author>
		<author>Marc Leman</author>
		<author>Gerhard Widmer</author>
		<authors>
			<first>Xavier</first>
		</authors>
		<authors>
			<last>Serra</last>
		</authors>
		<authors>
			<first>Marc</first>
		</authors>
		<authors>
			<last>Leman</last>
		</authors>
		<authors>
			<first>Gerhard</first>
		</authors>
		<authors>
			<last>Widmer</last>
		</authors>
		<editor>Nicola Bernardini</editor>
		<editor>Xavier Serra</editor>
		<editor>Marc Leman</editor>
		<editor>Gerhard Widmer</editor>
		<editors>
			<first>Gerhard</first>
		</editors>
		<editors>
			<last>Widmer</last>
		</editors>
		<editors>
			<first>Gerhard</first>
		</editors>
		<editors>
			<last>Widmer</last>
		</editors>
		<editors>
			<first>Gerhard</first>
		</editors>
		<editors>
			<last>Widmer</last>
		</editors>
		<editors>
			<first>Gerhard</first>
		</editors>
		<editors>
			<last>Widmer</last>
		</editors>
		<pages>1--167</pages>
		<abstract>Music is an important aspect of all human cultures. Music is meant
	to give new experiences, to give sense and meaning to life, to console
	and to promote social coherence and personal identity in and across
	very diverse social and ethnic groups. Rooted in the biology of every
	human being, music is a core occupation of our technological society.
	By 2020, music will have become a commodity as ubiquitous as water
	or electricity. Its content and the activities surrounding it will
	promote new business ventures, which in turn will bolster the music
	and cultural/creative industries. Sound and Music Computing (SMC)
	will provide the core technologies for this ongoing revolution in
	electronic music culture. Its major research contribution to advances
	in the eld will be to bridge the semantic gap, the hiatus that currently
	separates sound from sense. This contribution will stimulate fruitful
	interaction between culture, science and industry.</abstract>
		<title>A Roadmap for Sound and Music Computing</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/207d81d4cd466fd9bc8a85872d1c4e8be/sapo</id>
		<tags>Performance_Analysis</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>2</count>
		<booktitle>Proceedings of the 7th International Conference on New Interfaces for Musical Expression</booktitle>
		<series>NIME '07</series>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2007</year>
		<url></url>
		<author>Bo Bell</author>
		<author>Jim Kleban</author>
		<author>Dan Overholt</author>
		<author>Lance Putnam</author>
		<author>John Thompson</author>
		<author>JoAnn Kuchera-Morin</author>
		<authors>
			<first>Bo</first>
		</authors>
		<authors>
			<last>Bell</last>
		</authors>
		<authors>
			<first>Jim</first>
		</authors>
		<authors>
			<last>Kleban</last>
		</authors>
		<authors>
			<first>Dan</first>
		</authors>
		<authors>
			<last>Overholt</last>
		</authors>
		<authors>
			<first>Lance</first>
		</authors>
		<authors>
			<last>Putnam</last>
		</authors>
		<authors>
			<first>John</first>
		</authors>
		<authors>
			<last>Thompson</last>
		</authors>
		<authors>
			<first>JoAnn</first>
		</authors>
		<authors>
			<last>Kuchera-Morin</last>
		</authors>
		<pages>62--65</pages>
		<abstract>We present the Multimodal Music Stand (MMMS) for the untethered sensing
	of performance gestures and the interactive control of music. Using
	e-field sensing, audio analysis, and computer vision, the MMMS captures
	a performer's continuous expressive gestures and robustly identifies
	discrete cues in a musical performance. Continuous and discrete gestures
	are sent to an interactive music system featuring custom designed
	software that performs real-time spectral transformation of audio.</abstract>
		<doi>10.1145/1279740.1279750</doi>
		<title>The Multimodal Music Stand</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/294937fab779b7bcaec56816b0febf29c/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<publisher>Oxford University Press UK</publisher>
		<year>2015</year>
		<url></url>
		<author>Reinier de Valk</author>
		<author>Tillman Weyde</author>
		<authors>
			<first>Reinier</first>
		</authors>
		<authors>
			<last>de Valk</last>
		</authors>
		<authors>
			<first>Tillman</first>
		</authors>
		<authors>
			<last>Weyde</last>
		</authors>
		<volume>43</volume>
		<number>4</number>
		<pages>563--576</pages>
		<abstract>A large corpus of music written in lute tablature, spanning some three-and-a-half
	centuries, has survived. This music has so far escaped systematic
	musicological research because of its notational format. Being a
	practical instruction for the player, tablature reveals very little
	of the polyphonic structure of the music it encodes--and is therefore
	relatively inaccessible to non-specialists. Automatic polyphonic
	transcription into modern music notation can help unlock the corpus
	to a larger audience, and thus facilitate musicological research.
	In this study we present four variants of a machinelearning model
	for voice separation and duration reconstruction in 16th-century
	lute tablature. These models are intended to form the heart of an
	interactive system for automatic polyphonic transcription that can
	assist users in making editions tailored to their own preferences.
	Additionally, such models can provide new methods for analysing different
	aspects of polyphonic structure. We have experimented with modelling
	only voice and modelling voice and duration simultaneously, applying
	each in a forward- and in a backward-processing approach. The models
	are evaluated on a dataset containing 15 three- and four-voice intabulations.
	Each processing approach has its advantages, and the results vary
	between the models. With accuracy rates between approximately 80
	and 90 per cent, both for voice prediction and for duration prediction,
	the best models' performance is promising. Even in this early stage
	of the research, such models yield a useful initial transcription
	system. ABSTRACT FROM AUTHOR</abstract>
		<journaltitle>Early Music</journaltitle>
		<doi>10.1093/em/cau102</doi>
		<title>Bringing ‘Musicque into the Tableture': Machine-Learning Models for Polyphonic Transcription of 16th-century Lute Tablature</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27ca0266fae5165d7ab861e044a1b519c/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<publisher>Routledge</publisher>
		<year>2017</year>
		<url></url>
		<author>Emery Schubert</author>
		<author>Sergio Canazza</author>
		<author>Giovanni De Poli</author>
		<author>Antonio Rodà</author>
		<authors>
			<first>Emery</first>
		</authors>
		<authors>
			<last>Schubert</last>
		</authors>
		<authors>
			<first>Sergio</first>
		</authors>
		<authors>
			<last>Canazza</last>
		</authors>
		<authors>
			<first>Giovanni De</first>
		</authors>
		<authors>
			<last>Poli</last>
		</authors>
		<authors>
			<first>Antonio</first>
		</authors>
		<authors>
			<last>Rodà</last>
		</authors>
		<volume>46</volume>
		<number>2</number>
		<pages>175--186</pages>
		<abstract>AbstractCan a computer play a music score, e.g. via a Disklavier,
	in a way that cannot be distinguished from a human performance of
	the same music? One hundred and seventy-two participants with a wide
	range of music playing backgrounds rated sound recordings of 7 performances
	of piano music by Kuhlau, one played by a human, and six generated
	by algorithms, including a ‘mechanical’ and an ‘unmusical’ rendering.
	Participants rated the extent to which each performance was by a
	human and explained their answers. The mechanical performance had
	the lowest mean rating, but the human performance was rated as statistically
	identical to the other stimuli. There were no differences between
	ratings made by classical piano experts and lay listeners, but despite
	this, the musicians were more confident with their ratings. Qualitative
	analysis revealed five broad themes that contribute to judging whether
	a piece appears to be human. The themes were labelled (in descending
	order of frequency) intuitive, expressive, imperfections, halo (global
	preference) and empathy. This paper presents new evidence systematically
	demonstrating that algorithm generated performances of piano music
	can be indistinguishable from human performances, suggesting some
	parallels with the 1990s victory of the Deep Blue computer of the
	world champion (human) chess player.</abstract>
		<eprint>https://doi.org/10.1080/09298215.2016.1264976</eprint>
		<journaltitle>Journal of New Music Research</journaltitle>
		<doi>10.1080/09298215.2016.1264976</doi>
		<title>Algorithms Can Mimic Human Piano Performance: The Deep Blues of Music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/299b24a186d34d75402e27197f864ff41/sapo</id>
		<tags>Explorative_studies</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>4</count>
		<booktitle>Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017, Suzhou, China</booktitle>
		<year>2017</year>
		<url>https://ismir2017.smcnus.org/wp-content/uploads/2017/10/32_Paper.pdf</url>
		<author>Matthias Dorfer</author>
		<author>Andreas Arzt</author>
		<author>Gerhard Widmer</author>
		<authors>
			<first>Matthias</first>
		</authors>
		<authors>
			<last>Dorfer</last>
		</authors>
		<authors>
			<first>Andreas</first>
		</authors>
		<authors>
			<last>Arzt</last>
		</authors>
		<authors>
			<first>Gerhard</first>
		</authors>
		<authors>
			<last>Widmer</last>
		</authors>
		<editor>Sally Jo Cunningham</editor>
		<editor>Zhiyao Duan</editor>
		<editor>Xiao Hu</editor>
		<editor>Douglas Turnbull</editor>
		<editors>
			<first>Gerhard</first>
		</editors>
		<editors>
			<last>Widmer</last>
		</editors>
		<editors>
			<first>Gerhard</first>
		</editors>
		<editors>
			<last>Widmer</last>
		</editors>
		<editors>
			<first>Gerhard</first>
		</editors>
		<editors>
			<last>Widmer</last>
		</editors>
		<editors>
			<first>Gerhard</first>
		</editors>
		<editors>
			<last>Widmer</last>
		</editors>
		<pages>115--122</pages>
		<abstract>This work addresses the problem of matching short excerpts of audio
	with their respective counterparts in sheet music images. We show
	how to employ neural network-based cross-modality embedding spaces
	for solving the following two sheet music-related tasks: retrieving
	the correct piece of sheet music from a database when given a music
	audio as a search query; and aligning an audio recording of a piece
	with the corresponding images of sheet music. We demonstrate the
	feasibility of this in experiments on classical piano music by five
	different composers (Bach, Haydn, Mozart, Beethoven and Chopin),
	and additionally provide a discussion on why we expect multi-modal
	neural networks to be a fruitful paradigm for dealing with sheet
	music and audio at the same time.</abstract>
		<eprinttype>arXiv</eprinttype>
		<eprint>1707.09887</eprint>
		<bibsource>dblp computer science bibliography, https://dblp.org</bibsource>
		<title>Learning Audio-sheet Music Correspondences for Score Identification and Offline Alignment</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22aeb45372b6bca78832da6e80528980c/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2020-03-18 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#IslamK13a</url>
		<author>Md. Shohidul Islam</author>
		<author>Jong-Myon Kim</author>
		<authors>
			<first>Md. Shohidul</first>
		</authors>
		<authors>
			<last>Islam</last>
		</authors>
		<authors>
			<first>Jong-Myon</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Jong-Myon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Jong-Myon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Jong-Myon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Jong-Myon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<volume>274</volume>
		<pages>591-597</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Accelerating Adaptive Forward Error Correction Using Graphics Processing Units.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24b31eeab83fb60b71351ff94d871300e/bauerc</id>
		<tags>imported</tags>
		<tags>myown</tags>
		<tags>music</tags>
		<tags>mainstreaminess</tags>
		<tags>recsys</tags>
		<tags>region</tags>
		<description></description>
		<date>2019-04-27 18:20:03</date>
		<count>2</count>
		<journal>Journal of Mobile Multimedia</journal>
		<year>2018</year>
		<url></url>
		<author>Markus Schedl</author>
		<author>Christine Bauer</author>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<volume>14</volume>
		<number>1</number>
		<pages>95-112</pages>
		<abstract>The music mainstreaminess of a listener reflects how strong a person's listening preferences correspond to those of the larger population. Considering that music mainstream may be defined from different perspectives, we show country-specific differences and study how taking into account music mainstreaminess influences the quality of music recommendations.

In this paper, we first propose 11 novel mainstreaminess measures characterizing music listeners, considering both a global and a country-specific basis for mainstreaminess.
To this end, we model preference profiles (as a vector over artists) for users, countries, and globally, incorporating artist frequency, listener frequency, and a newly proposed TF-IDF-inspired weighting function, which we call artist frequency--inverse listener frequency (AF-ILF).
The resulting preference profile for each user $u$ is then related to the respective country-specific and global preference profile using fraction-based approaches, symmetrized Kullback-Leibler divergence, and Kendall's $\tau$ rank correlation, in order to quantify $u$'s mainstreaminess.
Second, we detail country-specific peculiarities concerning what defines the countries' mainstream and discuss the proposed mainstreaminess definitions.
Third, we show that incorporating the proposed global and country-specific mainstreaminess measures into the music recommendation process can notably improve accuracy of rating prediction.</abstract>
		<language>English</language>
		<doi>10.13052/jmm1550-4646.1415</doi>
		<title>An analysis of global and regional mainstreaminess for personalized music recommender systems</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/255e4b3f9c80860e6ca8fc3fec613bd4b/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2020-04-30 00:00:00</date>
		<count>1</count>
		<booktitle>The Oxford Handbook of Algorithmic Music</booktitle>
		<publisher>Oxford University Press</publisher>
		<year>2018</year>
		<url>http://dblp.uni-trier.de/db/books/collections/MD2018.html#McLeanD18</url>
		<author>Alex McLean</author>
		<author>Roger T. Dean</author>
		<authors>
			<first>Alex</first>
		</authors>
		<authors>
			<last>McLean</last>
		</authors>
		<authors>
			<first>Roger T.</first>
		</authors>
		<authors>
			<last>Dean</last>
		</authors>
		<editor>Alex McLean</editor>
		<editor>Roger T. Dean</editor>
		<editors>
			<first>Roger T.</first>
		</editors>
		<editors>
			<last>Dean</last>
		</editors>
		<editors>
			<first>Roger T.</first>
		</editors>
		<editors>
			<last>Dean</last>
		</editors>
		<isbn>978-0-19-022699-2</isbn>
		<title>Musical Algorithms as Tools, Languages and Partners: A Perspective.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/287e69c756af04f798fcbb3caec521214/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2020-04-30 00:00:00</date>
		<count>1</count>
		<booktitle>The Oxford Handbook of Algorithmic Music</booktitle>
		<publisher>Oxford University Press</publisher>
		<year>2018</year>
		<url>http://dblp.uni-trier.de/db/books/collections/MD2018.html#Collins18</url>
		<author>Nick Collins</author>
		<authors>
			<first>Nick</first>
		</authors>
		<authors>
			<last>Collins</last>
		</authors>
		<editor>Alex McLean</editor>
		<editor>Roger T. Dean</editor>
		<editors>
			<first>Nick</first>
		</editors>
		<editors>
			<last>Collins</last>
		</editors>
		<editors>
			<first>Nick</first>
		</editors>
		<editors>
			<last>Collins</last>
		</editors>
		<isbn>978-0-19-022699-2</isbn>
		<title>Origins of Algorithmic Thinking in Music.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/282ab10ae0634f3885c456df85fb9cc52/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2020-04-30 00:00:00</date>
		<count>1</count>
		<booktitle>The Oxford Handbook of Algorithmic Music</booktitle>
		<publisher>Oxford University Press</publisher>
		<year>2018</year>
		<url>http://dblp.uni-trier.de/db/books/collections/MD2018.html#Scaletti18</url>
		<author>Carla A. Scaletti</author>
		<authors>
			<first>Carla A.</first>
		</authors>
		<authors>
			<last>Scaletti</last>
		</authors>
		<editor>Alex McLean</editor>
		<editor>Roger T. Dean</editor>
		<editors>
			<first>Carla A.</first>
		</editors>
		<editors>
			<last>Scaletti</last>
		</editors>
		<editors>
			<first>Carla A.</first>
		</editors>
		<editors>
			<last>Scaletti</last>
		</editors>
		<isbn>978-0-19-022699-2</isbn>
		<title>Sonification != music.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2eb5d829df7359f54d7de296959e2136c/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2020-04-30 00:00:00</date>
		<count>1</count>
		<booktitle>The Oxford Handbook of Algorithmic Music</booktitle>
		<publisher>Oxford University Press</publisher>
		<year>2018</year>
		<url>http://dblp.uni-trier.de/db/books/collections/MD2018.html#Kanaga18</url>
		<author>David Kanaga</author>
		<authors>
			<first>David</first>
		</authors>
		<authors>
			<last>Kanaga</last>
		</authors>
		<editor>Alex McLean</editor>
		<editor>Roger T. Dean</editor>
		<editors>
			<first>David</first>
		</editors>
		<editors>
			<last>Kanaga</last>
		</editors>
		<editors>
			<first>David</first>
		</editors>
		<editors>
			<last>Kanaga</last>
		</editors>
		<isbn>978-0-19-022699-2</isbn>
		<title>Ecooperatic Music Game Theory.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/201c40f5a28625fe762a938468734ac55/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2020-04-30 00:00:00</date>
		<count>1</count>
		<booktitle>The Oxford Handbook of Algorithmic Music</booktitle>
		<publisher>Oxford University Press</publisher>
		<year>2018</year>
		<url>http://dblp.uni-trier.de/db/books/collections/MD2018.html#Bullock18</url>
		<author>Jamie Bullock</author>
		<authors>
			<first>Jamie</first>
		</authors>
		<authors>
			<last>Bullock</last>
		</authors>
		<editor>Alex McLean</editor>
		<editor>Roger T. Dean</editor>
		<editors>
			<first>Jamie</first>
		</editors>
		<editors>
			<last>Bullock</last>
		</editors>
		<editors>
			<first>Jamie</first>
		</editors>
		<editors>
			<last>Bullock</last>
		</editors>
		<isbn>978-0-19-022699-2</isbn>
		<title>Designing Interfaces for Musical Algorithms.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/247bfd8ae6292987e4074921c3e63a5c4/bauerc</id>
		<tags>imported</tags>
		<tags>fairness</tags>
		<tags>music</tags>
		<tags>mainstreaminess</tags>
		<tags>recsys</tags>
		<description></description>
		<date>2021-05-22 00:30:13</date>
		<count>1</count>
		<journal>EPJ Data Science</journal>
		<publisher>Springer Science and Business Media LLC</publisher>
		<year>2021</year>
		<url>https://doi.org/10.1140%2Fepjds%2Fs13688-021-00268-9</url>
		<author>Dominik Kowald</author>
		<author>Peter Muellner</author>
		<author>Eva Zangerle</author>
		<author>Christine Bauer</author>
		<author>Markus Schedl</author>
		<author>Elisabeth Lex</author>
		<authors>
			<first>Dominik</first>
		</authors>
		<authors>
			<last>Kowald</last>
		</authors>
		<authors>
			<first>Peter</first>
		</authors>
		<authors>
			<last>Muellner</last>
		</authors>
		<authors>
			<first>Eva</first>
		</authors>
		<authors>
			<last>Zangerle</last>
		</authors>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<authors>
			<first>Elisabeth</first>
		</authors>
		<authors>
			<last>Lex</last>
		</authors>
		<volume>10</volume>
		<number>1</number>
		<abstract>Music recommender systems have become an integral part of music streaming services such as Spotify and Last.fm to assist users navigating the extensive music collections offered by them. However, while music listeners interested in mainstream music are traditionally served well by music recommender systems, users interested in music beyond the mainstream (i.e., non-popular music) rarely receive relevant recommendations. In this paper, we study the characteristics of beyond-mainstream music and music listeners and analyze to what extent these characteristics impact the quality of music recommendations provided. Therefore, we create a novel dataset consisting of Last.fm listening histories of several thousand beyond-mainstream music listeners, which we enrich with additional metadata describing music tracks and music listeners. Our analysis of this dataset shows four subgroups within the group of beyond-mainstream music listeners that differ not only with respect to their preferred music but also with their demographic characteristics. Furthermore, we evaluate the quality of music recommendations that these subgroups are provided with four different recommendation algorithms where we find significant differences between the groups. Specifically, our results show a positive correlation between a subgroup’s openness towards music listened to by members of other subgroups and recommendation accuracy. We believe that our findings provide valuable insights for developing improved user models and recommendation approaches to better serve beyond-mainstream music listeners.</abstract>
		<language>English</language>
		<issn>2193-1127</issn>
		<doi>10.1140/epjds/s13688-021-00268-9</doi>
		<title>Support the underground: characteristics of beyond-mainstream music listeners</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e1f8e5191a2caefb9e3ef96ea5fd3f50/analyst</id>
		<tags>music</tags>
		<tags>machine-learning</tags>
		<tags>2020</tags>
		<description>Understanding Optical Music Recognition | ACM Computing Surveys</description>
		<date>2021-10-06 12:25:42</date>
		<count>4</count>
		<journal>ACM Computing Surveys</journal>
		<publisher>Association for Computing Machinery (ACM)</publisher>
		<year>2020</year>
		<url>https://doi.org/10.1145%2F3397499</url>
		<author>Jorge Calvo-Zaragoza</author>
		<author>Jan Hajic Jr.</author>
		<author>Alexander Pacha</author>
		<authors>
			<first>Jorge</first>
		</authors>
		<authors>
			<last>Calvo-Zaragoza</last>
		</authors>
		<authors>
			<first>Jan Hajic</first>
		</authors>
		<authors>
			<last>Jr.</last>
		</authors>
		<authors>
			<first>Alexander</first>
		</authors>
		<authors>
			<last>Pacha</last>
		</authors>
		<volume>53</volume>
		<number>4</number>
		<pages>1--35</pages>
		<doi>10.1145/3397499</doi>
		<title>Understanding Optical Music Recognition</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25b59a53ec175a9cfedfc43869d5f640b/brusilovsky</id>
		<tags>recsys2021</tags>
		<tags>user-control</tags>
		<tags>interactive-recommender</tags>
		<tags>information-exploration</tags>
		<description>The role of preference consistency, defaults and musical expertise in users’ exploration behavior in a genre exploration recommender | Fifteenth ACM Conference on Recommender Systems</description>
		<date>2021-10-06 20:46:51</date>
		<count>1</count>
		<booktitle>Fifteenth ACM Conference on Recommender Systems</booktitle>
		<publisher>ACM</publisher>
		<year>2021</year>
		<url>https://doi.org/10.1145%2F3460231.3474253</url>
		<author>Yu Liang</author>
		<author>Martijn C. Willemsen</author>
		<authors>
			<first>Yu</first>
		</authors>
		<authors>
			<last>Liang</last>
		</authors>
		<authors>
			<first>Martijn C.</first>
		</authors>
		<authors>
			<last>Willemsen</last>
		</authors>
		<pages>230-240</pages>
		<abstract>Recommender systems are efficient at predicting users’ current preferences, but how users’ preferences develop over time is still under-explored. In this work, we study the development of users’ musical preferences. Exploring musical preference consistency between short-term and long-term preferences in data from earlier studies, we find that users with higher musical expertise have more consistent preferences at their top-listened artists and tags than those with lower musical expertise. Users typically chose to explore genres that were close to their current preferences, and this effect was stronger for expert users. Based on these findings we conducted a user study on genre exploration to investigate (1) whether it is possible to nudge users to explore more distant genres, and (2) how users’ exploration behaviors within a genre are influenced by default recommendation settings that balance personalization with genre representativeness in different ways. Our results show that users were more likely to select the more distant genres if these were presented at the top of the list. However, users with high musical expertise were less likely to do so, consistent with our earlier findings. When given a representative or mixed (balanced) default for exploration within a genre, users selected less personalized recommendation settings and explored further away from their current preferences, than with a personalized default. However, this effect was moderated by users’ slider usage behaviors. Overall, our results suggest that (personalized) defaults can nudge users to explore new, more distant genres and songs. However, the effect is smaller for those with higher musical expertise levels.</abstract>
		<doi>10.1145/3460231.3474253</doi>
		<title>The role of preference consistency, defaults and musical expertise in users' exploration behavior in a genre exploration recommender</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/295bfaddd91750df43aa13ce5c6620283/simonha94</id>
		<tags>Retrieval;</tags>
		<tags>uncovr</tags>
		<tags>H33</tags>
		<tags>H55</tags>
		<tags>Terms:</tags>
		<tags>Re-trieval,context-based</tags>
		<tags>retrieval,cross-media</tags>
		<tags>retrieval,music</tags>
		<tags>Storage</tags>
		<tags>Algorithms</tags>
		<tags>[Information</tags>
		<tags>General</tags>
		<tags>Music</tags>
		<tags>Keywords:</tags>
		<tags>Systems]:</tags>
		<tags>musicsearch</tags>
		<tags>In-terfaces</tags>
		<tags>engine,Music</tags>
		<tags>Computing</tags>
		<tags>Information</tags>
		<tags>mir</tags>
		<tags>and</tags>
		<tags>similarity</tags>
		<tags>search</tags>
		<tags>Presentation-Sound</tags>
		<description></description>
		<date>2021-10-15 08:46:33</date>
		<count>3</count>
		<year>2007</year>
		<url>http://www.findsounds.com</url>
		<author>Peter Knees</author>
		<author>Tim Pohle</author>
		<author>Markus Schedl</author>
		<author>Gerhard Widmer</author>
		<authors>
			<first>Peter</first>
		</authors>
		<authors>
			<last>Knees</last>
		</authors>
		<authors>
			<first>Tim</first>
		</authors>
		<authors>
			<last>Pohle</last>
		</authors>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<authors>
			<first>Gerhard</first>
		</authors>
		<authors>
			<last>Widmer</last>
		</authors>
		<abstract>An approach is presented to automatically build a search engine for large-scale music collections that can be queried through natural language. While existing approaches depend on explicit manual annotations and meta-data assigned to the individual audio pieces, we automatically derive descriptions by making use of methods from Web Retrieval and Music Information Retrieval. Based on the ID3 tags of a collection of mp3 files, we retrieve relevant Web pages via Google queries and use the contents of these pages to characterize the music pieces and represent them by term vectors. By incorporating complementary information about acoustic similarity we are able to both reduce the dimensionality of the vector space and improve the performance of retrieval, i.e. the quality of the results. Furthermore, the usage of audio similarity allows us to also characterize audio pieces when there is no associated information found on the Web.</abstract>
		<isbn>9781595935977</isbn>
		<title>A Music Search Engine Built upon Audio-based and Web-based Similarity Measures</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b583865228b06bcbad56c57cfd70313d/jaeschke</id>
		<tags>identification</tags>
		<tags>version</tags>
		<tags>uncovr</tags>
		<tags>mir</tags>
		<tags>similarity</tags>
		<tags>audio</tags>
		<tags>variant</tags>
		<tags>duplicate</tags>
		<tags>ml</tags>
		<description>[2109.02472] Audio-based Musical Version Identification: Elements and Challenges</description>
		<date>2021-10-12 16:07:09</date>
		<count>1</count>
		<year>2021</year>
		<url>http://arxiv.org/abs/2109.02472</url>
		<author>Furkan Yesiler</author>
		<author>Guillaume Doras</author>
		<author>Rachel M. Bittner</author>
		<author>Christopher J. Tralie</author>
		<author>Joan Serrà</author>
		<authors>
			<first>Furkan</first>
		</authors>
		<authors>
			<last>Yesiler</last>
		</authors>
		<authors>
			<first>Guillaume</first>
		</authors>
		<authors>
			<last>Doras</last>
		</authors>
		<authors>
			<first>Rachel M.</first>
		</authors>
		<authors>
			<last>Bittner</last>
		</authors>
		<authors>
			<first>Christopher J.</first>
		</authors>
		<authors>
			<last>Tralie</last>
		</authors>
		<authors>
			<first>Joan</first>
		</authors>
		<authors>
			<last>Serrà</last>
		</authors>
		<abstract>In this article, we aim to provide a review of the key ideas and approaches
proposed in 20 years of scientific literature around musical version
identification (VI) research and connect them to current practice. For more
than a decade, VI systems suffered from the accuracy-scalability trade-off,
with attempts to increase accuracy that typically resulted in cumbersome,
non-scalable systems. Recent years, however, have witnessed the rise of deep
learning-based approaches that take a step toward bridging the
accuracy-scalability gap, yielding systems that can realistically be deployed
in industrial applications. Although this trend positively influences the
number of researchers and institutions working on VI, it may also result in
obscuring the literature before the deep learning era. To appreciate two
decades of novel ideas in VI research and to facilitate building better
systems, we now review some of the successful concepts and applications
proposed in the literature and study their evolution throughout the years.</abstract>
		<title>Audio-based Musical Version Identification: Elements and Challenges</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24a7f5e88fdd622e4815b0f0ce31665ae/sapo</id>
		<tags>myown</tags>
		<tags>phdthesis</tags>
		<description></description>
		<date>2021-12-10 17:30:49</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2022</year>
		<url></url>
		<author>Federico Simonetta</author>
		<authors>
			<first>Federico</first>
		</authors>
		<authors>
			<last>Simonetta</last>
		</authors>
		<abstract>In this paper, a novel approach aiming at the democratization of music production tools via economical devices is discussed. The approach consists of the extraction of features from a low-quality audio recording and in their resynthesis with digital synthesizers, placing a particular attention on the difference between the original and the synthesizer acoustical contexts. To this end and in line with recent literature, a distinction is proposed between the concept of "performance" and the one of "interpretation", which expresses the ärtistic intention". In the paper, first, the existing DSP techniques for the restoration of deteriorated music sources are reviewed while considering both
technical aspects and ethical implications. Then, their limitations are analyzed and the novel automated approach able to address the previous ethical issues is described. Finally, various synthesis methods are examined and possible solutions for the realization of the proposed approach are discussed.</abstract>
		<title>Towards Faithful Automatic Music Resynthesis</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2cffee804523131553b1d8355fa8ff23f/sapo</id>
		<tags>phdthesis</tags>
		<tags>neural_networks_literature</tags>
		<tags>automatic_transcription</tags>
		<description>[2010.09969] The Effect of Spectrogram Reconstruction on Automatic Music Transcription: An Alternative Approach to Improve Transcription Accuracy</description>
		<date>2021-01-29 12:27:25</date>
		<count>2</count>
		<year>2020</year>
		<url>http://arxiv.org/abs/2010.09969</url>
		<author>Kin Wai Cheuk</author>
		<author>Yin-Jyun Luo</author>
		<author>Emmanouil Benetos</author>
		<author>Dorien Herremans</author>
		<authors>
			<first>Kin Wai</first>
		</authors>
		<authors>
			<last>Cheuk</last>
		</authors>
		<authors>
			<first>Yin-Jyun</first>
		</authors>
		<authors>
			<last>Luo</last>
		</authors>
		<authors>
			<first>Emmanouil</first>
		</authors>
		<authors>
			<last>Benetos</last>
		</authors>
		<authors>
			<first>Dorien</first>
		</authors>
		<authors>
			<last>Herremans</last>
		</authors>
		<abstract>Most of the state-of-the-art automatic music transcription (AMT) models break
down the main transcription task into sub-tasks such as onset prediction and
offset prediction and train them with onset and offset labels. These
predictions are then concatenated together and used as the input to train
another model with the pitch labels to obtain the final transcription. We
attempt to use only the pitch labels (together with spectrogram reconstruction
loss) and explore how far this model can go without introducing supervised
sub-tasks. In this paper, we do not aim at achieving state-of-the-art
transcription accuracy, instead, we explore the effect that spectrogram
reconstruction has on our AMT model. Our proposed model consists of two U-nets:
the first U-net transcribes the spectrogram into a posteriorgram, and a second
U-net transforms the posteriorgram back into a spectrogram. A reconstruction
loss is applied between the original spectrogram and the reconstructed
spectrogram to constrain the second U-net to focus only on reconstruction. We
train our model on three different datasets: MAPS, MAESTRO, and MusicNet. Our
experiments show that adding the reconstruction loss can generally improve the
note-level transcription accuracy when compared to the same model without the
reconstruction part. Moreover, it can also boost the frame-level precision to
be higher than the state-of-the-art models. The feature maps learned by our
U-net contain gridlike structures (not present in the baseline model) which
implies that with the presence of the reconstruction loss, the model is
probably trying to count along both the time and frequency axis, resulting in a
higher note-level transcription accuracy.</abstract>
		<title>The Effect of Spectrogram Reconstruction on Automatic Music Transcription: An Alternative Approach to Improve Transcription Accuracy.</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/232336300f33d2c6b23356ea3222d1efc/sapo</id>
		<tags>phdthesis</tags>
		<tags>performance_analysis</tags>
		<tags>mia</tags>
		<description>Room Effect on Musicians’ Performance | SpringerLink https://link-springer-com.pros.lib.unimi.it/chapter/10.1007/978-3-030-00386-9_9</description>
		<date>2021-07-16 20:24:23</date>
		<count>1</count>
		<booktitle>The Technology of Binaural Understanding</booktitle>
		<publisher>Springer International Publishing</publisher>
		<address>Cham</address>
		<year>2020</year>
		<url>https://doi.org/10.1007/978-3-030-00386-9_9</url>
		<author>Malte Kob</author>
		<author>Sebastià V. Amengual Garí</author>
		<author>Zora Schärer Kalkandjiev</author>
		<authors>
			<first>Malte</first>
		</authors>
		<authors>
			<last>Kob</last>
		</authors>
		<authors>
			<first>Sebastià V.</first>
		</authors>
		<authors>
			<last>Amengual Garí</last>
		</authors>
		<authors>
			<first>Zora</first>
		</authors>
		<authors>
			<last>Schärer Kalkandjiev</last>
		</authors>
		<editor>Jens Blauert</editor>
		<editor>Jonas Braasch</editor>
		<editors>
			<first>Zora</first>
		</editors>
		<editors>
			<last>Schärer Kalkandjiev</last>
		</editors>
		<editors>
			<first>Zora</first>
		</editors>
		<editors>
			<last>Schärer Kalkandjiev</last>
		</editors>
		<pages>223--249</pages>
		<abstract>This chapter reviews the basics of music and room acoustics perception, an overview of auralization methods for the investigation of music performance and a series of studies related to the impact of room acoustics on listeners and musicians. The acoustics of the performance environment play a major role for musicians, both during rehearsals and concerts. However, systematic investigations of music performance are challenging due to the variety of conditions that determine the artists' performance. Set-ups that allow controlled studies with variable but well-defined acoustic conditions have been developed over the last decades with increasing naturalness and applicability. Current auralization methods allow the reproduction of measured or synthesized room acoustics in real-time, thus enabling the perceptual assessment of room acoustics in laboratory conditions, isolating acoustics from other potential impacting factors. Common methodologies, as well as advantages and limitations of such virtual environments for the study of music and room acoustics perception are discussed in the first section. The virtual environments enable studies that help to explain why and how room acoustics can affect the listener subjective impact of a musical performance and to what extent listeners can be classified depending on their individual taste. Recent studies have shown that musicians systematically adjust their musical performance and adapt to the room acoustical conditions. The most important findings from these studies are presented in the second section. Methods and results from recent investigations of the impact of room acoustics on music performance are discussed in the third section of this chapter.</abstract>
		<isbn>978-3-030-00386-9</isbn>
		<doi>10.1007/978-3-030-00386-9_9</doi>
		<title>Room Effect on Musicians' Performance</title>
		<pubtype>inbook</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2101d076a079b4db4af9b5255c9269cbc/sapo</id>
		<tags>phdthesis</tags>
		<tags>synthesis</tags>
		<description>[2011.06801] A Comprehensive Survey on Deep Music Generation: Multi-level Representations, Algorithms, Evaluations, and Future Directions https://arxiv.org/abs/2011.06801</description>
		<date>2021-12-03 17:40:52</date>
		<count>2</count>
		<year>2020</year>
		<url>http://arxiv.org/abs/2011.06801</url>
		<author>Shulei Ji</author>
		<author>Jing Luo</author>
		<author>Xinyu Yang</author>
		<authors>
			<first>Shulei</first>
		</authors>
		<authors>
			<last>Ji</last>
		</authors>
		<authors>
			<first>Jing</first>
		</authors>
		<authors>
			<last>Luo</last>
		</authors>
		<authors>
			<first>Xinyu</first>
		</authors>
		<authors>
			<last>Yang</last>
		</authors>
		<abstract>The utilization of deep learning techniques in generating various contents
(such as image, text, etc.) has become a trend. Especially music, the topic of
this paper, has attracted widespread attention of countless researchers.The
whole process of producing music can be divided into three stages,
corresponding to the three levels of music generation: score generation
produces scores, performance generation adds performance characteristics to the
scores, and audio generation converts scores with performance characteristics
into audio by assigning timbre or generates music in audio format directly.
Previous surveys have explored the network models employed in the field of
automatic music generation. However, the development history, the model
evolution, as well as the pros and cons of same music generation task have not
been clearly illustrated. This paper attempts to provide an overview of various
composition tasks under different music generation levels, covering most of the
currently popular music generation tasks using deep learning. In addition, we
summarize the datasets suitable for diverse tasks, discuss the music
representations, the evaluation methods as well as the challenges under
different levels, and finally point out several future directions.</abstract>
		<title>A Comprehensive Survey on Deep Music Generation: Multi-level
  Representations, Algorithms, Evaluations, and Future Directions</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2217587c7d2e8081d9a6688c3d9073d71/sapo</id>
		<tags>imported</tags>
		<tags>phdthesis</tags>
		<description></description>
		<date>2021-11-22 11:44:52</date>
		<count>3</count>
		<journal>Int. Work. on Multilayer Music Representation and Processing</journal>
		<booktitle>Proceedings of 2019 International Workshop on Multilayer Music Representation and Processing</booktitle>
		<publisher>IEEE Conference Publishing Services</publisher>
		<address>Milan, Italy</address>
		<year>2019</year>
		<url></url>
		<author>Federico Simonetta</author>
		<author>Stavros Ntalampiras</author>
		<author>Federico Avanzini</author>
		<authors>
			<first>Federico</first>
		</authors>
		<authors>
			<last>Simonetta</last>
		</authors>
		<authors>
			<first>Stavros</first>
		</authors>
		<authors>
			<last>Ntalampiras</last>
		</authors>
		<authors>
			<first>Federico</first>
		</authors>
		<authors>
			<last>Avanzini</last>
		</authors>
		<pages>10--18</pages>
		<title>Multimodal Music Information Processing and Retrieval: Survey and Future Challenges</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2cf56ac1cf3d1b2a7adf4127c7207e5c2/brusilovsky</id>
		<tags>fairness</tags>
		<tags>group-recommendation</tags>
		<tags>recommender</tags>
		<tags>iui2021</tags>
		<description>Perception of Fairness in Group Music Recommender Systems | 26th International Conference on Intelligent User Interfaces</description>
		<date>2022-01-16 20:59:07</date>
		<count>1</count>
		<booktitle>26th International Conference on Intelligent User Interfaces</booktitle>
		<publisher>ACM</publisher>
		<year>2021</year>
		<url>https://doi.org/10.1145%2F3397481.3450642</url>
		<author>Nyi Nyi Htun</author>
		<author>Elisa Lecluse</author>
		<author>Katrien Verbert</author>
		<authors>
			<first>Nyi Nyi</first>
		</authors>
		<authors>
			<last>Htun</last>
		</authors>
		<authors>
			<first>Elisa</first>
		</authors>
		<authors>
			<last>Lecluse</last>
		</authors>
		<authors>
			<first>Katrien</first>
		</authors>
		<authors>
			<last>Verbert</last>
		</authors>
		<pages>302-306</pages>
		<abstract>Fairness is an important aspect in group recommender systems (GRSs). They must ensure that potentially diverse preferences of all group members are taken into consideration when providing recommendations. Previous work has proposed a number of conflict elicitation and merging techniques to produce preferable recommendations for group members. However, we have yet to understand the influence of user personality on the perception of fairness in GRSs. To examine this gap, we use music recommendation as an example domain. We have developed a web-based group music recommender system using the Spotify API and two simple ranking algorithms: one based on the time the songs were voted by users (time-based) and the other based on a dissimilarity score (dissimilarity-based). A within-subjects experiment was conducted with 45 participants divided into groups of 3 (15 groups). Results showed that openness personality has a negative correlation with the perception that fairness is important in groups.</abstract>
		<doi>10.1145/3397481.3450642</doi>
		<title>Perception of Fairness in Group Music Recommender Systems</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d28ed089ba6b3c457f43a930e9879f93/kurtjx</id>
		<tags>music_similarity</tags>
		<description></description>
		<date>2008-04-07 17:50:12</date>
		<count>8</count>
		<booktitle>Speech and Audio Processing, IEEE Transactions on</booktitle>
		<year>2002</year>
		<url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1021072</url>
		<author>G. Tzanetakis</author>
		<author>P. Cook</author>
		<authors>
			<first>G.</first>
		</authors>
		<authors>
			<last>Tzanetakis</last>
		</authors>
		<authors>
			<first>P.</first>
		</authors>
		<authors>
			<last>Cook</last>
		</authors>
		<volume>10</volume>
		<pages>293- 302</pages>
		<abstract>Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61% for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.</abstract>
		<issn>1063-6676</issn>
		<doi>10.1109/TSA.2002.800560</doi>
		<title>Musical genre classification of audio signals</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26a0824d093e433664222e63fa570b674/brazovayeye</id>
		<tags>genetic</tags>
		<tags>algorithms,</tags>
		<tags>programming,</tags>
		<tags>Self-Organising</tags>
		<tags>Music</tags>
		<tags>Map,</tags>
		<tags>Features</tags>
		<tags>Automatic</tags>
		<tags>generation</tags>
		<description></description>
		<date>2008-06-19 17:46:40</date>
		<count>2</count>
		<booktitle>Applications of Evolutionary Computing,
                 EvoWorkshops2007: EvoCOMNET, EvoFIN, EvoIASP,
                 EvoInteraction, EvoMUSART, EvoSTOC,
                 EvoTransLog</booktitle>
		<series>LNCS</series>
		<publisher>Springer Verlag</publisher>
		<address>Valencia, Spain</address>
		<year>2007</year>
		<url></url>
		<author>Somnuk Phon-Amnuaisuk</author>
		<author>Edwin Hui Hean Law</author>
		<author>Chin Kuan Ho</author>
		<authors>
			<first>Somnuk</first>
		</authors>
		<authors>
			<last>Phon-Amnuaisuk</last>
		</authors>
		<authors>
			<first>Edwin Hui Hean</first>
		</authors>
		<authors>
			<last>Law</last>
		</authors>
		<authors>
			<first>Chin Kuan</first>
		</authors>
		<authors>
			<last>Ho</last>
		</authors>
		<editor>Mario Giacobini</editor>
		<editor>Anthony Brabazon</editor>
		<editor>Stefano Cagnoni</editor>
		<editor>Gianni A. Di Caro</editor>
		<editor>Rolf Drechsler</editor>
		<editor>Muddassar Farooq</editor>
		<editor>Andreas Fink</editor>
		<editor>Evelyne Lutton</editor>
		<editor>Penousal Machado</editor>
		<editor>Stefan Minner</editor>
		<editor>Michael O'Neill</editor>
		<editor>Juan Romero</editor>
		<editor>Franz Rothlauf</editor>
		<editor>Giovanni Squillero</editor>
		<editor>Hideyuki Takagi</editor>
		<editor>A. Sima Uyar</editor>
		<editor>Shengxiang Yang</editor>
		<editors>
			<first>Chin Kuan</first>
		</editors>
		<editors>
			<last>Ho</last>
		</editors>
		<editors>
			<first>Chin Kuan</first>
		</editors>
		<editors>
			<last>Ho</last>
		</editors>
		<editors>
			<first>Chin Kuan</first>
		</editors>
		<editors>
			<last>Ho</last>
		</editors>
		<editors>
			<first>Chin Kuan</first>
		</editors>
		<editors>
			<last>Ho</last>
		</editors>
		<editors>
			<first>Chin Kuan</first>
		</editors>
		<editors>
			<last>Ho</last>
		</editors>
		<editors>
			<first>Chin Kuan</first>
		</editors>
		<editors>
			<last>Ho</last>
		</editors>
		<editors>
			<first>Chin Kuan</first>
		</editors>
		<editors>
			<last>Ho</last>
		</editors>
		<editors>
			<first>Chin Kuan</first>
		</editors>
		<editors>
			<last>Ho</last>
		</editors>
		<editors>
			<first>Chin Kuan</first>
		</editors>
		<editors>
			<last>Ho</last>
		</editors>
		<editors>
			<first>Chin Kuan</first>
		</editors>
		<editors>
			<last>Ho</last>
		</editors>
		<editors>
			<first>Chin Kuan</first>
		</editors>
		<editors>
			<last>Ho</last>
		</editors>
		<editors>
			<first>Chin Kuan</first>
		</editors>
		<editors>
			<last>Ho</last>
		</editors>
		<editors>
			<first>Chin Kuan</first>
		</editors>
		<editors>
			<last>Ho</last>
		</editors>
		<editors>
			<first>Chin Kuan</first>
		</editors>
		<editors>
			<last>Ho</last>
		</editors>
		<editors>
			<first>Chin Kuan</first>
		</editors>
		<editors>
			<last>Ho</last>
		</editors>
		<editors>
			<first>Chin Kuan</first>
		</editors>
		<editors>
			<last>Ho</last>
		</editors>
		<editors>
			<first>Chin Kuan</first>
		</editors>
		<editors>
			<last>Ho</last>
		</editors>
		<volume>4448</volume>
		<pages>557--566</pages>
		<abstract>Most real life applications have huge search spaces.
                 Evolutionary Computation provides an advantage in the
                 form of parallel explorations of many parts of the
                 search space. In this report, Genetic Programming is
                 the technique we used to search for good melodic
                 fragments. It is generally accepted that knowledge is a
                 crucial factor to guide search. Here, we show that SOM
                 can be used to facilitate the encoding of domain
                 knowledge into the system. The SOM was trained with
                 music of desired quality and was used as fitness
                 functions. In this work, we are not interested in music
                 with complex rules but with simple music employed in
                 computer games. We argue that this technique provides a
                 flexible and adaptive means to capture the domain
                 knowledge in the system.</abstract>
		<isbn13>978-3-540-71804-8</isbn13>
		<notes>EvoWorkshops2007</notes>
		<doi>doi:10.1007/978-3-540-71805-5_61</doi>
		<title>Evolving Music Generation with SOM-fitness Genetic
                 Programming</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/274841b95975c8b5d32a1dcee9ea57f73/brazovayeye</id>
		<tags>music,</tags>
		<tags>genetic</tags>
		<tags>composition,</tags>
		<tags>languages</tags>
		<tags>grammars,</tags>
		<tags>automatic</tags>
		<tags>grammatical</tags>
		<tags>formal</tags>
		<tags>algorithms,</tags>
		<tags>computer</tags>
		<tags>evolution,</tags>
		<tags>programming,</tags>
		<tags>and</tags>
		<tags>evolutionary</tags>
		<tags>programming</tags>
		<description></description>
		<date>2008-06-19 17:35:00</date>
		<count>2</count>
		<booktitle>Proceedings of the 2002 conference on APL</booktitle>
		<publisher>ACM Press</publisher>
		<address>Madrid, Spain</address>
		<year>2002</year>
		<url>http://portal.acm.org/citation.cfm?id=602249&jmp=cit&dl=portal&dl=ACM</url>
		<author>Alfonso Ortega de la Puente</author>
		<author>Rafael Sanchez Alfonso</author>
		<author>Manuel Alfonseca Moreno</author>
		<authors>
			<first>Alfonso Ortega</first>
		</authors>
		<authors>
			<last>de la Puente</last>
		</authors>
		<authors>
			<first>Rafael Sanchez</first>
		</authors>
		<authors>
			<last>Alfonso</last>
		</authors>
		<authors>
			<first>Manuel Alfonseca</first>
		</authors>
		<authors>
			<last>Moreno</last>
		</authors>
		<pages>148--155</pages>
		<abstract>Grammatical evolution may be applied to the domain of
                 automatic composition. Our goal is to test this
                 technique as an alternate tool for automatic
                 composition. The AP440 auxiliary processor will be used
                 to play music, thus we shall use a grammar that
                 generates AP440 melodies. Grammar evolution will use
                 fitness functions defined from several well-known
                 single melodies to automatically generate AP440
                 compositions that are expected to sound like those
                 composed by human musicians.</abstract>
		<isbn>1-58113-577-7</isbn>
		<notes>Acknowledgement sponsored by the Spanish
                 Interdepartmental Commission of Science and Technology
                 (CICYT), project numbers TEL1999-0181 and
                 TIC2001-0685-C02-1.</notes>
		<doi>doi:10.1145/602231.602249</doi>
		<title>Automatic composition of music by means of grammatical
                 evolution</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a7642bc3228e7f99d2d55e0b83cffb4f/hanappe</id>
		<tags>imported</tags>
		<description></description>
		<date>2006-07-13 17:11:36</date>
		<count>2</count>
		<booktitle>Readings in Music and Artificial Intelligence</booktitle>
		<series>Contempoary Music Series</series>
		<publisher>Harwood Academic Publishers</publisher>
		<address>The Netherlands</address>
		<year>2000</year>
		<url></url>
		<author>E. R. Miranda</author>
		<authors>
			<first>E. R.</first>
		</authors>
		<authors>
			<last>Miranda</last>
		</authors>
		<editor>E. R. Miranda</editor>
		<editors>
			<first>E. R.</first>
		</editors>
		<editors>
			<last>Miranda</last>
		</editors>
		<volume>20</volume>
		<pages>1-13</pages>
		<title>Regarding Music, Machines, Intelligence and the Brain: An Introduction to Music and AI</title>
		<pubtype>inbook</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d8e1309fff10a8f3edac235b81f55699/lychen1109</id>
		<tags>matlab</tags>
		<tags>music</tags>
		<tags>audio</tags>
		<tags>toolbox</tags>
		<description></description>
		<date>2006-04-17 11:48:08</date>
		<count>4</count>
		<journal>Proceedings of the 5th International Conference on Music</journal>
		<year>2004</year>
		<url>http://www.ofai.at/cgi-bin/get-tr%3Fpaper%3Doefai-tr-2004-15.pdf</url>
		<author>E Pampalk</author>
		<authors>
			<first>E</first>
		</authors>
		<authors>
			<last>Pampalk</last>
		</authors>
		<title>A MATLAB TOOLBOX TO COMPUTE MUSIC SIMILARITY FROM AUDIO</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/253fd44637f70aa0bef28a841c9442aa1/bfields</id>
		<tags>music</tags>
		<tags>playlistResearch</tags>
		<tags>radio</tags>
		<description>Programming music: radio as mediator -- Hennion and Meadel 8 (3): 281 -- Media, Culture & Society</description>
		<date>2010-01-28 12:10:26</date>
		<count>1</count>
		<journal>Media Culture Society</journal>
		<year>1986</year>
		<url>http://mcs.sagepub.com</url>
		<author>Antoine Hennion</author>
		<author>Cecile Meadel</author>
		<authors>
			<first>Antoine</first>
		</authors>
		<authors>
			<last>Hennion</last>
		</authors>
		<authors>
			<first>Cecile</first>
		</authors>
		<authors>
			<last>Meadel</last>
		</authors>
		<volume>8</volume>
		<number>3</number>
		<pages>281-303</pages>
		<doi>10.1177/016344386008003003</doi>
		<eprint>http://mcs.sagepub.com/cgi/reprint/8/3/281.pdf</eprint>
		<title>Programming music: radio as mediator</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f052efe0deb58ca83910367872eb2f42/bfields</id>
		<tags>music</tags>
		<tags>role_analysis</tags>
		<description>The Aesthetic Gap Between Consumer and Composer -- Mueller 15 (2): 151 -- Journal of Research in Music Education</description>
		<date>2010-01-28 12:25:26</date>
		<count>1</count>
		<journal>Journal of Research in Music Education</journal>
		<year>1967</year>
		<url>http://jrm.sagepub.com</url>
		<author>John H. Mueller</author>
		<authors>
			<first>John H.</first>
		</authors>
		<authors>
			<last>Mueller</last>
		</authors>
		<volume>15</volume>
		<number>2</number>
		<pages>151-158</pages>
		<doi>10.2307/3344013</doi>
		<eprint>http://jrm.sagepub.com/cgi/reprint/15/2/151.pdf</eprint>
		<title>The Aesthetic Gap Between Consumer and Composer</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e688a126ba240f78726da49179dc5503/bfields</id>
		<tags>Playlist</tags>
		<tags>music_similarity</tags>
		<description>initial imports</description>
		<date>2010-01-28 11:42:45</date>
		<count>2</count>
		<booktitle>Proc. 7th ACM SIGMM international workshop on Multimedia information retrieva</booktitle>
		<year>2005</year>
		<url></url>
		<author>R. Ragno</author>
		<author>C.J.C. Burges</author>
		<author>C. Herley</author>
		<authors>
			<first>R.</first>
		</authors>
		<authors>
			<last>Ragno</last>
		</authors>
		<authors>
			<first>C.J.C.</first>
		</authors>
		<authors>
			<last>Burges</last>
		</authors>
		<authors>
			<first>C.</first>
		</authors>
		<authors>
			<last>Herley</last>
		</authors>
		<title>Inferring Similarity Between Music Objects with Application to Playlist Generation</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20ab701048df19d6cabfe93b864f9caa9/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>Popular Music and Society</journal>
		<year>2005</year>
		<url></url>
		<author>George Plasketes</author>
		<authors>
			<first>George</first>
		</authors>
		<authors>
			<last>Plasketes</last>
		</authors>
		<volume>28</volume>
		<number>2</number>
		<pages>137</pages>
		<abstract>Since the 1980s, "Re" has been the predominant cultural mode. This condition is an endless lifestyle loop of repeating, retrieving, rewinding, recycling, reciting, redesigning and reprocessing. Popular music's backward spin accelerated and diversified dramatically during the Re Era. The past quarter century's "like a version" loop invites "The Cover Age" as a fitting characterization. Standardization, interpretation, incorporation, adaptation, appropriation and appreciation have been manifest in a multitude of musical manners and methods, including retrospectives and reissues, the emergence of rap and sampling as commercially dominant pop styles, karaoke, and a steady flow, if not stream, of cover compilations and tribute recordings which revisit a significant cross section of musical periods, styles, genre and artists and their catalogs of compositions. This essay is a collage and chronicle of the continuous coverage, intertextuality, and issues (imitation, ownership, apprenticeship, and preservation) within the karaoke climate of the music, mass media and marketplace triad, with artists, producers, record companies and consumers cohorts in the massive cover up. PUBLICATION ABSTRACT</abstract>
		<issn>03007766</issn>
		<shorttitle>Re-flections on the Cover Age</shorttitle>
		<title>Re-flections on the Cover Age: A Collage of Continuous Coverage in Popular Music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f2868a310a4b44d6248214c9d2aef311/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>Popular Music</journal>
		<year>1988</year>
		<url>http://www.jstor.org/stable/853544</url>
		<author>Richard Middleton</author>
		<authors>
			<first>Richard</first>
		</authors>
		<authors>
			<last>Middleton</last>
		</authors>
		<volume>7</volume>
		<number>2</number>
		<pages>229--230</pages>
		<issn>02611430</issn>
		<shorttitle>Review</shorttitle>
		<title>Review: From Blues to Rock: An Analytical History of Pop Music by David Hatch; Stephen Millward</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/237682962de707a331ea3a46605642c1d/bfields</id>
		<tags>rss</tags>
		<tags>Music_recommendation</tags>
		<tags>semantic_web</tags>
		<tags>foaf</tags>
		<description>initial imports</description>
		<date>2010-01-28 11:42:45</date>
		<count>4</count>
		<booktitle>Proc. of Int. Symposium on Music Information Retrieval</booktitle>
		<year>2005</year>
		<url></url>
		<author>Oscar Celma</author>
		<author>Miquel Ramirez</author>
		<author>Perfecto Herrera</author>
		<authors>
			<first>Oscar</first>
		</authors>
		<authors>
			<last>Celma</last>
		</authors>
		<authors>
			<first>Miquel</first>
		</authors>
		<authors>
			<last>Ramirez</last>
		</authors>
		<authors>
			<first>Perfecto</first>
		</authors>
		<authors>
			<last>Herrera</last>
		</authors>
		<title>Foafing the Music: A Music Recommendation System Based On RSS Feeds and User Preferences</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29e5167a0babaf1c2031be6f2f9ae7651/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>Popular Music</journal>
		<year>1993</year>
		<url>http://www.jstor.org.ezproxy.library.yorku.ca/stable/931297</url>
		<author>Richard Middleton</author>
		<authors>
			<first>Richard</first>
		</authors>
		<authors>
			<last>Middleton</last>
		</authors>
		<volume>12</volume>
		<number>2</number>
		<pages>177--190</pages>
		<issn>02611430</issn>
		<shorttitle>Popular Music Analysis and Musicology</shorttitle>
		<title>Popular Music Analysis and Musicology: Bridging the Gap</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/294bd871995c4b7966519817185cbf9bc/bfields</id>
		<tags>commercial</tags>
		<tags>music</tags>
		<tags>playlistResearch</tags>
		<tags>manual_playlisting</tags>
		<tags>radio</tags>
		<description></description>
		<date>2010-01-28 12:00:26</date>
		<count>2</count>
		<journal>Qualitative Sociology</journal>
		<year>2002</year>
		<url>http://dx.doi.org/10.1023/A:1015494716804</url>
		<author>Jarl A. Ahlkvist</author>
		<author>Robert Faulkner</author>
		<authors>
			<first>Jarl A.</first>
		</authors>
		<authors>
			<last>Ahlkvist</last>
		</authors>
		<authors>
			<first>Robert</first>
		</authors>
		<authors>
			<last>Faulkner</last>
		</authors>
		<volume>25</volume>
		<number>2</number>
		<pages>189--215</pages>
		<abstract>How do radio stations decide what music to play on the air? Previous studies offer a single answer to this question. In contrast, this study examines the variety of ways that radio programmers answer this question by conceptualizing them as mediators between record companies and radio audiences. From interviews with programmers at commercial radio stations in the United States we identify key programming practices that programmers use to manage their stations' music formats and present a typology of repertoires of such practices. We then discuss the implications of each programming repertoire for stations' music formats and the structural conditions that promote the use of each programming repertoire. In conclusion, we consider the study's implications for understanding culture production in the commercial radio industry.
ER  -</abstract>
		<title>“Will This Record Work for Us?”: Managing Music Formats in Commercial Radio</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/204115c6fb8e96dca9b4007f988e918e5/bfields</id>
		<tags>music_similarity</tags>
		<tags>timbre_similarity</tags>
		<description>initial imports</description>
		<date>2010-01-28 11:42:45</date>
		<count>3</count>
		<booktitle>Int. Symposium on Music Information Retrieval</booktitle>
		<year>2004</year>
		<url></url>
		<author>J. Aucouturier</author>
		<author>F. Pachet</author>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Aucouturier</last>
		</authors>
		<authors>
			<first>F.</first>
		</authors>
		<authors>
			<last>Pachet</last>
		</authors>
		<title>Tools and Architecture for the Evaluation of Similarity Measures : Case Study of Timbre Similarity</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28b07e1af69ed0510dc064f2f69cede17/tb2332</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>1</count>
		<year>2004</year>
		<url></url>
		<author>D. Eck</author>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Eck</last>
		</authors>
		<abstract>Slides and musical examples available on request.</abstract>
		<source>OwnPublication</source>
		<title>Challenges for Machine Learning in the Domain of Music</title>
		<pubtype>unpublished</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fc9f3d649d77a3f6fb9480ea68b9d5cf/tb2332</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>1</count>
		<booktitle>Proceedings of the International Computer Music Conference</booktitle>
		<address>International Computer Music Association</address>
		<year>1984</year>
		<url></url>
		<author>P. Allen</author>
		<author>R. B. Dannenberg</author>
		<authors>
			<first>P.</first>
		</authors>
		<authors>
			<last>Allen</last>
		</authors>
		<authors>
			<first>R. B.</first>
		</authors>
		<authors>
			<last>Dannenberg</last>
		</authors>
		<pages>140--143</pages>
		<title>Tracking musical beats in real time.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2551564ad1cebf66961f1f864fe8ab9c9/andre@ismll</id>
		<tags>music</tags>
		<tags>toread</tags>
		<tags>similarity</tags>
		<tags>sequences</tags>
		<description>CiteSeerX — MUSIC SIMILARITY BASED ON SEQUENCES OF DESCRIPTORS: TONAL FEATURES APPLIED TO AUDIO COVER SONG</description>
		<date>2010-05-11 08:35:17</date>
		<count>1</count>
		<year>2007</year>
		<url></url>
		<author>Joan Serrà Julià</author>
		<authors>
			<first>Joan Serrà</first>
		</authors>
		<authors>
			<last>Julià</last>
		</authors>
		<title>MUSIC SIMILARITY BASED ON SEQUENCES OF DESCRIPTORS: TONAL FEATURES APPLIED TO AUDIO COVER SONG</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/288965b7628e6303b57d4c8228c3234b2/andre@ismll</id>
		<tags>music</tags>
		<tags>processing</tags>
		<tags>audio</tags>
		<tags>phd</tags>
		<tags>analysis</tags>
		<tags>classification</tags>
		<description></description>
		<date>2010-06-02 11:15:35</date>
		<count>1</count>
		<address>Tampere, Finland</address>
		<year>2009</year>
		<url>http://urn.fi/URN:NBN:fi:tty-200906301085</url>
		<author>Antti Eronen</author>
		<authors>
			<first>Antti</first>
		</authors>
		<authors>
			<last>Eronen</last>
		</authors>
		<abstract>Signal processing methods for audio classification and music content analysis are developed in this thesis. Audio classification is here understood as the process of assigning a discrete category label to an unknown recording. Two specific problems of audio classification are considered: musical instrument recognition and context recognition. In the former, the system classifies an audio recording according to the instrument, e.g. violin, flute, piano, that produced the sound. The latter task is about classifying an environment, such a car, restaurant, or library, based on its ambient audio background.
       
       In the field of music content analysis, methods are presented for music meter analysis and chorus detection. Meter analysis methods consider the estimation of the regular pattern of strong and weak beats in a piece of music. The goal of chorus detection is to locate the chorus segment in music which is often the catchiest and most memorable part of a song. These are among the most important and readily commercially applicable content attributes that can be automatically analyzed from music signals.
       
       For audio classification, several features and classification methods are proposed and evaluated. In musical instrument recognition, we consider methods to improve the performance of a baseline audio classification system that uses mel-frequency cepstral coefficients and their first derivatives as features, and continuous-density hidden Markov models (HMMs) for modeling the feature distributions. Two improvements are proposed to increase the performance of this baseline system. First, transforming the features to a base with maximal statistical independence using independent component analysis. Secondly, discriminative training is shown to further improve the recognition accuracy of the system.
       
       For musical meter analysis, three methods are proposed. The first performs meter analysis jointly at three different time scales: at the temporally atomic tatum pulse level, at the tactus pulse level, which corresponds to the tempo of a piece, and at the musical measure level. The features obtained from an accent feature analyzer and a bank of combfilter resonators are processed by a novel probabilistic model which represents primitive musical knowledge and performs joint estimation of the tatum, tactus, and measure pulses.
       
       The second method focuses on estimating the beat and the tatum. The design goal was to keep the method computationally very efficient while retaining sufficient analysis accuracy. Simplified probabilistic modeling is proposed for beat and tatum period and phase estimation, and ensuring the continuity of the estimates. A novel phase-estimator based on adaptive comb filtering is presented. The accuracy of the method is close to the first method but with a fraction of the computational cost.
       
       The third method for music rhythm analysis focuses on improving the accuracy in music tempo estimation. The method is based on estimating the tempo of periodicity vectors using locally weighted k-Nearest Neighbors (k-NN) regression. Regression closely relates to classification, the difference being that the goal of regression is to estimate the value of a continuous variable (the tempo), whereas in classification the value to be assigned is a discrete category label. We propose a resampling step applied to an unknown periodicity vector before finding the nearest neighbors to increase the likelihood of finding a good match from the training set. This step improves the performance of the method significantly. The tempo estimate is computed as a distance-weighted median of the nearest neighbor tempi. Experimental results show that the proposed method provides significantly better tempo estimation accuracies than three reference methods.
       
       Finally, we describe a computationally efficient method for detecting a chorus section in popular and rock music. The method utilizes a self-dissimilarity representation that is obtained by summing two separate distance matrices calculated using the mel-frequency cepstral coefficient and pitch chroma features. This is followed by the detection of off-diagonal segments of small distance in the distance matrix. From the detected segments, an initial chorus section is selected using a scoring mechanism utilizing several heuristics, and subjected to further processing.</abstract>
		<title>Signal Processing Methods for Audio Classification and Music Content Analysis</title>
		<pubtype>phdthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d30f5e86d0da247661209f58b3c70c1e/svrist</id>
		<tags>imported</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>1</count>
		<journal>Computing in Musicology</journal>
		<year>1994</year>
		<url></url>
		<author>E. Selfridge-Field</author>
		<authors>
			<first>E.</first>
		</authors>
		<authors>
			<last>Selfridge-Field</last>
		</authors>
		<volume>9</volume>
		<pages>109-45</pages>
		<title>Optical recognition of musical notation: A survey of current work</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/241e098cd7c5c78f2d3cd6c72992cd678/bfields</id>
		<tags>playlistResearch</tags>
		<tags>recom</tags>
		<description>r-MUSIC, A Collaborative Music DJ for Ad Hoc Networks</description>
		<date>2010-07-21 13:55:50</date>
		<count>2</count>
		<journal>Web Delivering of Music, International Conference on</journal>
		<publisher>IEEE Computer Society</publisher>
		<address>Los Alamitos, CA, USA</address>
		<year>2004</year>
		<url>http://www.computer.org/portal/web/csdl/doi/10.1109/WDM.2004.1358111</url>
		<author>Ursula Wolz</author>
		<author>Michael Massimi</author>
		<author>Eric Tarn</author>
		<authors>
			<first>Ursula</first>
		</authors>
		<authors>
			<last>Wolz</last>
		</authors>
		<authors>
			<first>Michael</first>
		</authors>
		<authors>
			<last>Massimi</last>
		</authors>
		<authors>
			<first>Eric</first>
		</authors>
		<authors>
			<last>Tarn</last>
		</authors>
		<volume>0</volume>
		<pages>144-150</pages>
		<isbn>0-7695-2157-6</isbn>
		<doi>10.1109/WDM.2004.1358111,</doi>
		<title>r-MUSIC, A Collaborative Music DJ for Ad Hoc Networks</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/206c2bbf921a07a77cc181632849138e8/mediadigits</id>
		<tags>music</tags>
		<tags>metadata</tags>
		<tags>mining</tags>
		<tags>genres</tags>
		<tags>aggregation</tags>
		<tags>classification</tags>
		<description></description>
		<date>2011-01-09 11:24:58</date>
		<count>7</count>
		<booktitle>Proceedings of the 7th International Conference on Music Information Retrieval</booktitle>
		<year>2006</year>
		<url></url>
		<author>M. Schedl</author>
		<author>T. Pohle</author>
		<author>P. Knees</author>
		<author>G. Widmer</author>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<authors>
			<first>T.</first>
		</authors>
		<authors>
			<last>Pohle</last>
		</authors>
		<authors>
			<first>P.</first>
		</authors>
		<authors>
			<last>Knees</last>
		</authors>
		<authors>
			<first>G.</first>
		</authors>
		<authors>
			<last>Widmer</last>
		</authors>
		<pages>260--265</pages>
		<title>Assigning and visualizing music genres by web-based co-occurrence analysis</title>
		<pubtype>conference</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e3253c86da33b9540d0f13edd307b92a/thitcherine</id>
		<tags>imported</tags>
		<description>Bibliography for a cultural history paper on "The Beggar's Opera"</description>
		<date>2010-07-03 12:22:11</date>
		<count>1</count>
		<journal>Revue d'histoire moderne et contemporaine</journal>
		<year>2002</year>
		<url>http://www.jstor.org/stable/20530850</url>
		<author>William Weber</author>
		<authors>
			<first>William</first>
		</authors>
		<authors>
			<last>Weber</last>
		</authors>
		<volume>49</volume>
		<number>3</number>
		<pages>119--139</pages>
		<abstract>Le monde musical qui apparaît à Londres au tournant du XVIIIe siècle engendre des institutions musicales publiques et des pratiques sociales dont certaines caractéristiques essentielles pour les capitales nationales modernes. À cette époque, les capitales attirent les gens riches et influents en nombre croissant, au point, le plus souvent, de dépasser la fréquentation des cours et de priver ces dernières du rôle central qu'elles jouaient dans le domaine politique et culturel. Ces individus se connaissent moins que les courtisans, mais mieux que l'élite qui se développera ultérieurement, à la fin du XIXe siècle, avec la croissance démographique et l'essor des mouvements politiques de masse. Ces gens s'appellent eux-mêmes le beau monde ou, simplement, le Monde. /// The musical world that evolved in London and then Paris at the turn of the eighteenth century acquired public musical institutions and social practices that became prototypical of those in national capitals in the modern era. Capitals drew a concentration of rich and influential people never found in cities before, depriving courts of their former central roles in politics and culture. People within the cosmopolitan elite of the capitals knew one another less than had courtesans but far more extensively than was the case among the elite that developed by the end of the nineteenth century, due to fundamental demographic and political change. These people called themselves the beau monde or simply the World.</abstract>
		<issn>00488003</issn>
		<translator>Daniel Argelès</translator>
		<shorttitle>La culture musicale d'une capitale</shorttitle>
		<title>La culture musicale d'une capitale: l'époque du beau monde à Londres, 1700-1870</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ff1dc2cb1ca0e13362f491955de9c9db/mediadigits</id>
		<tags>music</tags>
		<tags>search</tags>
		<tags>description</tags>
		<description></description>
		<date>2010-12-17 14:30:59</date>
		<count>2</count>
		<booktitle>Proceedings of the Third International Conference on Music Information Retrieval: ISMIR</booktitle>
		<year>2002</year>
		<url></url>
		<author>J.Y. Kim</author>
		<author>N.J. Belkin</author>
		<authors>
			<first>J.Y.</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>N.J.</first>
		</authors>
		<authors>
			<last>Belkin</last>
		</authors>
		<pages>209--214</pages>
		<title>Categories of music description and search terms and phrases used by non-music experts</title>
		<pubtype>conference</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c313b0691b800fa7dd0a4f713087a81a/yevb0</id>
		<tags>Skills:</tags>
		<tags>anatomy</tags>
		<tags>Skills,Motor</tags>
		<tags>histology,Brain:</tags>
		<tags>Mapping,Brain:</tags>
		<tags>physiology,Humans,Magnetic</tags>
		<tags>Resonance</tags>
		<tags>\&</tags>
		<tags>physiology,Music,music,musicality,neuro</tags>
		<tags>Adolescent,Adult,Brain,Brain</tags>
		<tags>Imaging,Male,Motor</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Journal of Neuroscience</journal>
		<year>2003</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/14534258</url>
		<author>Christian Gaser</author>
		<author>Gottfried Schlaug</author>
		<authors>
			<first>Christian</first>
		</authors>
		<authors>
			<last>Gaser</last>
		</authors>
		<authors>
			<first>Gottfried</first>
		</authors>
		<authors>
			<last>Schlaug</last>
		</authors>
		<volume>23</volume>
		<number>27</number>
		<pages>9240--5</pages>
		<abstract>From an early age, musicians learn complex motor and auditory skills
	(e.g., the translation of visually perceived musical symbols into
	motor commands with simultaneous auditory monitoring of output),
	which they practice extensively from childhood throughout their entire
	careers. Using a voxel-by-voxel morphometric technique, we found
	gray matter volume differences in motor, auditory, and visual-spatial
	brain regions when comparing professional musicians (keyboard players)
	with a matched group of amateur musicians and non-musicians. Although
	some of these multiregional differences could be attributable to
	innate predisposition, we believe they may represent structural adaptations
	in response to long-term skill acquisition and the repetitive rehearsal
	of those skills. This hypothesis is supported by the strong association
	we found between structural differences, musician status, and practice
	intensity, as well as the wealth of supporting animal data showing
	structural changes in response to long-term motor training. However,
	only future experiments can determine the relative contribution of
	predisposition and practice.</abstract>
		<issn>1529-2401</issn>
		<title>Brain structures differ between musicians and non-musicians</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2799eb8a557c998de19ea429edc43186d/yevb0</id>
		<tags>Processes,Mental</tags>
		<tags>Processes:</tags>
		<tags>Brain,Brain:</tags>
		<tags>physiology,Humans,Language,Mental</tags>
		<tags>physiology,Music,language,music</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Annals of the New York Academy of Sciences</journal>
		<year>2001</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/12499110</url>
		<author>Mireille Besson</author>
		<author>Daniele Schön</author>
		<authors>
			<first>Mireille</first>
		</authors>
		<authors>
			<last>Besson</last>
		</authors>
		<authors>
			<first>Daniele</first>
		</authors>
		<authors>
			<last>Schön</last>
		</authors>
		<volume>930</volume>
		<pages>232--58</pages>
		<abstract>Similarities and differences between language and music processing
	are examined from an evolutionary and a cognitive perspective. Language
	and music cannot be considered single entities; they need to be decomposed
	into different component operations or levels of processing. The
	central question concerns one of the most important claims of the
	generative grammar theory, that is, the specificity of language processing:
	do the computations performed to process language rely on specific
	linguistic processes or do they rely on general cognitive principles?
	Evidence from brain imaging results is reviewed, noting that this
	field is currently in need of metanalysis of the available results
	to precisely evaluate this claim. A series of experiments, mainly
	using the event-related brain potentials method, were conducted to
	compare different levels of processing in language and music. Overall,
	results favor language specificity when certain aspects of semantic
	processing in language are compared with certain aspects of melodic
	and harmonic processing in music. By contrast, results support the
	view that general cognitive principles are involved when aspects
	of syntactic processing in language are compared with aspects of
	harmonic processing in music. Moreover, analysis of the temporal
	structure led to similar effects in language and music. These tentative
	conclusions must be supported by other brain imaging results to shed
	further light on the spatiotemporal dynamics of the brain structure-function
	relationship.</abstract>
		<issn>0077-8923</issn>
		<title>Comparison between language and music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2dce1965d009e03a1924919f14c429a3c/yevb0</id>
		<tags>Stimulation:</tags>
		<tags>Stimulation,Acoustic</tags>
		<tags>Acoustic</tags>
		<tags>methods,Adult,Auditory,Auditory:</tags>
		<tags>Discrimination:</tags>
		<tags>physiology,Evoked</tags>
		<tags>Discrimination,Pitch</tags>
		<tags>Time,Reaction</tags>
		<tags>Time:</tags>
		<tags>Potentials,Female,Humans,Male,Music,Pitch</tags>
		<tags>physiology,Reaction</tags>
		<tags>physiology,music,musicality,neuro,perception,pitch</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>2</count>
		<journal>Experimental Brain Research</journal>
		<year>2005</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/15551089</url>
		<author>Mari Tervaniemi</author>
		<author>Viola Just</author>
		<author>Stefan Koelsch</author>
		<author>Andreas Widmann</author>
		<author>Erich Schröger</author>
		<authors>
			<first>Mari</first>
		</authors>
		<authors>
			<last>Tervaniemi</last>
		</authors>
		<authors>
			<first>Viola</first>
		</authors>
		<authors>
			<last>Just</last>
		</authors>
		<authors>
			<first>Stefan</first>
		</authors>
		<authors>
			<last>Koelsch</last>
		</authors>
		<authors>
			<first>Andreas</first>
		</authors>
		<authors>
			<last>Widmann</last>
		</authors>
		<authors>
			<first>Erich</first>
		</authors>
		<authors>
			<last>Schröger</last>
		</authors>
		<volume>161</volume>
		<number>1</number>
		<pages>1--10</pages>
		<abstract>Previously, professional violin players were found to automatically
	discriminate tiny pitch changes, not discriminable by nonmusicians.
	The present study addressed the pitch processing accuracy in musicians
	with expertise in playing a wide selection of instruments (e.g.,
	piano; wind and string instruments). Of specific interest was whether
	also musicians with such divergent backgrounds have facilitated accuracy
	in automatic and/or attentive levels of auditory processing. Thirteen
	professional musicians and 13 nonmusicians were presented with frequent
	standard sounds and rare deviant sounds (0.8, 2, or 4\% higher in
	frequency). Auditory event-related potentials evoked by these sounds
	were recorded while first the subjects read a self-chosen book and
	second they indicated behaviorally the detection of sounds with deviant
	frequency. Musicians detected the pitch changes faster and more accurately
	than nonmusicians. The N2b and P3 responses recorded during attentive
	listening had larger amplitude in musicians than in nonmusicians.
	Interestingly, the superiority in pitch discrimination accuracy in
	musicians over nonmusicians was observed not only with the 0.8\%
	but also with the 2\% frequency changes. Moreover, also nonmusicians
	detected quite reliably the smallest pitch changes of 0.8\%. However,
	the mismatch negativity (MMN) and P3a recorded during a reading condition
	did not differentiate musicians and nonmusicians. These results suggest
	that musical expertise may exert its effects merely at attentive
	levels of processing and not necessarily already at the preattentive
	levels.</abstract>
		<issn>0014-4819</issn>
		<doi>10.1007/s00221-004-2044-5</doi>
		<title>Pitch discrimination accuracy in musicians vs nonmusicians: an event-related
	potential and behavioral study.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c0043e1171db07256c9a494c4c866e1c/yevb0</id>
		<tags>Cortex:</tags>
		<tags>physiology,Auditory:</tags>
		<tags>physiology,Music,Music:</tags>
		<tags>Acoustic</tags>
		<tags>Cortex,Auditory</tags>
		<tags>Stimulation,Adolescent,Adult,Auditory,Auditory</tags>
		<tags>Imaging,Magnetoencephalography,Male,Memory,Memory:</tags>
		<tags>psychology,melody,memory,music,musicality,neuro,perception</tags>
		<tags>Resonance</tags>
		<tags>Potentials,Female,Humans,Magnetic</tags>
		<tags>physiology,Electroencephalography,Evoked</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Learning & Memory</journal>
		<year>2001</year>
		<url>http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=311383\&tool=pmcentrez\&rendertype=abstract</url>
		<author>Mari Tervaniemi</author>
		<author>M Rytkönen</author>
		<author>Erich Schröger</author>
		<author>R J Ilmoniemi</author>
		<author>Risto Näätänen</author>
		<authors>
			<first>Mari</first>
		</authors>
		<authors>
			<last>Tervaniemi</last>
		</authors>
		<authors>
			<first>M</first>
		</authors>
		<authors>
			<last>Rytkönen</last>
		</authors>
		<authors>
			<first>Erich</first>
		</authors>
		<authors>
			<last>Schröger</last>
		</authors>
		<authors>
			<first>R J</first>
		</authors>
		<authors>
			<last>Ilmoniemi</last>
		</authors>
		<authors>
			<first>Risto</first>
		</authors>
		<authors>
			<last>Näätänen</last>
		</authors>
		<volume>8</volume>
		<number>5</number>
		<pages>295--300</pages>
		<abstract>The human central auditory system has a remarkable ability to establish
	memory traces for invariant features in the acoustic environment
	despite continual acoustic variations in the sounds heard. By recording
	the memory-related mismatch negativity (MMN) component of the auditory
	electric and magnetic brain responses as well as behavioral performance,
	we investigated how subjects learn to discriminate changes in a melodic
	pattern presented at several frequency levels. In addition, we explored
	whether musical expertise facilitates this learning. Our data show
	that especially musicians who perform music primarily without a score
	learn easily to detect contour changes in a melodic pattern presented
	at variable frequency levels. After learning, their auditory cortex
	detects these changes even when their attention is directed away
	from the sounds. The present results thus show that, after perceptual
	learning during attentive listening has taken place, changes in a
	highly complex auditory pattern can be detected automatically by
	the human auditory cortex and, further, that this process is facilitated
	by musical expertise.</abstract>
		<issn>1072-0502</issn>
		<doi>10.1101/lm.39501</doi>
		<title>Superior formation of cortical memory traces for melodic patterns
	in musicians</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f909dfcb6701ae495344c6ced3678a97/yevb0</id>
		<tags>acquisition,music,musicality</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>The cognitive neuroscience of music</booktitle>
		<publisher>Oxford University Press</publisher>
		<address>New York</address>
		<year>2003</year>
		<url></url>
		<author>Sandra E. Trehub</author>
		<authors>
			<first>Sandra E.</first>
		</authors>
		<authors>
			<last>Trehub</last>
		</authors>
		<editor>Isabelle Peretz</editor>
		<editor>Robert J. Zatorre</editor>
		<editors>
			<first>Sandra E.</first>
		</editors>
		<editors>
			<last>Trehub</last>
		</editors>
		<editors>
			<first>Sandra E.</first>
		</editors>
		<editors>
			<last>Trehub</last>
		</editors>
		<pages>3--20</pages>
		<isbn>0198525192</isbn>
		<title>Musical predispositions in infancy: An update</title>
		<pubtype>inbook</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2769e297379ace7ea7901c6e7d18b1b6f/yevb0</id>
		<tags>memory,music,musicality,perception</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Music Perception</journal>
		<publisher>University of California Press</publisher>
		<year>2003</year>
		<url>http://caliber.ucpress.net/doi/abs/10.1525/mp.2003.20.3.307</url>
		<author>Lorna S. Jakobson</author>
		<author>Lola L. Cuddy</author>
		<author>Andrea R. Kilgour</author>
		<authors>
			<first>Lorna S.</first>
		</authors>
		<authors>
			<last>Jakobson</last>
		</authors>
		<authors>
			<first>Lola L.</first>
		</authors>
		<authors>
			<last>Cuddy</last>
		</authors>
		<authors>
			<first>Andrea R.</first>
		</authors>
		<authors>
			<last>Kilgour</last>
		</authors>
		<volume>20</volume>
		<number>3</number>
		<pages>307--313</pages>
		<issn>0730-7829</issn>
		<title>Time tagging: A key to musicians' superior memory</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/248dbcc51a282580c4e17497606d96229/yevb0</id>
		<tags>language,music,neuro</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>Language, music, and the brain</booktitle>
		<publisher>Oxford University Press</publisher>
		<address>Oxford</address>
		<year>2008</year>
		<url>http://www.oup.com/us/patel</url>
		<author>Aniruddh D. Patel</author>
		<authors>
			<first>Aniruddh D.</first>
		</authors>
		<authors>
			<last>Patel</last>
		</authors>
		<title>Language, music, and the brain</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24d2b0c06c859b0bcdf6fb3e6f30a9ca6/yevb0</id>
		<tags>evolution,music</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>The cognitive neuroscience of music</booktitle>
		<publisher>Oxford University Press</publisher>
		<address>New York</address>
		<year>2003</year>
		<url></url>
		<author>David Huron</author>
		<authors>
			<first>David</first>
		</authors>
		<authors>
			<last>Huron</last>
		</authors>
		<editor>Isabelle Peretz</editor>
		<editor>Robert J. Zatorre</editor>
		<editors>
			<first>David</first>
		</editors>
		<editors>
			<last>Huron</last>
		</editors>
		<editors>
			<first>David</first>
		</editors>
		<editors>
			<last>Huron</last>
		</editors>
		<pages>57--75</pages>
		<title>Is music an evolutionary adaptation?</title>
		<pubtype>inbook</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25d864487b0a3734b239631dcf6184cbb/yevb0</id>
		<tags>System:</tags>
		<tags>physiology,Auditory</tags>
		<tags>physiology,Music,Psychoacoustics,Sound</tags>
		<tags>Pathways,Auditory</tags>
		<tags>Mapping,Brain:</tags>
		<tags>System,Autonomic</tags>
		<tags>physiology,Emotions,Emotions:</tags>
		<tags>Pathways:</tags>
		<tags>Nervous</tags>
		<tags>physiology,Autonomic</tags>
		<tags>Perception,Auditory</tags>
		<tags>Perception:</tags>
		<tags>Spectrography,music,neuro,perception</tags>
		<tags>Potentials,Evoked</tags>
		<tags>Arousal,Arousal:</tags>
		<tags>physiology,Evoked</tags>
		<tags>physiology,Brain,Brain</tags>
		<tags>Theory,Humans,Intelligence,Intelligence:</tags>
		<tags>Potentials:</tags>
		<tags>physiology,Memory,Memory:</tags>
		<tags>physiology,Gestalt</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Trends in Cognitive Sciences</journal>
		<year>2005</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/16271503</url>
		<author>Stefan Koelsch</author>
		<author>Walter A Siebel</author>
		<authors>
			<first>Stefan</first>
		</authors>
		<authors>
			<last>Koelsch</last>
		</authors>
		<authors>
			<first>Walter A</first>
		</authors>
		<authors>
			<last>Siebel</last>
		</authors>
		<volume>9</volume>
		<number>12</number>
		<pages>578--84</pages>
		<abstract>Music perception involves complex brain functions underlying acoustic
	analysis, auditory memory, auditory scene analysis, and processing
	of musical syntax and semantics. Moreover, music perception potentially
	affects emotion, influences the autonomic nervous system, the hormonal
	and immune systems, and activates (pre)motor representations. During
	the past few years, research activities on different aspects of music
	processing and their neural correlates have rapidly progressed. This
	article provides an overview of recent developments and a framework
	for the perceptual side of music processing. This framework lays
	out a model of the cognitive modules involved in music perception,
	and incorporates information about the time course of activity of
	some of these modules, as well as research findings about where in
	the brain these modules might be located.</abstract>
		<issn>1364-6613</issn>
		<doi>10.1016/j.tics.2005.10.001</doi>
		<title>Towards a neural basis of music perception</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/240dc8ed8aa03bfee16e69bb39f4d684c/yevb0</id>
		<tags>fmri,language,music\_cognition,neuroscience,syntax</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>2</count>
		<journal>NeuroImage</journal>
		<year>2005</year>
		<url>http://www.sciencedirect.com/science/article/B6WNP-4FKYDTN-3/2/cee3a757268509eb7f60cba3513a3def</url>
		<author>Stefan Koelsch</author>
		<author>Thomas Fritz</author>
		<author>Katrin Schulze</author>
		<author>David Alsop</author>
		<author>Gottfried Schlaug</author>
		<authors>
			<first>Stefan</first>
		</authors>
		<authors>
			<last>Koelsch</last>
		</authors>
		<authors>
			<first>Thomas</first>
		</authors>
		<authors>
			<last>Fritz</last>
		</authors>
		<authors>
			<first>Katrin</first>
		</authors>
		<authors>
			<last>Schulze</last>
		</authors>
		<authors>
			<first>David</first>
		</authors>
		<authors>
			<last>Alsop</last>
		</authors>
		<authors>
			<first>Gottfried</first>
		</authors>
		<authors>
			<last>Schlaug</last>
		</authors>
		<volume>25</volume>
		<number>4</number>
		<pages>1068--1076</pages>
		<abstract>The present study investigates the functional neuroanatomy of music
	perception with functional magnetic resonance imaging (fMRI). Three
	different subject groups were investigated to examine developmental
	aspects and effects of musical training: 10-year-old children with
	varying degrees of musical training, adults without formal musical
	training (nonmusicians), and adult musicians. Subjects made judgements
	on sequences that ended on chords that were music-syntactically either
	regular or irregular. In adults, irregular chords activated the inferior
	frontal gyrus, orbital frontolateral cortex, the anterior insula,
	ventrolateral premotor cortex, anterior and posterior areas of the
	superior temporal gyrus, the superior temporal sulcus, and the supramarginal
	gyrus. These structures presumably form different networks mediating
	cognitive aspects of music processing (such as processing of musical
	syntax and musical meaning, as well as auditory working memory),
	and possibly emotional aspects of music processing. In the right
	hemisphere, the activation pattern of children was similar to that
	of adults. In the left hemisphere, adults showed larger activations
	than children in prefrontal areas, in the supramarginal gyrus, and
	in temporal areas. In both adults and children, musical training
	was correlated with stronger activations in the frontal operculum
	and the anterior portion of the superior temporal gyrus.</abstract>
		<title>Adults and Children Processing Music: \A\n \fMRI\ Study</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2aba3e75e0f8f29e7eb13d6debb87813b/yevb0</id>
		<tags>music,perception,tonality</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Journal of New Music Research</journal>
		<year>2004</year>
		<url>http://www.informaworld.com/openurl?genre=article\&doi=10.1080/0929821042000317831\&magic=crossref||D404A21C5BB053405B1A640AFFD44AE3</url>
		<author>Carol L Krumhansl</author>
		<authors>
			<first>Carol L</first>
		</authors>
		<authors>
			<last>Krumhansl</last>
		</authors>
		<volume>33</volume>
		<number>3</number>
		<pages>253--268</pages>
		<issn>0929-8215</issn>
		<doi>10.1080/0929821042000317831</doi>
		<title>The cognition of tonality - as we know it today</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25a1ee462f3cde67577e9375d15177a83/yevb0</id>
		<tags>Uganda,amadinda,music,scale</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>African Music</journal>
		<year>1960</year>
		<url>http://www.jstor.org/stable/30249930</url>
		<author>Gerhard Kubik</author>
		<authors>
			<first>Gerhard</first>
		</authors>
		<authors>
			<last>Kubik</last>
		</authors>
		<volume>2</volume>
		<number>3</number>
		<pages>6--30</pages>
		<title>The structure of Kiganda xylophone music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2255ecb134f03facd59cd4bd1bef7b5dd/yevb0</id>
		<tags>language,modularity,music</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>Language and music as cognitive systems</booktitle>
		<publisher>Oxford University Press</publisher>
		<address>Oxford</address>
		<year>2008</year>
		<url></url>
		<author>Isabelle Peretz</author>
		<authors>
			<first>Isabelle</first>
		</authors>
		<authors>
			<last>Peretz</last>
		</authors>
		<editor>P Rebuschat</editor>
		<editor>Martin Rohrmeier</editor>
		<editor>John Hawkins</editor>
		<editor>Ian Cross</editor>
		<editors>
			<first>Isabelle</first>
		</editors>
		<editors>
			<last>Peretz</last>
		</editors>
		<editors>
			<first>Isabelle</first>
		</editors>
		<editors>
			<last>Peretz</last>
		</editors>
		<editors>
			<first>Isabelle</first>
		</editors>
		<editors>
			<last>Peretz</last>
		</editors>
		<editors>
			<first>Isabelle</first>
		</editors>
		<editors>
			<last>Peretz</last>
		</editors>
		<title>Music, language and modularity in action</title>
		<pubtype>inbook</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28d9d9544aef8e5e31508efa6253720e4/yevb0</id>
		<tags>physiology,Humans,Models,Motor</tags>
		<tags>Skills:</tags>
		<tags>Laterality:</tags>
		<tags>anatomy</tags>
		<tags>physiology,Music,Neurological,Neuronal</tags>
		<tags>Cerebral</tags>
		<tags>Plasticity:</tags>
		<tags>Callosum:</tags>
		<tags>Perception,Pitch</tags>
		<tags>\&</tags>
		<tags>physiology,Pitch</tags>
		<tags>Callosum,Corpus</tags>
		<tags>Cortex:</tags>
		<tags>physiology,Functional</tags>
		<tags>Perception:</tags>
		<tags>Skills,Motor</tags>
		<tags>physiology,music,musicality,neuro,plasticity</tags>
		<tags>physiology,Corpus</tags>
		<tags>Cortex,Cerebral</tags>
		<tags>histology,Cerebral</tags>
		<tags>histology,Corpus</tags>
		<tags>Plasticity,Neuronal</tags>
		<tags>Laterality,Functional</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Nature Reviews Neuroscience</journal>
		<year>2002</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/12042882</url>
		<author>Thomas F Münte</author>
		<author>Eckart O Altenmüller</author>
		<author>Lutz Jäncke</author>
		<authors>
			<first>Thomas F</first>
		</authors>
		<authors>
			<last>Münte</last>
		</authors>
		<authors>
			<first>Eckart O</first>
		</authors>
		<authors>
			<last>Altenmüller</last>
		</authors>
		<authors>
			<first>Lutz</first>
		</authors>
		<authors>
			<last>Jäncke</last>
		</authors>
		<volume>3</volume>
		<number>6</number>
		<pages>473--8</pages>
		<abstract>Studies of experience-driven neuroplasticity at the behavioural, ensemble,
	cellular and molecular levels have shown that the structure and significance
	of the eliciting stimulus can determine the neural changes that result.
	Studying such effects in humans is difficult, but professional musicians
	represent an ideal model in which to investigate plastic changes
	in the human brain. There are two advantages to studying plasticity
	in musicians: the complexity of the eliciting stimulus music and
	the extent of their exposure to this stimulus. Here, we focus on
	the functional and anatomical differences that have been detected
	in musicians by modern neuroimaging methods.</abstract>
		<issn>1471-003X</issn>
		<doi>10.1038/nrn843</doi>
		<title>The musician's brain as a model of neuroplasticity</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e9c6a937e101de958e9de920e08361b5/yevb0</id>
		<tags>harmony,language,music,neuro,syntax</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>2</count>
		<journal>Nature Neuroscience</journal>
		<year>2001</year>
		<url></url>
		<author>Burkhard Maess</author>
		<author>Stefan Koelsch</author>
		<author>Thomas C. Gunter</author>
		<author>Angela D. Friederici</author>
		<authors>
			<first>Burkhard</first>
		</authors>
		<authors>
			<last>Maess</last>
		</authors>
		<authors>
			<first>Stefan</first>
		</authors>
		<authors>
			<last>Koelsch</last>
		</authors>
		<authors>
			<first>Thomas C.</first>
		</authors>
		<authors>
			<last>Gunter</last>
		</authors>
		<authors>
			<first>Angela D.</first>
		</authors>
		<authors>
			<last>Friederici</last>
		</authors>
		<volume>4</volume>
		<number>5</number>
		<pages>540--545</pages>
		<abstract>The present experiment was designed to localize the neural substrates
	that process music-syntactic incongruities, using magnetoencephalography
	(MEG). Electrically, such processing has been proposed to be indicated
	by early right-anterior negativity (ERAN), which is elicited by harmonically
	inappropriate chords occurring within a major-minor tonal context.
	In the present experiment, such chords elicited an early effect,
	taken as the magnetic equivalent of the ERAN (termed mERAN). The
	source of mERAN activity was localized in Broca's area and its right-hemisphere
	homologue, areas involved in syntactic analysis during auditory language
	comprehension. We find that these areas are also responsible for
	an analysis of incoming harmonic sequences, indicating that these
	regions process syntactic information that is less language-specific
	than previously believed.</abstract>
		<doi>10.1038/87502</doi>
		<title>Musical syntax is processed in Broca's area: An MEG study.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29a4c41f1016402e4bf28f580951f149e/yevb0</id>
		<tags>Perception,Reaction</tags>
		<tags>Variance,Cues,Discrimination</tags>
		<tags>(Psychology),Female,Humans,L1,Language,Male,Mandarin,Music,Occupations,Pattern</tags>
		<tags>Recognition,Physiological,Pitch</tags>
		<tags>Perception,Young</tags>
		<tags>Time,Speech</tags>
		<tags>Acoustic</tags>
		<tags>Acoustics,Speech</tags>
		<tags>of</tags>
		<tags>Adult,language,music,musicality,perception,tone</tags>
		<tags>Stimulation,Adolescent,Aging,Analysis</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>The Journal of the Acoustical Society of America</journal>
		<year>2010</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/20058993</url>
		<author>Chao-Yang Lee</author>
		<author>Yuh-Fang Lee</author>
		<authors>
			<first>Chao-Yang</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Yuh-Fang</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<volume>127</volume>
		<number>1</number>
		<pages>481--90</pages>
		<abstract>The relationship between music and language processing was explored
	in two perception experiments on the identification of musical notes
	and Mandarin tones. In the music task, Mandarin-speaking musicians
	were asked to identify musical notes of three timbres without a reference
	pitch. 72\% of the musicians met the criterion for absolute pitch
	when an exact match was required, and 82\% met the criterion when
	one-semitone errors were allowed. Accuracy of identification was
	negatively correlated with age of onset of musical training, and
	piano notes were identified more accurately than viola and pure tone
	stimuli. In the Mandarin task, the musicians were able to identify,
	beyond chance, brief Mandarin tone stimuli that were devoid of dynamic
	F0 information and cues commonly considered necessary for speaker
	normalization. Although F0 height detection was involved in both
	musical note and Mandarin tone identification, performances in the
	two tasks were not correlated. The putative link between absolute
	pitch and tone language experience was discussed.</abstract>
		<issn>1520-8524</issn>
		<doi>10.1121/1.3266683</doi>
		<title>Perception of musical pitch and lexical tones by Mandarin-speaking
	musicians</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ee97b83f9ab23adb6b96fdf3460f9646/yevb0</id>
		<tags>Stimulation:</tags>
		<tags>physiology,Electroencephalography,Electroencephalography:</tags>
		<tags>methods,Adult,Auditory,Auditory</tags>
		<tags>Stimulation,Acoustic</tags>
		<tags>methods,Evoked</tags>
		<tags>Adult,interval,music,musicality,neuro,perception</tags>
		<tags>Acoustic</tags>
		<tags>Analysis,Statistics</tags>
		<tags>Factors,Young</tags>
		<tags>Stem,Brain</tags>
		<tags>as</tags>
		<tags>Perception,Auditory</tags>
		<tags>Potentials,Female,Humans,Male,Music,Occupations,Reaction</tags>
		<tags>Topic,Time</tags>
		<tags>Perception:</tags>
		<tags>Stem:</tags>
		<tags>Time,Reaction</tags>
		<tags>physiology,Brain</tags>
		<tags>Time:</tags>
		<tags>physiology,Spectrum</tags>
		<tags>Mapping,Brain</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Journal of Neuroscience</journal>
		<year>2009</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/19420250</url>
		<author>Kyung Myun Lee</author>
		<author>Erika Skoe</author>
		<author>Nina Kraus</author>
		<author>Richard Ashley</author>
		<authors>
			<first>Kyung Myun</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Erika</first>
		</authors>
		<authors>
			<last>Skoe</last>
		</authors>
		<authors>
			<first>Nina</first>
		</authors>
		<authors>
			<last>Kraus</last>
		</authors>
		<authors>
			<first>Richard</first>
		</authors>
		<authors>
			<last>Ashley</last>
		</authors>
		<volume>29</volume>
		<number>18</number>
		<pages>5832--40</pages>
		<abstract>By measuring the auditory brainstem response to two musical intervals,
	the major sixth (E3 and G2) and the minor seventh (E3 and F\#2),
	we found that musicians have a more specialized sensory system for
	processing behaviorally relevant aspects of sound. Musicians had
	heightened responses to the harmonics of the upper tone (E), as well
	as certain combination tones (sum tones) generated by nonlinear processing
	in the auditory system. In music, the upper note is typically carried
	by the upper voice, and the enhancement of the upper tone likely
	reflects musicians' extensive experience attending to the upper voice.
	Neural phase locking to the temporal periodicity of the amplitude-modulated
	envelope, which underlies the perception of musical harmony, was
	also more precise in musicians than nonmusicians. Neural enhancements
	were strongly correlated with years of musical training, and our
	findings, therefore, underscore the role that long-term experience
	with music plays in shaping auditory sensory encoding.</abstract>
		<issn>1529-2401</issn>
		<doi>10.1523/JNEUROSCI.6133-08.2009</doi>
		<title>Selective subcortical enhancement of musical intervals in musicians</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fd9bffe99d4dd0d0cf3339309f4d68ad/yevb0</id>
		<tags>musical,perception,pitch,pitch</tags>
		<tags>memory,rapidly,tone,tones</tags>
		<tags>pitch,academic</tags>
		<tags>produce</tags>
		<tags>achievement,accurately,and</tags>
		<tags>effortlessly,concert</tags>
		<tags>absolute</tags>
		<tags>and</tags>
		<tags>c,music,music</tags>
		<tags>isolated</tags>
		<tags>children,or</tags>
		<tags>a,cross-cultural,differences,e,g,language,memory,middle</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Music Perception</journal>
		<year>2008</year>
		<url></url>
		<author>E. Glenn Schellenberg</author>
		<author>Sandra E. Trehub</author>
		<authors>
			<first>E. Glenn</first>
		</authors>
		<authors>
			<last>Schellenberg</last>
		</authors>
		<authors>
			<first>Sandra E.</first>
		</authors>
		<authors>
			<last>Trehub</last>
		</authors>
		<volume>25</volume>
		<number>3</number>
		<pages>241--252</pages>
		<doi>10.1525/MP.2008.25.3.241</doi>
		<title>Is there an Asian advantage for pitch memory?</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a6a0dff6657872e494c34bd4099336a7/yevb0</id>
		<tags>Imaging,Magnetoencephalography,Male,Middle</tags>
		<tags>Acoustic</tags>
		<tags>anatomy</tags>
		<tags>histology,Auditory</tags>
		<tags>Cortex,Auditory</tags>
		<tags>histology,Temporal</tags>
		<tags>Aged,Music,Predictive</tags>
		<tags>\&</tags>
		<tags>Lobe,Temporal</tags>
		<tags>physiology,music,musicality,neuro,perception</tags>
		<tags>Cortex:</tags>
		<tags>physiology,Auditory:</tags>
		<tags>Tests,Temporal</tags>
		<tags>Lobe:</tags>
		<tags>physiology,Auditory,Auditory</tags>
		<tags>Stimulation,Adult,Aptitude,Aptitude:</tags>
		<tags>of</tags>
		<tags>physiology,Evoked</tags>
		<tags>Value</tags>
		<tags>Resonance</tags>
		<tags>Potentials,Female,Humans,Magnetic</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Nature Neuroscience</journal>
		<year>2002</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/12068300</url>
		<author>Peter Schneider</author>
		<author>Michael Scherg</author>
		<author>H Günter Dosch</author>
		<author>Hans J Specht</author>
		<author>Alexander Gutschalk</author>
		<author>André Rupp</author>
		<authors>
			<first>Peter</first>
		</authors>
		<authors>
			<last>Schneider</last>
		</authors>
		<authors>
			<first>Michael</first>
		</authors>
		<authors>
			<last>Scherg</last>
		</authors>
		<authors>
			<first>H Günter</first>
		</authors>
		<authors>
			<last>Dosch</last>
		</authors>
		<authors>
			<first>Hans J</first>
		</authors>
		<authors>
			<last>Specht</last>
		</authors>
		<authors>
			<first>Alexander</first>
		</authors>
		<authors>
			<last>Gutschalk</last>
		</authors>
		<authors>
			<first>André</first>
		</authors>
		<authors>
			<last>Rupp</last>
		</authors>
		<volume>5</volume>
		<number>7</number>
		<pages>688--94</pages>
		<abstract>Using magnetoencephalography (MEG), we compared the processing of
	sinusoidal tones in the auditory cortex of 12 non-musicians, 12 professional
	musicians and 13 amateur musicians. We found neurophysiological and
	anatomical differences between groups. In professional musicians
	as compared to non-musicians, the activity evoked in primary auditory
	cortex 19-30 ms after stimulus onset was 102\% larger, and the gray
	matter volume of the anteromedial portion of Heschl's gyrus was 130\%
	larger. Both quantities were highly correlated with musical aptitude,
	as measured by psychometric evaluation. These results indicate that
	both the morphology and neurophysiology of Heschl's gyrus have an
	essential impact on musical aptitude.</abstract>
		<issn>1097-6256</issn>
		<doi>10.1038/nn871</doi>
		<title>Morphology of Heschl's gyrus reflects enhanced activation in the
	auditory cortex of musicians</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fd75ac490581cd1382fc8d95fcfa9d38/mediadigits</id>
		<tags>listening</tags>
		<tags>audience</tags>
		<tags>collaborative</tags>
		<tags>histories</tags>
		<tags>broadcasting</tags>
		<tags>generation</tags>
		<tags>conventional</tags>
		<tags>techniques</tags>
		<tags>online</tags>
		<tags>Internet</tags>
		<tags>musical</tags>
		<tags>information</tags>
		<tags>request</tags>
		<tags>broadcast</tags>
		<tags>tastes</tags>
		<tags>flycasting</tags>
		<tags>instantaneous</tags>
		<tags>entertainment</tags>
		<tags>medium</tags>
		<tags>radio</tags>
		<tags>groupware</tags>
		<tags>retrieval</tags>
		<tags>music</tags>
		<tags>systems</tags>
		<tags>real-time</tags>
		<tags>playlist</tags>
		<tags>filtering</tags>
		<description></description>
		<date>2010-10-04 13:25:50</date>
		<count>1</count>
		<journal>Web Delivering of Music, 2001. Proceedings. First International Conference on</journal>
		<year>23-24 Nov. 2001</year>
		<url>http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=990167</url>
		<author>D.B. Hauver</author>
		<author>J.C. French</author>
		<authors>
			<first>D.B.</first>
		</authors>
		<authors>
			<last>Hauver</last>
		</authors>
		<authors>
			<first>J.C.</first>
		</authors>
		<authors>
			<last>French</last>
		</authors>
		<pages>123-130</pages>
		<abstract>In recent years, the popularity of online radio has exploded. This new entertainment medium affords an opportunity not available to conventional broadcast radio: the instantaneous listening audience can be known, or what is more important, the musical tastes of the current listening audience can be known. Thus, it is possible in the new medium to tailor the playlist in real-time to the musical tastes of the listening audience. We discuss a method, termed flycasting, for using collaborative filtering techniques to generate a playlist in real-time based on the request histories of the current listening audience. We also describe a concrete implementation of the technique.</abstract>
		<doi>10.1109/WDM.2001.990167</doi>
		<title>Flycasting: using collaborative filtering to generate a playlist for online radio</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d1e9f3a02a4af3a6f607f121e9757143/muhe</id>
		<tags>imported</tags>
		<description></description>
		<date>2012-01-27 14:10:42</date>
		<count>1</count>
		<journal>Nature.</journal>
		<year>1998</year>
		<url></url>
		<author>C. Pantev</author>
		<author>R. Oostenveld</author>
		<author>Ross A, E.</author>
		<author>Roberts B.</author>
		<author>L. E.</author>
		<author>M. Hoke</author>
		<authors>
			<first>C.</first>
		</authors>
		<authors>
			<last>Pantev</last>
		</authors>
		<authors>
			<first>R.</first>
		</authors>
		<authors>
			<last>Oostenveld</last>
		</authors>
		<authors>
			<first>Ross</first>
		</authors>
		<authors>
			<last>A, E.</last>
		</authors>
		<authors>
			<first>Roberts</first>
		</authors>
		<authors>
			<last>B.</last>
		</authors>
		<authors>
			<first>L.</first>
		</authors>
		<authors>
			<last>E.</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Hoke</last>
		</authors>
		<volume>392</volume>
		<pages>811-814</pages>
		<abstract>Acoustic stimuli are processed throughout the auditory projection
	pathway, including the neocortex, by neurons that are aggregated
	into 'tonotopic' maps according to their specific frequency tunings1,
	2, 3. Research on animals has shown that tonotopic representations
	are not statically fixed in the adult organism but can reorganize
	after damage to the cochlea4 or after training the intact subject
	to discriminate between auditory stimuli5. Here we used functional
	magnetic source imaging (single dipole model) to measure cortical
	representations in highly skilled musicians. Dipole moments for piano
	tones, but not for pure tones of similar fundamental frequency (matched
	in loudness), were found to be enlarged by about 25% in musicians
	compared with control subjects who had never played an instrument.
	Enlargement was correlated with the age at which musicians began
	to practise and did not differ between musicians with absolute or
	relative pitch. These results, when interpreted with evidence for
	modified somatosensory representations of the fingering digits in
	skilled violinists6, suggest that use-dependent functional reorganization
	extends across the sensory cortices to reflect the pattern of sensory
	input processed by the subject during development of musical skill.</abstract>
		<title>Increased auditory cortical representation in musicians</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/257f39df40b074f96c278c35254bff32b/baltazhar</id>
		<tags>music</tags>
		<tags>study</tags>
		<tags>urban</tags>
		<tags>medi</tags>
		<description></description>
		<date>2008-10-09 01:54:46</date>
		<count>1</count>
		<journal>Media Culture Society</journal>
		<year>2002</year>
		<url>http://mcs.sagepub.com/cgi/reprint/24/1/87.pdf</url>
		<author>Andy Benett</author>
		<authors>
			<first>Andy</first>
		</authors>
		<authors>
			<last>Benett</last>
		</authors>
		<volume>24</volume>
		<number>1</number>
		<pages>87-100</pages>
		<abstract>The purpose of this article is twofold. First, to illustrate how recently developed technologies are giving rise to new ways of conceptualizing the relationship between music and place. Applying the concept of mythscapes, developed from Appadurai's work, to the 'Canterbury Sound', a term recently revived and adapted by a website-centred fanbase to describe a loosely defined back-catalogue of albums, songs and home-recorded musical experiments, the article argues that the city of Canterbury is being inscribed with a series of urban myths relating to its perceived role in the creation of a musical style deemed by fans to be locally specific. Second, through its analysis of the Canterbury Sound's 'construction' on the Internet, the article considers the extent to which the Canterbury Sound can be considered a 'virtual' scene, Internet communication replacing more conventional forms of celebrating collective musical taste as these emerge through the sociality of club, concert hall and festival-based scenes.</abstract>
		<title>Music, medi and urban mythscapes: a study of the 'Canterbury Sound'</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c9dc230c67bd4119e98a078d150ab994/kochm</id>
		<tags>science</tags>
		<tags>primes</tags>
		<tags>mathematics</tags>
		<tags>ownprivate</tags>
		<description>Music of the Primes: Why an Unsolved Problem in Mathematics Matters: Marcus du Sautoy: Amazon.de: Englische Bücher</description>
		<date>2008-12-24 10:17:34</date>
		<count>1</count>
		<publisher>Harperperennial</publisher>
		<year>2004</year>
		<url>http://www.amazon.de/Music-Primes-Unsolved-Problem-Mathematics/dp/1841155802%3FSubscriptionId%3D13CT5CVB80YFWJEPWS02%26tag%3Dws%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D1841155802</url>
		<author>Marcus du Sautoy</author>
		<authors>
			<first>Marcus</first>
		</authors>
		<authors>
			<last>du Sautoy</last>
		</authors>
		<ean>9781841155807</ean>
		<asin>1841155802</asin>
		<isbn>1841155802</isbn>
		<dewey>600</dewey>
		<title>Music of the Primes: Why an Unsolved Problem in Mathematics Matters</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d04cfaade177b192e2c7014ef175d6d7/msn</id>
		<tags>research.mining.classification</tags>
		<tags>research.conceptual.generation</tags>
		<tags>humanities.music</tags>
		<tags>research.genres</tags>
		<description>Automatic genre classification of music content: a survey</description>
		<date>2008-08-18 13:59:47</date>
		<count>2</count>
		<journal>Signal Processing Magazine, IEEE</journal>
		<year>2006</year>
		<url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1598089</url>
		<author>N. Scaringella</author>
		<author>G. Zoia</author>
		<author>D. Mlynek</author>
		<authors>
			<first>N.</first>
		</authors>
		<authors>
			<last>Scaringella</last>
		</authors>
		<authors>
			<first>G.</first>
		</authors>
		<authors>
			<last>Zoia</last>
		</authors>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Mlynek</last>
		</authors>
		<volume>23</volume>
		<number>2</number>
		<pages>133--141</pages>
		<abstract>This paper reviews the state-of-the-art in automatic genre classification of music collections through three main paradigms: expert systems, unsupervised classification, and supervised classification. The paper discusses the importance of music genres with their definitions and hierarchies. It also presents techniques to extract meaningful information from audio data to characterize musical excerpts. The paper also presents the results of new emerging research fields and techniques that investigate the proximity of music genres.</abstract>
		<title>Automatic genre classification of music content: a survey</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29c016184a25087b8aad59911afb168d0/slegroux</id>
		<tags>imported</tags>
		<description></description>
		<date>2012-02-20 13:49:26</date>
		<count>1</count>
		<booktitle>Society for Music Perception and Cognition</booktitle>
		<address>Rochester, NY</address>
		<year>2011</year>
		<url></url>
		<author>Sylvain Le Groux</author>
		<author>Paul F. M. J. Verschure</author>
		<authors>
			<first>Sylvain</first>
		</authors>
		<authors>
			<last>Le Groux</last>
		</authors>
		<authors>
			<first>Paul F. M. J.</first>
		</authors>
		<authors>
			<last>Verschure</last>
		</authors>
		<abstract>Music appears to deeply affect emotional, cerebral and physiological states.Yet, the relationship between specific musical parameters and emotional responses is still not clear. Even if it is difficult to expect reproducible and independent control of musical parameters from human performers, synthetic music systems now allow to generate expressive musical pieces. In this study, we use a system, called the SMuSe, to generate a set of well-controlled musical stimuli, and analyze the influence of parameters of musical structure, performance and timbre on emotional responses.

13 students (5 women, M: 25.8 , range: 22-31) took part in the experiment. They were asked to rate three blocks of sound samples in terms of the emotion they felt on a 5 points SAM scale of valence, arousal and dominance. These blocks corresponded to changes in the structure parameter: 3 modes (Minor, Major, Random ) * 3 registers (Bass, Tenor, Soprano); performance level: 3 tempi (Lento, Moderato, Presto) * 3 dynamics (Piano, Mezzo Forte, Forte) * 3 articulations (Staccato, Regular, Legato); and timbre: 3 Attack time (Short, Medium, Long) * 3 Brightness (Dull, Regular, Bright) * 3 Damping (Low, Medium, High). For each block, we followed a repeated measure design where the conditions were presented in random order.

Repeated measure MANOVAs showed that minor and random modes were more negative, while soprano register was more arousing. Staccato articulations, presto tempi and forte dynamics felt more arousing but also more negative. Presto tempi and forte dynamics were perceived as more dominant. Bright sounds with short-attack and low damping were more arousing. Longer attacks and brighter sounds felt more negative. Finally, bright and low damping sounds were perceived as more dominant.

This study shows the potential of synthetic music systems for analyzing and inducing musical emotion. In the future, such systems will be highly relevant for therapeutic applications but also for sound-based diagnosis, interactive gaming, and physiologically-based musical instruments.</abstract>
		<title>A Synthetic Approach to the Study of Musically-induced Emotions</title>
		<pubtype>conference</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/249ddd7b1965188e0aecb535fc2c45eab/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#LeeY13</url>
		<author>Gangin Lee</author>
		<author>Unil Yun</author>
		<authors>
			<first>Gangin</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Unil</first>
		</authors>
		<authors>
			<last>Yun</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Unil</first>
		</editors>
		<editors>
			<last>Yun</last>
		</editors>
		<editors>
			<first>Unil</first>
		</editors>
		<editors>
			<last>Yun</last>
		</editors>
		<editors>
			<first>Unil</first>
		</editors>
		<editors>
			<last>Yun</last>
		</editors>
		<editors>
			<first>Unil</first>
		</editors>
		<editors>
			<last>Yun</last>
		</editors>
		<volume>274</volume>
		<pages>19-23</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Frequent Graph Mining Based on Multiple Minimum Support Constraints.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2cd204c3cd1c16567e9cd069608960a0a/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#ZhangX13</url>
		<author>Lichen Zhang</author>
		<author>Bingqing Xu</author>
		<authors>
			<first>Lichen</first>
		</authors>
		<authors>
			<last>Zhang</last>
		</authors>
		<authors>
			<first>Bingqing</first>
		</authors>
		<authors>
			<last>Xu</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Bingqing</first>
		</editors>
		<editors>
			<last>Xu</last>
		</editors>
		<editors>
			<first>Bingqing</first>
		</editors>
		<editors>
			<last>Xu</last>
		</editors>
		<editors>
			<first>Bingqing</first>
		</editors>
		<editors>
			<last>Xu</last>
		</editors>
		<editors>
			<first>Bingqing</first>
		</editors>
		<editors>
			<last>Xu</last>
		</editors>
		<volume>274</volume>
		<pages>63-68</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Specification of Communication Based Train Control System Using AADL.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fb3757e97f02df5b4ffc5fa22a991ac1/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#XuZ13</url>
		<author>Bingqing Xu</author>
		<author>Lichen Zhang</author>
		<authors>
			<first>Bingqing</first>
		</authors>
		<authors>
			<last>Xu</last>
		</authors>
		<authors>
			<first>Lichen</first>
		</authors>
		<authors>
			<last>Zhang</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Lichen</first>
		</editors>
		<editors>
			<last>Zhang</last>
		</editors>
		<editors>
			<first>Lichen</first>
		</editors>
		<editors>
			<last>Zhang</last>
		</editors>
		<editors>
			<first>Lichen</first>
		</editors>
		<editors>
			<last>Zhang</last>
		</editors>
		<editors>
			<first>Lichen</first>
		</editors>
		<editors>
			<last>Zhang</last>
		</editors>
		<volume>274</volume>
		<pages>131-136</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Specification of Train Control Systems Using Formal Methods.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27ebe20968129233aee1e2bf433d9f009/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#WangZQ13</url>
		<author>Zehan Wang</author>
		<author>Lijun Zhu</author>
		<author>Jiandong Qi</author>
		<authors>
			<first>Zehan</first>
		</authors>
		<authors>
			<last>Wang</last>
		</authors>
		<authors>
			<first>Lijun</first>
		</authors>
		<authors>
			<last>Zhu</last>
		</authors>
		<authors>
			<first>Jiandong</first>
		</authors>
		<authors>
			<last>Qi</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Jiandong</first>
		</editors>
		<editors>
			<last>Qi</last>
		</editors>
		<editors>
			<first>Jiandong</first>
		</editors>
		<editors>
			<last>Qi</last>
		</editors>
		<editors>
			<first>Jiandong</first>
		</editors>
		<editors>
			<last>Qi</last>
		</editors>
		<editors>
			<first>Jiandong</first>
		</editors>
		<editors>
			<last>Qi</last>
		</editors>
		<volume>274</volume>
		<pages>197-203</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>ROI Extraction in Dermatosis Images Using a Method of Chan-Vese Segmentation Based on Saliency Detection.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/285b02f91f1b4a3be754a13fd1a4769fe/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#ZhangZXL13</url>
		<author>Haodong Zhang</author>
		<author>Lijun Zhu</author>
		<author>Shuo Xu</author>
		<author>Weifeng Li</author>
		<authors>
			<first>Haodong</first>
		</authors>
		<authors>
			<last>Zhang</last>
		</authors>
		<authors>
			<first>Lijun</first>
		</authors>
		<authors>
			<last>Zhu</last>
		</authors>
		<authors>
			<first>Shuo</first>
		</authors>
		<authors>
			<last>Xu</last>
		</authors>
		<authors>
			<first>Weifeng</first>
		</authors>
		<authors>
			<last>Li</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Weifeng</first>
		</editors>
		<editors>
			<last>Li</last>
		</editors>
		<editors>
			<first>Weifeng</first>
		</editors>
		<editors>
			<last>Li</last>
		</editors>
		<editors>
			<first>Weifeng</first>
		</editors>
		<editors>
			<last>Li</last>
		</editors>
		<editors>
			<first>Weifeng</first>
		</editors>
		<editors>
			<last>Li</last>
		</editors>
		<volume>274</volume>
		<pages>211-217</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>XML-Based Document Retrieval in Chinese Diseases Question Answering System.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23b497f3b7d4a5e1317d0f29128c58bd5/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#CuiKKJLLC13</url>
		<author>Yun Cui</author>
		<author>Myoungjin Kim</author>
		<author>Seung woo Kum</author>
		<author>Jong jin Jung</author>
		<author>Tae-Beom Lim</author>
		<author>Hanku Lee</author>
		<author>Okkyung Choi</author>
		<authors>
			<first>Yun</first>
		</authors>
		<authors>
			<last>Cui</last>
		</authors>
		<authors>
			<first>Myoungjin</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Seung</first>
		</authors>
		<authors>
			<last>woo Kum</last>
		</authors>
		<authors>
			<first>Jong</first>
		</authors>
		<authors>
			<last>jin Jung</last>
		</authors>
		<authors>
			<first>Tae-Beom</first>
		</authors>
		<authors>
			<last>Lim</last>
		</authors>
		<authors>
			<first>Hanku</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Okkyung</first>
		</authors>
		<authors>
			<last>Choi</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Okkyung</first>
		</editors>
		<editors>
			<last>Choi</last>
		</editors>
		<editors>
			<first>Okkyung</first>
		</editors>
		<editors>
			<last>Choi</last>
		</editors>
		<editors>
			<first>Okkyung</first>
		</editors>
		<editors>
			<last>Choi</last>
		</editors>
		<editors>
			<first>Okkyung</first>
		</editors>
		<editors>
			<last>Choi</last>
		</editors>
		<volume>274</volume>
		<pages>353-357</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Home Appliance Control and Monitoring System Model Based on Cloud Computing Technology.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29cc305556cd58fea742b7da94fa35f38/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#HosenC13</url>
		<author>A. S. M. Sanwar Hosen</author>
		<author>Gihwan Cho</author>
		<authors>
			<first>A. S. M. Sanwar</first>
		</authors>
		<authors>
			<last>Hosen</last>
		</authors>
		<authors>
			<first>Gihwan</first>
		</authors>
		<authors>
			<last>Cho</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Gihwan</first>
		</editors>
		<editors>
			<last>Cho</last>
		</editors>
		<editors>
			<first>Gihwan</first>
		</editors>
		<editors>
			<last>Cho</last>
		</editors>
		<editors>
			<first>Gihwan</first>
		</editors>
		<editors>
			<last>Cho</last>
		</editors>
		<editors>
			<first>Gihwan</first>
		</editors>
		<editors>
			<last>Cho</last>
		</editors>
		<volume>274</volume>
		<pages>433-439</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Eccentricity-Based Data Gathering and Diameter-Based Data Forwarding in 3D Wireless Sensor Networks.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b7a12e40f6f2cf43bab3ab2113f5c23c/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#Sug13</url>
		<author>Hyontai Sug</author>
		<authors>
			<first>Hyontai</first>
		</authors>
		<authors>
			<last>Sug</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Hyontai</first>
		</editors>
		<editors>
			<last>Sug</last>
		</editors>
		<editors>
			<first>Hyontai</first>
		</editors>
		<editors>
			<last>Sug</last>
		</editors>
		<editors>
			<first>Hyontai</first>
		</editors>
		<editors>
			<last>Sug</last>
		</editors>
		<editors>
			<first>Hyontai</first>
		</editors>
		<editors>
			<last>Sug</last>
		</editors>
		<volume>274</volume>
		<pages>185-189</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Better Induction Models for Classification of Forest Cover.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/267f7c24c9d6218ab6e8c068343c8c337/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#LiuLZ13</url>
		<author>Dongpei Liu</author>
		<author>Hengzhu Liu</author>
		<author>Li Zhou</author>
		<authors>
			<first>Dongpei</first>
		</authors>
		<authors>
			<last>Liu</last>
		</authors>
		<authors>
			<first>Hengzhu</first>
		</authors>
		<authors>
			<last>Liu</last>
		</authors>
		<authors>
			<first>Li</first>
		</authors>
		<authors>
			<last>Zhou</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Li</first>
		</editors>
		<editors>
			<last>Zhou</last>
		</editors>
		<editors>
			<first>Li</first>
		</editors>
		<editors>
			<last>Zhou</last>
		</editors>
		<editors>
			<first>Li</first>
		</editors>
		<editors>
			<last>Zhou</last>
		</editors>
		<editors>
			<first>Li</first>
		</editors>
		<editors>
			<last>Zhou</last>
		</editors>
		<volume>274</volume>
		<pages>413-419</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Enhanced Implementation of Max ∗ Operator for Turbo Decoding.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/269508df12e4464197f44a182c0fc1b8e/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#TranLHC13</url>
		<author>Nhat-Phuong Tran</author>
		<author>Myungho Lee</author>
		<author>Sugwon Hong</author>
		<author>Dong Hoon Choi</author>
		<authors>
			<first>Nhat-Phuong</first>
		</authors>
		<authors>
			<last>Tran</last>
		</authors>
		<authors>
			<first>Myungho</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Sugwon</first>
		</authors>
		<authors>
			<last>Hong</last>
		</authors>
		<authors>
			<first>Dong Hoon</first>
		</authors>
		<authors>
			<last>Choi</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Dong Hoon</first>
		</editors>
		<editors>
			<last>Choi</last>
		</editors>
		<editors>
			<first>Dong Hoon</first>
		</editors>
		<editors>
			<last>Choi</last>
		</editors>
		<editors>
			<first>Dong Hoon</first>
		</editors>
		<editors>
			<last>Choi</last>
		</editors>
		<editors>
			<first>Dong Hoon</first>
		</editors>
		<editors>
			<last>Choi</last>
		</editors>
		<volume>274</volume>
		<pages>307-313</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Multi-stream Parallel String Matching on Kepler Architecture.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ed85fa249408bb391873497b5edf1019/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#HsiaoL13</url>
		<author>Yuan-Kai Hsiao</author>
		<author>Yen-Wen Lin</author>
		<authors>
			<first>Yuan-Kai</first>
		</authors>
		<authors>
			<last>Hsiao</last>
		</authors>
		<authors>
			<first>Yen-Wen</first>
		</authors>
		<authors>
			<last>Lin</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Yen-Wen</first>
		</editors>
		<editors>
			<last>Lin</last>
		</editors>
		<editors>
			<first>Yen-Wen</first>
		</editors>
		<editors>
			<last>Lin</last>
		</editors>
		<editors>
			<first>Yen-Wen</first>
		</editors>
		<editors>
			<last>Lin</last>
		</editors>
		<editors>
			<first>Yen-Wen</first>
		</editors>
		<editors>
			<last>Lin</last>
		</editors>
		<volume>274</volume>
		<pages>569-575</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>A Mobility Management Scheme for Internet of Things.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20146346a1121607e504d3aea9218f7c3/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#ChuangY13</url>
		<author>Po-Jen Chuang</author>
		<author>Chu-Sing Yang</author>
		<authors>
			<first>Po-Jen</first>
		</authors>
		<authors>
			<last>Chuang</last>
		</authors>
		<authors>
			<first>Chu-Sing</first>
		</authors>
		<authors>
			<last>Yang</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Chu-Sing</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<editors>
			<first>Chu-Sing</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<editors>
			<first>Chu-Sing</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<editors>
			<first>Chu-Sing</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<volume>274</volume>
		<pages>637-642</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>The Originality of a Leader for Cooperative Learning.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a4ad95de9c40a7e93e9c54e729aaead7/keinstein</id>
		<tags>Informatik</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2006</date>
		<count>2</count>
		<journal>Computer Music Modeling and Retrieval</journal>
		<year>2006</year>
		<url>http://dx.doi.org/10.1007/11751069_3</url>
		<author>Leonello Tarabella</author>
		<authors>
			<first>Leonello</first>
		</authors>
		<authors>
			<last>Tarabella</last>
		</authors>
		<pages>34--44</pages>
		<abstract>The pureCMusic (pCM++) framework gives the possibility to write a piece of music in terms of an algorithmic-composition-based program -also controlled by data streaming from external devices for giving expressiveness in electro-acoustic music performances- and of synthesis algorithms. Everything is written following the C language syntax and compiled into machine code that runs at CPU speed. The framework provides a number of predefined functions for sound processing, for generating complex events and for managing external data coming from standard Midi controllers and/or other special gesture interfaces. I'm going to propose pCM++ as open-source code.</abstract>
		<doi>10.1007/11751069_3</doi>
		<title>The pureCMusic (pCM++) Framework as Open-Source Music Language</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ed096d46ba9061a5820448930427ca2f/ks-plugin-devel</id>
		<tags>Physik</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2013-02-02 14:43:04</date>
		<count>2</count>
		<journal>Science & Education</journal>
		<year>2008/04/26/</year>
		<url>http://dx.doi.org/10.1007/s11191-007-9090-x</url>
		<author>Imelda Caleon</author>
		<author>Subramaniam Ramanathan</author>
		<authors>
			<first>Imelda</first>
		</authors>
		<authors>
			<last>Caleon</last>
		</authors>
		<authors>
			<first>Subramaniam</first>
		</authors>
		<authors>
			<last>Ramanathan</last>
		</authors>
		<volume>17</volume>
		<number>4</number>
		<pages>449--456</pages>
		<abstract>Abstract~~This paper presents the early investigations about the nature of sound of the Pythagoreans, and how they started a tradition that remains valid up to present times---the use of numbers in representing natural reality. It will touch on the Pythagorean notion of musical harmony, which was extended to the notion of universal harmony. How the Pythagorean ideas have inspired many great works in physics, such as those of Galileo, Kepler and Newton, will also be presented. In exploring the legacy of Pythagoras to physics and the study of the universe, some valuable insights on the nature of science that can inspire budding physicists are extracted.</abstract>
		<ty>JOUR</ty>
		<doi>10.1007/s11191-007-9090-x</doi>
		<title>From Music to Physics: The Undervalued Legacy of Pythagoras</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25af091e48a9e19b63422574b54b1806c/ks-plugin-devel</id>
		<tags>Mathematik</tags>
		<tags>Musiktheorie</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2013-02-02 14:43:02</date>
		<count>3</count>
		<journal>Science</journal>
		<year>2007</year>
		<url>http://www.sciencemag.org/cgi/content/abstract/315/5810/330b</url>
		<author>Dave Headlam</author>
		<author>Matthew Brown</author>
		<authors>
			<first>Dave</first>
		</authors>
		<authors>
			<last>Headlam</last>
		</authors>
		<authors>
			<first>Matthew</first>
		</authors>
		<authors>
			<last>Brown</last>
		</authors>
		<volume>315</volume>
		<number>5810</number>
		<pages>330b-</pages>
		<abstract>Tymoczko (Reports, 7 July 2006, p. 72) proposed that the familiar sonorities of Western tonal music cluster around the center of a multidimensional orbifold. However, this is not true for all tonal progressions. When prototypical three-voice cadential progressions by Bach converge on the tonic, the chords migrate from the center to the edge of the orbifold.</abstract>
		<doi>10.1126/science.1134013</doi>
		<eprint>http://www.sciencemag.org/cgi/reprint/315/5810/330b.pdf</eprint>
		<title>Comment on "The Geometry of Musical Chords"</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21f0f19bce05b718400572c28f4772477/ks-plugin-devel</id>
		<description></description>
		<date>2013-02-02 14:41:41</date>
		<count>2</count>
		<journal>Computer Music Modeling and Retrieval. Sense of Sounds</journal>
		<year>2009</year>
		<url></url>
		<author>Tuukka Ilomäki</author>
		<authors>
			<first>Tuukka</first>
		</authors>
		<authors>
			<last>Ilomäki</last>
		</authors>
		<pages>98--109</pages>
		<abstract>Computer applications are an everyday tool for music analysts, composers, and music theory students. While these applications are a welcome tool to be used in the classrooms and research labs, their effectiveness could be improved by focusing on their usability. The usability of a user interface can be evaluated and even measured with respect to the goals of its users. In order to demonstrate the evaluation of a user interface, I present an experiment in which the efficiency of user interfaces is assessed in the context of three scenarios or ``use cases.'' Based on the experiment, I discuss some basic principles of usability theory, such as affordances, minimization of navigation, error handling, immediate feedback, and data visibility. The evaluation of these principles suggests some new types of music theory applications.</abstract>
		<doi>10.1007/978-3-540-85035-9_6</doi>
		<title>The Usability of Music Theory Software: The Analysis of Twelve-Tone Music as a Case Study</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ac83b4bd088c74429638a9ed8f609ca3/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-10-01 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#HongSCJC13</url>
		<author>Seung-Tae Hong</author>
		<author>Young-Sung Shin</author>
		<author>Dong Hoon Choi</author>
		<author>Heeseung Jo</author>
		<author>Jae-Woo Chang</author>
		<authors>
			<first>Seung-Tae</first>
		</authors>
		<authors>
			<last>Hong</last>
		</authors>
		<authors>
			<first>Young-Sung</first>
		</authors>
		<authors>
			<last>Shin</last>
		</authors>
		<authors>
			<first>Dong Hoon</first>
		</authors>
		<authors>
			<last>Choi</last>
		</authors>
		<authors>
			<first>Heeseung</first>
		</authors>
		<authors>
			<last>Jo</last>
		</authors>
		<authors>
			<first>Jae-Woo</first>
		</authors>
		<authors>
			<last>Chang</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Jae-Woo</first>
		</editors>
		<editors>
			<last>Chang</last>
		</editors>
		<editors>
			<first>Jae-Woo</first>
		</editors>
		<editors>
			<last>Chang</last>
		</editors>
		<editors>
			<first>Jae-Woo</first>
		</editors>
		<editors>
			<last>Chang</last>
		</editors>
		<editors>
			<first>Jae-Woo</first>
		</editors>
		<editors>
			<last>Chang</last>
		</editors>
		<volume>274</volume>
		<pages>301-306</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>A Semi-clustering Scheme for Large-Scale Graph Analysis on Hadoop.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23de4adbbd9d8e65f6796bebde90719c1/algebradresden</id>
		<tags>slub</tags>
		<tags>handbib</tags>
		<tags>verschiedenes</tags>
		<tags>regalmus</tags>
		<tags>musiktheorie</tags>
		<tags>schmidt</tags>
		<description>The Topos of Music</description>
		<date>2009-11-03 16:12:26</date>
		<count>3</count>
		<publisher>Birkhäuser</publisher>
		<address>Basel; Boston</address>
		<year>2002</year>
		<url>http://www.worldcat.org/search?qt=worldcat_org_all&q=9783764357313</url>
		<author>G. Mazzola</author>
		<author>Stefan. Göller</author>
		<author>Stefan. Müller</author>
		<authors>
			<first>G.</first>
		</authors>
		<authors>
			<last>Mazzola</last>
		</authors>
		<authors>
			<first>Stefan.</first>
		</authors>
		<authors>
			<last>Göller</last>
		</authors>
		<authors>
			<first>Stefan.</first>
		</authors>
		<authors>
			<last>Müller</last>
		</authors>
		<abstract>The Topos of Music is the upgraded and vastly deepened English extension of the seminal German Geometrie der Tone. It reflects the dramatic progress of mathematical music theory and its operationalization by information technology since the publication of Geometrie der Tone in 1990. The conceptual basis has been vastly generalized to topos-theoretic foundations, including a corresponding thoroughly geometric musical logic. The theoretical models and results now include topologies for rhythm, melody, and harmony, as well as a classification theory of musical objects that comprises the topos-theoretic concept framework. Classification also implies techniques of algebraic moduli theory. The classical models of modulation and counterpoint have been extended to exotic scales and counterpoint interval dichotomies. The probably most exciting new field of research deals with musical performance and its implementation on advanced object-oriented software environments. This subject not only uses extensively the existing mathematical music theory, it also opens the language to differential equations and tools of differential geometry, such as Lie derivatives. Mathematical performance theory is the key to inverse performance theory, an advanced new research field which deals with the calculation of varieties of parameters which give rise to a determined performance. This field uses techniques of algebraic geometry and statistics, approaches which have already produced significant results in the understanding of highest-ranked human performances. The book's formal language and models are currently being used by leading researchers in Europe and Northern America and have become a foundation of music software design. This is also testified by the book's nineteen collaborators and the included CD-ROM containing software and music examples.</abstract>
		<added-at>2009-11-03T16:12:26.000+0100</added-at>
		<biburl>http://www.bibsonomy.org/bibtex/2b0da2ee3c24883adcc6034626384a4e2/algebradresden</biburl>
		<title>The topos of music : geometric logic of concepts, theory, and performance</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29851819ac8fe6a897fd22a10068bc31a/keinstein</id>
		<tags>Neurowissenschaften</tags>
		<tags>Musiktheorie</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2005</date>
		<count>2</count>
		<journal>Annals of the New York Academy of Sciences</journal>
		<booktitle>The Neurosciences and Music II: From Perception to Performance</booktitle>
		<publisher>Center for Mind and Brain, University of California, Davis, Davis, California 95616, USA</publisher>
		<year>2005</year>
		<url>http://dx.doi.org/10.1196/annals.1360.008</url>
		<author>Petr Janata</author>
		<authors>
			<first>Petr</first>
		</authors>
		<authors>
			<last>Janata</last>
		</authors>
		<volume>1060</volume>
		<pages>111-124</pages>
		<abstract>As the functional neuroimaging literature grows, it becomes increasingly apparent that music and musical activities engage diverse regions of the brain. In this paper I discuss two studies to illustrate that exactly which brain areas are observed to be responsive to musical stimuli and tasks depends on the tasks and the methods used to describe the tasks and the stimuli. In one study, subjects listened to polyphonic music and were asked to either orient their attention selectively to individual instruments or in a divided or holistic manner across multiple instruments. The network of brain areas that was recruited changed subtly with changes in the task instructions. The focus of the second study was to identify brain regions that follow the pattern of movement of a continuous melody through the tonal space defined by the major and minor keys of Western tonal music. Such an area was identified in the rostral medial prefrontal cortex. This observation is discussed in the context of other neuroimaging studies that implicate this region in inwardly directed mental states involving decisions about the self, autobiographical memory, the cognitive regulation of emotion, affective responses to musical stimuli, and familiarity judgments about musical stimuli. Together with observations that these regions are among the last to atrophy in Alzheimer disease, and that these patients appear to remain responsive to autobiographically salient musical stimuli, very early evidence is emerging from the literature for the hypothesis that the rostral medial prefrontal cortex is a node that is important for binding music with memories within a broader music-responsive network.</abstract>
		<issn>1749-6632</issn>
		<doi>10.1196/annals.1360.008</doi>
		<title>Brain Networks That Track Musical Structure</title>
		<pubtype>inbook</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2440d034bdc36b070322caf1a468fbd4c/keinstein</id>
		<tags>DLfrage</tags>
		<tags>Musiktheorie</tags>
		<tags>MaMu</tags>
		<tags>Musik</tags>
		<description>JSTOR: Music Theory Spectrum, Vol. 11, No. 2 (Autumn, 1989), pp. 187-206</description>
		<date>2010-09-14 11:04:28</date>
		<count>2</count>
		<journal>Music Theory Spectrum</journal>
		<publisher>University of California Press on behalf of the Society for Music Theory</publisher>
		<year>1989</year>
		<url>http://www.jstor.org/stable/745935</url>
		<author>Norman Carey</author>
		<author>David Clampitt</author>
		<authors>
			<first>Norman</first>
		</authors>
		<authors>
			<last>Carey</last>
		</authors>
		<authors>
			<first>David</first>
		</authors>
		<authors>
			<last>Clampitt</last>
		</authors>
		<volume>11</volume>
		<number>2</number>
		<pages>pp. 187-206</pages>
		<abstract>Pentatonic, diatonic, and chromatic scales share the same underlying structure, that of the well-formed scale. Well-formedness is defined in terms of a relationship between the order in which a single interval generates the elements of a pitch-class set and the order in which those elements appear in a scale. Another characterization provides a recursive procedure for organizing all well-formed scales into hierarchies. Finally, well-formed scales are defined in terms of scale-step measure, and aspects of the diatonic set are examined.</abstract>
		<issn>01956167</issn>
		<language>English</language>
		<copyright>Copyright © 1989 University of California Press</copyright>
		<doi>10.1525/mts.1989.11.2.02a00030</doi>
		<title>Aspects of Well-Formed Scales</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/201a0b53c7024b1cb5abe17197055bc6b/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-06-03 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#JeongSYY13</url>
		<author>Gowun Jeong</author>
		<author>Yong-Ho Seo</author>
		<author>Sang-Soo Yeo</author>
		<author>Hyun Seung Yang</author>
		<authors>
			<first>Gowun</first>
		</authors>
		<authors>
			<last>Jeong</last>
		</authors>
		<authors>
			<first>Yong-Ho</first>
		</authors>
		<authors>
			<last>Seo</last>
		</authors>
		<authors>
			<first>Sang-Soo</first>
		</authors>
		<authors>
			<last>Yeo</last>
		</authors>
		<authors>
			<first>Hyun Seung</first>
		</authors>
		<authors>
			<last>Yang</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Hyun Seung</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<editors>
			<first>Hyun Seung</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<editors>
			<first>Hyun Seung</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<editors>
			<first>Hyun Seung</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<volume>274</volume>
		<pages>475-481</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Serial Dictatorial Rule-Based Games for Camera Selection.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28401c31a4ae0803fc620b0eb3152c965/alexarje</id>
		<tags>imported</tags>
		<description></description>
		<date>2016-02-26 20:58:13</date>
		<count>1</count>
		<booktitle>Proceedings of Music Education Research, Values and Initiatives</booktitle>
		<year>2007</year>
		<url></url>
		<author>Maree MacMillan</author>
		<authors>
			<first>Maree</first>
		</authors>
		<authors>
			<last>MacMillan</last>
		</authors>
		<pages>104</pages>
		<title>Autonomous Learning within a Learning Community?: Musicians Have Been Doing It for Years!</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c61bfea5e518df03ec579547ca79f2ce/dawsontucker85</id>
		<tags>music</tags>
		<tags>musicality</tags>
		<tags>perception</tags>
		<description></description>
		<date>2017-01-19 20:09:54</date>
		<count>2</count>
		<journal>Annals of the New York Academy of Sciences</journal>
		<year>2003</year>
		<url></url>
		<author>Emmanuel Bigand</author>
		<authors>
			<first>Emmanuel</first>
		</authors>
		<authors>
			<last>Bigand</last>
		</authors>
		<volume>999</volume>
		<pages>304--312</pages>
		<doi>10.1196/annals.1284.041</doi>
		<title>More about the musical expertise of musically untrained listeners</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29c16c2b7f11d4b82f21c02f760791d3c/becker</id>
		<tags>music</tags>
		<tags>inthesis</tags>
		<tags>diss</tags>
		<tags>analysis</tags>
		<tags>temporal</tags>
		<tags>descriptive</tags>
		<description></description>
		<date>2017-01-17 12:05:06</date>
		<count>2</count>
		<booktitle>2010 IEEE/ACIS 9th International Conference on Computer and Information Science</booktitle>
		<year>2010</year>
		<url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5591002</url>
		<author>C. H. Park</author>
		<author>M. Kahng</author>
		<authors>
			<first>C. H.</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Kahng</last>
		</authors>
		<pages>573-578</pages>
		<abstract>Although temporal context may significantly affect the popularity of items and user preference over items, traditional information filtering techniques such as recommender systems have not sufficiently considered temporal factors. Modeling temporal dynamics in user behavior is not trivial, and it is challenging to study its effect in order to provide better recommendation results to users. To incorporate temporal effects into information filtering systems, we analyze a large sized real-world usage log data gathered from Bugs Music, which is one of the well-known online music service in Korea, and study temporal dynamics in users' music listening behaviors considering periodicity of time dimension and popularity change. We insist that the result of our analysis can be a useful guideline to the industry which delivers music items to users and tries to consider temporal context in their recommendations.</abstract>
		<doi>10.1109/ICIS.2010.142</doi>
		<title>Temporal Dynamics in Music Listening Behavior: A Case Study of Online Music Service</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a55e6434253d3bd3a26e8986c5bdb24e/joaakive</id>
		<tags>ethnomusicology</tags>
		<tags>music</tags>
		<tags>theory</tags>
		<description></description>
		<date>2014-04-27 09:11:04</date>
		<count>1</count>
		<journal>Journal of Music Theory</journal>
		<year>1957</year>
		<url></url>
		<author>Kazu Nakaseko</author>
		<authors>
			<first>Kazu</first>
		</authors>
		<authors>
			<last>Nakaseko</last>
		</authors>
		<volume>Vol. 1</volume>
		<number>No. 2</number>
		<title>Symbolism in Ancient Chinese Music Theory</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e07c6fcdfe14d8542a4152ab6c46bc8f/joaakive</id>
		<tags>music</tags>
		<tags>technology</tags>
		<description></description>
		<date>2014-04-27 09:28:27</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2011</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj35.html#Geers11</url>
		<author>Dan Hosken</author>
		<authors>
			<first>Dan</first>
		</authors>
		<authors>
			<last>Hosken</last>
		</authors>
		<volume>35</volume>
		<number>4</number>
		<pages>99-101</pages>
		<title>An Introduction to Music Technology</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/249ee2b5147f713b81c71fc1030d0ae55/ar0berts</id>
		<tags>Therapy</tags>
		<tags>Humans;</tags>
		<tags>Brain</tags>
		<tags>Adult;</tags>
		<tags>Cerebral</tags>
		<tags>Gymnastics;</tags>
		<tags>Male;</tags>
		<tags>Reference</tags>
		<tags>Movement;</tags>
		<tags>Sports;</tags>
		<tags>Ataxia;</tags>
		<tags>Physical</tags>
		<tags>Accident;</tags>
		<tags>Spinal</tags>
		<tags>Palsy;</tags>
		<tags>Female;</tags>
		<tags>Weight-Bearing</tags>
		<tags>Cerebrovascular</tags>
		<tags>(Specialty);</tags>
		<tags>Injuries;</tags>
		<tags>Cord</tags>
		<tags>Music;</tags>
		<tags>Values;</tags>
		<description></description>
		<date>2014-07-19 21:14:28</date>
		<count>1</count>
		<journal>Electromyogr Clin Neurophysiol</journal>
		<year>2003</year>
		<url></url>
		<author>G. Schalow</author>
		<author>M. P??suke</author>
		<authors>
			<first>G.</first>
		</authors>
		<authors>
			<last>Schalow</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>P??suke</last>
		</authors>
		<volume>43</volume>
		<number>4</number>
		<pages>195--201</pages>
		<abstract>Low-load coordination dynamics were measured in athletes, physiotherapists, gymnasts, musicians and patients after stroke, traumatic brain injury and spinal cord lesion during exercise on a special coordination dynamic therapy device to quantify differences in central nervous system (CNS) organization between healthy subjects and patients with CNS injury. In healthy humans coordination dynamics (arrhythmicity of turning) varied between 5.2 and 6.0 for forward and between 6.9 and 10.7 1/s for backward turning. The frequency of turning varied between 1.24 (athletes) and 1.49 Hz (musicians) for forward and between 1.11 and 1.25 Hz for backward turning. Apart from the poor rhythmicity of backward turning among physiotherapists, gymnasts and musicians, inter-group differences were small in comparison to intra-group variation. In patients with spinal cord lesion the coordination dynamics value was 8.3 for forward and 11.0 for backward turning. The frequencies for forward and backward turning were 1.20 and 1.20 Hz respectively. The values for coordination dynamics and frequency of turning thus did only slightly differ from those measured for healthy subjects. The patients after stroke, traumatic brain injury and cerebral palsy had much higher coordination dynamic values (20.4, 22.9 and 30 1/s respectively) and lower forward (0.85, 0.93, and 0.52 Hz) and backward turning frequencies (0.98, 1.06, 0.42 Hz), suggesting strongly pathologic CNS organization. Low-load coordination dynamics (20N) are thus useful to measure progress in CNS organization due to therapy in patients with CNS injury.</abstract>
		<title>Low-load coordination dynamics in athletes, physiotherapists, gymnasts, musicians and patients with spinal cord injury, after stroke, traumatic brain lesion and with cerebral palsy.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2024c913d87e4fed65268b43b5148c6a7/choko</id>
		<tags>phone,music</tags>
		<tags>TV,media</tags>
		<tags>interaction,interactive</tags>
		<tags>TV,voting</tags>
		<tags>studies,mobile</tags>
		<tags>Usability,human-computer</tags>
		<description></description>
		<date>2013-12-30 18:09:17</date>
		<count>2</count>
		<booktitle>Proceedings of the 6th European conference on Changing Television Environments</booktitle>
		<series>Lecture Notes in Computer Science</series>
		<publisher>Springer Berlin Heidelberg</publisher>
		<address>Berlin, Heidelberg</address>
		<year>2008</year>
		<url>http://dx.doi.org/10.1007/978-3-540-69478-6\_36</url>
		<author>Ralph Riecke</author>
		<author>Alex Juers</author>
		<author>Konstantinos Chorianopoulos</author>
		<authors>
			<first>Ralph</first>
		</authors>
		<authors>
			<last>Riecke</last>
		</authors>
		<authors>
			<first>Alex</first>
		</authors>
		<authors>
			<last>Juers</last>
		</authors>
		<authors>
			<first>Konstantinos</first>
		</authors>
		<authors>
			<last>Chorianopoulos</last>
		</authors>
		<editor>Manfred Tscheligi</editor>
		<editor>Marianna Obrist</editor>
		<editor>Artur Lugmayr</editor>
		<editors>
			<first>Konstantinos</first>
		</editors>
		<editors>
			<last>Chorianopoulos</last>
		</editors>
		<editors>
			<first>Konstantinos</first>
		</editors>
		<editors>
			<last>Chorianopoulos</last>
		</editors>
		<editors>
			<first>Konstantinos</first>
		</editors>
		<editors>
			<last>Chorianopoulos</last>
		</editors>
		<volume>5066</volume>
		<pages>268--272</pages>
		<abstract>The aim of this work is to study the usability of voting on music TV channels. We asked subjects to perform a voting-task on two different music TV shows. The results indicate, that 1) there are small differences in acceptance and understanding of the voting-instructions between users and non-users, 2) the mobile phone is a familiar and the most preferred voting-device and 3) sociability features is a way to support the pricing model of voting services for entertainment applications in TV.</abstract>
		<issn>0302-9743</issn>
		<isbn>978-3-540-69477-9</isbn>
		<doi>10.1007/978-3-540-69478-6</doi>
		<title>Interaction Design in Television Voting: A Usability Study on Music TV and Input Devices</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fb98d04715b7fce804bd6d2ae3a50655/mandrean</id>
		<tags>timbre</tags>
		<tags>LWINDSOR</tags>
		<description></description>
		<date>2015-12-19 09:36:39</date>
		<count>1</count>
		<journal>Contemporary Music Review</journal>
		<year>1994</year>
		<url>/brokenurl#         http://dx.doi.org/10.1080/07494469400640321    </url>
		<author>W. Luke Windsor</author>
		<authors>
			<first>W. Luke</first>
		</authors>
		<authors>
			<last>Windsor</last>
		</authors>
		<volume>10</volume>
		<number>2</number>
		<pages>85-93</pages>
		<abstract>This essay uses research in ecological acoustics to perform a critique of the rejection of worldly meaning and significance inherent in the classical notion of reduced or acousmatic listening, showing how the direct perception of events and their meanings, or affordances, can be exploited by electroacoustic musicians without betraying the spirit of an aural compositional technique. Our dynamic relationship with a meaningful environment and the structured auditory information that it produces is shown to provide the grounds for developing compositional techniques and tools that exploit a closer relationship to sounds through their direct perceptual links to causal actions and events. These links are also shown to provide the basis for a common ground between composer and listener that redefines the acousmatic by reinstating and reassessing the importance of mimesis in electroacoustic music.</abstract>
		<eprint>http://dx.doi.org/10.1080/07494469400640321</eprint>
		<doi>10.1080/07494469400640321</doi>
		<title>Using auditory information for events in electroacoustic music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f09cf7ba2fd748bf682ce48c3ac67295/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2020-04-30 00:00:00</date>
		<count>1</count>
		<booktitle>The Oxford Handbook of Algorithmic Music</booktitle>
		<publisher>Oxford University Press</publisher>
		<year>2018</year>
		<url>http://dblp.uni-trier.de/db/books/collections/MD2018.html#CoxR18</url>
		<author>Geoff Cox</author>
		<author>Morten Riis</author>
		<authors>
			<first>Geoff</first>
		</authors>
		<authors>
			<last>Cox</last>
		</authors>
		<authors>
			<first>Morten</first>
		</authors>
		<authors>
			<last>Riis</last>
		</authors>
		<editor>Alex McLean</editor>
		<editor>Roger T. Dean</editor>
		<editors>
			<first>Morten</first>
		</editors>
		<editors>
			<last>Riis</last>
		</editors>
		<editors>
			<first>Morten</first>
		</editors>
		<editors>
			<last>Riis</last>
		</editors>
		<isbn>978-0-19-022699-2</isbn>
		<title>(Micro) Politics of Algorithmic Music: Towards a Tactical Media Archaeology.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2032e4a3b13ab4e53387ad79f886f5618/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2020-04-30 00:00:00</date>
		<count>1</count>
		<booktitle>The Oxford Handbook of Algorithmic Music</booktitle>
		<publisher>Oxford University Press</publisher>
		<year>2018</year>
		<url>http://dblp.uni-trier.de/db/books/collections/MD2018.html#Levtov18</url>
		<author>Yuli Levtov</author>
		<authors>
			<first>Yuli</first>
		</authors>
		<authors>
			<last>Levtov</last>
		</authors>
		<editor>Alex McLean</editor>
		<editor>Roger T. Dean</editor>
		<editors>
			<first>Yuli</first>
		</editors>
		<editors>
			<last>Levtov</last>
		</editors>
		<editors>
			<first>Yuli</first>
		</editors>
		<editors>
			<last>Levtov</last>
		</editors>
		<isbn>978-0-19-022699-2</isbn>
		<title>Algorithmic Music for Mass Consumption and Universal Production.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/236fd1b1ecf4b503f7863d56047a65891/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2020-04-30 00:00:00</date>
		<count>1</count>
		<booktitle>The Oxford Handbook of Algorithmic Music</booktitle>
		<publisher>Oxford University Press</publisher>
		<year>2018</year>
		<url>http://dblp.uni-trier.de/db/books/collections/MD2018.html#Beran18</url>
		<author>Jan Beran</author>
		<authors>
			<first>Jan</first>
		</authors>
		<authors>
			<last>Beran</last>
		</authors>
		<editor>Alex McLean</editor>
		<editor>Roger T. Dean</editor>
		<editors>
			<first>Jan</first>
		</editors>
		<editors>
			<last>Beran</last>
		</editors>
		<editors>
			<first>Jan</first>
		</editors>
		<editors>
			<last>Beran</last>
		</editors>
		<isbn>978-0-19-022699-2</isbn>
		<title>Mathematical Theory in Music Practice.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/265dd67354f70c1170422913511e2ceb8/sapo</id>
		<tags>perceptual_evaluation</tags>
		<tags>excerpt_duration</tags>
		<description>Effects of Excerpt Duration, Tempo, and Performance Level on Musicians' Ratings of Wind Band Performances - John M. Geringer, Christopher M. Johnson, 2007</description>
		<date>2020-03-24 12:14:35</date>
		<count>2</count>
		<journal>Journal of Research in Music Education</journal>
		<publisher>SAGE Publications</publisher>
		<year>2007</year>
		<url>https://doi.org/10.1177%2F0022429408317366</url>
		<author>John M. Geringer</author>
		<author>Christopher M. Johnson</author>
		<authors>
			<first>John M.</first>
		</authors>
		<authors>
			<last>Geringer</last>
		</authors>
		<authors>
			<first>Christopher M.</first>
		</authors>
		<authors>
			<last>Johnson</last>
		</authors>
		<volume>55</volume>
		<number>4</number>
		<pages>289--301</pages>
		<doi>10.1177/0022429408317366</doi>
		<title>Effects of Excerpt Duration, Tempo, and Performance Level on Musicians Ratings of Wind Band Performances</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2212dee2879a020860f0752a2543c4b10/bauerc</id>
		<tags>imported</tags>
		<tags>myown</tags>
		<tags>country</tags>
		<tags>music</tags>
		<tags>mainstreaminess</tags>
		<tags>recsys</tags>
		<description></description>
		<date>2019-04-27 18:22:14</date>
		<count>2</count>
		<booktitle>51st Hawaii International Conference on System Sciences (HICSS 2018)</booktitle>
		<series>HICSS 2018</series>
		<publisher>ScholarSpace / AIS Electronic Library (AISeL)</publisher>
		<year>2018</year>
		<url>http://hdl.handle.net/10125/50349</url>
		<author>Christine Bauer</author>
		<author>Markus Schedl</author>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<pages>3647-3656</pages>
		<abstract>In the field of music recommender systems, country-specific aspects have received little attention, although it is known that music perception and preferences are shaped by culture; and culture varies across countries. Based on the LFM-1b dataset (including 53,258 users from 47 countries), we show that there are significant country-specific differences in listeners' music consumption behavior with respect to the most popular artists listened to. Results indicate that, for instance, Finnish users' listening behavior is farther away from the global mainstream, while United States' listeners are close to the global mainstream. Relying on rating prediction experiments, we tailor recommendations to a user's level of preference for mainstream (defined on a global level and on a country level) and the user's country. Results suggest that, in terms of rating prediction accuracy, a combination of these two filtering strategies works particularly well for users of countries far away from the global mainstream.</abstract>
		<isbn>978-0-9981331-1-9</isbn>
		<language>English</language>
		<doi>10.24251/HICSS.2018.461</doi>
		<title>On the importance of considering country-specific aspects on the online-market: an example of music recommendation considering country-specific mainstream</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d688f5890a328f732701344162371379/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2017-05-21 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2014</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html</url>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Markus</first>
		</editors>
		<editors>
			<last>Schedl</last>
		</editors>
		<editors>
			<first>Markus</first>
		</editors>
		<editors>
			<last>Schedl</last>
		</editors>
		<editors>
			<first>Markus</first>
		</editors>
		<editors>
			<last>Schedl</last>
		</editors>
		<editors>
			<first>Markus</first>
		</editors>
		<editors>
			<last>Schedl</last>
		</editors>
		<volume>274</volume>
		<isbn>978-3-642-40674-4</isbn>
		<title>Mobile, Ubiquitous, and Intelligent Computing - MUSIC 2013, FTRA 4th International Conference on Mobile, Ubiquitous, and Intelligent Computing, September 4-6, 2013, Gwangju, Korea</title>
		<pubtype>proceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c27e115aed56d01e790550ff5bbbe1ac/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2020-03-18 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#IslamK13</url>
		<author>Md. Shohidul Islam</author>
		<author>Jong-Myon Kim</author>
		<authors>
			<first>Md. Shohidul</first>
		</authors>
		<authors>
			<last>Islam</last>
		</authors>
		<authors>
			<first>Jong-Myon</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Jong-Myon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Jong-Myon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Jong-Myon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Jong-Myon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<volume>274</volume>
		<pages>47-53</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>End-to-End High Speed Forward Error Correction Using Graphics Processing Units.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b866320b4ce41731ba76d786e4230efe/lepsky</id>
		<tags>musik</tags>
		<tags>information_retrieval</tags>
		<description></description>
		<date>2018-11-04 17:02:36</date>
		<count>2</count>
		<journal>Journal of the Association for Information Science & Technology</journal>
		<year>2016</year>
		<url></url>
		<author>Jin Ha Lee</author>
		<author>Hyerim Cho</author>
		<author>Yea-Seul Kim</author>
		<authors>
			<first>Jin Ha</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Hyerim</first>
		</authors>
		<authors>
			<last>Cho</last>
		</authors>
		<authors>
			<first>Yea-Seul</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<volume>67</volume>
		<number>6</number>
		<pages>1301--1330</pages>
		<abstract>User studies in the music information retrieval ( MIR) domain tend to be exploratory and qualitative in nature, involving a small number of users, which makes it difficult to derive broader implications for system design. In order to fill this gap, we conducted a large-scale user survey questioning various aspects of people's music information needs and behaviors. In particular, we investigated if general music users' needs and behaviors have significantly changed over time by comparing our current survey results with a similar survey conducted in 2004. In this paper, we present the key findings from the survey data and discuss 4 emergent themes-(a) the shift in access and use of personal music collections; (b) the growing need for tools to support collaborative music seeking, listening, and sharing; (c) the importance of 'visual' music experiences; and (d) the need for ontologies for providing rich contextual information. We conclude by making specific recommendations for improving the design of MIR systems and services.</abstract>
		<shorttitle>Users' music information needs and behaviors</shorttitle>
		<issn>23301635</issn>
		<doi>10.1002/asi.23471</doi>
		<title>Users' music information needs and behaviors : design implications for music information retrieval systems</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f91205a4c6701d3c78bb2c7cc8d24e1e/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2018-11-02 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#GirsangTY13</url>
		<author>Abba Suganda Girsang</author>
		<author>Chun-Wei Tsai</author>
		<author>Chu-Sing Yang</author>
		<authors>
			<first>Abba Suganda</first>
		</authors>
		<authors>
			<last>Girsang</last>
		</authors>
		<authors>
			<first>Chun-Wei</first>
		</authors>
		<authors>
			<last>Tsai</last>
		</authors>
		<authors>
			<first>Chu-Sing</first>
		</authors>
		<authors>
			<last>Yang</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Chu-Sing</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<editors>
			<first>Chu-Sing</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<editors>
			<first>Chu-Sing</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<editors>
			<first>Chu-Sing</first>
		</editors>
		<editors>
			<last>Yang</last>
		</editors>
		<volume>274</volume>
		<pages>643-648</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>A Hybrid Ant-Bee Colony Optimization for Solving Traveling Salesman Problem with Competitive Agents.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c10c716ec8dbe384e265b70af64e14d9/chwick</id>
		<tags>myown</tags>
		<tags>just_intonation</tags>
		<description>Playing Music in Just Intonation - A Dynamically Adapting Tuning Scheme</description>
		<date>2017-07-31 11:05:32</date>
		<count>1</count>
		<year>2017</year>
		<url>http://arxiv.org/abs/1706.04338</url>
		<author>Karolin Stange</author>
		<author>Christoph Wick</author>
		<author>Haye Hinrichsen</author>
		<authors>
			<first>Karolin</first>
		</authors>
		<authors>
			<last>Stange</last>
		</authors>
		<authors>
			<first>Christoph</first>
		</authors>
		<authors>
			<last>Wick</last>
		</authors>
		<authors>
			<first>Haye</first>
		</authors>
		<authors>
			<last>Hinrichsen</last>
		</authors>
		<abstract>We introduce a dynamically adapting tuning scheme for microtonal tuning of
musical instruments, allowing the performer to play music in just intonation in
any key. Unlike previous methods, which are based on a procedural analysis of
the chordal structure, the suggested tuning scheme continually solves a system
of linear equations without making explicit decisions. In complex situations,
where not all intervals of a chord can be tuned according to just frequency
ratios, the method automatically yields a tempered compromise. We outline the
implementation of the algorithm in an open-source software project that we have
provided in order to demonstrate the feasibility of the tuning method.</abstract>
		<title>Playing Music in Just Intonation - A Dynamically Adapting Tuning Scheme</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2660e70d2cc6540909faa94dae9b6a8d7/marianorico</id>
		<tags>datos4.0</tags>
		<description>Automatic Music Generation by Deep Learning | SpringerLink</description>
		<date>2019-02-26 13:05:06</date>
		<count>1</count>
		<booktitle>Distributed Computing and Artificial Intelligence, 15th International Conference</booktitle>
		<publisher>Springer International Publishing</publisher>
		<address>Cham</address>
		<year>2019</year>
		<url></url>
		<author>Juan Carlos García</author>
		<author>Emilio Serrano</author>
		<authors>
			<first>Juan Carlos</first>
		</authors>
		<authors>
			<last>García</last>
		</authors>
		<authors>
			<first>Emilio</first>
		</authors>
		<authors>
			<last>Serrano</last>
		</authors>
		<editor>Fernando De La Prieta</editor>
		<editor>Sigeru Omatu</editor>
		<editor>Antonio Fernández-Caballero</editor>
		<editors>
			<first>Emilio</first>
		</editors>
		<editors>
			<last>Serrano</last>
		</editors>
		<editors>
			<first>Emilio</first>
		</editors>
		<editors>
			<last>Serrano</last>
		</editors>
		<editors>
			<first>Emilio</first>
		</editors>
		<editors>
			<last>Serrano</last>
		</editors>
		<pages>284--291</pages>
		<abstract>This paper presents a model capable of generating and completing musical compositions automatically. The model is based on generative learning paradigms of machine learning and deep learning, such as recurrent neural networks. Related works consider music as a text of a natural language, requiring the network to learn the syntax of the sheet music completely and the dependencies among symbols. This involves a very intense training and may produce overfitting in many cases. This paper contributes with a data preprocessing that eliminates the most complex dependencies allowing the musical content to be abstracted from the syntax. Moreover, a web application based on the trained models is presented. The tool allows inexperienced users to generate automatic music from scratch or from a given fragment of sheet music.</abstract>
		<isbn>978-3-319-94649-8</isbn>
		<title>Automatic Music Generation by Deep Learning</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f11e4140683a05d1b70830afe3f38d46/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>Proceedings of the 6th Sound and Music Computing Conference</booktitle>
		<series>Proceedings of the 6th Sound and Music Computing Conference</series>
		<year>2009</year>
		<url></url>
		<author>Dimitris Rafailidis</author>
		<author>Emilios Cambouropoulos</author>
		<author>Yannis Manolopoulos</author>
		<authors>
			<first>Dimitris</first>
		</authors>
		<authors>
			<last>Rafailidis</last>
		</authors>
		<authors>
			<first>Emilios</first>
		</authors>
		<authors>
			<last>Cambouropoulos</last>
		</authors>
		<authors>
			<first>Yannis</first>
		</authors>
		<authors>
			<last>Manolopoulos</last>
		</authors>
		<number>July</number>
		<title>Musical Voice Integration / Segregation : VISA revisited</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/211c3b07c166b3f58343bfb1e50561217/sapo</id>
		<tags>music_analysis</tags>
		<tags>music_understanding</tags>
		<tags>web_service</tags>
		<tags>music_interface</tags>
		<tags>active_music_listening</tags>
		<tags>music_information_research</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>2</count>
		<booktitle>23rd International Conference on Intelligent User Interfaces</booktitle>
		<series>IUI '18</series>
		<publisher>ACM</publisher>
		<year>2018</year>
		<url></url>
		<author>Masataka Goto</author>
		<authors>
			<first>Masataka</first>
		</authors>
		<authors>
			<last>Goto</last>
		</authors>
		<pages>3--4</pages>
		<abstract>Automatic music-understanding technologies (automatic analysis of
	music signals) make possible the creation of intelligent music interfaces
	that enrich music experiences and open up new ways of listening to
	music. In the past, it was common to listen to music in a somewhat
	passive manner; in the future, people will be able to enjoy music
	in a more active manner by using music technologies. Listening to
	music through active interactions is called active music listening.
	In this keynote speech I first introduce active music listening interfaces
	demonstrating how end users can benefit from music-understanding
	technologies based on signal processing and/or machine learning.
	By analyzing the music structure (chorus sections), for example,
	the SmartMusicKIOSK interface enables people to access their favorite
	part of a song directly (skipping other parts) while viewing a visual
	representation of the song's structure. I then introduce our recent
	challenge of deploying such research-level music interfaces as web
	services open to the public. Those services augment people's understanding
	of music, enable music-synchronized control of computer-graphics
	animation and robots, and provide various bird's-eye views on a large
	music collection. In the future, further advances in music-understanding
	technologies and music interfaces based on them will make interaction
	between people and music even more active and enriching.</abstract>
		<doi>10.1145/3172944.3176184</doi>
		<title>Intelligent Music Interfaces</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/275f910a236f2aab5791ecfc94b8dd2cb/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>9</count>
		<year>2003</year>
		<url></url>
		<author>Jean-julien Aucouturier</author>
		<author>Francois Pachet</author>
		<authors>
			<first>Jean-julien</first>
		</authors>
		<authors>
			<last>Aucouturier</last>
		</authors>
		<authors>
			<first>Francois</first>
		</authors>
		<authors>
			<last>Pachet</last>
		</authors>
		<volume>32</volume>
		<number>February 2015</number>
		<pages>83--93</pages>
		<abstract>Musical genre is probably the most popular music descrip- tor. In
	the context of large musical databases and Electronic Music Distribution,
	genre is therefore a crucial metadata for the description of music
	content. However, genre is intrinsi- cally ill-defined and attempts
	at defining genre precisely have a strong tendency to end up in circular,
	ungrounded projec- tions of fantasies. Is genre an intrinsic attribute
	of music titles, as, say, tempo? Or is genre a extrinsic description
	of the whole piece? In this article, we discuss the various approaches
	in representing musical genre, and propose to classify these approaches
	in three main categories: manual, prescriptive and emergent approaches.
	We discuss the pros and cons of each approach, and illustrate our
	study with results of the Cuidado IST project.</abstract>
		<journaltitle>Journal of New Music Research</journaltitle>
		<doi>10.1076/jnmr.32.1.83.16801</doi>
		<title>Representing Musical Genre : A State of the Art</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26acfe92dcd36803bc289e2fd7d9e6de8/sapo</id>
		<tags>Off-line</tags>
		<tags>Score-to-audio_alignment</tags>
		<tags>OL_chord_asynchronies</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>2</count>
		<booktitle>Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009, Kobe International Conference Center, Kobe, Japan, October 26-30, 2009</booktitle>
		<publisher>International Society for Music Information Retrieval</publisher>
		<year>2009</year>
		<url>http://ismir2009.ismir.net/proceedings/PS4-2.pdf</url>
		<author>Bernhard Niedermayer</author>
		<authors>
			<first>Bernhard</first>
		</authors>
		<authors>
			<last>Niedermayer</last>
		</authors>
		<editor>Keiji Hirata</editor>
		<editor>George Tzanetakis</editor>
		<editor>Kazuyoshi Yoshii</editor>
		<editors>
			<first>Bernhard</first>
		</editors>
		<editors>
			<last>Niedermayer</last>
		</editors>
		<editors>
			<first>Bernhard</first>
		</editors>
		<editors>
			<last>Niedermayer</last>
		</editors>
		<editors>
			<first>Bernhard</first>
		</editors>
		<editors>
			<last>Niedermayer</last>
		</editors>
		<pages>585--590</pages>
		<__markedentry>sapo:1</__markedentry>
		<bibsource>dblp computer science bibliography, https://dblp.org</bibsource>
		<title>Improving Accuracy of Polyphonic Music-to-score Alignment</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a0b4c1f11e9d311671b16b8c52ef3f4c/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2019-09-16 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#ShiehLG13</url>
		<author>Wann-Yun Shieh</author>
		<author>An-Peng Liu</author>
		<author>Tyng-Tyng Guu</author>
		<authors>
			<first>Wann-Yun</first>
		</authors>
		<authors>
			<last>Shieh</last>
		</authors>
		<authors>
			<first>An-Peng</first>
		</authors>
		<authors>
			<last>Liu</last>
		</authors>
		<authors>
			<first>Tyng-Tyng</first>
		</authors>
		<authors>
			<last>Guu</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Tyng-Tyng</first>
		</editors>
		<editors>
			<last>Guu</last>
		</editors>
		<editors>
			<first>Tyng-Tyng</first>
		</editors>
		<editors>
			<last>Guu</last>
		</editors>
		<editors>
			<first>Tyng-Tyng</first>
		</editors>
		<editors>
			<last>Guu</last>
		</editors>
		<editors>
			<first>Tyng-Tyng</first>
		</editors>
		<editors>
			<last>Guu</last>
		</editors>
		<volume>274</volume>
		<pages>81-85</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Detect Spatial and Temporal Gait Parameters by Dual Accelerometers.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2423eaf78d4df84bfa549413d3889fac2/mariap3636</id>
		<tags>normalization</tags>
		<tags>coda</tags>
		<tags>amplicon</tags>
		<tags>16S</tags>
		<description>MUSiCC: a marker genes based framework for metagenomic normalization and accurate profiling of gene abundances in the microbiome</description>
		<date>2019-08-09 17:26:22</date>
		<count>1</count>
		<journal>Genome Biol</journal>
		<year>2015</year>
		<url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4391136/</url>
		<author>O Manor</author>
		<author>E Borenstein</author>
		<authors>
			<first>O</first>
		</authors>
		<authors>
			<last>Manor</last>
		</authors>
		<authors>
			<first>E</first>
		</authors>
		<authors>
			<last>Borenstein</last>
		</authors>
		<volume>16</volume>
		<pages>53-53</pages>
		<abstract>Functional metagenomic analyses commonly involve a normalization step, where measured levels of genes or pathways are converted into relative abundances. Here, we demonstrate that this normalization scheme introduces marked biases both across and within human microbiome samples, and identify sample- and gene-specific properties that contribute to these biases. We introduce an alternative normalization paradigm, MUSiCC, which combines universal single-copy genes with machine learning methods to correct these biases and to obtain an accurate and biologically meaningful measure of gene abundances. Finally, we demonstrate that MUSiCC significantly improves downstream discovery of functional shifts in the microbiome.MUSiCC is available at http://elbo.gs.washington.edu/software.html .</abstract>
		<doi>10.1186/s13059-015-0610-8</doi>
		<title>MUSiCC: a marker genes based framework for metagenomic normalization and accurate profiling of gene abundances in the microbiome</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e29e7fc451517c20b70442d50edab08a/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2019-10-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#ShoaibS13</url>
		<author>Muhammad Shoaib</author>
		<author>Wang-Cheol Song</author>
		<authors>
			<first>Muhammad</first>
		</authors>
		<authors>
			<last>Shoaib</last>
		</authors>
		<authors>
			<first>Wang-Cheol</first>
		</authors>
		<authors>
			<last>Song</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Wang-Cheol</first>
		</editors>
		<editors>
			<last>Song</last>
		</editors>
		<editors>
			<first>Wang-Cheol</first>
		</editors>
		<editors>
			<last>Song</last>
		</editors>
		<editors>
			<first>Wang-Cheol</first>
		</editors>
		<editors>
			<last>Song</last>
		</editors>
		<editors>
			<first>Wang-Cheol</first>
		</editors>
		<editors>
			<last>Song</last>
		</editors>
		<volume>274</volume>
		<pages>179-184</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Clustering Objects in Heterogeneous Information Network Using Fuzzy C-Mean.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2cf2224835e7cb9de20054fae1dd26bf7/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2018-11-14 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2012.html#AbdelkaderY12</url>
		<author>Reem Abdelkader</author>
		<author>Moustafa Youssef</author>
		<authors>
			<first>Reem</first>
		</authors>
		<authors>
			<last>Abdelkader</last>
		</authors>
		<authors>
			<first>Moustafa</first>
		</authors>
		<authors>
			<last>Youssef</last>
		</authors>
		<pages>72-77</pages>
		<isbn>978-1-4673-1956-0</isbn>
		<title>UVote: A Ubiquitous E-voting System.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e64d1e4cdc748fefb6075c26c78e62c9/sapo</id>
		<tags>source_separation</tags>
		<description>Deep clustering and conventional networks for music separation: Stronger together - IEEE Conference Publication</description>
		<date>2020-11-12 16:54:12</date>
		<count>1</count>
		<booktitle>2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</booktitle>
		<year>2017</year>
		<url>https://ieeexplore.ieee.org/abstract/document/7952118</url>
		<author>Y. Luo</author>
		<author>Z. Chen</author>
		<author>J. R. Hershey</author>
		<author>J. Le Roux</author>
		<author>N. Mesgarani</author>
		<authors>
			<first>Y.</first>
		</authors>
		<authors>
			<last>Luo</last>
		</authors>
		<authors>
			<first>Z.</first>
		</authors>
		<authors>
			<last>Chen</last>
		</authors>
		<authors>
			<first>J. R.</first>
		</authors>
		<authors>
			<last>Hershey</last>
		</authors>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Le Roux</last>
		</authors>
		<authors>
			<first>N.</first>
		</authors>
		<authors>
			<last>Mesgarani</last>
		</authors>
		<pages>61-65</pages>
		<abstract>Deep clustering is the first method to handle general audio separation scenarios with multiple sources of the same type and an arbitrary number of sources, performing impressively in speaker-independent speech separation tasks. However, little is known about its effectiveness in other challenging situations such as music source separation. Contrary to conventional networks that directly estimate the source signals, deep clustering generates an embedding for each time-frequency bin, and separates sources by clustering the bins in the embedding space. We show that deep clustering outperforms conventional networks on a singing voice separation task, in both matched and mismatched conditions, even though conventional networks have the advantage of end-to-end training for best signal approximation, presumably because its more flexible objective engenders better regularization. Since the strengths of deep clustering and conventional network architectures appear complementary, we explore combining them in a single hybrid network trained via an approach akin to multi-task learning. Remarkably, the combination significantly outperforms either of its components.</abstract>
		<issn>2379-190X</issn>
		<doi>10.1109/ICASSP.2017.7952118</doi>
		<title>Deep clustering and conventional networks for music separation: Stronger together</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24584e7586c0020bf899251142436daa4/brusilovsky</id>
		<tags>neural-network</tags>
		<tags>recommender</tags>
		<tags>knowledge</tags>
		<description>What does BERT know about books, movies and music? Probing BERT for Conversational Recommendation | Fourteenth ACM Conference on Recommender Systems</description>
		<date>2020-09-24 05:42:10</date>
		<count>5</count>
		<booktitle>Fourteenth ACM Conference on Recommender Systems</booktitle>
		<publisher>ACM</publisher>
		<year>2020</year>
		<url>https://doi.org/10.1145%2F3383313.3412249</url>
		<author>Gustavo Penha</author>
		<author>Claudia Hauff</author>
		<authors>
			<first>Gustavo</first>
		</authors>
		<authors>
			<last>Penha</last>
		</authors>
		<authors>
			<first>Claudia</first>
		</authors>
		<authors>
			<last>Hauff</last>
		</authors>
		<pages>388-397</pages>
		<abstract>Heavily pre-trained transformer models such as BERT have recently shown to be remarkably powerful at language modelling, achieving impressive results on numerous downstream tasks. It has also been shown that they implicitly store factual knowledge in their parameters after pre-training. Understanding what the pre-training procedure of LMs actually learns is a crucial step for using and improving them for Conversational Recommender Systems (CRS). We first study how much off-the-shelf pre-trained BERT “knows” about recommendation items such as books, movies and music. In order to analyze the knowledge stored in BERT’s parameters, we use different probes (i.e., tasks to examine a trained model regarding certain properties) that require different types of knowledge to solve, namely content-based and collaborative-based. Content-based knowledge is knowledge that requires the model to match the titles of items with their content information, such as textual descriptions and genres. In contrast, collaborative-based knowledge requires the model to match items with similar ones, according to community interactions such as ratings. We resort to BERT’s Masked Language Modelling (MLM) head to probe its knowledge about the genre of items, with cloze style prompts. In addition, we employ BERT’s Next Sentence Prediction (NSP) head and representations’ similarity (SIM) to compare relevant and non-relevant search and recommendation query-document inputs to explore whether BERT can, without any fine-tuning, rank relevant items first. Finally, we study how BERT performs in a conversational recommendation downstream task. To this end, we fine-tune BERT to act as a retrieval-based CRS. Overall, our experiments show that: (i) BERT has knowledge stored in its parameters about the content of books, movies and music; (ii) it has more content-based knowledge than collaborative-based knowledge; and (iii) fails on conversational recommendation when faced with adversarial data.</abstract>
		<doi>10.1145/3383313.3412249</doi>
		<title>What does BERT know about books, movies and music? Probing BERT for Conversational Recommendation</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/226df046fa672e9d6d2faabcb77181fe6/researchparks</id>
		<tags>culturalheritage</tags>
		<tags>nationalspirituality</tags>
		<tags>tradition</tags>
		<tags>musician</tags>
		<description></description>
		<date>2021-02-27 08:49:46</date>
		<count>1</count>
		<journal>International Journal on Integrated Education</journal>
		<year>2020</year>
		<url>https://journals.researchparks.org/index.php/IJIE/article/view/226</url>
		<author>S. B. Мurtozova</author>
		<authors>
			<first>S. B.</first>
		</authors>
		<authors>
			<last>Мurtozova</last>
		</authors>
		<volume>2</volume>
		<number>4</number>
		<pages>32-38</pages>
		<abstract>This article is devoted to the study of the history of music education in Uzbekistan. Generalized questions about the changes in the field of music that occurred after the establishment of Soviet power in Uzbekistan, the subordination of music education to the ideas of communist ideology, the organization of local music, choral schools, schools of folk music, which focused on the promotion of European music.  dutar Pdf Url : https://journals.researchparks.org/index.php/IJIE/article/view/226/219 Paper Url : https://journals.researchparks.org/index.php/IJIE/article/view/226</abstract>
		<language>English</language>
		<issn>2620-3502</issn>
		<title>From the history of music education in Uzbekistan</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25817d9f7fdde6aa6f0dbb2969c350cb6/researchparks</id>
		<tags>musicalwork</tags>
		<tags>composer</tags>
		<tags>pianoperformance</tags>
		<description></description>
		<date>2021-03-03 06:20:05</date>
		<count>1</count>
		<journal>International Journal on Integrated Education</journal>
		<year>2020</year>
		<url>https://journals.researchparks.org/index.php/IJIE/article/view/1030</url>
		<author>Kholmatov Sherzod Khushbekovich</author>
		<authors>
			<first>Kholmatov Sherzod</first>
		</authors>
		<authors>
			<last>Khushbekovich</last>
		</authors>
		<volume>3</volume>
		<number>12</number>
		<pages>333-335</pages>
		<abstract>professional skill Kholmatov Sherzod Khushbekovich 2020. MUSIC-WIDE DEVELOPMENT OF SPECIAL EXERCISES AND PEDAGOGICAL SCIENTIFIC METHODS IN THE PERFORMANCE OF PIANO MUSICAL. International Journal on Integrated Education. 3, 12 (Dec. 2020), 333-335. DOI:https://doi.org/10.31149/ijie.v3i12.1030 Pdf Url : https://journals.researchparks.org/index.php/IJIE/article/view/1030/977 Paper Url : https://journals.researchparks.org/index.php/IJIE/article/view/1030</abstract>
		<language>English</language>
		<issn>2620-3502</issn>
		<title>MUSIC-WIDE DEVELOPMENT OF SPECIAL EXERCISES AND PEDAGOGICAL SCIENTIFIC METHODS IN THE PERFORMANCE OF PIANO MUSICAL</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/223b3b0952fb4d6a650963773887d37ca/simonha94</id>
		<tags>imported</tags>
		<tags>musicsearch</tags>
		<tags>uncovr</tags>
		<tags>mir</tags>
		<description></description>
		<date>2021-10-15 08:46:33</date>
		<count>1</count>
		<year>2009</year>
		<url>http://lucene.apache.org/nutch</url>
		<author>P Knees</author>
		<author>T Pohle</author>
		<author>M Schedl</author>
		<author>D Schnitzer</author>
		<author>K Seyerlehner</author>
		<author>G Widmer</author>
		<authors>
			<first>P</first>
		</authors>
		<authors>
			<last>Knees</last>
		</authors>
		<authors>
			<first>T</first>
		</authors>
		<authors>
			<last>Pohle</last>
		</authors>
		<authors>
			<first>M</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<authors>
			<first>D</first>
		</authors>
		<authors>
			<last>Schnitzer</last>
		</authors>
		<authors>
			<first>K</first>
		</authors>
		<authors>
			<last>Seyerlehner</last>
		</authors>
		<authors>
			<first>G</first>
		</authors>
		<authors>
			<last>Widmer</last>
		</authors>
		<abstract>We investigate an approach to a music search engine that indexes music pieces based on related Web documents. This allows for searching for relevant music pieces by issuing descriptive textual queries. In this paper, we examine the effects of incorporating audio-based similarity into the text-based ranking process-either by directly modifying the retrieval process or by performing post-hoc audio-based re-ranking of the search results. The aim of this combination is to improve ranking quality by including relevant tracks that are left out by text-based retrieval approaches. Our evaluations show overall improvements but also expose limitations of these unsupervised approaches to combining sources. Evaluations are carried out on two collections , one large real-world collection containing about 35,000 tracks and on the CAL500 set.</abstract>
		<title>AUGMENTING TEXT-BASED MUSIC RETRIEVAL WITH AUDIO SIMILARITY</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>sigarra:343966, https://hdl.handle.net/10216/121352, 202392015</id>
		<tags>synthesis</tags>
		<description>Repositório Aberto da Universidade do Porto: Musical Cross Synthesis using Matrix Factorisation https://repositorio-aberto.up.pt/handle/10216/121352</description>
		<date>2021-11-20 17:33:35</date>
		<count>2</count>
		<year>2019</year>
		<url>https://repositorio-aberto.up.pt/handle/10216/121352</url>
		<author>Francisco Daniel Andrade Fonseca</author>
		<authors>
			<first>Francisco Daniel Andrade</first>
		</authors>
		<authors>
			<last>Fonseca</last>
		</authors>
		<editor>Faculdade de Engenharia</editor>
		<editors>
			<first>Francisco Daniel Andrade</first>
		</editors>
		<editors>
			<last>Fonseca</last>
		</editors>
		<abstract>The focus of this work is to explore a new method for the creative analysis and manipulation of musical audio content. Given a target song and a source song, the goal is reconstruct the harmonic and rhythmic structure of the target with the timbral components from the source, in such a way that so that both the target and the source material are recognizable by the listener. We refer to this operation as musical cross-synthesis. For this purpose, we propose the use of a Matrix Factorisation method, more specifically, Shift-Invariant Probabilistic Latent Component Analysis (PLCA). The input to the PLCA algorithm are beat synchronous CQT basis functions of the source whose temporal activations are used to approximate the CQT of the target. Using the shift invariant property of the PLCA allows each basis function to be subjected to a range of possible pitch shifts which increases the flexibility of the source to represent the target. To create the resulting musical cross-synthesis the beat synchronous, pitch-shifted CQT basis functions are inverted and concatenated in time.</abstract>
		<title>Musical Cross Synthesis using Matrix Factorisation</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ca23560e37e20c7d77eaeb3387dd1c1d/sapo</id>
		<tags>audio_restoration</tags>
		<description>Reduction of parasitic pitch variations in archival musical recordings - ScienceDirect https://www.sciencedirect.com/science/article/pii/S016516840900406X</description>
		<date>2021-11-16 14:41:13</date>
		<count>3</count>
		<journal>Signal Processing</journal>
		<year>2010</year>
		<url>https://www.sciencedirect.com/science/article/pii/S016516840900406X</url>
		<author>Andrzej Czyzewski</author>
		<author>Przemyslaw Maziewski</author>
		<author>Adam Kupryjanow</author>
		<authors>
			<first>Andrzej</first>
		</authors>
		<authors>
			<last>Czyzewski</last>
		</authors>
		<authors>
			<first>Przemyslaw</first>
		</authors>
		<authors>
			<last>Maziewski</last>
		</authors>
		<authors>
			<first>Adam</first>
		</authors>
		<authors>
			<last>Kupryjanow</last>
		</authors>
		<volume>90</volume>
		<number>4</number>
		<pages>981-990</pages>
		<abstract>A new method for reducing parasitic pitch variations in archival audio recordings is presented. The method is intended for analyzing movie soundtracks recorded in optical films. It utilizes image processing for calculating and reducing effects of tape shrinkage being one of the main reasons for parasitic pitch variations in audio accompanying moving images. As long as the film tape characteristics are known the new method can be easily tuned to analyze archival recordings. The new method is also compared to some previous approaches to pitch variation correction.</abstract>
		<issn>0165-1684</issn>
		<doi>https://doi.org/10.1016/j.sigpro.2009.09.015</doi>
		<title>Reduction of parasitic pitch variations in archival musical recordings</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/239176980fd0f45a33ec6007fcbef0562/bibsonomolli</id>
		<tags>typology</tags>
		<tags>musical-bow</tags>
		<tags>musicology</tags>
		<description></description>
		<date>2021-03-18 15:52:56</date>
		<count>1</count>
		<booktitle>The New Grove dictionary of music and musicians</booktitle>
		<publisher>Macmillan</publisher>
		<address>London</address>
		<year>1984</year>
		<url></url>
		<author>D. K. Rycroft</author>
		<authors>
			<first>D. K.</first>
		</authors>
		<authors>
			<last>Rycroft</last>
		</authors>
		<editor>S. Sadie</editor>
		<editors>
			<first>D. K.</first>
		</editors>
		<editors>
			<last>Rycroft</last>
		</editors>
		<pages>719-723</pages>
		<title>Musical bow</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2cb345f27d3c6f4d50a0bf7e9d44e9963/sapo</id>
		<tags>myown</tags>
		<tags>conference</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>3</count>
		<booktitle>Proceedings of 2019 International Workshop on Multilayer Music Representation and Processing</booktitle>
		<publisher>IEEE Conference Publishing Services</publisher>
		<year>2019</year>
		<url>https://air.unimi.it/handle/2434/633063</url>
		<author>Federico Simonetta</author>
		<author>Stavros Ntalampiras</author>
		<author>Federico Avanzini</author>
		<authors>
			<first>Federico</first>
		</authors>
		<authors>
			<last>Simonetta</last>
		</authors>
		<authors>
			<first>Stavros</first>
		</authors>
		<authors>
			<last>Ntalampiras</last>
		</authors>
		<authors>
			<first>Federico</first>
		</authors>
		<authors>
			<last>Avanzini</last>
		</authors>
		<pages>10--18</pages>
		<abstract>Towards improving the performance in various music information processing tasks, recent studies exploit different modalities able to capture diverse aspects of music. Such modalities include audio recordings, symbolic music scores, mid-level representations, motion and gestural data, video recordings, editorial or cultural tags, lyrics and album cover arts. This paper critically reviews the various approaches adopted in Music Information Processing and Retrieval, and highlights how multimodal algorithms can help Music Computing applications. First, we categorize the related literature based on the application they address. Subsequently, we analyze existing information fusion approaches, and we conclude with the set of challenges that Music Information Retrieval and Sound and Music Computing research communities should focus in the next years.</abstract>
		<doi>10.1109/MMRP.2019.00012</doi>
		<title>Multimodal Music Information Processing and Retrieval: Survey and Future Challenges</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a77809cc8dc3460804dcf89b4cf96e11/jaeschke</id>
		<tags>identification</tags>
		<tags>cover</tags>
		<tags>music</tags>
		<tags>version</tags>
		<tags>uncovr</tags>
		<tags>mir</tags>
		<description>Audio Cover Song Identification and Similarity: Background, Approaches, Evaluation, and Beyond | SpringerLink</description>
		<date>2021-06-02 13:13:57</date>
		<count>2</count>
		<booktitle>Advances in Music Information Retrieval</booktitle>
		<publisher>Springer</publisher>
		<address>Berlin, Heidelberg</address>
		<year>2010</year>
		<url>https://doi.org/10.1007/978-3-642-11674-2_14</url>
		<author>Joan Serrà</author>
		<author>Emilia Gómez</author>
		<author>Perfecto Herrera</author>
		<authors>
			<first>Joan</first>
		</authors>
		<authors>
			<last>Serrà</last>
		</authors>
		<authors>
			<first>Emilia</first>
		</authors>
		<authors>
			<last>Gómez</last>
		</authors>
		<authors>
			<first>Perfecto</first>
		</authors>
		<authors>
			<last>Herrera</last>
		</authors>
		<editor>Zbigniew W. Raś</editor>
		<editor>Alicja A. Wieczorkowska</editor>
		<editors>
			<first>Perfecto</first>
		</editors>
		<editors>
			<last>Herrera</last>
		</editors>
		<editors>
			<first>Perfecto</first>
		</editors>
		<editors>
			<last>Herrera</last>
		</editors>
		<pages>307--332</pages>
		<abstract>A cover version is an alternative rendition of a previously recorded song. Given that a cover may differ from the original song in timbre, tempo, structure, key, arrangement, or language of the vocals, automatically identifying cover songs in a given music collection is a rather difficult task. The music information retrieval (MIR) community has paid much attention to this task in recent years and many approaches have been proposed. This chapter comprehensively summarizes the work done in cover song identification while encompassing the background related to this area of research. The most promising strategies are reviewed and qualitatively compared under a common framework, and their evaluation methodologies are critically assessed. A discussion on the remaining open issues and future lines of research closes the chapter.</abstract>
		<isbn>978-3-642-11674-2</isbn>
		<doi>10.1007/978-3-642-11674-2_14</doi>
		<title>Audio Cover Song Identification and Similarity: Background, Approaches, Evaluation, and Beyond</title>
		<pubtype>inbook</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24419b3abaf3e439d268bae2e13626ba9/sapo</id>
		<tags>instruments</tags>
		<description>Hands-Free Accessible Digital Musical Instruments: Conceptual Framework, Challenges, and Perspectives | IEEE Journals & Magazine | IEEE Xplore https://ieeexplore.ieee.org/abstract/document/9179732</description>
		<date>2021-11-25 16:13:20</date>
		<count>2</count>
		<journal>IEEE Access</journal>
		<year>2020</year>
		<url>https://ieeexplore.ieee.org/abstract/document/9179732</url>
		<author>Nicola Davanzo</author>
		<author>Federico Avanzini</author>
		<authors>
			<first>Nicola</first>
		</authors>
		<authors>
			<last>Davanzo</last>
		</authors>
		<authors>
			<first>Federico</first>
		</authors>
		<authors>
			<last>Avanzini</last>
		</authors>
		<volume>8</volume>
		<pages>163975-163995</pages>
		<abstract>Exponential increases of available computational resources, miniaturization, and sensors, are enabling the development of digital musical instruments that use non-conventional interaction paradigms and interfaces. This scenario opens up new opportunities and challenges in the creation of accessible instruments to include persons with disabilities into music practice. This work focuses in particular on instruments dedicated to people who can not use limbs, for whom the only means for musical expression are the voice and a small number of traditional instruments. First, a modular and adaptable conceptual framework is discussed for the design of accessible digital musical instruments targeted at performers with motor impairments. Physical interaction channels available from the neck upwards (head, mouth, eyes, brain) are analyzed in terms of potential and limitations for musical interaction. Second, a systematic survey of previously developed instruments is presented: each is analyzed in terms of design choices, physical interaction channels and related sensors, mapping strategies, performer interface and feedback. As a result of this survey, several open research directions are discussed, including the use of unconventional interaction channels, musical control mappings, multisensory feedback, design, evaluation, and adaptation.</abstract>
		<issn>2169-3536</issn>
		<doi>10.1109/ACCESS.2020.3019978</doi>
		<title>Hands-Free Accessible Digital Musical Instruments: Conceptual Framework, Challenges, and Perspectives</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2cd3b73784f4e386a2a8b9417cb4e70c2/sapo</id>
		<tags>phdthesis</tags>
		<tags>automatic_transcription</tags>
		<description>Polyphonic Music Transcription with Semantic Segmentation - IEEE Conference Publication</description>
		<date>2020-08-25 10:29:06</date>
		<count>3</count>
		<journal>ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</journal>
		<booktitle>ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</booktitle>
		<year>2019</year>
		<url>https://ieeexplore.ieee.org/abstract/document/8682605/references#references</url>
		<author>Y. Wu</author>
		<author>B. Chen</author>
		<author>L. Su</author>
		<authors>
			<first>Y.</first>
		</authors>
		<authors>
			<last>Wu</last>
		</authors>
		<authors>
			<first>B.</first>
		</authors>
		<authors>
			<last>Chen</last>
		</authors>
		<authors>
			<first>L.</first>
		</authors>
		<authors>
			<last>Su</last>
		</authors>
		<pages>166-170</pages>
		<abstract>The multi-instrument transcription task refers to joint recognition of instrument and pitch of every event in polyphonic music signals generated by one or more classes of music instruments. In this paper, we leverage multi-object semantic segmentation techniques to solve this problem. We design a time-frequency representation, which has multiple channels to jointly represent the harmonic structure and pitch saliency of a pitch activation. The transcription task therefore becomes a pixel-wise multi-task classification problem including pitch activity detection and instrument recognition. Experiments on both single- and multi-instrument data verify the competitiveness of the proposed method.</abstract>
		<issn>2379-190X</issn>
		<doi>10.1109/ICASSP.2019.8682605</doi>
		<title>Polyphonic Music Transcription with Semantic Segmentation</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d13aec56a201adda1b605012636bcfb4/mosamusique</id>
		<tags>k-12</tags>
		<tags>children</tags>
		<tags>pedagogy</tags>
		<description></description>
		<date>2010-08-27 07:00:37</date>
		<count>1</count>
		<journal>Music Education Research</journal>
		<publisher>Taylor & Francis</publisher>
		<year>2000</year>
		<url></url>
		<author>Pamela Burnard</author>
		<authors>
			<first>Pamela</first>
		</authors>
		<authors>
			<last>Burnard</last>
		</authors>
		<volume>2</volume>
		<number>1</number>
		<pages>7--</pages>
		<abstract>This paper is taken from doctoral research which sought to discover how children engage in and reflect on their experiences of improvising and composing. The study was carried out at a comprehensive Middle         School in West London where 18 self-selected 12-year-old children participated in weekly music making sessions. Data collected over a six-month period included observations, interviews and the examination         of musical artefacts. This paper reports on interview methodology based on constructivist elicitation tools to understand how children ascribe meaning to improvisation and composition. It was found that         children represented these phenomena in three ways: (i) distinct forms distinguished by bodily intention; (ii) interrelated forms co-existing functionally in context; and (iii) inseparable processes. The         pedagogical significance of what is under description here will be discussed.</abstract>
		<issn>14613808</issn>
		<title>How Children Ascribe Meaning to Improvisation and Composition: rethinking pedagogy in music education</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24b4478da031c48f3ee9b21dcb9444f96/svrist</id>
		<tags>imported</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>1</count>
		<year>2009</year>
		<url>http://diku.dk/hjemmesider/studerende/zerrez/sections/projects/music.html</url>
		<author>Johan Sejr Brinch Nielsen</author>
		<authors>
			<first>Johan Sejr Brinch</first>
		</authors>
		<authors>
			<last>Nielsen</last>
		</authors>
		<title>Statistical Analysis of Music Corpora</title>
		<pubtype>techreport</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20a9593f3d71b4f3266a95c39e4350507/mediadigits</id>
		<tags>folksonomy</tags>
		<tags>genre</tags>
		<tags>taxonomy</tags>
		<tags>classification</tags>
		<description></description>
		<date>2009-02-04 14:46:57</date>
		<count>3</count>
		<journal>Proceedings of the 9th International Conference on Music Information Retrieval</journal>
		<address>Philadelphia, USA</address>
		<year>2008</year>
		<url></url>
		<author>Mohamed Sordo</author>
		<author>Oscar Celma</author>
		<author>Martin Blech</author>
		<author>Enric Guaus</author>
		<authors>
			<first>Mohamed</first>
		</authors>
		<authors>
			<last>Sordo</last>
		</authors>
		<authors>
			<first>Oscar</first>
		</authors>
		<authors>
			<last>Celma</last>
		</authors>
		<authors>
			<first>Martin</first>
		</authors>
		<authors>
			<last>Blech</last>
		</authors>
		<authors>
			<first>Enric</first>
		</authors>
		<authors>
			<last>Guaus</last>
		</authors>
		<abstract>This paper presents some findings around musical genres. The main goal is to analyse whether there is any agreement between a group of experts and a community, when defining a set of genres and their relationships. For this purpose, three different experiments are conducted using two datasets: the MP3.com expert taxonomy, and last.fm tags at artist level. The experimental results show a clear agreement for some components of the taxonomy (Blues, Hip-Hop), whilst in other cases (e.g. Rock) there is no correlations. Interestingly enough, the same results are found in the MIREX2007 results for audio genre classification task. Therefore, a multi–faceted approach for musical genre using expert based classifications, dynamic associations derived from the wisdom of crowds, and content–based analysis can improve genre classification, as well as other relevant MIR tasks such as music similarity or music recommendation.</abstract>
		<title>The Quest for Musical Genres: Do the Experts and the Wisdom of Crowds Agree?</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f9737aae7c29d3f70c37e396eb632071/ocelma</id>
		<tags>PhD</tags>
		<description>PhD</description>
		<date>2009-01-08 17:06:55</date>
		<count>6</count>
		<booktitle>Proceedings of the 9th International Conference on Music Information Retrieval</booktitle>
		<address>Philadelphia, USA</address>
		<year>2008</year>
		<url></url>
		<author>D. Turnbull</author>
		<author>L. Barrington</author>
		<author>G. Lanckriet</author>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Turnbull</last>
		</authors>
		<authors>
			<first>L.</first>
		</authors>
		<authors>
			<last>Barrington</last>
		</authors>
		<authors>
			<first>G.</first>
		</authors>
		<authors>
			<last>Lanckriet</last>
		</authors>
		<pages>225--230</pages>
		<abstract>We compare five approaches to collecting tags for music:
conducting a survey, harvesting social tags, deploying annotation
games, mining web documents, and autotagging audio
content. The comparison includes a discussion of both scalability
(financial cost, human involvement, and computational
resources) and quality (the cold start problem & popularity
bias, strong vs. weak labeling, vocabulary structure & size,
and annotation accuracy). We then describe one state-ofthe-
art system for each approach. The performance of each
system is evaluated using a tag-based music information
retrieval task. Using this task, we are able to quantify the
effect of popularity bias on each approach by making use
of a subset of more popular (short-head) songs and a set of
less popular (long-tail) songs. Lastly, we propose a simple
hybrid context-content system that combines our individual
approaches and produces superior retrieval results.</abstract>
		<title>Five Approaches to Collecting Tags for Music</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2828c0f78353df367876dc89f9b6335b8/iblis</id>
		<tags>imported</tags>
		<description></description>
		<date>2007-03-19 17:08:38</date>
		<count>1</count>
		<journal>Revue de musicologie</journal>
		<series>3</series>
		<publisher>Societe Francaise de Musicologie</publisher>
		<year>1990</year>
		<url>http://links.jstor.org/sici?sici=0035-1601%281990%293%3A76%3A2%3C213%3ALDRDLM%3E2.0.CO%3B2-2</url>
		<author>Jean During</author>
		<authors>
			<first>Jean</first>
		</authors>
		<authors>
			<last>During</last>
		</authors>
		<volume>76</volume>
		<number>2</number>
		<pages>213--225</pages>
		<issn>0035-1601</issn>
		<group>Notes et Documents</group>
		<jstor_date>1990</jstor_date>
		<language>fre</language>
		<copyright>Copyright 1990 Societe Francaise de Musicologie</copyright>
		<title>L'organisation du rythme dans la musique de transe baloutche</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a0e06d4513b83fe281802571d0f0f723/wiljami74</id>
		<tags>information_theory</tags>
		<tags>musical_meaning</tags>
		<description>Meyer's Music, the Arts, and Ideas : Patterns and Predictions in Twentieth-Century Culture, information theory and meaning in music</description>
		<date>2007-08-15 09:16:23</date>
		<count>1</count>
		<publisher>The University Of Chicago Press</publisher>
		<address>Chicago and London</address>
		<year>1967</year>
		<url></url>
		<author>Leonard B. Meyer</author>
		<authors>
			<first>Leonard B.</first>
		</authors>
		<authors>
			<last>Meyer</last>
		</authors>
		<title>Music, the Arts, and Ideas: Patterns and Predictions in Twentieth-Century Culture</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20df002369314f3ef28e548b2b13baa00/moustaki</id>
		<tags>music</tags>
		<tags>surface</tags>
		<tags>reprenstation</tags>
		<tags>knowledge</tags>
		<description>My bibtex file</description>
		<date>2007-03-28 16:44:39</date>
		<count>2</count>
		<journal>Readings in Music and Artificial Intelligence ISBN 90-5755-094-6</journal>
		<year>2000</year>
		<url>citeseer.ist.psu.edu/677739.html</url>
		<author>Geraint Wiggins</author>
		<author>Alan Smaill</author>
		<authors>
			<first>Geraint</first>
		</authors>
		<authors>
			<last>Wiggins</last>
		</authors>
		<authors>
			<first>Alan</first>
		</authors>
		<authors>
			<last>Smaill</last>
		</authors>
		<pdf>/home/moustaki/work/phd/papers/ai/miranda_collection.pdf</pdf>
		<title>Musical Knowledge: what can Artificial Intelligence bring to the
	musician?</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e7f3a48ff6e1923abfe473204fd0ad5d/ch</id>
		<tags>langzeitarchivierung</tags>
		<tags>musik</tags>
		<tags>drm</tags>
		<description>DRMs, fair use and users' experience of sharing music</description>
		<date>2007-07-06 09:20:16</date>
		<count>3</count>
		<booktitle>DRM '05: Proceedings of the 5th ACM workshop on Digital rights management</booktitle>
		<publisher>ACM Press</publisher>
		<address>New York, NY, USA</address>
		<year>2005</year>
		<url>http://portal.acm.org/citation.cfm?id=1102549&coll=Portal&dl=ACM&CFID=27628983&CFTOKEN=96340807</url>
		<author>Margaret Jackson</author>
		<author>Supriya Singh</author>
		<author>Jenine Beekhuyzen</author>
		<author>Jenny Waycott</author>
		<authors>
			<first>Margaret</first>
		</authors>
		<authors>
			<last>Jackson</last>
		</authors>
		<authors>
			<first>Supriya</first>
		</authors>
		<authors>
			<last>Singh</last>
		</authors>
		<authors>
			<first>Jenine</first>
		</authors>
		<authors>
			<last>Beekhuyzen</last>
		</authors>
		<authors>
			<first>Jenny</first>
		</authors>
		<authors>
			<last>Waycott</last>
		</authors>
		<pages>8--16</pages>
		<abstract>There is a mismatch between the law relating to fair use, personal use and copying; the central thrust of Digital Rights Management (DRM) and users' behavior relating to the listening and sharing of music. This paper reports on the different copyright regimes in the United States and Australia. It describes some of the current DRM systems. Against this background, the paper draws on a qualitative study to explore Australian users' experience of listening to and sharing music. A design for a good DRM has to take into account the schism between the copyright regimes and users' sharing behavior.</abstract>
		<doi>http://doi.acm.org/10.1145/1102546.1102549</doi>
		<isbn>1-59593-230-5</isbn>
		<title>DRMs, fair use and users' experience of sharing music</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ae79840385e3b61ad0723c163d9d993c/kurtjx</id>
		<tags>musical_structure</tags>
		<description></description>
		<date>2008-05-13 18:47:37</date>
		<count>2</count>
		<booktitle>Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference on</booktitle>
		<year>2006</year>
		<url>http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1661200</url>
		<author>M. Levy</author>
		<author>M. Sandier</author>
		<author>M. Casey</author>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Levy</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Sandier</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Casey</last>
		</authors>
		<volume>5</volume>
		<pages>V-V</pages>
		<abstract>A method for segmenting musical audio with a hierarchical timbre model is introduced. New evidence is presented to show that music segmentation can be recast as clustering of timbre features, and a new clustering algorithm is described. A prototype thumbnail-generating application is described and evaluated. Experimental results are given, including comparison of machine and human segmentations</abstract>
		<issn>1520-6149</issn>
		<isbn>1-4244-0469-X</isbn>
		<doi>10.1109/ICASSP.2006.1661200</doi>
		<title>Extraction of High-Level Musical Structure From Audio Data and Its Application to Thumbnail Generation</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e3bec615d5e50884343cbc5b59f28007/baltazhar</id>
		<tags>music</tags>
		<tags>sociology</tags>
		<tags>popular</tags>
		<tags>musicology</tags>
		<description></description>
		<date>2008-10-13 02:04:24</date>
		<count>1</count>
		<publisher>Cambridge University Press</publisher>
		<address>The Edinburgh Building, Cambridge CB2 2RU, United Kingdom</address>
		<year>2003</year>
		<url>http://www.cambridge.org/catalogue/catalogue.asp?isbn=9780521771207</url>
		<editor>Allan F. Moore.</editor>
		<editors>
			<first>M.</first>
		</editors>
		<editors>
			<last>Casey</last>
		</editors>
		<abstract>How do we know music? We perform it, we compose it, we sing it in the shower, we cook, sleep and dance to it. Eventually we think and write about it. This book represents the culmination of such shared processes. Each of these essays, written by leading writers on popular music, is analytical in some sense, but none of them treats analysis as an end in itself. The books presents a wide range of genres (rock, dance, TV soundtracks, country, pop, soul, easy listening, Turkish Arabesk) and deals with issues as broad as methodology, modernism, postmodernism, Marxism and communication. It aims to encourage listeners to think more seriously about the ‘social’ consequences of the music they spend time with and is the first collection of such essays to incorporate contextualisation in this way</abstract>
		<title>Analyzing popular music</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bc3522a0e9d3c3e5e3a5b94ec0e741b9/yevb0</id>
		<tags>Development,Male,Music,Phonetics,Preschool,Reading,Task</tags>
		<tags>Perception,Awareness,Child,Child</tags>
		<tags>Auditory</tags>
		<tags>Performance</tags>
		<tags>and</tags>
		<tags>Analysis,acquisition,language,music,musicality,perception,speech</tags>
		<tags>Development,Female,Humans,Language</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Journal of Experimental Child Psychology</journal>
		<year>2002</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/12408958</url>
		<author>Sima H Anvari</author>
		<author>Laurel J. Trainor</author>
		<author>Jennifer Woodside</author>
		<author>Betty Ann Levy</author>
		<authors>
			<first>Sima H</first>
		</authors>
		<authors>
			<last>Anvari</last>
		</authors>
		<authors>
			<first>Laurel J.</first>
		</authors>
		<authors>
			<last>Trainor</last>
		</authors>
		<authors>
			<first>Jennifer</first>
		</authors>
		<authors>
			<last>Woodside</last>
		</authors>
		<authors>
			<first>Betty Ann</first>
		</authors>
		<authors>
			<last>Levy</last>
		</authors>
		<volume>83</volume>
		<number>2</number>
		<pages>111--30</pages>
		<abstract>We examined the relations among phonological awareness, music perception
	skills, and early reading skills in a population of 100 4- and 5-year-old
	children. Music skills were found to correlate significantly with
	both phonological awareness and reading development. Regression analyses
	indicated that music perception skills contributed unique variance
	in predicting reading ability, even when variance due to phonological
	awareness and other cognitive abilities (math, digit span, and vocabulary)
	had been accounted for. Thus, music perception appears to tap auditory
	mechanisms related to reading that only partially overlap with those
	related to phonological awareness, suggesting that both linguistic
	and nonlinguistic general auditory mechanisms are involved in reading.</abstract>
		<issn>0022-0965</issn>
		<title>Relations among musical skills, phonological processing, and early
	reading ability in preschool children</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23c5700b036ea8822f80d9449008b586e/yevb0</id>
		<tags>Brain,Brain</tags>
		<tags>Mapping,Brain:</tags>
		<tags>physiology,Humans,Models,Music,Neurological,music,musicality,neuro</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Annals of the New York Academy of Sciences</journal>
		<year>2001</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/11458834</url>
		<author>Eckart O Altenmüller</author>
		<authors>
			<first>Eckart O</first>
		</authors>
		<authors>
			<last>Altenmüller</last>
		</authors>
		<volume>930</volume>
		<number>0</number>
		<pages>273--80</pages>
		<abstract>When reviewing the literature on brain substrates of music processing,
	a puzzling variety of findings can be stated. The traditional view
	of a left-right dichotomy of brain organization--assuming that in
	contrast to language, music is primarily processed in the right hemisphere--was
	challenged 20 years ago, when the influence of music education on
	brain lateralization was demonstrated. Modern concepts emphasize
	the modular organization of music cognition. According to this viewpoint,
	different aspects of music are processed in different, although partly
	overlapping neuronal networks of both hemispheres. However, even
	when isolating a single "module," such as, for example, the perception
	of contours, the interindividual variance of brain substrates is
	enormous. To clarify the factors contributing to this variability,
	we conducted a longitudinal experiment comparing the effects of procedural
	versus explicit music teaching on brain networks. We demonstrated
	that cortical activation during music processing reflects the auditory
	"learning biography," the personal experiences accumulated over time.
	Listening to music, learning to play an instrument, formal instruction,
	and professional training result in multiple, in many instances multisensory,
	representations of music, which seem to be partly interchangeable
	and rapidly adaptive. In summary, as soon as we consider "real music"
	apart from laboratory experiments, we have to expect individually
	formed and quickly adaptive brain substrates, including widely distributed
	neuronal networks in both hemispheres.</abstract>
		<issn>0077-8923</issn>
		<title>How many music centers are in the brain?</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26c50cb4a4c32110665e25e39b1e98a55/yevb0</id>
		<tags>methods,Adult,Auditory,Auditory</tags>
		<tags>Potentials,Female,Functional</tags>
		<tags>Stimulation,Acoustic</tags>
		<tags>physiology,Auditory</tags>
		<tags>Laterality:</tags>
		<tags>Acoustic</tags>
		<tags>effects,Cognition,Cognition:</tags>
		<tags>Perception,Pitch</tags>
		<tags>physiology,Dose-Response</tags>
		<tags>Cortex:</tags>
		<tags>physiology,Auditory:</tags>
		<tags>radiation</tags>
		<tags>physiology,Radiation,Time</tags>
		<tags>Stimulation:</tags>
		<tags>physiology,Humans,Magnetoencephalography,Magnetoencephalography:</tags>
		<tags>Cortex,Auditory</tags>
		<tags>Discrimination:</tags>
		<tags>effects,Auditory:</tags>
		<tags>Discrimination,Pitch</tags>
		<tags>physiology,Pitch</tags>
		<tags>Perception:</tags>
		<tags>methods,Male,Music,Pitch</tags>
		<tags>Relationship,Evoked</tags>
		<tags>Laterality,Functional</tags>
		<tags>Factors,contour,interval,melody,music,musicality,perception,scale</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Journal of Cognitive Neuroscience</journal>
		<year>2004</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/15298788</url>
		<author>Takako Fujioka</author>
		<author>Laurel J. Trainor</author>
		<author>Bernhard Ross</author>
		<author>Ryusuke Kakigi</author>
		<author>Christo Pantev</author>
		<authors>
			<first>Takako</first>
		</authors>
		<authors>
			<last>Fujioka</last>
		</authors>
		<authors>
			<first>Laurel J.</first>
		</authors>
		<authors>
			<last>Trainor</last>
		</authors>
		<authors>
			<first>Bernhard</first>
		</authors>
		<authors>
			<last>Ross</last>
		</authors>
		<authors>
			<first>Ryusuke</first>
		</authors>
		<authors>
			<last>Kakigi</last>
		</authors>
		<authors>
			<first>Christo</first>
		</authors>
		<authors>
			<last>Pantev</last>
		</authors>
		<volume>16</volume>
		<number>6</number>
		<pages>1010--1021</pages>
		<abstract>In music, melodic information is thought to be encoded in two forms,
	a contour code (up/down pattern of pitch changes) and an interval
	code (pitch distances between successive notes). A recent study recording
	the mismatch negativity (MMN) evoked by pitch contour and interval
	deviations in simple melodies demonstrated that people with no formal
	music education process both contour and interval information in
	the auditory cortex automatically. However, it is still unclear whether
	musical experience enhances both strategies of melodic encoding.
	We designed stimuli to examine contour and interval information separately.
	In the contour condition there were eight different standard melodies
	(presented on 80\% of trials), each consisting of five notes all
	ascending in pitch, and the corresponding deviant melodies (20\%)
	were altered to descending on their final note. The interval condition
	used one five-note standard melody transposed to eight keys from
	trial to trial, and on deviant trials the last note was raised by
	one whole tone without changing the pitch contour. There was also
	a control condition, in which a standard tone (990.7 Hz) and a deviant
	tone (1111.0 Hz) were presented. The magnetic counterpart of the
	MMN (MMNm) from musicians and nonmusicians was obtained as the difference
	between the dipole moment in response to the standard and deviant
	trials recorded by magnetoencephalography. Significantly larger MMNm
	was present in musicians in both contour and interval conditions
	than in nonmusicians, whereas MMNm in the control condition was similar
	for both groups. The interval MMNm was larger than the contour MMNm
	in musicians. No hemispheric difference was found in either group.
	The results suggest that musical training enhances the ability to
	automatically register abstract changes in the relative pitch structure
	of melodies.</abstract>
		<issn>0898-929X</issn>
		<doi>10.1162/0898929041502706</doi>
		<title>Musical training enhances automatic encoding of melodic contour and
	interval structure</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c61bfea5e518df03ec579547ca79f2ce/yevb0</id>
		<tags>music,musicality,perception</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>2</count>
		<journal>Annals of the New York Academy of Sciences</journal>
		<year>2003</year>
		<url></url>
		<author>Emmanuel Bigand</author>
		<authors>
			<first>Emmanuel</first>
		</authors>
		<authors>
			<last>Bigand</last>
		</authors>
		<volume>999</volume>
		<pages>304--312</pages>
		<doi>10.1196/annals.1284.041</doi>
		<title>More about the musical expertise of musically untrained listeners</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b1646e24fe2a54d425b459dde91eb6e2/yevb0</id>
		<tags>music,acquisition,music,perception,scale</tags>
		<tags>M1,Western</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Journal of Experimental Psychology: Human Perception and Performance</journal>
		<year>1992</year>
		<url>http://psycserv.mcmaster.ca/ljt/trainor\_trehub\_1992.pdf</url>
		<author>Laurel J. Trainor</author>
		<author>Sandra E. Trehub</author>
		<authors>
			<first>Laurel J.</first>
		</authors>
		<authors>
			<last>Trainor</last>
		</authors>
		<authors>
			<first>Sandra E.</first>
		</authors>
		<authors>
			<last>Trehub</last>
		</authors>
		<volume>18</volume>
		<number>2</number>
		<pages>394--402</pages>
		<title>A comparison of infants' and adults' sensitivity to Western musical
	structure</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/216bf6d3e494c84cdb9d066122aafd5a7/yevb0</id>
		<tags>Development,Child</tags>
		<tags>Perception,Time</tags>
		<tags>Auditory</tags>
		<tags>Perception,Auditory</tags>
		<tags>Perception:</tags>
		<tags>Perception</tags>
		<tags>Psychology,Humans,Infant,Music,Music:</tags>
		<tags>psychology,Pitch</tags>
		<tags>physiology,Child</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Cognition</journal>
		<year>2006</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/16380107</url>
		<author>Sandra E Trehub</author>
		<author>Erin E Hannon</author>
		<authors>
			<first>Sandra E</first>
		</authors>
		<authors>
			<last>Trehub</last>
		</authors>
		<authors>
			<first>Erin E</first>
		</authors>
		<authors>
			<last>Hannon</last>
		</authors>
		<volume>100</volume>
		<number>1</number>
		<pages>73--99</pages>
		<abstract>We review the literature on infants' perception of pitch and temporal
	patterns, relating it to comparable research with human adult and
	non-human listeners. Although there are parallels in relative pitch
	processing across age and species, there are notable differences.
	Infants accomplish such tasks with ease, but non-human listeners
	require extensive training to achieve very modest levels of performance.
	In general, human listeners process auditory sequences in a holistic
	manner, and non-human listeners focus on absolute aspects of individual
	tones. Temporal grouping processes and categorization on the basis
	of rhythm are evident in non-human listeners and in human infants
	and adults. Although synchronization to sound patterns is thought
	to be uniquely human, tapping to music, synchronous firefly flashing,
	and other cyclic behaviors can be described by similar mathematical
	principles. We conclude that infants' music perception skills are
	a product of general perceptual mechanisms that are neither music-
	nor species-specific. Along with general-purpose mechanisms for the
	perceptual foundations of music, we suggest unique motivational mechanisms
	that can account for the perpetuation of musical behavior in all
	human societies.</abstract>
		<issn>0010-0277</issn>
		<doi>10.1016/j.cognition.2005.11.006</doi>
		<title>Infant music perception: domain-general or domain-specific mechanisms?</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/252ca6241e2460079193a1759b35c036b/yevb0</id>
		<tags>Stimulation:</tags>
		<tags>physiology,Occupations,Pitch</tags>
		<tags>Stimulation,Acoustic</tags>
		<tags>Potentials,Female,Humans,Male,Music,Neuronal</tags>
		<tags>Acoustic</tags>
		<tags>Cortex,Auditory</tags>
		<tags>Plasticity:</tags>
		<tags>Discrimination:</tags>
		<tags>Discrimination,Pitch</tags>
		<tags>physiology,Electroencephalography,Evoked</tags>
		<tags>physiology,music,musicality,neuro,perception,plasticity</tags>
		<tags>Cortex:</tags>
		<tags>physiology,Auditory:</tags>
		<tags>physiology,Auditory,Auditory</tags>
		<tags>physiology,Sound</tags>
		<tags>Time,Reaction</tags>
		<tags>Plasticity,Neuronal</tags>
		<tags>Time:</tags>
		<tags>Localization,Sound</tags>
		<tags>methods,Adult,Attention,Attention:</tags>
		<tags>Localization:</tags>
		<tags>physiology,Reaction</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Journal of Neuroscience</journal>
		<year>2003</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/12843255</url>
		<author>Antoine Shahin</author>
		<author>Daniel J Bosnyak</author>
		<author>Laurel J. Trainor</author>
		<author>Larry E Roberts</author>
		<authors>
			<first>Antoine</first>
		</authors>
		<authors>
			<last>Shahin</last>
		</authors>
		<authors>
			<first>Daniel J</first>
		</authors>
		<authors>
			<last>Bosnyak</last>
		</authors>
		<authors>
			<first>Laurel J.</first>
		</authors>
		<authors>
			<last>Trainor</last>
		</authors>
		<authors>
			<first>Larry E</first>
		</authors>
		<authors>
			<last>Roberts</last>
		</authors>
		<volume>23</volume>
		<number>13</number>
		<pages>5545--52</pages>
		<abstract>P2 and N1c components of the auditory evoked potential (AEP) have
	been shown to be sensitive to remodeling of the auditory cortex by
	training at pitch discrimination in nonmusician subjects. Here, we
	investigated whether these neuroplastic components of the AEP are
	enhanced in musicians in accordance with their musical training histories.
	Highly skilled violinists and pianists and nonmusician controls listened
	under conditions of passive attention to violin tones, piano tones,
	and pure tones matched in fundamental frequency to the musical tones.
	Compared with nonmusician controls, both musician groups evidenced
	larger N1c (latency, 138 msec) and P2 (latency, 185 msec) responses
	to the three types of tonal stimuli. As in training studies with
	nonmusicians, N1c enhancement was expressed preferentially in the
	right hemisphere, where auditory neurons may be specialized for processing
	of spectral pitch. Equivalent current dipoles fitted to the N1c and
	P2 field patterns localized to spatially differentiable regions of
	the secondary auditory cortex, in agreement with previous findings.
	These results suggest that the tuning properties of neurons are modified
	in distributed regions of the auditory cortex in accordance with
	the acoustic training history (musical- or laboratory-based) of the
	subject. Enhanced P2 and N1c responses in musicians need not be considered
	genetic or prenatal markers for musical skill.</abstract>
		<issn>1529-2401</issn>
		<title>Enhancement of neuroplastic P2 and N1c auditory evoked potentials
	in musicians</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e9a6075344c0ed4e2271539d05a14611/yevb0</id>
		<tags>Recall,Middle</tags>
		<tags>Aged,Music,Pitch</tags>
		<tags>Allocation,acquisition,interval,music,musicality,perception,pitch</tags>
		<tags>(Psychology),Psychoacoustics,Random</tags>
		<tags>Adolescent,Adult,Attention,Female,Humans,Individuality,Male,Mental</tags>
		<tags>Discrimination,Practice</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Cognition</journal>
		<year>1994</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/7924198</url>
		<author>J David Smith</author>
		<author>D G Nelson</author>
		<author>L A Grohskopf</author>
		<author>T Appleton</author>
		<authors>
			<first>J David</first>
		</authors>
		<authors>
			<last>Smith</last>
		</authors>
		<authors>
			<first>D G</first>
		</authors>
		<authors>
			<last>Nelson</last>
		</authors>
		<authors>
			<first>L A</first>
		</authors>
		<authors>
			<last>Grohskopf</last>
		</authors>
		<authors>
			<first>T</first>
		</authors>
		<authors>
			<last>Appleton</last>
		</authors>
		<volume>52</volume>
		<number>1</number>
		<pages>23--54</pages>
		<abstract>In the laboratory, musical novices often seem insensitive even to
	basic structural elements of music (octaves, intervals, etc.), undermining
	long-held theories of music perception, and threatening to leave
	current theories applicable only to experts. Consequently it is important
	to demonstrate novices' basic listening competence where possible.
	Two experiments examined the perception of musical intervals (minor
	thirds, major thirds and perfect fourths) by musical novices. Subjects
	received either standard instructions or familiar folk-tune labels
	to aid performance. The folk-tune labels greatly improved identification
	performance, producing expert-caliber performance by some musically
	inexperienced subjects. The effectiveness of the folk-tune manipulation
	was much more limited in a difficult discrimination task. The results
	suggest that novices do have some basic competence when assayed appropriately,
	and that familiar musical tokens may be a critical element in such
	assays. Larger implications of the role of familiarity in novices'
	competence are discussed, including those that relate to music cognition
	and aesthetics.</abstract>
		<issn>0010-0277</issn>
		<title>What child is this? What interval was that? Familiar tunes and music
	perception in novice listeners</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bfae1c9869bfa8e0357bbb3748e74259/yevb0</id>
		<tags>imported</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>The psychology of music</booktitle>
		<publisher>Academic Press</publisher>
		<address>New York</address>
		<year>1982</year>
		<url></url>
		<editor>Diana Deutsch</editor>
		<editors>
			<first>T</first>
		</editors>
		<editors>
			<last>Appleton</last>
		</editors>
		<title>The psychology of music</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29d789e339820158a511e38f495b0ecfa/yevb0</id>
		<tags>Adult,Auditory</tags>
		<tags>Pathways,Auditory</tags>
		<tags>physiology,Cognition,Cognition:</tags>
		<tags>Imaging,Music,language,music,neuro,perception</tags>
		<tags>Resonance</tags>
		<tags>Pathways:</tags>
		<tags>Characteristics,Humans,Language,M1,Magnetic</tags>
		<tags>physiology,Cultural</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Annals of the New York Academy of Sciences</journal>
		<year>2009</year>
		<url>http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2906120\&tool=pmcentrez\&rendertype=abstract</url>
		<author>Patrick C.M. Wong</author>
		<author>Tyler K Perrachione</author>
		<author>Elizabeth Hellmuth Margulis</author>
		<authors>
			<first>Patrick C.M.</first>
		</authors>
		<authors>
			<last>Wong</last>
		</authors>
		<authors>
			<first>Tyler K</first>
		</authors>
		<authors>
			<last>Perrachione</last>
		</authors>
		<authors>
			<first>Elizabeth Hellmuth</first>
		</authors>
		<authors>
			<last>Margulis</last>
		</authors>
		<volume>1169</volume>
		<pages>157--63</pages>
		<abstract>Cultural experiences come in many different forms, such as immersion
	in a particular linguistic community, exposure to faces of people
	with different racial backgrounds, or repeated encounters with music
	of a particular tradition. In most circumstances, these cultural
	experiences are asymmetric, meaning one type of experience occurs
	more frequently than other types (e.g., a person raised in India
	will likely encounter the Indian todi scale more so than a Westerner).
	In this paper, we will discuss recent findings from our laboratories
	that reveal the impact of short- and long-term asymmetric musical
	experiences on how the nervous system responds to complex sounds.
	We will discuss experiments examining how musical experience may
	facilitate the learning of a tone language, how musicians develop
	neural circuitries that are sensitive to musical melodies played
	on their instrument of expertise, and how even everyday listeners
	who have little formal training are particularly sensitive to music
	of their own culture(s). An understanding of these cultural asymmetries
	is useful in formulating a more comprehensive model of auditory perceptual
	expertise that considers how experiences shape auditory skill levels.
	Such a model has the potential to aid in the development of rehabilitation
	programs for the efficacious treatment of neurologic impairments.</abstract>
		<issn>1749-6632</issn>
		<doi>10.1111/j.1749-6632.2009.04548.x</doi>
		<title>Effects of asymmetric cultural experiences on the auditory pathway:
	evidence from music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/210ee6cebd5013aca370c2c2980dbcb96/yevb0</id>
		<tags>Stimulation:</tags>
		<tags>Variance,Animals,Auditory</tags>
		<tags>Stimulation,Acoustic</tags>
		<tags>metabolism,Newborn,Rats,Receptors,Sprague-Dawley,Western,music,neuro,perception</tags>
		<tags>physiology,Auditory</tags>
		<tags>Acoustic</tags>
		<tags>Cortex,Auditory</tags>
		<tags>Colliculi,Inferior</tags>
		<tags>metabolism,Auditory</tags>
		<tags>Cortex:</tags>
		<tags>Learning,Discrimination</tags>
		<tags>Perception,Auditory</tags>
		<tags>Perception:</tags>
		<tags>physiology,Inferior</tags>
		<tags>physiology,Male,Music,N-Methyl-D-Aspartate,N-Methyl-D-Aspartate:</tags>
		<tags>Colliculi:</tags>
		<tags>of</tags>
		<tags>Learning:</tags>
		<tags>methods,Analysis</tags>
		<tags>physiology,Blotting,Discrimination</tags>
		<tags>metabolism,Inferior</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Behavioural Brain Research</journal>
		<year>2009</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/18706452</url>
		<author>Jinghong Xu</author>
		<author>Liping Yu</author>
		<author>Rui Cai</author>
		<author>Jiping Zhang</author>
		<author>Xinde Sun</author>
		<authors>
			<first>Jinghong</first>
		</authors>
		<authors>
			<last>Xu</last>
		</authors>
		<authors>
			<first>Liping</first>
		</authors>
		<authors>
			<last>Yu</last>
		</authors>
		<authors>
			<first>Rui</first>
		</authors>
		<authors>
			<last>Cai</last>
		</authors>
		<authors>
			<first>Jiping</first>
		</authors>
		<authors>
			<last>Zhang</last>
		</authors>
		<authors>
			<first>Xinde</first>
		</authors>
		<authors>
			<last>Sun</last>
		</authors>
		<volume>196</volume>
		<number>1</number>
		<pages>49--54</pages>
		<abstract>Previous studies have shown that the functional development of auditory
	system is substantially influenced by the structure of environmental
	acoustic inputs in early life. In our present study, we investigated
	the effects of early auditory enrichment with music on rat auditory
	discrimination learning. We found that early auditory enrichment
	with music from postnatal day (PND) 14 enhanced learning ability
	in auditory signal-detection task and in sound duration-discrimination
	task. In parallel, a significant increase was noted in NMDA receptor
	subunit NR2B protein expression in the auditory cortex. Furthermore,
	we found that auditory enrichment with music starting from PND 28
	or 56 did not influence NR2B expression in the auditory cortex. No
	difference was found in the NR2B expression in the inferior colliculus
	(IC) between music-exposed and normal rats, regardless of when the
	auditory enrichment with music was initiated. Our findings suggest
	that early auditory enrichment with music influences NMDA-mediated
	neural plasticity, which results in enhanced auditory discrimination
	learning.</abstract>
		<issn>1872-7549</issn>
		<doi>10.1016/j.bbr.2008.07.018</doi>
		<title>Early auditory enrichment with music enhances auditory discrimination
	learning and alters NR2B protein expression in rat auditory cortex</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a91d3971b00a8ba1acd804316628cec9/yevb0</id>
		<tags>Perception,Time</tags>
		<tags>physiology,Auditory</tags>
		<tags>(Psychology),Time</tags>
		<tags>anatomy</tags>
		<tags>physiology,Recognition</tags>
		<tags>physiology,Cerebellum,Cerebellum:</tags>
		<tags>Perception,Pitch</tags>
		<tags>Net,Nerve</tags>
		<tags>\&</tags>
		<tags>Ganglia:</tags>
		<tags>physiology,Pitch</tags>
		<tags>physiology,music,musicality,neuro</tags>
		<tags>Ganglia,Basal</tags>
		<tags>Perception,Auditory</tags>
		<tags>Perception:</tags>
		<tags>physiology,Music,Nerve</tags>
		<tags>physiology,Cognition,Cognition:</tags>
		<tags>histology,Brain:</tags>
		<tags>Net:</tags>
		<tags>Affect,Affect:</tags>
		<tags>physiology,Brain,Brain:</tags>
		<tags>physiology,Basal</tags>
		<tags>physiology,Humans,Memory,Memory:</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Annual review of psychology</journal>
		<year>2005</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/15709930</url>
		<author>Isabelle Peretz</author>
		<author>Robert J. Zatorre</author>
		<authors>
			<first>Isabelle</first>
		</authors>
		<authors>
			<last>Peretz</last>
		</authors>
		<authors>
			<first>Robert J.</first>
		</authors>
		<authors>
			<last>Zatorre</last>
		</authors>
		<volume>56</volume>
		<pages>89--114</pages>
		<abstract>Research on how the brain processes music is emerging as a rich and
	stimulating area of investigation of perception, memory, emotion,
	and performance. Results emanating from both lesion studies and neuroimaging
	techniques are reviewed and integrated for each of these musical
	functions. We focus our attention on the common core of musical abilities
	shared by musicians and nonmusicians alike. Hence, the effect of
	musical training on brain plasticity is examined in a separate section,
	after a review of the available data regarding music playing and
	reading skills that are typically cultivated by musicians. Finally,
	we address a currently debated issue regarding the putative existence
	of music-specific neural networks. Unfortunately, due to scarcity
	of research on the macrostructure of music organization and on cultural
	differences, the musical material under focus is at the level of
	the musical phrase, as typically used in Western popular music.</abstract>
		<issn>0066-4308</issn>
		<doi>10.1146/annurev.psych.56.091103.070225</doi>
		<title>Brain organization for music processing</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29c85aaa485e5af7a4ad7bb15594233de/yevb0</id>
		<tags>imported</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>Empirical musicology: Aims, methods, prospects</booktitle>
		<publisher>Oxford University Press</publisher>
		<address>New York</address>
		<year>2004</year>
		<url></url>
		<editor>Eric Clarke</editor>
		<editor>Nicholas Cook</editor>
		<editors>
			<first>Robert J.</first>
		</editors>
		<editors>
			<last>Zatorre</last>
		</editors>
		<editors>
			<first>Robert J.</first>
		</editors>
		<editors>
			<last>Zatorre</last>
		</editors>
		<title>Empirical musicology: Aims, methods, prospects</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2dcd544f809d682f41b837bd8b927afed/yevb0</id>
		<tags>M1,M2,acquisition,music,perception,scale</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Music Perception</journal>
		<publisher>JSTOR</publisher>
		<year>1991</year>
		<url>http://www.jstor.org/stable/40286162</url>
		<author>Michael P. Lynch</author>
		<author>Rebecca E. Eilers</author>
		<authors>
			<first>Michael P.</first>
		</authors>
		<authors>
			<last>Lynch</last>
		</authors>
		<authors>
			<first>Rebecca E.</first>
		</authors>
		<authors>
			<last>Eilers</last>
		</authors>
		<volume>9</volume>
		<number>1</number>
		<pages>121--131</pages>
		<title>Children's perception of native and nonnative musical scales</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/235b904c0702b0e1ee6ad8be33ad0054e/yevb0</id>
		<tags>Auditory</tags>
		<tags>States,absolute</tags>
		<tags>Acoustics,United</tags>
		<tags>pitch,language,music,musicality,perception,pitch,tone</tags>
		<tags>Perception,China,England,English,Language,Learning,Linguistics,Mandarin,Music,Occupations,Pitch</tags>
		<tags>Perception,Speech,Speech</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>The Journal of the Acoustical Society of America</journal>
		<year>2008</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/19045807</url>
		<author>Chao-Yang Lee</author>
		<author>Tsun-Hui Hung</author>
		<authors>
			<first>Chao-Yang</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Tsun-Hui</first>
		</authors>
		<authors>
			<last>Hung</last>
		</authors>
		<volume>124</volume>
		<number>5</number>
		<pages>3235--48</pages>
		<abstract>This study examined Mandarin tone identification by 36 English-speaking
	musicians and 36 nonmusicians and musical note identification by
	the musicians. In the Mandarin task, participants were given a brief
	tutorial on Mandarin tones and identified the tones of the syllable
	sa produced by 32 speakers. The stimuli included intact syllables
	and acoustically modified syllables with limited F0 information.
	Acoustic analyses showed considerable overlap in F0 range among the
	tones due to the presence of multiple speakers. Despite no prior
	experience with Mandarin, the musicians identified intact tones at
	68\% and silent-center tones at 54\% correct, both exceeding chance
	(25\%). The musicians also outperformed the nonmusicians, who identified
	intact tones at 44\% and silent-center tones at 36\% correct. These
	results indicate musical training facilitated lexical tone identification,
	although the facilitation varied as a function of tone and the type
	of acoustic input. In the music task, the musicians listened to synthesized
	musical notes of three timbres and identified the notes without a
	reference pitch. Average identification accuracy was at chance level
	even when multiple semitone errors were allowed. Since none of the
	musicians possessed absolute pitch, the role of absolute pitch in
	Mandarin tone identification remains inconclusive.</abstract>
		<issn>1520-8524</issn>
		<doi>10.1121/1.2990713</doi>
		<title>Identification of Mandarin tones by English-speaking musicians and
	nonmusicians</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d186d9d148133a74c3101c9ee51e24e6/yevb0</id>
		<tags>interval,music,perception,tonality</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Music Perception</journal>
		<year>2001</year>
		<url>http://caliber.ucpress.net/doi/abs/10.1525/mp.2001.19.2.223</url>
		<author>E. Glenn Schellenberg</author>
		<authors>
			<first>E. Glenn</first>
		</authors>
		<authors>
			<last>Schellenberg</last>
		</authors>
		<volume>19</volume>
		<number>2</number>
		<pages>223--248</pages>
		<issn>0730-7829</issn>
		<doi>10.1525/mp.2001.19.2.223</doi>
		<title>Asymmetries in the discrimination of musical intervals: Going out-of-tune
	is more noticeable than going in-tune</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/281a2d282828d0327053b517d7a1001d2/yevb0</id>
		<tags>acquisition,language,modularity,music,perception</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>The cognitive neuroscience of music</booktitle>
		<publisher>Oxford University Press</publisher>
		<address>New York</address>
		<year>2003</year>
		<url></url>
		<author>E. Glenn Schellenberg</author>
		<authors>
			<first>E. Glenn</first>
		</authors>
		<authors>
			<last>Schellenberg</last>
		</authors>
		<editor>Isabelle Peretz</editor>
		<editor>Robert J. Zatorre</editor>
		<editors>
			<first>E. Glenn</first>
		</editors>
		<editors>
			<last>Schellenberg</last>
		</editors>
		<editors>
			<first>E. Glenn</first>
		</editors>
		<editors>
			<last>Schellenberg</last>
		</editors>
		<pages>430--448</pages>
		<title>Does exposure to music have beneficial side effects?</title>
		<pubtype>inbook</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26d3a9090a80cebcf465a93f478bc71dc/aucelum</id>
		<tags>harmony</tags>
		<tags>music</tags>
		<tags>mem</tags>
		<tags>gttm</tags>
		<description></description>
		<date>2010-02-02 17:03:01</date>
		<count>1</count>
		<journal>Journal of New Music Research</journal>
		<year>2006</year>
		<url></url>
		<author>Masatoshi Hamanaka</author>
		<author>Keiji Hirata</author>
		<author>Satoshi Tojo</author>
		<authors>
			<first>Masatoshi</first>
		</authors>
		<authors>
			<last>Hamanaka</last>
		</authors>
		<authors>
			<first>Keiji</first>
		</authors>
		<authors>
			<last>Hirata</last>
		</authors>
		<authors>
			<first>Satoshi</first>
		</authors>
		<authors>
			<last>Tojo</last>
		</authors>
		<volume>35</volume>
		<number>4</number>
		<pages>249--277</pages>
		<title>Implementing Ä Generative Theory of Tonal Music"</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/251e3e09c63c3f75c5282a391e54e7af3/bfields</id>
		<tags>imported</tags>
		<description>JSTOR: Popular Music, Vol. 9, No. 2 (Apr., 1990), pp. 179-192</description>
		<date>2010-01-28 11:54:50</date>
		<count>1</count>
		<journal>Popular Music</journal>
		<publisher>Cambridge University Press</publisher>
		<year>1990</year>
		<url>http://www.jstor.org/stable/853500</url>
		<author>Jody Berland</author>
		<authors>
			<first>Jody</first>
		</authors>
		<authors>
			<last>Berland</last>
		</authors>
		<volume>9</volume>
		<number>2</number>
		<pages>179--192</pages>
		<issn>02611430</issn>
		<copyright>Copyright © 1990 Cambridge University Press</copyright>
		<title>Radio Space and Industrial Time: Music Formats, Local Narratives and Technological Mediation</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2339783e3c99d51139accbbc86bff4364/bfields</id>
		<tags>matlab</tags>
		<tags>music_similarity</tags>
		<description>initial imports</description>
		<date>2010-01-28 11:42:45</date>
		<count>4</count>
		<booktitle>Proc. of Int. Symposium on Music Information Retrieval</booktitle>
		<year>2004</year>
		<url></url>
		<author>E. Pampalk</author>
		<authors>
			<first>E.</first>
		</authors>
		<authors>
			<last>Pampalk</last>
		</authors>
		<title>A Matlab Toolbox To Compute Music Similarity From Audio</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c6f6e23d0ce2a0a9203224e45eb794f4/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<year>2005</year>
		<url></url>
		<author>Andrew author Kania</author>
		<authors>
			<first>Andrew</first>
		</authors>
		<authors>
			<last>author Kania</last>
		</authors>
		<abstract>Unedited I investigate the nature of, and relationships between, works, performances, and recordings in the Western musical traditions of classical, rock, and jazz music. I begin in chapter one by defending the study of musical ontology against a recent attack by Aaron Ridley. This leads into a discussion of the appropriate methodology for investigating the ontology of art, and the reasons for doing musical ontology, particularly in a comparative way. In chapter two I review and reject several theories of what a classical musical work is. I defend the view that such a work is an abstract object--a type of performance--against several objections, most notably that abstract objects cannot be created, while musical works are. In chapter three I argue that classical recordings, as they are typically made, are correctly conceived of as giving access to performances of the works they purport to be of, despite the fact that they are not records of any single performance event in the studio. Before tackling rock and jazz, in chapter four I investigate the concept of a work of art in general, arguing that there are two necessary conditions an art object must meet to be a work: (1) it must be of a kind that is a primary focus of critical attention in a given art form or tradition, and (2) it must be a persisting object. I argue further that (i) there is no need to subsume all art under the work concept, and that (ii) drawing a distinction between works and other art objects need not lead to valuing the former over the latter. In chapter five, I argue that the work of art in rock music is a track for playback, constructed in the studio. Tracks usually manifest songs, which can be performed live. A cover version is a track (successfully) intended to manifest the same song as some other track. In chapter six, I discuss various proposals for the ontology of jazz. I argue that in jazz there are no works, only performances.</abstract>
		<shorttitle>Pieces of music</shorttitle>
		<title>Pieces of music: The ontology of classical, rock, and jazz music</title>
		<pubtype>phdthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21750d454a8ff1707352b49229832dc43/syslogd</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-07 13:57:09</date>
		<count>4</count>
		<booktitle>Proceedings of the 2002 International Computer Music Conference</booktitle>
		<year>2002</year>
		<url>http://www.media.mit.edu/~bwhitman/whitman02inferring.pdf</url>
		<author>Brian Whitman</author>
		<author>Steve Lawrence</author>
		<authors>
			<first>Brian</first>
		</authors>
		<authors>
			<last>Whitman</last>
		</authors>
		<authors>
			<first>Steve</first>
		</authors>
		<authors>
			<last>Lawrence</last>
		</authors>
		<pages>591--598</pages>
		<abstract>We propose methods for unsupervised learning of text profiles for music from unstructured text obtained from the web. The profiles can be used for classification, recommendation, and understanding, and may be used in conjunction with existing methods such as audio analysis and collaborative filtering to improve performance. A formal method for analyzing the quality of the learned profiles is given, and results indicate that they perform well when used to find similar artists.</abstract>
		<title>Inferring descriptions and similarity for music from community metadata</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/246280b12ba0abdb6935da393787c01ee/joaakive</id>
		<tags>music</tags>
		<tags>history</tags>
		<tags>electronic</tags>
		<description></description>
		<date>2014-04-27 09:35:20</date>
		<count>1</count>
		<publisher>Routledge</publisher>
		<year>1985</year>
		<url></url>
		<author>Thom Holmes</author>
		<authors>
			<first>Thom</first>
		</authors>
		<authors>
			<last>Holmes</last>
		</authors>
		<abstract>Technology, music and culture</abstract>
		<title>Electronic and Experimental Music</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d3194d49e99d06a465a9fc65723e7c35/thesaiorg</id>
		<tags>transcription;</tags>
		<tags>audio;</tags>
		<tags>synthesis;</tags>
		<tags>monophonic;</tags>
		<tags>frequency;</tags>
		<tags>strategy;</tags>
		<tags>detection;</tags>
		<tags>pitch</tags>
		<tags>MIDI;</tags>
		<tags>FFT;</tags>
		<tags>fundamental</tags>
		<tags>evolution</tags>
		<tags>signal;</tags>
		<tags>contours</tags>
		<tags>F0;</tags>
		<tags>chords;</tags>
		<tags>music</tags>
		<tags>polyphonic</tags>
		<tags>notes;</tags>
		<tags>electronic</tags>
		<description></description>
		<date>2014-02-21 08:00:08</date>
		<count>2</count>
		<journal>International Journal of Advanced Computer Science and Applications(IJACSA)</journal>
		<year>2013</year>
		<url>http://ijacsa.thesai.org/</url>
		<author>Herve Kabamba Mbikayi</author>
		<authors>
			<first>Herve Kabamba</first>
		</authors>
		<authors>
			<last>Mbikayi</last>
		</authors>
		<volume>4</volume>
		<number>3</number>
		<abstract>We present in this paper a new approach for polyphonic music transcription using evolution strategies (ES). Automatic music transcription is a complex process that still remains an open challenge. Using an audio signal to be transcribed as target for our ES, information needed to generate a MIDI file can be extracted from this latter one. Many techniques presented in the literature at present exist and a few of them have applied evolutionary algorithms to address this problem in the context of considering it as a search space problem. However, ES have never been applied until now. The experiments showed that by using these machines learning tools, some shortcomings presented by other evolutionary algorithms based approaches for transcription can be solved. They include the computation cost and the time for convergence. As evolution strategies use self-adapting parameters, we show in this paper that by correctly tuning the value of its strategy parameter that controls the standard deviation, a fast convergence can be triggered toward the optima, which from the results performs the transcription of the music with good accuracy and in a short time. In the same context, the computation task is tackled using parallelization techniques thus reducing the computation time and the transcription time in the overall.</abstract>
		<title>Toward Evolution Strategies Application in Automatic Polyphonic Music Transcription using Electronic Synthesis</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/277b8676bc6edda8c7ec326fedf746ac6/ar0berts</id>
		<tags>Mental</tags>
		<tags>Humans;</tags>
		<tags>Adul;</tags>
		<tags>Intelligence;</tags>
		<tags>Cerebral</tags>
		<tags>Male;</tags>
		<tags>Aptitude;</tags>
		<tags>Retardation;</tags>
		<tags>t</tags>
		<tags>Palsy;</tags>
		<tags>Blindness;</tags>
		<tags>Creativeness;</tags>
		<tags>Music;</tags>
		<description></description>
		<date>2014-07-19 20:21:03</date>
		<count>1</count>
		<journal>Psychol Med</journal>
		<year>1989</year>
		<url></url>
		<author>B. Hermelin</author>
		<author>N. O'Connor</author>
		<author>S. Lee</author>
		<author>D. Treffert</author>
		<authors>
			<first>B.</first>
		</authors>
		<authors>
			<last>Hermelin</last>
		</authors>
		<authors>
			<first>N.</first>
		</authors>
		<authors>
			<last>O'Connor</last>
		</authors>
		<authors>
			<first>S.</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Treffert</last>
		</authors>
		<volume>19</volume>
		<number>2</number>
		<pages>447--457</pages>
		<abstract>We investigated whether somebody with a severe mental impairment could not only remember and reproduce music, but was also able to generate it. Musical improvisation requires the ability to recognize constraints and also demands inventiveness. Musical improvisations on a traditional, tonal and also on a whole tone scale composition were produced by a mentally handicapped and by a normal control musician. It was found that not only the control but also the handicapped subject could improvise appropriately within structural constraints, although with the tonal music the idiot-savant showed some stylistic latitude. It is concluded that cognitive processes such as musical input analysis, decision making, and output monitoring are independent of general intellectual status.</abstract>
		<title>Intelligence and musical improvisation.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d17522343b278e826db4f71052a66053/keinstein</id>
		<tags>Mathematik</tags>
		<tags>Geometrie</tags>
		<tags>Musiktheorie</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2012-09-20 17:57:47</date>
		<count>3</count>
		<booktitle>The Topos of Music: geometric logic of concepts, theory, and performance</booktitle>
		<publisher>Birkhäuser</publisher>
		<address>Basel ; Boston</address>
		<year>2002</year>
		<url>http://www.loc.gov/catdir/enhancements/fy0812/2002033209-d.html</url>
		<author>Guerino Mazzola</author>
		<author>Stefan Göller</author>
		<author>Stefan Müller</author>
		<authors>
			<first>Guerino</first>
		</authors>
		<authors>
			<last>Mazzola</last>
		</authors>
		<authors>
			<first>Stefan</first>
		</authors>
		<authors>
			<last>Göller</last>
		</authors>
		<authors>
			<first>Stefan</first>
		</authors>
		<authors>
			<last>Müller</last>
		</authors>
		<genre>Music theory</genre>
		<isbn>3-7643-5731-2; 0-8176-5731-2 (alk. paper)</isbn>
		<title>The Topos of Music: geometric logic of concepts, theory, and performance</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21f0f19bce05b718400572c28f4772477/keinstein</id>
		<description></description>
		<date>2012-02-16 17:14:29</date>
		<count>2</count>
		<journal>Computer Music Modeling and Retrieval. Sense of Sounds</journal>
		<year>2009</year>
		<url></url>
		<author>Tuukka Ilomäki</author>
		<authors>
			<first>Tuukka</first>
		</authors>
		<authors>
			<last>Ilomäki</last>
		</authors>
		<pages>98--109</pages>
		<abstract>Computer applications are an everyday tool for music analysts, composers, and music theory students. While these applications are a welcome tool to be used in the classrooms and research labs, their effectiveness could be improved by focusing on their usability. The usability of a user interface can be evaluated and even measured with respect to the goals of its users. In order to demonstrate the evaluation of a user interface, I present an experiment in which the efficiency of user interfaces is assessed in the context of three scenarios or ``use cases.'' Based on the experiment, I discuss some basic principles of usability theory, such as affordances, minimization of navigation, error handling, immediate feedback, and data visibility. The evaluation of these principles suggests some new types of music theory applications.</abstract>
		<doi>10.1007/978-3-540-85035-9_6</doi>
		<title>The Usability of Music Theory Software: The Analysis of Twelve-Tone Music as a Case Study</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/230d55243304c1b35a7377822d3ce1a51/keinstein</id>
		<tags>Tonnetz</tags>
		<tags>Musiktheorie</tags>
		<tags>MaMu</tags>
		<description></description>
		<date>2012-02-16 17:14:32</date>
		<count>2</count>
		<booktitle>Mathematics and Computation in Music</booktitle>
		<series>Communications in Computer and Information Science</series>
		<publisher>Springer Berlin Heidelberg</publisher>
		<year>2009</year>
		<url>http://dmitri.tymoczko.com/publications.html</url>
		<author>Dmitri Tymoczko</author>
		<authors>
			<first>Dmitri</first>
		</authors>
		<authors>
			<last>Tymoczko</last>
		</authors>
		<editor>Elaine Chew</editor>
		<editor>Adrian Childs</editor>
		<editor>Ching-Hua Chuan</editor>
		<editors>
			<first>Dmitri</first>
		</editors>
		<editors>
			<last>Tymoczko</last>
		</editors>
		<editors>
			<first>Dmitri</first>
		</editors>
		<editors>
			<last>Tymoczko</last>
		</editors>
		<editors>
			<first>Dmitri</first>
		</editors>
		<editors>
			<last>Tymoczko</last>
		</editors>
		<volume>38</volume>
		<pages>258-272</pages>
		<abstract>This paper considers three conceptions of musical distance (or inverse similarity ) that produce three different musico-geometrical spaces: the first, based on voice leading, yields a collection of continuous quotient spaces or orbifolds; the second, based on acoustics, gives rise to the Tonnetz and related tuning lattices ; while the third, based on the total interval content of a group of notes, generates a six-dimensional quality space first described by Ian Quinn. I will show that although these three measures are in principle quite distinct, they are in practice surprisingly interrelated. This produces the challenge of determining which model is appropriate to a given music-theoretical circumstance. Since the different models can yield comparable results, unwary theorists could potentially find themselves using one type of structure (such as a tuning lattice) to investigate properties more perspicuously represented by another (for instance, voice-leading relationships).</abstract>
		<isbn>978-3-642-02394-1</isbn>
		<doi>10.1007/978-3-642-02394-1_24</doi>
		<title>Three Conceptions of Musical Distance</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f05e5abef1186b5485281fe0e1a1bab4/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#LeeKC13</url>
		<author>Sungju Lee</author>
		<author>Heegon Kim</author>
		<author>Yongwha Chung</author>
		<authors>
			<first>Sungju</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Heegon</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Yongwha</first>
		</authors>
		<authors>
			<last>Chung</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Yongwha</first>
		</editors>
		<editors>
			<last>Chung</last>
		</editors>
		<editors>
			<first>Yongwha</first>
		</editors>
		<editors>
			<last>Chung</last>
		</editors>
		<editors>
			<first>Yongwha</first>
		</editors>
		<editors>
			<last>Chung</last>
		</editors>
		<editors>
			<first>Yongwha</first>
		</editors>
		<editors>
			<last>Chung</last>
		</editors>
		<volume>274</volume>
		<pages>157-163</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Power-Time Tradeoff of Parallel Execution on Multi-core Platforms.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27587b5a53f2d7404f5e8fa8384e64444/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#KangPLB13</url>
		<author>Taegyeong Kang</author>
		<author>Namje Park</author>
		<author>Hyungkyu Lee</author>
		<author>Hyo-Chan Bang</author>
		<authors>
			<first>Taegyeong</first>
		</authors>
		<authors>
			<last>Kang</last>
		</authors>
		<authors>
			<first>Namje</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<authors>
			<first>Hyungkyu</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Hyo-Chan</first>
		</authors>
		<authors>
			<last>Bang</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Hyo-Chan</first>
		</editors>
		<editors>
			<last>Bang</last>
		</editors>
		<editors>
			<first>Hyo-Chan</first>
		</editors>
		<editors>
			<last>Bang</last>
		</editors>
		<editors>
			<first>Hyo-Chan</first>
		</editors>
		<editors>
			<last>Bang</last>
		</editors>
		<editors>
			<first>Hyo-Chan</first>
		</editors>
		<editors>
			<last>Bang</last>
		</editors>
		<volume>274</volume>
		<pages>463-468</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Use-Cases and Service Modeling Analysis of Open Ubiquitous Sensor Network Platform in Semantic Environment.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a2dbd3e60d227d0c57fb38537cec82bc/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#LiZJZJ13</url>
		<author>Weimin Li</author>
		<author>Xiaohua Zhao</author>
		<author>Jiulei Jiang</author>
		<author>Xiaokang Zhou</author>
		<author>Qun Jin</author>
		<authors>
			<first>Weimin</first>
		</authors>
		<authors>
			<last>Li</last>
		</authors>
		<authors>
			<first>Xiaohua</first>
		</authors>
		<authors>
			<last>Zhao</last>
		</authors>
		<authors>
			<first>Jiulei</first>
		</authors>
		<authors>
			<last>Jiang</last>
		</authors>
		<authors>
			<first>Xiaokang</first>
		</authors>
		<authors>
			<last>Zhou</last>
		</authors>
		<authors>
			<first>Qun</first>
		</authors>
		<authors>
			<last>Jin</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Qun</first>
		</editors>
		<editors>
			<last>Jin</last>
		</editors>
		<editors>
			<first>Qun</first>
		</editors>
		<editors>
			<last>Jin</last>
		</editors>
		<editors>
			<first>Qun</first>
		</editors>
		<editors>
			<last>Jin</last>
		</editors>
		<editors>
			<first>Qun</first>
		</editors>
		<editors>
			<last>Jin</last>
		</editors>
		<volume>274</volume>
		<pages>501-508</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>A Probabilistic Timing Constraint Modeling and Functional Validation Approach to Dynamic Service Composition for LBS.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/298134d7ece65099c56c2faa6ac2c312b/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#KimPAK13</url>
		<author>Jong-Ho Kim</author>
		<author>Young-Su Park</author>
		<author>Sang-Ho Ahn</author>
		<author>Sang-Kyoon Kim</author>
		<authors>
			<first>Jong-Ho</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Young-Su</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<authors>
			<first>Sang-Ho</first>
		</authors>
		<authors>
			<last>Ahn</last>
		</authors>
		<authors>
			<first>Sang-Kyoon</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Sang-Kyoon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Sang-Kyoon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Sang-Kyoon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Sang-Kyoon</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<volume>274</volume>
		<pages>541-548</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>A Feature-Based Small Target Detection System.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/269a14629cff37f33f1e2caa11698e2ba/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#LeeYYC13</url>
		<author>Ha-Kyung Jennifer Lee</author>
		<author>Young-Mi Yun</author>
		<author>Kee-Hyung Yoon</author>
		<author>Dong-Sub Cho</author>
		<authors>
			<first>Ha-Kyung Jennifer</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Young-Mi</first>
		</authors>
		<authors>
			<last>Yun</last>
		</authors>
		<authors>
			<first>Kee-Hyung</first>
		</authors>
		<authors>
			<last>Yoon</last>
		</authors>
		<authors>
			<first>Dong-Sub</first>
		</authors>
		<authors>
			<last>Cho</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Dong-Sub</first>
		</editors>
		<editors>
			<last>Cho</last>
		</editors>
		<editors>
			<first>Dong-Sub</first>
		</editors>
		<editors>
			<last>Cho</last>
		</editors>
		<editors>
			<first>Dong-Sub</first>
		</editors>
		<editors>
			<last>Cho</last>
		</editors>
		<editors>
			<first>Dong-Sub</first>
		</editors>
		<editors>
			<last>Cho</last>
		</editors>
		<volume>274</volume>
		<pages>25-30</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Design of Automatic Paper Identification System with QR Code for Digital Forensics.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e2a771a2f77945c6f09d09292f807bf0/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#JungCK13</url>
		<author>Euihyun Jung</author>
		<author>IlKwon Cho</author>
		<author>Sun Moo Kang</author>
		<authors>
			<first>Euihyun</first>
		</authors>
		<authors>
			<last>Jung</last>
		</authors>
		<authors>
			<first>IlKwon</first>
		</authors>
		<authors>
			<last>Cho</last>
		</authors>
		<authors>
			<first>Sun Moo</first>
		</authors>
		<authors>
			<last>Kang</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Sun Moo</first>
		</editors>
		<editors>
			<last>Kang</last>
		</editors>
		<editors>
			<first>Sun Moo</first>
		</editors>
		<editors>
			<last>Kang</last>
		</editors>
		<editors>
			<first>Sun Moo</first>
		</editors>
		<editors>
			<last>Kang</last>
		</editors>
		<editors>
			<first>Sun Moo</first>
		</editors>
		<editors>
			<last>Kang</last>
		</editors>
		<volume>274</volume>
		<pages>69-74</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>An Agent Modeling for Overcoming the Heterogeneity in the IoT with Design Patterns.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2324ed058af8fd3bef9be00237b7662b1/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#TsaiHC13</url>
		<author>Chun-Wei Tsai</author>
		<author>Bo-Chi Huang</author>
		<author>Ming-Chao Chiang</author>
		<authors>
			<first>Chun-Wei</first>
		</authors>
		<authors>
			<last>Tsai</last>
		</authors>
		<authors>
			<first>Bo-Chi</first>
		</authors>
		<authors>
			<last>Huang</last>
		</authors>
		<authors>
			<first>Ming-Chao</first>
		</authors>
		<authors>
			<last>Chiang</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Ming-Chao</first>
		</editors>
		<editors>
			<last>Chiang</last>
		</editors>
		<editors>
			<first>Ming-Chao</first>
		</editors>
		<editors>
			<last>Chiang</last>
		</editors>
		<editors>
			<first>Ming-Chao</first>
		</editors>
		<editors>
			<last>Chiang</last>
		</editors>
		<editors>
			<first>Ming-Chao</first>
		</editors>
		<editors>
			<last>Chiang</last>
		</editors>
		<volume>274</volume>
		<pages>621-628</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>A Novel Spiral Optimization for Clustering.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23e32c1d130fe403b6a4a51c378925ef8/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#VladimirJK13</url>
		<author>Tyan Vladimir</author>
		<author>Dongwoon Jeon</author>
		<author>Doo-Hyun Kim</author>
		<authors>
			<first>Tyan</first>
		</authors>
		<authors>
			<last>Vladimir</last>
		</authors>
		<authors>
			<first>Dongwoon</first>
		</authors>
		<authors>
			<last>Jeon</last>
		</authors>
		<authors>
			<first>Doo-Hyun</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Doo-Hyun</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Doo-Hyun</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Doo-Hyun</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<editors>
			<first>Doo-Hyun</first>
		</editors>
		<editors>
			<last>Kim</last>
		</editors>
		<volume>274</volume>
		<pages>341-345</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>A Vision-Based Robust Hovering Control System for UAV.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27410126a575dedf6fe592c19b82568aa/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#TurkesSH13</url>
		<author>Okan Turkes</author>
		<author>Hans Scholten</author>
		<author>Paul J. M. Havinga</author>
		<authors>
			<first>Okan</first>
		</authors>
		<authors>
			<last>Turkes</last>
		</authors>
		<authors>
			<first>Hans</first>
		</authors>
		<authors>
			<last>Scholten</last>
		</authors>
		<authors>
			<first>Paul J. M.</first>
		</authors>
		<authors>
			<last>Havinga</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Paul J. M.</first>
		</editors>
		<editors>
			<last>Havinga</last>
		</editors>
		<editors>
			<first>Paul J. M.</first>
		</editors>
		<editors>
			<last>Havinga</last>
		</editors>
		<editors>
			<first>Paul J. M.</first>
		</editors>
		<editors>
			<last>Havinga</last>
		</editors>
		<editors>
			<first>Paul J. M.</first>
		</editors>
		<editors>
			<last>Havinga</last>
		</editors>
		<volume>274</volume>
		<pages>397-403</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Introspection-Based Periodicity Awareness Model for Intermittently Connected Mobile Networks.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/299247ca0947d98401a85250ab8d9296c/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#KimKP13a</url>
		<author>Jeongyeun Kim</author>
		<author>Yilip Kim</author>
		<author>Namje Park</author>
		<authors>
			<first>Jeongyeun</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Yilip</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Namje</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Namje</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Namje</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Namje</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<editors>
			<first>Namje</first>
		</editors>
		<editors>
			<last>Park</last>
		</editors>
		<volume>274</volume>
		<pages>529-534</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Result of Implementing STEAM Program and Analysis of Effectiveness for Smart Grid's Education.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c936556b1b927fe3bf1698b9e7ad868b/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#ParkPL13</url>
		<author>Jong-Eun Park</author>
		<author>Jongmoon Park</author>
		<author>Myung-Joon Lee</author>
		<authors>
			<first>Jong-Eun</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<authors>
			<first>Jongmoon</first>
		</authors>
		<authors>
			<last>Park</last>
		</authors>
		<authors>
			<first>Myung-Joon</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Myung-Joon</first>
		</editors>
		<editors>
			<last>Lee</last>
		</editors>
		<editors>
			<first>Myung-Joon</first>
		</editors>
		<editors>
			<last>Lee</last>
		</editors>
		<editors>
			<first>Myung-Joon</first>
		</editors>
		<editors>
			<last>Lee</last>
		</editors>
		<editors>
			<first>Myung-Joon</first>
		</editors>
		<editors>
			<last>Lee</last>
		</editors>
		<volume>274</volume>
		<pages>55-61</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>DirectSpace: A Collaborative Framework for Supporting Group Workspaces over Wi-Fi Direct.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29555b6612190a879d66e4410bb073f93/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#KimY13</url>
		<author>Jiwon Kim</author>
		<author>Unil Yun</author>
		<authors>
			<first>Jiwon</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Unil</first>
		</authors>
		<authors>
			<last>Yun</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Unil</first>
		</editors>
		<editors>
			<last>Yun</last>
		</editors>
		<editors>
			<first>Unil</first>
		</editors>
		<editors>
			<last>Yun</last>
		</editors>
		<editors>
			<first>Unil</first>
		</editors>
		<editors>
			<last>Yun</last>
		</editors>
		<editors>
			<first>Unil</first>
		</editors>
		<editors>
			<last>Yun</last>
		</editors>
		<volume>274</volume>
		<pages>13-17</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>The Blog Ranking Algorithm Using Analysis of Both Blog Influence and Characteristics of Blog Posts.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c5c5b10cb2c606a028dba78e585e4ed8/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-09-30 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#DegefaW13</url>
		<author>Fikadu B. Degefa</author>
		<author>Dongho Won</author>
		<authors>
			<first>Fikadu B.</first>
		</authors>
		<authors>
			<last>Degefa</last>
		</authors>
		<authors>
			<first>Dongho</first>
		</authors>
		<authors>
			<last>Won</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Dongho</first>
		</editors>
		<editors>
			<last>Won</last>
		</editors>
		<editors>
			<first>Dongho</first>
		</editors>
		<editors>
			<last>Won</last>
		</editors>
		<editors>
			<first>Dongho</first>
		</editors>
		<editors>
			<last>Won</last>
		</editors>
		<editors>
			<first>Dongho</first>
		</editors>
		<editors>
			<last>Won</last>
		</editors>
		<volume>274</volume>
		<pages>521-527</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Scalable Key Management for Dynamic Group in Multi-cast Communication.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f8c87c6e693e175a2f302cdf4fd341a2/choko</id>
		<tags>model,copyright,cross-media,digital</tags>
		<tags>rights,media</tags>
		<tags>business,business</tags>
		<tags>media</tags>
		<tags>technology,mp3,music,social</tags>
		<description></description>
		<date>2013-12-30 18:09:17</date>
		<count>1</count>
		<booktitle>First International Conference on Automated Production of Cross Media Content for Multi-Channel Distribution (AXMEDIS'05)</booktitle>
		<publisher>IEEE</publisher>
		<address>Washington</address>
		<year>2005</year>
		<url>http://portal.acm.org/citation.cfm?id=1114279.1114356</url>
		<author>Konstantinos Chorianopoulos</author>
		<author>J. Barria</author>
		<author>T. Regner</author>
		<author>J. Pitt</author>
		<authors>
			<first>Konstantinos</first>
		</authors>
		<authors>
			<last>Chorianopoulos</last>
		</authors>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Barria</last>
		</authors>
		<authors>
			<first>T.</first>
		</authors>
		<authors>
			<last>Regner</last>
		</authors>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Pitt</last>
		</authors>
		<pages>257--260</pages>
		<abstract>Digital music consumers have to choose between illegal file swapping services and online music stores. The latter impose various restrictions to the established music consumption behaviour, such as limitations on the number of devices and proprietary music formats. We describe a business model that is based on a liberal management of music rights, instead of the dominant restrictions of access. The proposed business model facilitates the free flow of music content between different client devices (PC, mobile phone, portable player) and between heterogeneous networks (Web, P2P, wireless, broadcast), but it controls the flow of rights for added value music bundles. The business model is presented over two stages of the customer activity cycle and along the revenue, process and technology elements.</abstract>
		<isbn>0-7695-2348-X</isbn>
		<doi>10.1109/AXMEDIS.2005.16</doi>
		<title>Cross Media Digital Rights Management for Online Music Stores</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f30b170edec4586c21c9c5d8d297b654/joaakive</id>
		<tags>music</tags>
		<tags>computer</tags>
		<tags>controller</tags>
		<description></description>
		<date>2014-11-10 11:49:44</date>
		<count>3</count>
		<publisher>New interfaces for musical expression</publisher>
		<year>2001</year>
		<url></url>
		<author>Perry Cook</author>
		<authors>
			<first>Perry</first>
		</authors>
		<authors>
			<last>Cook</last>
		</authors>
		<title>Principles for Designing Computer Music Controllers</title>
		<pubtype>proceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20681ab4879e2378295f724eb73e7360c/dbenz</id>
		<tags>tagging</tags>
		<tags>music</tags>
		<tags>taggingsurvey</tags>
		<tags>ol_web2.0</tags>
		<tags>semantics</tags>
		<description></description>
		<date>2011-07-29 09:40:39</date>
		<count>1</count>
		<journal>Journal of New Music Research</journal>
		<publisher>Routledge, part of the Taylor & Francis Group</publisher>
		<year>2008</year>
		<url></url>
		<author>M. Levy</author>
		<author>M. Sandler</author>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Levy</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Sandler</last>
		</authors>
		<volume>37</volume>
		<number>2</number>
		<pages>137--150</pages>
		<title>Learning latent semantic models for music from social tags</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/289bc2f45416a99d459ae97fd46015256/zazi</id>
		<tags>myown</tags>
		<tags>informationquality</tags>
		<tags>informationmanagement</tags>
		<tags>counterontology</tags>
		<tags>orderedlistontology</tags>
		<tags>musicontology</tags>
		<tags>associationontology</tags>
		<tags>musicrecommendation</tags>
		<tags>playbackontology</tags>
		<tags>knowledgerepresentation</tags>
		<tags>recommendationontology</tags>
		<tags>musicinformationretrieval</tags>
		<tags>knowledgemanagement</tags>
		<tags>infoserviceontology</tags>
		<tags>personalisation</tags>
		<tags>semanticweb</tags>
		<tags>linkeddata</tags>
		<tags>personalmusicknowledgebase</tags>
		<tags>informationfederation</tags>
		<tags>weightingontology</tags>
		<tags>informationintegration</tags>
		<tags>cognitivepattern</tags>
		<tags>informationaggregation</tags>
		<tags>propertyreificationvocabulary</tags>
		<tags>cognitivecharacteristicsontology</tags>
		<tags>ontology</tags>
		<description></description>
		<date>2011-08-19 12:31:01</date>
		<count>1</count>
		<address>Dresden, Germany</address>
		<year>2011</year>
		<url>http://zazi.smiy.org/2011_-_Thomas_Gaengler_-_Personal_Music_Knowledge_Base.pdf</url>
		<author>Thomas Gängler</author>
		<authors>
			<first>Thomas</first>
		</authors>
		<authors>
			<last>Gängler</last>
		</authors>
		<abstract>Music is perceived and described very subjectively by every individual. Nowadays, people often get lost in their steadily growing, multi-placed, digital music collection. Existing music player and management applications get in trouble when dealing with poor metadata that is predominant in personal music collections. There are several music information services available that assist users by providing tools for precisely organising their music collection, or for presenting them new insights into their own music library and listening habits. However, it is still not the case that music consumers can seamlessly interact with all these auxiliary services directly from the place where they access their music individually. To profit from the manifold music and music-related knowledge that is or can be available via various information services, this information has to be gathered up, semantically federated, and integrated into a uniform knowledge base that can personalised represent this data in an appropriate visualisation to the users. This personalised semantic aggregation of music metadata from several sources is the gist of this thesis. The outlined solution particularly concentrates on users’ needs regarding music collection management which can strongly alternate between single human beings. The author’s proposal, the personal music knowledge base (PMKB), consists of a client-server architecture with uniform communication endpoints and an ontological knowledge representation model format that is able to represent the versatile information of its use cases. The PMKB concept is appropriate to cover the complete information flow life cycle, including the processes of user account initialisation, information service choice, individual information extraction, and proactive update notification. The PMKB implementation makes use of SemanticWeb technologies. Particularly the knowledge representation part of the PMKB vision is explained in this work. Several new Semantic Web ontologies are defined or existing ones are massively modified to meet the requirements of a personalised semantic federation of music and music-related data for managing personal music collections. The outcome is, amongst others, • a new vocabulary for describing the play back domain, • another one for representing information service categorisations and quality ratings, and • one that unites the beneficial parts of the existing advanced user modelling ontologies. The introduced vocabularies can be perfectly utilised in conjunction with the existing Music Ontology framework. Some RDFizers that also make use of the outlined ontologies in their mapping definitions, illustrate the fitness in practise of these specifications. A social evaluation method is applied to carry out an examination dealing with the reutilisation, application and feedback of the vocabularies that are explained in this work. This analysis shows that it is a good practise to properly publish Semantic Web ontologies with the help of some Linked Data principles and further basic SEO techniques to easily reach the searching audience, to avoid duplicates of such KR specifications, and, last but not least, to directly establish a "shared understanding". Due to their project-independence, the proposed vocabularies can be deployed in every knowledge representation model that needs their knowledge representation capacities. This thesis added its value to make the vision of a personal music knowledge base come true.</abstract>
		<title>Semantic Federation of Musical and Music-Related Information for Establishing a Personal Music Knowledge Base</title>
		<pubtype>mastersthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e7136e59b7f1a5cdddf45cd1d4688396/lvieira</id>
		<tags>listening</tags>
		<tags>music</tags>
		<tags>enjoyment</tags>
		<tags>activity</tags>
		<tags>physical</tags>
		<description></description>
		<date>2013-02-22 22:59:45</date>
		<count>1</count>
		<year>2006</year>
		<url></url>
		<author>Ting-Chun Weng</author>
		<authors>
			<first>Ting-Chun</first>
		</authors>
		<authors>
			<last>Weng</last>
		</authors>
		<abstract>Research has suggested that listening to preferred music may be helpful in increasing exercise enjoyment, positive mood, and endurance enhancement. This survey research aimed to explore the reasons why people listen to their personal stereo while engaged in physical activity, and to understand the effect this has on their level of enjoyment. Three hundred and forty six students participated in this study via convenience sampling. Participants were divided into groups depending on their active usage of a personal stereo. The Physical Activity Enjoyment Scale (PACES) was administered as the instrument. An independent-samples t test comparing the PACES mean scores of the Non-Personal Stereo and the Personal Stereo groups found a significant difference between the two groups (t(322)= -5.338, p<.0005). Participants in the Personal Stereo group reported that they felt more energized, they enjoyed their experiences more, and that they preferred being in control of the music they listened to. Overall the results suggested that that listening to self-selected music can enhance individuals’ physical activity enjoyment.</abstract>
		<title>Effect of music-listening on the enjoyment of physical activity experience</title>
		<pubtype>mastersthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2440d034bdc36b070322caf1a468fbd4c/ks-plugin-devel</id>
		<tags>DLfrage</tags>
		<tags>Musiktheorie</tags>
		<tags>MaMu</tags>
		<tags>Musik</tags>
		<description>JSTOR: Music Theory Spectrum, Vol. 11, No. 2 (Autumn, 1989), pp. 187-206</description>
		<date>2013-02-02 14:42:55</date>
		<count>2</count>
		<journal>Music Theory Spectrum</journal>
		<publisher>University of California Press on behalf of the Society for Music Theory</publisher>
		<year>1989</year>
		<url>http://www.jstor.org/stable/745935</url>
		<author>Norman Carey</author>
		<author>David Clampitt</author>
		<authors>
			<first>Norman</first>
		</authors>
		<authors>
			<last>Carey</last>
		</authors>
		<authors>
			<first>David</first>
		</authors>
		<authors>
			<last>Clampitt</last>
		</authors>
		<volume>11</volume>
		<number>2</number>
		<pages>pp. 187-206</pages>
		<abstract>Pentatonic, diatonic, and chromatic scales share the same underlying structure, that of the well-formed scale. Well-formedness is defined in terms of a relationship between the order in which a single interval generates the elements of a pitch-class set and the order in which those elements appear in a scale. Another characterization provides a recursive procedure for organizing all well-formed scales into hierarchies. Finally, well-formed scales are defined in terms of scale-step measure, and aspects of the diatonic set are examined.</abstract>
		<issn>01956167</issn>
		<language>English</language>
		<copyright>Copyright © 1989 University of California Press</copyright>
		<doi>10.1525/mts.1989.11.2.02a00030</doi>
		<title>Aspects of Well-Formed Scales</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d17522343b278e826db4f71052a66053/ks-plugin-devel</id>
		<tags>Mathematik</tags>
		<tags>Geometrie</tags>
		<tags>Musiktheorie</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2013-02-02 14:41:00</date>
		<count>3</count>
		<booktitle>The Topos of Music: geometric logic of concepts, theory, and performance</booktitle>
		<publisher>Birkhäuser</publisher>
		<address>Basel ; Boston</address>
		<year>2002</year>
		<url>http://www.loc.gov/catdir/enhancements/fy0812/2002033209-d.html</url>
		<author>Guerino Mazzola</author>
		<author>Stefan Göller</author>
		<author>Stefan Müller</author>
		<authors>
			<first>Guerino</first>
		</authors>
		<authors>
			<last>Mazzola</last>
		</authors>
		<authors>
			<first>Stefan</first>
		</authors>
		<authors>
			<last>Göller</last>
		</authors>
		<authors>
			<first>Stefan</first>
		</authors>
		<authors>
			<last>Müller</last>
		</authors>
		<genre>Music theory</genre>
		<isbn>3-7643-5731-2; 0-8176-5731-2 (alk. paper)</isbn>
		<title>The Topos of Music: geometric logic of concepts, theory, and performance</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29851819ac8fe6a897fd22a10068bc31a/ks-plugin-devel</id>
		<tags>Neurowissenschaften</tags>
		<tags>Musiktheorie</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2005</date>
		<count>2</count>
		<journal>Annals of the New York Academy of Sciences</journal>
		<booktitle>The Neurosciences and Music II: From Perception to Performance</booktitle>
		<publisher>Center for Mind and Brain, University of California, Davis, Davis, California 95616, USA</publisher>
		<year>2005</year>
		<url>http://dx.doi.org/10.1196/annals.1360.008</url>
		<author>Petr Janata</author>
		<authors>
			<first>Petr</first>
		</authors>
		<authors>
			<last>Janata</last>
		</authors>
		<volume>1060</volume>
		<pages>111-124</pages>
		<abstract>As the functional neuroimaging literature grows, it becomes increasingly apparent that music and musical activities engage diverse regions of the brain. In this paper I discuss two studies to illustrate that exactly which brain areas are observed to be responsive to musical stimuli and tasks depends on the tasks and the methods used to describe the tasks and the stimuli. In one study, subjects listened to polyphonic music and were asked to either orient their attention selectively to individual instruments or in a divided or holistic manner across multiple instruments. The network of brain areas that was recruited changed subtly with changes in the task instructions. The focus of the second study was to identify brain regions that follow the pattern of movement of a continuous melody through the tonal space defined by the major and minor keys of Western tonal music. Such an area was identified in the rostral medial prefrontal cortex. This observation is discussed in the context of other neuroimaging studies that implicate this region in inwardly directed mental states involving decisions about the self, autobiographical memory, the cognitive regulation of emotion, affective responses to musical stimuli, and familiarity judgments about musical stimuli. Together with observations that these regions are among the last to atrophy in Alzheimer disease, and that these patients appear to remain responsive to autobiographically salient musical stimuli, very early evidence is emerging from the literature for the hypothesis that the rostral medial prefrontal cortex is a node that is important for binding music with memories within a broader music-responsive network.</abstract>
		<issn>1749-6632</issn>
		<doi>10.1196/annals.1360.008</doi>
		<title>Brain Networks That Track Musical Structure</title>
		<pubtype>inbook</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e3ffdf5c3b6c9ec1eee823acf6d67042/ks-plugin-devel</id>
		<tags>zitiert_wille</tags>
		<tags>MaMu</tags>
		<description></description>
		<date>2013-02-02 14:39:53</date>
		<count>2</count>
		<journal>Acta Musicologica</journal>
		<publisher>International Musicological Society</publisher>
		<year>1986</year>
		<url>http://www.jstor.org/stable/932818</url>
		<author>Albrecht Schneider</author>
		<author>Uwe Seifert</author>
		<authors>
			<first>Albrecht</first>
		</authors>
		<authors>
			<last>Schneider</last>
		</authors>
		<authors>
			<first>Uwe</first>
		</authors>
		<authors>
			<last>Seifert</last>
		</authors>
		<volume>58</volume>
		<number>2</number>
		<pages>pp. 305-338</pages>
		<issn>00016241</issn>
		<language>English</language>
		<copyright>Copyright © 1986 International Musicological Society</copyright>
		<title>Zu einigen Ansätzen und Verfahren in neueren musiktheoretischen Konzepten</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2403e691af5fcabb14790c83eabafdc86/pasmoi144</id>
		<tags>software</tags>
		<tags>Music</tags>
		<tags>e-Learning</tags>
		<description></description>
		<date>2012-11-07 16:33:57</date>
		<count>1</count>
		<journal>British Journal of Music Education</journal>
		<year>2009</year>
		<url>http://journals.cambridge.org/action/displayFulltext?pageCode=100101&type=1&fid=4012272&jid=BME&volumeId=26&issueId=01&aid=4012264</url>
		<author>Steve Cooper</author>
		<author>Crispin Dale</author>
		<author>Steve Spencer</author>
		<authors>
			<first>Steve</first>
		</authors>
		<authors>
			<last>Cooper</last>
		</authors>
		<authors>
			<first>Crispin</first>
		</authors>
		<authors>
			<last>Dale</last>
		</authors>
		<authors>
			<first>Steve</first>
		</authors>
		<authors>
			<last>Spencer</last>
		</authors>
		<volume>26</volume>
		<pages>85-97</pages>
		<title>A tutor in your back pocket. Reflections on the use of iPods and podcasting in an undergraduate popular music programme</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/261cb69572b3e6aae22e138527e51c543/merlo</id>
		<tags>myown</tags>
		<description></description>
		<date>2015-05-04 22:06:34</date>
		<count>1</count>
		<series>Asociación Internacional de Bibliotecas Musicales, Archivos y Centros de Documentación. Congreso (18º. 1998. San Sebastián). Actas: ponencias españolas e hispanoamericanas C1  - Madrid</series>
		<publisher>Asociación Española de Documentación Musical</publisher>
		<year>1999</year>
		<url>http://gredos.usal.es/jspui/handle/10366/18018</url>
		<author>José Antonio Merlo Vega</author>
		<authors>
			<first>José Antonio</first>
		</authors>
		<authors>
			<last>Merlo Vega</last>
		</authors>
		<pages>307--326</pages>
		<title>Formación de los bibliotecarios musicales en España</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b3a9f47f32d1728abe509c1447ed7623/ijritcc</id>
		<tags>gloves</tags>
		<tags>transceiver</tags>
		<tags>Space</tags>
		<tags>Musical</tags>
		<tags>instruments</tags>
		<tags>Data</tags>
		<tags>Virtual</tags>
		<description></description>
		<date>2015-08-06 09:10:57</date>
		<count>1</count>
		<journal>International Journal on Recent and Innovation Trends in Computing and Communication</journal>
		<publisher>Auricle Technologies, Pvt., Ltd.</publisher>
		<year>2015</year>
		<url>http://dx.doi.org/10.17762/ijritcc2321-8169.150351</url>
		<author>Pritesh K. Patil</author>
		<author>Mohit B. Patil</author>
		<author>Kalyani V. Patil</author>
		<author>Tejaswini S.Aware</author>
		<authors>
			<first>Pritesh K.</first>
		</authors>
		<authors>
			<last>Patil</last>
		</authors>
		<authors>
			<first>Mohit B.</first>
		</authors>
		<authors>
			<last>Patil</last>
		</authors>
		<authors>
			<first>Kalyani V.</first>
		</authors>
		<authors>
			<last>Patil</last>
		</authors>
		<authors>
			<first>Tejaswini</first>
		</authors>
		<authors>
			<last>S.Aware</last>
		</authors>
		<volume>3</volume>
		<number>3</number>
		<pages>1125--1128</pages>
		<abstract>In this paper we describe the Data Glove Controlled Virtual Musical Instruments. Traditionally musical instruments are very bulky and expensive to carrying it, whenever user wants to play it. Now the mew system is generated which helps user to play musical instruments virtually, using data glove and computer system. The data glove is input device to the system which is made with the help of flex based register and which helps to tracking gesture made by user. These gestures are playing various musical instruments. Data glove produces the triggered signal which is receives by music generation system. This triggered signal used to play the predefined musical notes of instruments.</abstract>
		<doi>10.17762/ijritcc2321-8169.150351</doi>
		<title>Wireless Data Gloves Controlled Virtual Musical Instrument</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21485f6521c6ae2db520d1a7c3c429f07/hangdong</id>
		<tags>retrieval</tags>
		<tags>tagging_behaviour</tags>
		<tags>experiment</tags>
		<tags>folksonomy</tags>
		<tags>interesting</tags>
		<tags>last.fm</tags>
		<description>Analysis of Music Tagging and Listening Patterns: Do Tags Really Function as Retrieval Aids? - Springer</description>
		<date>2015-07-10 03:26:46</date>
		<count>2</count>
		<booktitle>Social Computing, Behavioral-Cultural Modeling, and Prediction</booktitle>
		<series>Lecture Notes in Computer Science</series>
		<publisher>Springer International Publishing</publisher>
		<year>2015</year>
		<url>http://dx.doi.org/10.1007/978-3-319-16268-3_15</url>
		<author>Jared Lorince</author>
		<author>Kenneth Joseph</author>
		<author>PeterM. Todd</author>
		<authors>
			<first>Jared</first>
		</authors>
		<authors>
			<last>Lorince</last>
		</authors>
		<authors>
			<first>Kenneth</first>
		</authors>
		<authors>
			<last>Joseph</last>
		</authors>
		<authors>
			<first>PeterM.</first>
		</authors>
		<authors>
			<last>Todd</last>
		</authors>
		<editor>Nitin Agarwal</editor>
		<editor>Kevin Xu</editor>
		<editor>Nathaniel Osgood</editor>
		<editors>
			<first>PeterM.</first>
		</editors>
		<editors>
			<last>Todd</last>
		</editors>
		<editors>
			<first>PeterM.</first>
		</editors>
		<editors>
			<last>Todd</last>
		</editors>
		<editors>
			<first>PeterM.</first>
		</editors>
		<editors>
			<last>Todd</last>
		</editors>
		<volume>9021</volume>
		<pages>141-152</pages>
		<abstract>In collaborative tagging systems, it is generally assumed that users assign tags to facilitate retrieval of content at a later time. There is, however, little behavioral evidence that tags actually serve this purpose. Using a large-scale dataset from the social music website Last.fm, we explore how patterns of music tagging and subsequent listening interact to determine if there exist measurable signals of tags functioning as retrieval aids. Specifically, we describe our methods for testing if the assignment of a tag tends to lead to an increase in listening behavior. Results suggest that tagging, on average, leads to only very small increases in listening rates, and overall the data do</abstract>
		<isbn>978-3-319-16267-6</isbn>
		<language>English</language>
		<doi>10.1007/978-3-319-16268-3_15</doi>
		<title>Analysis of Music Tagging and Listening Patterns: Do Tags Really Function as Retrieval Aids?</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24553270366757d547e86adc5bc86a28b/alexarje</id>
		<tags>imported</tags>
		<description></description>
		<date>2016-02-26 20:58:13</date>
		<count>1</count>
		<journal>Twentieth-Century Music</journal>
		<year>2015</year>
		<url>http://www.journals.cambridge.org/abstract_S1478572215000018</url>
		<author>Georgina Born</author>
		<author>Kyle Devine</author>
		<authors>
			<first>Georgina</first>
		</authors>
		<authors>
			<last>Born</last>
		</authors>
		<authors>
			<first>Kyle</first>
		</authors>
		<authors>
			<last>Devine</last>
		</authors>
		<volume>12</volume>
		<number>02</number>
		<pages>135--172</pages>
		<issn>1478-5722, 1478-5730</issn>
		<shorttitle>Music Technology, Gender, and Class</shorttitle>
		<doi>10.1017/S1478572215000018</doi>
		<title>Music Technology, Gender, and Class: Digitization, Educational and Social Change in Britain</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25174d01a2eae6f747da0cca21f66f39b/mandrean</id>
		<tags>timbre</tags>
		<description></description>
		<date>2015-12-19 09:34:40</date>
		<count>1</count>
		<journal>Contemporary Music Review</journal>
		<year>1994</year>
		<url>/brokenurl#         http://dx.doi.org/10.1080/07494469400640271    </url>
		<author>Elena Ungeheuer</author>
		<authors>
			<first>Elena</first>
		</authors>
		<authors>
			<last>Ungeheuer</last>
		</authors>
		<volume>10</volume>
		<number>2</number>
		<pages>25-33</pages>
		<abstract>Attempts to achieve a continuous change of timbre can be found all over the landscape of electronic composing. In as much as timbre is a complex parameter that fuses together all the different properties of sound into its “character”, the Klangfarbenkontinuum figures not only as an exotic effect in the universe of electronic sounds but links a musical utopia with an acoustic premise and electrotechnical reality. This can be shown in the works of Karlheinz Stockhausen and Gottfried Michael Koenig of the 1950s that were created in the course of an intensive interchange. Serial compositional methods — developed dialectically in the search for continual transitions with the help of distinct musical elements — are enriched by the scientific model of the unity of the acoustic world as it was formulated by Werner Meyer-Eppler. Electrotechnical transformations articulate the continuum of musical derivation.</abstract>
		<eprint>http://dx.doi.org/10.1080/07494469400640271</eprint>
		<doi>10.1080/07494469400640271</doi>
		<title>From the elements to the continuum: Timbre composition in early electronic music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fbd096ca51c8e175b291b75b3beeeb25/becker</id>
		<tags>song</tags>
		<tags>music</tags>
		<tags>inthesis</tags>
		<tags>playlist</tags>
		<tags>recommendation</tags>
		<tags>prediction</tags>
		<tags>diss</tags>
		<description></description>
		<date>2017-01-16 14:12:43</date>
		<count>6</count>
		<booktitle>Proceedings of the Sixth ACM Conference on Recommender Systems</booktitle>
		<series>RecSys '12</series>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2012</year>
		<url>http://doi.acm.org/10.1145/2365952.2365979</url>
		<author>Negar Hariri</author>
		<author>Bamshad Mobasher</author>
		<author>Robin Burke</author>
		<authors>
			<first>Negar</first>
		</authors>
		<authors>
			<last>Hariri</last>
		</authors>
		<authors>
			<first>Bamshad</first>
		</authors>
		<authors>
			<last>Mobasher</last>
		</authors>
		<authors>
			<first>Robin</first>
		</authors>
		<authors>
			<last>Burke</last>
		</authors>
		<pages>131--138</pages>
		<abstract>Contextual factors can greatly influence the users' preferences in listening to music. Although it is hard to capture these factors directly, it is possible to see their effects on the sequence of songs liked by the user in his/her current interaction with the system. In this paper, we present a context-aware music recommender system which infers contextual information based on the most recent sequence of songs liked by the user. Our approach mines the top frequent tags for songs from social tagging Web sites and uses topic modeling to determine a set of latent topics for each song, representing different contexts. Using a database of human-compiled playlists, each playlist is mapped into a sequence of topics and frequent sequential patterns are discovered among these topics. These patterns represent frequent sequences of transitions between the latent topics representing contexts. Given a sequence of songs in a user's current interaction, the discovered patterns are used to predict the next topic in the playlist. The predicted topics are then used to post-filter the initial ranking produced by a traditional recommendation algorithm. Our experimental evaluation suggests that our system can help produce better recommendations in comparison to a conventional recommender system based on collaborative or content-based filtering. Furthermore, the topic modeling approach proposed here is also useful in providing better insight into the underlying reasons for song selection and in applications such as playlist construction and context prediction.</abstract>
		<isbn>978-1-4503-1270-7</isbn>
		<doi>10.1145/2365952.2365979</doi>
		<title>Context-aware Music Recommendation Based on Latenttopic Sequential Patterns</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21b2dd2b93e5be62a23d2c4201d5aee55/krevelen</id>
		<tags>imported</tags>
		<tags>thesis</tags>
		<description></description>
		<date>2017-03-16 11:50:55</date>
		<count>2</count>
		<booktitle>Readings in Music and Artificial Intelligence</booktitle>
		<series>Contemporary Music Studies</series>
		<publisher>Harwood Academic</publisher>
		<address>Amsterdam, NL</address>
		<year>2000</year>
		<url></url>
		<author>Simon Holland</author>
		<authors>
			<first>Simon</first>
		</authors>
		<authors>
			<last>Holland</last>
		</authors>
		<editor>E. Miranda</editor>
		<editors>
			<first>Simon</first>
		</editors>
		<editors>
			<last>Holland</last>
		</editors>
		<volume>20</volume>
		<pages>239--274</pages>
		<issn>0891-5415</issn>
		<isbn>90-5755-094-6</isbn>
		<title>Artificial Intelligence in Music Education: A critical review</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f64ecf25855955a5ba879977cdf93a6c/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2020-04-30 00:00:00</date>
		<count>1</count>
		<booktitle>The Oxford Handbook of Algorithmic Music</booktitle>
		<publisher>Oxford University Press</publisher>
		<year>2018</year>
		<url>http://dblp.uni-trier.de/db/books/collections/MD2018.html#Ogborn18</url>
		<author>David Ogborn</author>
		<authors>
			<first>David</first>
		</authors>
		<authors>
			<last>Ogborn</last>
		</authors>
		<editor>Alex McLean</editor>
		<editor>Roger T. Dean</editor>
		<editors>
			<first>David</first>
		</editors>
		<editors>
			<last>Ogborn</last>
		</editors>
		<editors>
			<first>David</first>
		</editors>
		<editors>
			<last>Ogborn</last>
		</editors>
		<isbn>978-0-19-022699-2</isbn>
		<title>Network music and the algorithmic ensemble.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/210c1406eb97d6ec64ebd639b34a31345/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2020-04-30 00:00:00</date>
		<count>1</count>
		<booktitle>The Oxford Handbook of Algorithmic Music</booktitle>
		<publisher>Oxford University Press</publisher>
		<year>2018</year>
		<url>http://dblp.uni-trier.de/db/books/collections/MD2018.html#EldridgeB18</url>
		<author>Alice Eldridge</author>
		<author>Oliver Bown</author>
		<authors>
			<first>Alice</first>
		</authors>
		<authors>
			<last>Eldridge</last>
		</authors>
		<authors>
			<first>Oliver</first>
		</authors>
		<authors>
			<last>Bown</last>
		</authors>
		<editor>Alex McLean</editor>
		<editor>Roger T. Dean</editor>
		<editors>
			<first>Oliver</first>
		</editors>
		<editors>
			<last>Bown</last>
		</editors>
		<editors>
			<first>Oliver</first>
		</editors>
		<editors>
			<last>Bown</last>
		</editors>
		<isbn>978-0-19-022699-2</isbn>
		<title>Biologically-Inspired and Agent-Based Algorithms for Music.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/257ce0cfa9cba02e4701664fe117c2ba3/sapo</id>
		<tags>optical_music_recognition_systems</tags>
		<description>From Optical Music Recognition to Handwritten Music Recognition: A baseline - ScienceDirect</description>
		<date>2020-06-15 19:06:42</date>
		<count>2</count>
		<journal>Pattern Recognit. Lett.</journal>
		<year>2019</year>
		<url>http://www.sciencedirect.com/science/article/pii/S0167865518303386</url>
		<author>Arnau Baró</author>
		<author>Pau Riba</author>
		<author>Jorge Calvo-Zaragoza</author>
		<author>Alicia Fornés</author>
		<authors>
			<first>Arnau</first>
		</authors>
		<authors>
			<last>Baró</last>
		</authors>
		<authors>
			<first>Pau</first>
		</authors>
		<authors>
			<last>Riba</last>
		</authors>
		<authors>
			<first>Jorge</first>
		</authors>
		<authors>
			<last>Calvo-Zaragoza</last>
		</authors>
		<authors>
			<first>Alicia</first>
		</authors>
		<authors>
			<last>Fornés</last>
		</authors>
		<volume>123</volume>
		<pages>1 - 8</pages>
		<abstract>Optical Music Recognition (OMR) is the branch of document image analysis that aims to convert images of musical scores into a computer-readable format. Despite decades of research, the recognition of handwritten music scores, concretely the Western notation, is still an open problem, and the few existing works only focus on a specific stage of OMR. In this work, we propose a full Handwritten Music Recognition (HMR) system based on Convolutional Recurrent Neural Networks, data augmentation and transfer learning, that can serve as a baseline for the research community.</abstract>
		<issn>0167-8655</issn>
		<doi>https://doi.org/10.1016/j.patrec.2019.02.029</doi>
		<title>From Optical Music Recognition to Handwritten Music Recognition: A baseline</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c0220a75eb87b798dfa89b028a16e621/sapo</id>
		<tags>automatic_transcription</tags>
		<description>Improving Music Transcription by Pre-Stacking A U-Net - IEEE Conference Publication</description>
		<date>2020-05-07 09:42:43</date>
		<count>3</count>
		<journal>ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</journal>
		<booktitle>ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</booktitle>
		<year>2020</year>
		<url>https://ieeexplore.ieee.org/document/9052987</url>
		<author>F. Pedersoli</author>
		<author>G. Tzanetakis</author>
		<author>K. M. Yi</author>
		<authors>
			<first>F.</first>
		</authors>
		<authors>
			<last>Pedersoli</last>
		</authors>
		<authors>
			<first>G.</first>
		</authors>
		<authors>
			<last>Tzanetakis</last>
		</authors>
		<authors>
			<first>K. M.</first>
		</authors>
		<authors>
			<last>Yi</last>
		</authors>
		<pages>506-510</pages>
		<abstract>We propose to pre-stack a U-Net as a way of improving the polyphonic music transcription performance of various baseline Convolutional Neural Networks (CNNS). The U-Net, a network architecture based on skip-connections between layers acts as a transformation network followed by a transcription network. Notably, we do not introduce any additional loss terms specific to the transformation network, but instead, jointly train the entire combined model with the original loss function that was designed for the back-end transcription network. We argue that this U-Net network transforms the input signal into a representation that is more effective for transcription, and thus enables the observed improvements in accuracy. We empirically confirm with several experiments using the MusicNet dataset, that the proposed configuration consistently improves the accuracy of transcription networks. This enhancement cannot be achieved by simply introducing more neurons or more layers to the baseline CNNs. Moreover, we show that using the proposed architecture we can go beyond general music transcription and perform transcription in an instrument-specific fashion. By doing so, the original general transcription performance is also increased.</abstract>
		<issn>2379-190X</issn>
		<doi>10.1109/ICASSP40776.2020.9052987</doi>
		<title>Improving Music Transcription by Pre-Stacking A U-Net</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2091f7cf92ce7b81bc23b9fb6acc3927f/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>2</count>
		<booktitle>Proceedings of the 21st ACM international conference on Multimedia - MM '13</booktitle>
		<publisher>International Society for Music Information Retrieval (ISMIR)</publisher>
		<year>2013</year>
		<url>http://dl.acm.org/citation.cfm?doid=2502081.2502229</url>
		<author>Dmitry Bogdanov</author>
		<author>Xavier Serra</author>
		<author>Nicolas Wack</author>
		<author>Emilia Gómez</author>
		<author>Sankalp Gulati</author>
		<author>Perfecto Herrera</author>
		<author>Oscar Mayor</author>
		<author>Gerard Roma</author>
		<author>Justin Salamon</author>
		<author>José Zapata</author>
		<authors>
			<first>Dmitry</first>
		</authors>
		<authors>
			<last>Bogdanov</last>
		</authors>
		<authors>
			<first>Xavier</first>
		</authors>
		<authors>
			<last>Serra</last>
		</authors>
		<authors>
			<first>Nicolas</first>
		</authors>
		<authors>
			<last>Wack</last>
		</authors>
		<authors>
			<first>Emilia</first>
		</authors>
		<authors>
			<last>Gómez</last>
		</authors>
		<authors>
			<first>Sankalp</first>
		</authors>
		<authors>
			<last>Gulati</last>
		</authors>
		<authors>
			<first>Perfecto</first>
		</authors>
		<authors>
			<last>Herrera</last>
		</authors>
		<authors>
			<first>Oscar</first>
		</authors>
		<authors>
			<last>Mayor</last>
		</authors>
		<authors>
			<first>Gerard</first>
		</authors>
		<authors>
			<last>Roma</last>
		</authors>
		<authors>
			<first>Justin</first>
		</authors>
		<authors>
			<last>Salamon</last>
		</authors>
		<authors>
			<first>José</first>
		</authors>
		<authors>
			<last>Zapata</last>
		</authors>
		<number>November</number>
		<pages>855--858</pages>
		<abstract>We present Essentia 2.0, an open-source C++ library for audio analysis
	and audio-based music information retrieval released under the Affero
	GPL license. It contains an extensive collection of reusable algorithms
	which implement audio input/output functionality, standard digital
	signal processing blocks, statistical characterization of data, and
	a large set of spectral, temporal, tonal and high-level music descriptors.
	The library is also wrapped in Python and includes a number of predefined
	executable extractors for the available music descriptors, which
	facilitates its use for fast prototyping and allows setting up research
	experiments very rapidly. Furthermore, it includes a Vamp plugin
	to be used with Sonic Visualiser for visualization purposes. The
	library is cross-platform and currently supports Linux, Mac OS X,
	and Windows systems. Essentia is designed with a focus on the robustness
	of the provided music descriptors and is optimized in terms of the
	computational cost of the algorithms. The provided functionality,
	specifically the music descriptors included in-the-box and signal
	processing algorithms, is easily expandable and allows for both research
	experiments and development of large-scale industrial applications.</abstract>
		<doi>10.1145/2502081.2502229</doi>
		<title>ESSENTIA: an open-source library for sound and music analysis</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29ef0b85decbf24ce7062a1bc13f0f704/meneteqel</id>
		<tags>norms</tags>
		<tags>artistic_work</tags>
		<tags>creative_industries</tags>
		<tags>freelance_work</tags>
		<tags>labour_markets</tags>
		<tags>musicians</tags>
		<tags>France</tags>
		<tags>project-based_work</tags>
		<tags>United_Kingdom</tags>
		<description></description>
		<date>2019-04-29 10:40:44</date>
		<count>1</count>
		<journal>Human Relations</journal>
		<publisher>SAGE Publications</publisher>
		<year>2015</year>
		<url>https://journals.sagepub.com/doi/10.1177/0018726715596803</url>
		<author>Charles Umney</author>
		<authors>
			<first>Charles</first>
		</authors>
		<authors>
			<last>Umney</last>
		</authors>
		<volume>69</volume>
		<number>3</number>
		<pages>711--729</pages>
		<abstract>This article examines the normative expectations freelance jazz musicians have about the material conditions of live performance work, taking London and Paris as case studies. It shows how price norms constitute an important reference point for individual workers in navigating the labour market. However, only rarely do they take ‘stronger’ form as a collective demand. Two further arguments are made: first, that the strength of norms varies very widely across labour markets, being much stronger on jobs where other qualitative attractions (such as the scope for creative autonomy) are weak. Second, in the Paris case, an ostensibly solidaristic social insurance mechanism (the Intermittence du Spectacle system) had the seemingly paradoxical effect of further weakening social norms around working conditions. Workers’ individual efforts to meet the system’s eligibility criteria often disrupted the emergence of collective expectations around pricing, and in some cases the existence of formal regulation itself was stigmatized as stifling creativity.</abstract>
		<language>eng</language>
		<doi>10.1177/0018726715596803</doi>
		<title>The labour market for jazz musicians in Paris and London: Formal regulation and informal norms</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/231790fd30547346b0bc6fc099f05c76e/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2018-11-15 00:00:00</date>
		<count>1</count>
		<booktitle>MUSIC</booktitle>
		<series>Lecture Notes in Electrical Engineering</series>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/conf/music/music2013.html#ChoiLJ13</url>
		<author>Junyoung Choi</author>
		<author>Daesung Lee</author>
		<author>Hanmin Jung</author>
		<authors>
			<first>Junyoung</first>
		</authors>
		<authors>
			<last>Choi</last>
		</authors>
		<authors>
			<first>Daesung</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Hanmin</first>
		</authors>
		<authors>
			<last>Jung</last>
		</authors>
		<editor>James J. Park</editor>
		<editor>Hojjat Adeli</editor>
		<editor>Namje Park</editor>
		<editor>Isaac Woungang</editor>
		<editors>
			<first>Hanmin</first>
		</editors>
		<editors>
			<last>Jung</last>
		</editors>
		<editors>
			<first>Hanmin</first>
		</editors>
		<editors>
			<last>Jung</last>
		</editors>
		<editors>
			<first>Hanmin</first>
		</editors>
		<editors>
			<last>Jung</last>
		</editors>
		<editors>
			<first>Hanmin</first>
		</editors>
		<editors>
			<last>Jung</last>
		</editors>
		<volume>274</volume>
		<pages>287-291</pages>
		<isbn>978-3-642-40674-4</isbn>
		<title>Knowledge Discovery and Integration: A Case Study of Housing Planning Support System.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26f080ad266b706fde71d68d2246ee979/sapo</id>
		<tags>datasets</tags>
		<description>mtg.upf.edu/biblio/export/bibtex/3490</description>
		<date>2019-07-01 12:24:04</date>
		<count>1</count>
		<booktitle>17th International Society for Music Information Retrieval Conference (ISMIR 2016)</booktitle>
		<address>New York</address>
		<year>2016</year>
		<url>http://hdl.handle.net/10230/33063</url>
		<author>Sergio Oramas</author>
		<author>L. Espinosa-Anke</author>
		<author>A. Lawlor</author>
		<author>Xavier Serra</author>
		<author>H. Saggion</author>
		<authors>
			<first>Sergio</first>
		</authors>
		<authors>
			<last>Oramas</last>
		</authors>
		<authors>
			<first>L.</first>
		</authors>
		<authors>
			<last>Espinosa-Anke</last>
		</authors>
		<authors>
			<first>A.</first>
		</authors>
		<authors>
			<last>Lawlor</last>
		</authors>
		<authors>
			<first>Xavier</first>
		</authors>
		<authors>
			<last>Serra</last>
		</authors>
		<authors>
			<first>H.</first>
		</authors>
		<authors>
			<last>Saggion</last>
		</authors>
		<pages>150-156</pages>
		<abstract>In this paper, we explore a large multimodal dataset of about 65k albums constructed from a combination of Amazon customer reviews, MusicBrainz metadata and AcousticBrainz audio descriptors. Review texts are further enriched with named entity disambiguation along with polarity information derived from an aspect-based sentiment analysis framework. This dataset constitutes the cornerstone of two main contributions: First, we perform experiments on music genre classification, exploring a variety of feature types, including semantic, sentimental and acoustic features. These experiments show that modeling semantic information contributes to outperforming strong bag-of-words baselines. Second, we provide a diachronic study of the criticism of music genres via a quantitative analysis of the polarity associated to musical aspects over time. Our analysis hints at a potential correlation between key cultural and geopolitical events and the language and evolving sentiments found in music reviews.</abstract>
		<title>Exploring Customer Reviews for Music Genre Classification and Evolutionary Studies</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28e9d6f68ac2298162640b3bffeefd410/sapo</id>
		<tags>Off-line</tags>
		<tags>Score-to-audio_alignment</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>2</count>
		<booktitle>Proceedings of the 6th International Conference on Music Information Retrieval (ISMIR 2005), London, UK</booktitle>
		<year>2005</year>
		<url></url>
		<author>Simon Dixon</author>
		<author>Gerhard Widmer</author>
		<authors>
			<first>Simon</first>
		</authors>
		<authors>
			<last>Dixon</last>
		</authors>
		<authors>
			<first>Gerhard</first>
		</authors>
		<authors>
			<last>Widmer</last>
		</authors>
		<abstract>We present MATCH, a toolkit for aligning audio recordings of different renditions of the same piece of music, based on an efficient implementation of a dynamic time warping algorithm. A forward path estimation algorithm constrains the alignment path so that dynamic time warping can be performed with time and space costs that are linear in the size of the audio files. Frames of audio are represented by a positive spectral difference vector, which emphasises note onsets in the alignment process. MATCH was tested on several hundred test cases and had a median error of 20 ms, with less than 1% of test cases failing to align at all. The software is useful for content-based indexing of audio files and for the study of performance interpretation; it can also be used in real-time for tracking live performances. Another possible application is in an intelligent audio recording and editing system for aligning splice points. The toolkit also provides functions for displaying the cost matrix, the forward and backward paths, and any metadata associated with the recordings, which can be shown in real time as the alignment is computed.</abstract>
		<language>EN</language>
		<title>Match: A Music Alignment Tool Chest</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/243e3dca38787326b647b98ef8195b822/sapo</id>
		<tags>Off-line</tags>
		<tags>Score-to-audio_alignment</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>2</count>
		<booktitle>Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010, Utrecht, Netherlands, August 9-13, 2010</booktitle>
		<publisher>International Society for Music Information Retrieval</publisher>
		<year>2010</year>
		<url>http://ismir2010.ismir.net/proceedings/ismir2010-9.pdf</url>
		<author>Cyril Joder</author>
		<author>Slim Essid</author>
		<author>Gaël Richard</author>
		<authors>
			<first>Cyril</first>
		</authors>
		<authors>
			<last>Joder</last>
		</authors>
		<authors>
			<first>Slim</first>
		</authors>
		<authors>
			<last>Essid</last>
		</authors>
		<authors>
			<first>Gaël</first>
		</authors>
		<authors>
			<last>Richard</last>
		</authors>
		<editor>J. Stephen Downie</editor>
		<editor>Remco C. Veltkamp</editor>
		<editors>
			<first>Gaël</first>
		</editors>
		<editors>
			<last>Richard</last>
		</editors>
		<editors>
			<first>Gaël</first>
		</editors>
		<editors>
			<last>Richard</last>
		</editors>
		<pages>39--45</pages>
		<bibsource>dblp computer science bibliography, https://dblp.org</bibsource>
		<title>An Improved Hierarchical Approach for Music-to-symbolic Score Alignment</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/271b622d98faa8d2628da1180e3a3fca6/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>International Symposium on Music Information Retrieval (ISMIR)</booktitle>
		<series>ISMIR</series>
		<year>2000</year>
		<url>http://music.ece.drexel.edu/files/Navigation/Publications/ismir2000.pdf</url>
		<author>Y. E. Kim</author>
		<author>Wei Chai</author>
		<author>Ricardo Garcia</author>
		<author>Barry Vercoe</author>
		<authors>
			<first>Y. E.</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Wei</first>
		</authors>
		<authors>
			<last>Chai</last>
		</authors>
		<authors>
			<first>Ricardo</first>
		</authors>
		<authors>
			<last>Garcia</last>
		</authors>
		<authors>
			<first>Barry</first>
		</authors>
		<authors>
			<last>Vercoe</last>
		</authors>
		<pages>3000</pages>
		<abstract>Identifying a musical work from a melodic fragment is a task that
	most people are able to accomplish with relative ease. For some time
	now researchers have worked to give computers this ability as well,
	as it would be the cornerstone of any query-by-humming system. To
	accomplish this, it i s reasonable to study how humans are able to
	perform this task, and to assess what features we use to determine
	melodic similarity. Research has shown that melodic contour is an
	important feature in determining melodic similarity, but it i s also
	clear that rhythmic information is important as well. The goal of
	this research is to explore what variation of contour and rhythmic
	information can result in the most efficient, robust, and scalable
	representation for melody. We intend for this to be the basis of
	a query-by-humming system that will be used to test the validity
	of our proposed representation.</abstract>
		<title>Analysis of a Contour-based Representation for Melody</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f41eea901848a47f052aef2ca95d07cb/sapo</id>
		<tags>Mastery_learning</tags>
		<tags>Musical_instrument_instruction</tags>
		<tags>Violin_--_Instruction_{\&}_study</tags>
		<tags>Games_in_art_education</tags>
		<tags>Computer_assisted_instruction</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>Proceedings of the European Conference on Games Based Learning</booktitle>
		<year>2016</year>
		<url>http://crai.uniquindio.edu.co/login?url=http://search.ebscohost.com/login.aspx?direct=true{\&}db=eue{\&}AN=118287687{\&}lang=es{\&}site=eds-live{\&}scope=site</url>
		<author>Maria1 Margoudi</author>
		<author>Manuel1 Oliveira</author>
		<author>George2 Waddell</author>
		<authors>
			<first>Maria1</first>
		</authors>
		<authors>
			<last>Margoudi</last>
		</authors>
		<authors>
			<first>Manuel1</first>
		</authors>
		<authors>
			<last>Oliveira</last>
		</authors>
		<authors>
			<first>George2</first>
		</authors>
		<authors>
			<last>Waddell</last>
		</authors>
		<volume>1</volume>
		<pages>426--433</pages>
		<abstract>The process of learning and mastering a musical instrument is extremely
	complex and prone to challenges. Students engage with their teacher
	for a relatively limited time and subsequently undertake long periods
	of private study based on the guidance received. This time away from
	formal feedback leaves them prone to the development of bad technical
	and practice habits, to burnout from the solitary nature of practice,
	and to a lack of external motivation to improve. Furthermore, the
	great deal of time and effort required to master a musical instrument
	amplifies the effects of inefficient practice, and the highly complex
	nature of the material being learned can both lead to subjective
	and vague and the belief in the need for excessively long hours of
	repetitive practice to acquire technical skills. These factors can
	have negative consequences for the learner, ranging from frustration
	stemming from lack of progress, high abandonment rates, and psychological
	and physiological health problems. Game-based learning solutions
	are well-suited to address the challenges experienced by student,
	capitalizing on constant advances in technology such as the use of
	immersive or embodied controllers and interfaces that use gesture
	recognition as input. This paper reviews the current state-of-the-art
	in the use of game-based learning practices and tools (e.g. serious
	games) in music learning, taking the violin as a case study. It finds
	that, while progress is being made, relatively little has been done
	to leverage game-based learning to improve musicians' experience.
	The paper will outline the requirements and pitfalls associated to
	the culture of high performance and mainstream music students and
	conclude with a series of recommendations for those developing-game
	based learning solutions. ABSTRACT FROM AUTHOR</abstract>
		<title>Game-Based Learning of Musical Instruments: A Review and Recommendations.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c828705eba2f44a9a1321b907aa7ee5d/sapo</id>
		<tags>Emotion/mood_recognition</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>3</count>
		<booktitle>Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010, Utrecht, Netherlands, August 9-13, 2010</booktitle>
		<publisher>International Society for Music Information Retrieval</publisher>
		<year>2010</year>
		<url>http://ismir2010.ismir.net/proceedings/ismir2010-45.pdf</url>
		<author>Youngmoo E. Kim</author>
		<author>Erik M. Schmidt</author>
		<author>Raymond Migneco</author>
		<author>Brandon G. Morton</author>
		<author>Patrick Richardson</author>
		<author>Jeffrey J. Scott</author>
		<author>Jacquelin A. Speck</author>
		<author>Douglas Turnbull</author>
		<authors>
			<first>Youngmoo E.</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Erik M.</first>
		</authors>
		<authors>
			<last>Schmidt</last>
		</authors>
		<authors>
			<first>Raymond</first>
		</authors>
		<authors>
			<last>Migneco</last>
		</authors>
		<authors>
			<first>Brandon G.</first>
		</authors>
		<authors>
			<last>Morton</last>
		</authors>
		<authors>
			<first>Patrick</first>
		</authors>
		<authors>
			<last>Richardson</last>
		</authors>
		<authors>
			<first>Jeffrey J.</first>
		</authors>
		<authors>
			<last>Scott</last>
		</authors>
		<authors>
			<first>Jacquelin A.</first>
		</authors>
		<authors>
			<last>Speck</last>
		</authors>
		<authors>
			<first>Douglas</first>
		</authors>
		<authors>
			<last>Turnbull</last>
		</authors>
		<editor>J. Stephen Downie</editor>
		<editor>Remco C. Veltkamp</editor>
		<editors>
			<first>Douglas</first>
		</editors>
		<editors>
			<last>Turnbull</last>
		</editors>
		<editors>
			<first>Douglas</first>
		</editors>
		<editors>
			<last>Turnbull</last>
		</editors>
		<pages>255--266</pages>
		<bibsource>dblp computer science bibliography, https://dblp.org</bibsource>
		<title>State of the Art Report: Music Emotion Recognition: A State of the Art Review</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/244aeb8b49d3bc76d317d544e9f8231fb/sapo</id>
		<tags>voice_separation</tags>
		<tags>auditory_streaming</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>Int. Conf. on Music Perception and Cognition (ICMPC)</booktitle>
		<series>Int. Conf. on Music Perception and Cognition (ICMPC)</series>
		<year>2006</year>
		<url></url>
		<author>Emilios Cambouropoulos</author>
		<authors>
			<first>Emilios</first>
		</authors>
		<authors>
			<last>Cambouropoulos</last>
		</authors>
		<pages>987--997</pages>
		<abstract>The notions of ‘voice', as well as, homophony and polyphony, are thought
	to be well understood by musicians. Listeners are thought to be capable
	of perceiving multiple ‘voices' in music. However, there exists no
	systematic theory that describes how ‘voices' can be identified,
	especially, when polyphonic and homophonic elements are mixed together.
	The paper presents different views of what ‘voice' means and how
	the problem of voice separation can be described systematically,
	with a view to understanding the problem better and developing a
	systematic perceptually-based description of the cognitive task of
	segregating ‘voices' in music. Vague (or even contradicting) treatments
	of this issue will be presented. Elements of a systematic theory
	that can be implemented as a computer program are also proposed.</abstract>
		<title>‘Voice' separation: theoretical, perceptual and computational perspectives</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/285b4190e33cec1f12b7262785f129ced/sapo</id>
		<tags>Multimedia</tags>
		<tags>Signal_processing</tags>
		<tags>Music</tags>
		<tags>rank5</tags>
		<tags>Audio</tags>
		<tags>Expression_communication</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<year>2004</year>
		<url></url>
		<author>Sergio Canazza</author>
		<author>Giovanni De Poli</author>
		<author>Carlo Drioli</author>
		<author>Antonio Rodà</author>
		<author>Alvise Vidolin</author>
		<authors>
			<first>Sergio</first>
		</authors>
		<authors>
			<last>Canazza</last>
		</authors>
		<authors>
			<first>Giovanni</first>
		</authors>
		<authors>
			<last>De Poli</last>
		</authors>
		<authors>
			<first>Carlo</first>
		</authors>
		<authors>
			<last>Drioli</last>
		</authors>
		<authors>
			<first>Antonio</first>
		</authors>
		<authors>
			<last>Rodà</last>
		</authors>
		<authors>
			<first>Alvise</first>
		</authors>
		<authors>
			<last>Vidolin</last>
		</authors>
		<volume>92</volume>
		<number>4</number>
		<pages>686--701</pages>
		<abstract>Expression is an important aspect of music performance. It is the
	added value of a performance and is part of the reason that music
	is interesting to listen to and sounds alive. Understanding and modeling
	expressive content communication is important for many engineering
	applications in information technology. For example, in multimedia
	products, textual information is enriched by means of graphical and
	audio objects. In this paper, we present an original approach to
	modify the expressive content of a performance in a gradual way,
	both at the symbolic and signal levels. To this purpose, we discuss
	a model that applies a smooth morphing among performances with different
	expressive content, adapting the audio expressive character to the
	user's desires. Morphing can be realized with a wide range of graduality
	(from abrupt to very smooth), allowing adaptation of the system to
	different situations. The sound rendering is obtained by interfacing
	the expressiveness model with a dedicated postprocessing environment,
	which allows for the transformation of the event cues. The processing
	is based on the organized control of basic audio effects. Among the
	basic effects used, an original method for the spectral processing
	of audio is introduced.</abstract>
		<journaltitle>Proceedings of the Ieee</journaltitle>
		<doi>10.1109/JPROC.2004.825889</doi>
		<title>Modeling and control of expressiveness in music performance</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/284b3bfc7195620210b2e422dbb802892/sapo</id>
		<tags>TuneRank</tags>
		<tags>PageRank</tags>
		<tags>Melody_extraction</tags>
		<tags>Musical_score</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>Proceedings - 2014 6th International Conference on Intelligent Human-Machine Systems and Cybernetics, IHMSC 2014</booktitle>
		<series>Intelligent Human-Machine Systems and Cybernetics (IHMSC), 2014 Sixth International Conference on</series>
		<publisher>IEEE</publisher>
		<year>2014</year>
		<url></url>
		<author>Hanqing Zhao</author>
		<author>Zengchang Qin</author>
		<authors>
			<first>Hanqing</first>
		</authors>
		<authors>
			<last>Zhao</last>
		</authors>
		<authors>
			<first>Zengchang</first>
		</authors>
		<authors>
			<last>Qin</last>
		</authors>
		<volume>2</volume>
		<pages>176--180</pages>
		<abstract>A musical part is a strand of music played by an individual instrument
	within a larger music work, main melody is a single musical part
	which consists of the most significant melodic elements of multi-part
	musical score. This part is typically related to dissonance between
	musical parts and musical instruments. In this paper, we propose
	a novel model for extracting the main melody from multi-part musical
	scores. This model is referred to as the TuneRank model that has
	the conceptually similar idea of the PageRank model. If each musical
	note can be considered like a web page in the Internet, and the dissonance
	value between two notes is like the quantity of links between two
	web pages. The TuneRank (rank of becoming main melody) of each note
	is calculated using Markov transition probability. This model is
	tested on the ECPK4 database. By comparing to the previous work,
	we find that this note-based model is more effective for processing
	scores containing main melody in multiple parts. Also, the accuracy
	does not change with the increase of the number of parts. In general,
	this model can be used for extracting the single-part main melody
	of digital musical scores.</abstract>
		<doi>10.1109/IHMSC.2014.145</doi>
		<title>Tunerank model for main melody extraction from multi-part musical scores</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/244aa3febc7c9e23149bf29a84a78158a/sapo</id>
		<tags>Real-time</tags>
		<tags>Score-to-audio_alignment</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>2</count>
		<booktitle>Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009, Kobe International Conference Center, Kobe, Japan, October 26-30, 2009</booktitle>
		<publisher>International Society for Music Information Retrieval</publisher>
		<year>2009</year>
		<url>http://ismir2009.ismir.net/proceedings/PS3-18.pdf</url>
		<author>Nicola Montecchio</author>
		<author>Nicola Orio</author>
		<authors>
			<first>Nicola</first>
		</authors>
		<authors>
			<last>Montecchio</last>
		</authors>
		<authors>
			<first>Nicola</first>
		</authors>
		<authors>
			<last>Orio</last>
		</authors>
		<editor>Keiji Hirata</editor>
		<editor>George Tzanetakis</editor>
		<editor>Kazuyoshi Yoshii</editor>
		<editors>
			<first>Nicola</first>
		</editors>
		<editors>
			<last>Orio</last>
		</editors>
		<editors>
			<first>Nicola</first>
		</editors>
		<editors>
			<last>Orio</last>
		</editors>
		<editors>
			<first>Nicola</first>
		</editors>
		<editors>
			<last>Orio</last>
		</editors>
		<pages>495--500</pages>
		<bibsource>dblp computer science bibliography, https://dblp.org</bibsource>
		<title>A Discrete Filter Bank Approach to Audio to Score Matching for Polyphonic Music</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/265f470cf813bb69c6d1a9261e875a55a/sapo</id>
		<tags>Computer_music</tags>
		<tags>Music_performance</tags>
		<tags>Machine_learning</tags>
		<tags>Image_processing</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<publisher>Springer-Verlag</publisher>
		<year>2012</year>
		<url></url>
		<author>Ana Rebelo</author>
		<author>Ichiro Fujinaga</author>
		<author>Filipe Paszkiewicz</author>
		<author>Andre R. S. Marcal</author>
		<author>Carlos Guedes</author>
		<author>Jaime S. Cardoso</author>
		<authors>
			<first>Ana</first>
		</authors>
		<authors>
			<last>Rebelo</last>
		</authors>
		<authors>
			<first>Ichiro</first>
		</authors>
		<authors>
			<last>Fujinaga</last>
		</authors>
		<authors>
			<first>Filipe</first>
		</authors>
		<authors>
			<last>Paszkiewicz</last>
		</authors>
		<authors>
			<first>Andre R. S.</first>
		</authors>
		<authors>
			<last>Marcal</last>
		</authors>
		<authors>
			<first>Carlos</first>
		</authors>
		<authors>
			<last>Guedes</last>
		</authors>
		<authors>
			<first>Jaime S.</first>
		</authors>
		<authors>
			<last>Cardoso</last>
		</authors>
		<volume>1</volume>
		<number>3</number>
		<pages>173--190</pages>
		<abstract>For centuries, music has been shared and remem- bered by two traditions:
	aural transmission and in the form of written documents normally
	called musical scores. Many of these scores exist in the form of
	unpublished manuscripts and hence they are in danger of being lost
	through the nor- mal ravages of time. To preserve the music some
	form of typesetting or, ideally, a computer system that can automat-
	ically decode the symbolic images and create new scores is required.
	Programs analogous to optical character recogni- tion systems called
	optical music recognition (OMR) sys- tems have been under intensive
	development for many years. However, the results to date are far
	from ideal. Each of the proposed methods emphasizes different properties
	and there- fore makes it difficult to effectively evaluate its competitive
	advantages.This article provides an overviewof the literature concerning
	the automatic analysis of images of printed and handwritten musical
	scores. For self-containment and for the benefit of the reader, an
	introduction toOMRprocessing sys- tems precedes the literature overview.
	The following study presents a reference scheme for any researcher
	wanting to compare new OMR algorithms against well-known ones.</abstract>
		<journaltitle>International Journal of Multimedia Information Retrieval</journaltitle>
		<doi>10.1007/s13735-012-0004-6</doi>
		<title>Optical music recognition: state-of-the-art and open issues</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a57a6289b40a91c3fbb6165c6de3f270/sapo</id>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>1</count>
		<booktitle>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</booktitle>
		<series>International Symposium on Computer Music Modeling and Retrieval</series>
		<publisher>Springer, Berlin, Heidelberg</publisher>
		<year>2009</year>
		<url>https://doi.org/10.1007/978-3-642-02518-1{\_}12</url>
		<author>David Rizo</author>
		<author>Kjell Lemström</author>
		<author>José M. Iñesta</author>
		<authors>
			<first>David</first>
		</authors>
		<authors>
			<last>Rizo</last>
		</authors>
		<authors>
			<first>Kjell</first>
		</authors>
		<authors>
			<last>Lemström</last>
		</authors>
		<authors>
			<first>José M.</first>
		</authors>
		<authors>
			<last>Iñesta</last>
		</authors>
		<volume>5493 LNCS</volume>
		<pages>177--195</pages>
		<abstract>Identifying copies or different versions of a same musical work is
	a focal problem in maintaining large music databases. In this paper
	we introduce novel ideas and methods that are applicable to metered,
	symbolically encoded polyphonic music. We show how to represent and
	compare polyphonic music using a tree structure. Moreover, we put
	for trial various comparison methods and observe whether better comparison
	results can be obtained by combining distinct similarity measures.
	Our experiments show that the proposed representation is adequate
	for the task with good quality results and processing times, and
	when combined with other methods it becomes more robust against various
	types of music.</abstract>
		<doi>10.1007/978-3-642-02518-1_12</doi>
		<title>Tree representation in combined polyphonic music comparison</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20ddcc20735990bbcf91f24fc440221a9/nonancourt</id>
		<tags>music,</tags>
		<tags>geography,</tags>
		<tags>social-networks</tags>
		<tags>influence</tags>
		<description></description>
		<date>2019-06-10 14:53:09</date>
		<count>3</count>
		<booktitle>2012 International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2012)</booktitle>
		<publisher>IEEE</publisher>
		<year>2012</year>
		<url>http://dx.doi.org/10.1109/ASONAM.2012.237</url>
		<author>Conrad Lee</author>
		<author>Padraig Cunningham</author>
		<authors>
			<first>Conrad</first>
		</authors>
		<authors>
			<last>Lee</last>
		</authors>
		<authors>
			<first>Padraig</first>
		</authors>
		<authors>
			<last>Cunningham</last>
		</authors>
		<pages>691--695</pages>
		<abstract>The social media website last.fm provides a detailed snapshot of what its
users in hundreds of cities listen to each week. After suitably normalizing
this data, we use it to test three hypotheses related to the geographic flow of
music. The first is that although many of the most popular artists are listened
to around the world, music preferences are closely related to nationality,
language, and geographic location. We find support for this hypothesis, with a
couple of minor, yet interesting, exceptions. Our second hypothesis is that
some cities are consistently early adopters of new music (and early to snub
stale music). To test this hypothesis, we adapt a method previously used to
detect the leadership networks present in flocks of birds. We find empirical
support for the claim that a similar leadership network exists among cities,
and this finding is the main contribution of the paper. Finally, we test the
hypothesis that large cities tend to be ahead of smaller cities-we find only
weak support for this hypothesis.</abstract>
		<isbn>978-1-4673-2497-7</isbn>
		<eprint>1204.2677</eprint>
		<doi>10.1109/ASONAM.2012.237</doi>
		<title>The geographic flow of music</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/221379abf7921ffe5375f925a5e660b42/sapo</id>
		<tags>performance_analysis</tags>
		<tags>midi_quantization</tags>
		<description>Towards Complete Polyphonic Music Transcription: Integrating Multi-Pitch Detection and Rhythm Quantization - IEEE Conference Publication</description>
		<date>2019-08-08 14:39:21</date>
		<count>2</count>
		<booktitle>2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</booktitle>
		<publisher>IEEE</publisher>
		<year>2018</year>
		<url>https://ieeexplore.ieee.org/abstract/document/8461914</url>
		<author>Eita Nakamura</author>
		<author>Emmanouil Benetos</author>
		<author>Kazuyoshi Yoshii</author>
		<author>Simon Dixon</author>
		<authors>
			<first>Eita</first>
		</authors>
		<authors>
			<last>Nakamura</last>
		</authors>
		<authors>
			<first>Emmanouil</first>
		</authors>
		<authors>
			<last>Benetos</last>
		</authors>
		<authors>
			<first>Kazuyoshi</first>
		</authors>
		<authors>
			<last>Yoshii</last>
		</authors>
		<authors>
			<first>Simon</first>
		</authors>
		<authors>
			<last>Dixon</last>
		</authors>
		<pages>101-105</pages>
		<abstract>Most work on automatic transcription produces “piano roll” data with no musical interpretation of the rhythm or pitches. We present a polyphonic transcription method that converts a music audio signal into a human-readable musical score, by integrating multi-pitch detection and rhythm quantization methods. This integration is made difficult by the fact that the multi-pitch detection produces erroneous notes such as extra notes and introduces timing errors that are added to temporal deviations due to musical expression. Thus, we propose a rhythm quantization method that can remove extra notes by extending the metrical hidden Markov model and optimize the model parameters. We also improve the note-tracking process of multi-pitch detection by refining the treatment of repeated notes and adjustment of onset times. Finally, we propose evaluation measures for transcribed scores. Systematic evaluations on commonly used classical piano data show that these treatments improve the performance of transcription, which can be used as benchmarks for further studies.</abstract>
		<issn>2379-190X</issn>
		<doi>10.1109/ICASSP.2018.8461914</doi>
		<title>Towards Complete Polyphonic Music Transcription: Integrating Multi-Pitch Detection and Rhythm Quantization</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d49179491ec1c718beb2ceeab4b51c25/cheriell</id>
		<tags>AMT</tags>
		<description>Automatic music transcription: challenges and future directions | SpringerLink</description>
		<date>2019-10-02 22:08:36</date>
		<count>3</count>
		<journal>Journal of Intelligent Information Systems</journal>
		<year>2013</year>
		<url>https://doi.org/10.1007/s10844-013-0258-3</url>
		<author>Emmanouil Benetos</author>
		<author>Simon Dixon</author>
		<author>Dimitrios Giannoulis</author>
		<author>Holger Kirchhoff</author>
		<author>Anssi Klapuri</author>
		<authors>
			<first>Emmanouil</first>
		</authors>
		<authors>
			<last>Benetos</last>
		</authors>
		<authors>
			<first>Simon</first>
		</authors>
		<authors>
			<last>Dixon</last>
		</authors>
		<authors>
			<first>Dimitrios</first>
		</authors>
		<authors>
			<last>Giannoulis</last>
		</authors>
		<authors>
			<first>Holger</first>
		</authors>
		<authors>
			<last>Kirchhoff</last>
		</authors>
		<authors>
			<first>Anssi</first>
		</authors>
		<authors>
			<last>Klapuri</last>
		</authors>
		<volume>41</volume>
		<number>3</number>
		<pages>407--434</pages>
		<abstract>Automatic music transcription is considered by many to be a key enabling technology in music signal processing. However, the performance of transcription systems is still significantly below that of a human expert, and accuracies reported in recent years seem to have reached a limit, although the field is still very active. In this paper we analyse limitations of current methods and identify promising directions for future research. Current transcription methods use general purpose models which are unable to capture the rich diversity found in music signals. One way to overcome the limited performance of transcription systems is to tailor algorithms to specific use-cases. Semi-automatic approaches are another way of achieving a more reliable transcription. Also, the wealth of musical scores and corresponding audio data now available are a rich potential source of training data, via forced alignment of audio to scores, but large scale utilisation of such data has yet to be attempted. Other promising approaches include the integration of information from multiple algorithms and different musical aspects.</abstract>
		<issn>1573-7675</issn>
		<doi>10.1007/s10844-013-0258-3</doi>
		<title>Automatic music transcription: challenges and future directions</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ab893897310871df0be31ca4b3eff7af/sapo</id>
		<tags>performance_analysis</tags>
		<description>Understanding features and distance functions for music sequence alignment</description>
		<date>2019-11-14 10:06:29</date>
		<count>2</count>
		<booktitle>Proceedings of International Society for Music Information Retrieval Conf. (ISMIR</booktitle>
		<publisher>ISMIR</publisher>
		<year>2010</year>
		<url>http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.649.311</url>
		<author>Özgür Izmirli</author>
		<author>Roger B. Dannenberg</author>
		<authors>
			<first>Özgür</first>
		</authors>
		<authors>
			<last>Izmirli</last>
		</authors>
		<authors>
			<first>Roger B.</first>
		</authors>
		<authors>
			<last>Dannenberg</last>
		</authors>
		<editor>J. Stephen Downie</editor>
		<editor>Remco C. Veltkamp</editor>
		<editors>
			<first>Roger B.</first>
		</editors>
		<editors>
			<last>Dannenberg</last>
		</editors>
		<editors>
			<first>Roger B.</first>
		</editors>
		<editors>
			<last>Dannenberg</last>
		</editors>
		<pages>411--416</pages>
		<abstract>We investigate the problem of matching symbolic representations directly to audio based representations for applications that use data from both domains. One such application is score alignment, which aligns a sequence of frames based on features such as chroma vectors and distance functions such as Euclidean distance. Good representations are critical, yet current systems use ad hoc constructions such as the chromagram that have been shown to work quite well. We investigate ways to learn chromagram-like representations that optimize the classification of “matching ” vs. “non-matching ” frame pairs of audio and MIDI. New representations learned automatically from examples not only perform better than the chromagram representation but they also reveal interesting projection structures that differ distinctly from the traditional chromagram. 1.</abstract>
		<title>Understanding Features and Distance Functions for Music Sequence Alignment.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f2296ede67e47b7fed08a0b45650cff6/bauerc</id>
		<tags>myown</tags>
		<tags>country</tags>
		<tags>music</tags>
		<tags>rural</tags>
		<tags>culture</tags>
		<tags>urban</tags>
		<tags>region</tags>
		<tags>visualisation</tags>
		<description>This research is supported by the Austrian Science Fund (FWF): V579.</description>
		<date>2019-10-26 22:35:32</date>
		<count>2</count>
		<booktitle>Proceedings of the 27th ACM International Conference on Multimedia</booktitle>
		<series>ACM MM 2019</series>
		<publisher>ACM</publisher>
		<address>Nice, france</address>
		<year>2019</year>
		<url>http://dblp.uni-trier.de/db/conf/mm/mm2019.html#0001SAW19</url>
		<author>Christine Bauer</author>
		<author>Markus Schedl</author>
		<author>Vera Angerer</author>
		<author>Stefan Wegenkittl</author>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<authors>
			<first>Vera</first>
		</authors>
		<authors>
			<last>Angerer</last>
		</authors>
		<authors>
			<first>Stefan</first>
		</authors>
		<authors>
			<last>Wegenkittl</last>
		</authors>
		<editor>Laurent Amsaleg</editor>
		<editor>Benoit Huet</editor>
		<editor>Martha Larson</editor>
		<editor>Guillaume Gravier</editor>
		<editor>Hayley Hung</editor>
		<editor>Chong-Wah Ngo</editor>
		<editor>Wei Tsang Ooi</editor>
		<editors>
			<first>Stefan</first>
		</editors>
		<editors>
			<last>Wegenkittl</last>
		</editors>
		<editors>
			<first>Stefan</first>
		</editors>
		<editors>
			<last>Wegenkittl</last>
		</editors>
		<editors>
			<first>Stefan</first>
		</editors>
		<editors>
			<last>Wegenkittl</last>
		</editors>
		<editors>
			<first>Stefan</first>
		</editors>
		<editors>
			<last>Wegenkittl</last>
		</editors>
		<editors>
			<first>Stefan</first>
		</editors>
		<editors>
			<last>Wegenkittl</last>
		</editors>
		<editors>
			<first>Stefan</first>
		</editors>
		<editors>
			<last>Wegenkittl</last>
		</editors>
		<editors>
			<first>Stefan</first>
		</editors>
		<editors>
			<last>Wegenkittl</last>
		</editors>
		<pages>1044-1046</pages>
		<abstract>We present a browsing interface that allows for an audiovisual exploration of regional music taste around the world. We exploit a total of 10,758,121 geolocated tweets about music. The web-based geo-aware visualization and auralization called Tastalyzer enables exploring and analyzing music taste on a fine-grained geographical level, such as (i) comparing rural and corresponding urban music taste within an agglomeration (city) or (ii) comparing the music taste in a target region (agglomeration) to the taste of the country the region is part of and (iii) to the global music taste.</abstract>
		<isbn>978-1-4503-6889-6</isbn>
		<language>English</language>
		<doi>10.1145/3343031.3350585</doi>
		<title>Tastalyzer: Audiovisual Exploration of Urban and Rural Variations in Music Taste.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2454da6199c95a8b07a8d4b841da1fd59/ijtsrd</id>
		<tags>symptom</tags>
		<tags>nerve</tags>
		<tags>roots</tags>
		<tags>spine</tags>
		<tags>osteochondrosis</tags>
		<tags>Syndrome</tags>
		<description></description>
		<date>2020-12-18 11:14:03</date>
		<count>1</count>
		<journal>International Journal of Trend in Scientific Research and Development</journal>
		<year>2020</year>
		<url>https://www.ijtsrd.com/medicine/other/37953/peculiarities-of-neuromyographic-indicators-in-adolescent-musicians-with-pain-syndrome-of-the-cervicothoracic-spine/xamrakulova-faraxnoz-muradovna</url>
		<author>Xamrakulova Faraxnoz Muradovna</author>
		<authors>
			<first>Xamrakulova Faraxnoz</first>
		</authors>
		<authors>
			<last>Muradovna</last>
		</authors>
		<volume>4</volume>
		<number>6</number>
		<pages>107-110</pages>
		<abstract>The article deals with the clinical manifestations of juvenile spinal osteochondrosis. The main causes of the development of the disease. Pain syndrome, a typical symptom of adult patients, is rare in adolescents. Discomfort and quick fatigue of the back is often noted, which disappears after rest and does not hold the parents attention. Neurological disorders adolescents complain of headache, dizziness, increased general fatigue. Reflected pain and numbness of the limbs are almost never encountered. And what is the approach to treatment. Xamrakulova Faraxnoz Muradovna "Peculiarities of Neuromyographic Indicators in Adolescent Musicians with Pain Syndrome of the Cervicothoracic Spine" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Special Issue | Modern Trends in Scientific Research and Development, Case of Asia , October 2020, URL: https://www.ijtsrd.com/papers/ijtsrd37953.pdf Paper Url :https://www.ijtsrd.com/medicine/other/37953/peculiarities-of-neuromyographic-indicators-in-adolescent-musicians-with-pain-syndrome-of-the-cervicothoracic-spine/xamrakulova-faraxnoz-muradovna</abstract>
		<language>English</language>
		<issn>2456-6470</issn>
		<title>Peculiarities of Neuromyographic Indicators in Adolescent Musicians with Pain Syndrome of the Cervicothoracic Spine</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d462df8a286786995053e307316e5d7d/brusilovsky</id>
		<tags>carousel</tags>
		<tags>bandits</tags>
		<description>Carousel Personalization in Music Streaming Apps with Contextual Bandits | Fourteenth ACM Conference on Recommender Systems</description>
		<date>2021-07-09 23:08:25</date>
		<count>3</count>
		<booktitle>Fourteenth ACM Conference on Recommender Systems</booktitle>
		<publisher>ACM</publisher>
		<year>2020</year>
		<url>https://doi.org/10.1145%2F3383313.3412217</url>
		<author>Walid Bendada</author>
		<author>Guillaume Salha</author>
		<author>Théo Bontempelli</author>
		<authors>
			<first>Walid</first>
		</authors>
		<authors>
			<last>Bendada</last>
		</authors>
		<authors>
			<first>Guillaume</first>
		</authors>
		<authors>
			<last>Salha</last>
		</authors>
		<authors>
			<first>Théo</first>
		</authors>
		<authors>
			<last>Bontempelli</last>
		</authors>
		<pages>420-425</pages>
		<abstract>Media services providers, such as music streaming platforms, frequently leverage swipeable carousels to recommend personalized content to their users. However, selecting the most relevant items (albums, artists, playlists...) to display in these carousels is a challenging task, as items are numerous and as users have different preferences. In this paper, we model carousel personalization as a contextual multi-armed bandit problem with multiple plays, stochastic arm display and delayed batch feedback. We empirically show the effectiveness of our framework at capturing characteristics of real-world carousels by addressing a large-scale playlist recommendation task on a global music streaming mobile app. Along with this paper, we publicly release industrial data from our experiments, as well as an open-source environment to simulate comparable carousel personalization learning problems.</abstract>
		<doi>10.1145/3383313.3412217</doi>
		<title>Carousel Personalization in Music Streaming Apps with Contextual Bandits</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21f38658184f62d78341965aaa1bbf9f2/vaniave</id>
		<tags>music</tags>
		<tags>STEAM</tags>
		<tags>Mathematics</tags>
		<tags>education</tags>
		<tags>mathematical_modeling</tags>
		<description></description>
		<date>2021-04-21 11:45:34</date>
		<count>1</count>
		<journal>LCMAT-UENF</journal>
		<publisher>UENF</publisher>
		<year>2004</year>
		<url>http://doi.org/10.13140/2.1.2849.5360</url>
		<author>Vania V. Estrela</author>
		<author>Vania Vieira Estrela</author>
		<author>Mikhail VISHNEVSKI</author>
		<author>Talmise Franco Dos Santos</author>
		<authors>
			<first>Vania V.</first>
		</authors>
		<authors>
			<last>Estrela</last>
		</authors>
		<authors>
			<first>Vania Vieira</first>
		</authors>
		<authors>
			<last>Estrela</last>
		</authors>
		<authors>
			<first>Mikhail</first>
		</authors>
		<authors>
			<last>VISHNEVSKI</last>
		</authors>
		<authors>
			<first>Talmise Franco Dos</first>
		</authors>
		<authors>
			<last>Santos</last>
		</authors>
		<abstract>Geralmente, ao comentar sobre música, dificilmente pensamos na possibilidade da interatividade das ciências exatas como a física e a matemática, necessárias para entender as propriedades e os detalhes que ocorrem na música.(...) A ligação da música com a ciência teve grande importância no período clássicono séc.VI a.C. por Pitágoras, onde houve a descoberta das razões matemáticas por trásdos sons depois de observado o comprimento dos martelos dos ferreiros.A autoria de Pitágoras em sua obra é dado a descoberta do intervalo de umaoitava como sendo referente a uma relação de freqüência (altura: grave e agudo). Osseguidores de Pitágoras aplicaram essas razões ao comprimento de um instrumento chamado monocórdio. Pitágoras buscava relações de comprimentos/razões de números inteiros que produzissem determinados intervalos sonoros e, portanto, foram capazes de determinar matematicamente a entonação (bom tom) de todo sistema musical. (...)A música tornou-se uma extensão da matemática, bem como a arte. A matemática e as descobertas musicais de Pitágoras foram, desta forma, de grande influência no desenvolvimento da música através da idade média na Europa. Essa monografia despertará o interesse de muitos leitores para a obtenção deum melhor conhecimento e enriquecimento na aprendizagem, quando raramentepensamos na análise científica com a arte musical.Conhecer essas influências matemáticas é, antes de tudo, conhecer a essência da própria música.</abstract>
		<language>Portuguese</language>
		<doi>10.13140/2.1.2849.5360</doi>
		<title>Matematica, Musica e Educacao</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c22233a62afb44cdc2f1272dcb2d3119/bauerc</id>
		<tags>fairness</tags>
		<tags>myown</tags>
		<tags>streaming</tags>
		<tags>music</tags>
		<tags>artists</tags>
		<tags>recsys</tags>
		<tags>fair</tags>
		<description></description>
		<date>2021-09-02 00:31:03</date>
		<count>1</count>
		<booktitle>Human-Computer Interaction – INTERACT 2021</booktitle>
		<series>Lecture Notes in Computer Science</series>
		<publisher>Springer International Publishing</publisher>
		<address>Cham, Germany</address>
		<year>2021</year>
		<url>https://doi.org/10.1007/978-3-030-85616-8_33</url>
		<author>Andrés Ferraro</author>
		<author>Xavier Serra</author>
		<author>Christine Bauer</author>
		<authors>
			<first>Andrés</first>
		</authors>
		<authors>
			<last>Ferraro</last>
		</authors>
		<authors>
			<first>Xavier</first>
		</authors>
		<authors>
			<last>Serra</last>
		</authors>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<editor>Carmelo Ardito</editor>
		<editor>Rosa Lanzilotti</editor>
		<editor>Alessio Malizia</editor>
		<editor>Helen Petrie</editor>
		<editor>Antonio Piccinno</editor>
		<editor>Giuseppe Desolda</editor>
		<editor>Kori Inkpen</editor>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<volume>12933</volume>
		<pages>562-584</pages>
		<abstract>Music streaming platforms are currently among the main sources of music consumption, and the embedded recommender systems significantly influence what the users consume. There is an increasing interest to ensure that those platforms and systems are fair. Yet, we first need to understand what fairness means in such a context. Although artists are the main content providers for music platforms, there is a research gap concerning the artists' perspective. To fill this gap, we conducted interviews with music artists to understand how they are affected by current platforms and what improvements they deem necessary. Using a Qualitative Content Analysis, we identify the aspects that the artists consider relevant for fair platforms. In this paper, we discuss the following aspects derived from the interviews: fragmented presentation, reaching an audience, transparency, influencing users' listening behavior, popularity bias, artists' repertoire size, quotas for local music, gender balance, and new music. For some topics, our findings do not indicate a clear direction about the best way how music platforms should act and function; for other topics, though, there is a clear consensus among our interviewees: for these, the artists have a clear idea of the actions that should be taken so that music platforms will be fair also for the artists.</abstract>
		<isbn>978-3-030-85615-1
978-3-030-85616-8</isbn>
		<doi>10.1007/978-3-030-85616-8_33</doi>
		<title>What is fair? Exploring the artists' perspective on the fairness of music streaming platforms</title>
		<pubtype>inbook</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27294886382520a3f7eefcf633f7edff5/ls_leimeister</id>
		<tags>blackbox</tags>
		<tags>pub_peb</tags>
		<tags>artificial_intelligence</tags>
		<tags>pub_cen</tags>
		<tags>pub_eel</tags>
		<tags>hybrind_intelligence</tags>
		<tags>music_industry</tags>
		<tags>itegpub</tags>
		<description></description>
		<date>2021-05-22 13:45:26</date>
		<count>1</count>
		<booktitle>Hawaii International Conference on System Sciences (HICCS)</booktitle>
		<year>2021</year>
		<url>http://pubs.wi-kassel.de/wp-content/uploads/2021/05/JML_827.pdf</url>
		<author>Edona Elshan</author>
		<author>Christian Engel</author>
		<author>Philipp Ebel</author>
		<authors>
			<first>Edona</first>
		</authors>
		<authors>
			<last>Elshan</last>
		</authors>
		<authors>
			<first>Christian</first>
		</authors>
		<authors>
			<last>Engel</last>
		</authors>
		<authors>
			<first>Philipp</first>
		</authors>
		<authors>
			<last>Ebel</last>
		</authors>
		<abstract>The ever-increasing complexity of the music industry and the intensified resentment of artists towards collecting societies call for a transformation and a change of behavior within the music ecosystem. This article introduces a hybrid intelligence system, that ameliorates the current situation by combining the intelligence of humans and machines. This study proposes design requirements for hybrid intelligence systems in the music industry. Using a design science research approach, we identify design requirements both inductively from expert interviews and deductively from theory and present a first prototypical instantiation of a respective hybrid intelligence system. Overall, this shall enrich the body of knowledge of hybrid intelligence research by transferring its concepts into a new context. Furthermore, the identified design requirements shall serve as a foundation for researchers and practitioners to further explore and design hybrid intelligence in the music industry and beyond.</abstract>
		<doi>10.24251/HICSS.2021.671</doi>
		<title>Opening the Black Box of Music Royalties with the Help of Hybrid Intelligence</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29ba6087ce0406be5ec3f3c9abf352659/simonha94</id>
		<tags>imported</tags>
		<tags>musicsearch</tags>
		<tags>uncovr</tags>
		<tags>mir</tags>
		<description></description>
		<date>2021-10-15 08:46:33</date>
		<count>2</count>
		<journal>2013 IEEE International Workshop on Multimedia Signal Processing, MMSP 2013</journal>
		<year>2013</year>
		<url></url>
		<author>Michele Buccoli</author>
		<author>Massimiliano Zanoni</author>
		<author>Augusto Sarti</author>
		<author>Stefano Tubaro</author>
		<authors>
			<first>Michele</first>
		</authors>
		<authors>
			<last>Buccoli</last>
		</authors>
		<authors>
			<first>Massimiliano</first>
		</authors>
		<authors>
			<last>Zanoni</last>
		</authors>
		<authors>
			<first>Augusto</first>
		</authors>
		<authors>
			<last>Sarti</last>
		</authors>
		<authors>
			<first>Stefano</first>
		</authors>
		<authors>
			<last>Tubaro</last>
		</authors>
		<pages>254-259</pages>
		<abstract>Search and retrieval of songs from a large music repository usually relies on added meta-information (e.g., title, artist or musical genre); or on specific descriptors (e.g. mood); or on categorical music descriptors; none of which can specify the desired intensity. In this work, we propose an early example of semantic text-based music search engine. The semantic description takes into account emotional and non-emotional musical aspects. The method also includes a query-by-similarity search approach performed using semantic cues. We model both concepts and musical content in dimensional spaces that are suitable for carrying intensity information on the descriptors. We process the semantic query with a Natural Language parser to capture only the relevant words and qualifiers. We rely on Bayesian Decision theory to model concepts and songs as probability distributions. The resulted ranked list of songs are produced through a posterior probability model. A prototype of the system has been proposed to 53 subjects for evaluation, with good ratings on performance, usefulness and potential. © 2013 IEEE.</abstract>
		<doi>10.1109/MMSP.2013.6659297</doi>
		<title>A music search engine based on semantic text-based query</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29b2c51ffe670236e1a59d5ab86e2ce40/sapo</id>
		<tags>phdthesis</tags>
		<tags>automatic_transcription</tags>
		<description>[1906.08512] Adversarial Learning for Improved Onsets and Frames Music Transcription</description>
		<date>2021-01-29 12:31:04</date>
		<count>3</count>
		<year>2019</year>
		<url>http://arxiv.org/abs/1906.08512</url>
		<author>Jong Wook Kim</author>
		<author>Juan Pablo Bello</author>
		<authors>
			<first>Jong Wook</first>
		</authors>
		<authors>
			<last>Kim</last>
		</authors>
		<authors>
			<first>Juan Pablo</first>
		</authors>
		<authors>
			<last>Bello</last>
		</authors>
		<abstract>Automatic music transcription is considered to be one of the hardest problems
in music information retrieval, yet recent deep learning approaches have
achieved substantial improvements on transcription performance. These
approaches commonly employ supervised learning models that predict various
time-frequency representations, by minimizing element-wise losses such as the
cross entropy function. However, applying the loss in this manner assumes
conditional independence of each label given the input, and thus cannot
accurately express inter-label dependencies. To address this issue, we
introduce an adversarial training scheme that operates directly on the
time-frequency representations and makes the output distribution closer to the
ground-truth. Through adversarial learning, we achieve a consistent improvement
in both frame-level and note-level metrics over Onsets and Frames, a
state-of-the-art music transcription model. Our results show that adversarial
learning can significantly reduce the error rate while increasing the
confidence of the model estimations. Our approach is generic and applicable to
any transcription model based on multi-label predictions, which are very common
in music signal analysis.</abstract>
		<title>Adversarial Learning for Improved Onsets and Frames Music Transcription</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/258110c759b8f87efb8b8e790250578ee/sapo</id>
		<tags>phdthesis</tags>
		<tags>mia</tags>
		<description>The Influence of Room Acoustics on Solo Music Performance: An Experimental Study | Semantic Scholar</description>
		<date>2021-12-17 18:04:58</date>
		<count>3</count>
		<journal>Psychomusicology: Music, Mind and Brain</journal>
		<year>2015</year>
		<url></url>
		<author>Zora Schärer Kalkandjiev</author>
		<author>Stefan Weinzierl</author>
		<authors>
			<first>Zora Schärer</first>
		</authors>
		<authors>
			<last>Kalkandjiev</last>
		</authors>
		<authors>
			<first>Stefan</first>
		</authors>
		<authors>
			<last>Weinzierl</last>
		</authors>
		<volume>25</volume>
		<pages>195</pages>
		<title>The Influence of Room Acoustics on Solo Music Performance: An Experimental Study</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d332572b2bf2b6b6edfd3ce6c233f676/hanappe</id>
		<tags>imported</tags>
		<tags>sonycsl</tags>
		<description></description>
		<date>2006-07-13 17:56:29</date>
		<count>1</count>
		<journal>Journal of New Music Research</journal>
		<year>2005</year>
		<url></url>
		<author>J.-J. Aucouturier</author>
		<author>F. Pachet</author>
		<authors>
			<first>J.-J.</first>
		</authors>
		<authors>
			<last>Aucouturier</last>
		</authors>
		<authors>
			<first>F.</first>
		</authors>
		<authors>
			<last>Pachet</last>
		</authors>
		<title>Jamming With Plunderphonics: Interactive Contatenative Synthesis of Music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27576a5bd253f3fc8fccef6c3295983a4/hanappe</id>
		<tags>imported</tags>
		<description></description>
		<date>2006-07-13 17:11:36</date>
		<count>1</count>
		<booktitle>Proceedings of the 2nd International Symposium on Music Information Retrieval</booktitle>
		<year>2001</year>
		<url></url>
		<author>J. Reiss</author>
		<author>J.-J. Aucouturier</author>
		<author>M. Sandler</author>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Reiss</last>
		</authors>
		<authors>
			<first>J.-J.</first>
		</authors>
		<authors>
			<last>Aucouturier</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Sandler</last>
		</authors>
		<title>Efficient Multidimentional Searching Routines for Music Information retrieval</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/275ae7ab288c5badf12a58284e3487784/hanappe</id>
		<tags>imported</tags>
		<description></description>
		<date>2006-07-13 17:11:36</date>
		<count>1</count>
		<series>Music Technology Series</series>
		<publisher>Focal Press</publisher>
		<year>2001</year>
		<url></url>
		<author>E. R. Miranda</author>
		<authors>
			<first>E. R.</first>
		</authors>
		<authors>
			<last>Miranda</last>
		</authors>
		<title>Composing Music with Computers</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bae07f59e77682916459016a8980ec38/hanappe</id>
		<tags>imported</tags>
		<description></description>
		<date>2006-07-13 17:11:36</date>
		<count>1</count>
		<booktitle>Proceedings of the Workshop on Artificial Life Models for Musical Applications - ECAL 2001</booktitle>
		<year>2001</year>
		<url></url>
		<author>E. R. Miranda</author>
		<authors>
			<first>E. R.</first>
		</authors>
		<authors>
			<last>Miranda</last>
		</authors>
		<title>Evolving Cellular Automata Music: From Sound Synthesis to Composition</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25c3366792285178f022d2aa5f25427a0/yaxu</id>
		<tags>music</tags>
		<tags>language</tags>
		<description></description>
		<date>2007-05-05 14:31:03</date>
		<count>4</count>
		<booktitle>Proceedings of PPIG05</booktitle>
		<publisher>University of Sussex</publisher>
		<year>2005</year>
		<url></url>
		<author>Alan Blackwell</author>
		<author>Nick Collins</author>
		<authors>
			<first>Alan</first>
		</authors>
		<authors>
			<last>Blackwell</last>
		</authors>
		<authors>
			<first>Nick</first>
		</authors>
		<authors>
			<last>Collins</last>
		</authors>
		<title>The Programming Language as a Musical Instrument</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/239879ad8292e1b8f577734d5f10a47a9/yaxu</id>
		<tags>toplap</tags>
		<description></description>
		<date>2007-05-05 14:31:03</date>
		<count>1</count>
		<year>2006</year>
		<url></url>
		<author>Nick Collins</author>
		<authors>
			<first>Nick</first>
		</authors>
		<authors>
			<last>Collins</last>
		</authors>
		<title>Towards Autonomous Agents for Live Computer Music: Realtime Machine Listening and Interactive Music Systems</title>
		<pubtype>phdthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26cd205025566f09b73a73997f6e47a60/yaxu</id>
		<tags>music</tags>
		<tags>tabla</tags>
		<tags>msc</tags>
		<description></description>
		<date>2007-05-05 14:31:03</date>
		<count>1</count>
		<booktitle>Grove music online (accessed March 2007)</booktitle>
		<publisher>Oxford University Press</publisher>
		<year>2007</year>
		<url></url>
		<author>Sen Devdan</author>
		<authors>
			<first>Sen</first>
		</authors>
		<authors>
			<last>Devdan</last>
		</authors>
		<editor>L. Macy</editor>
		<editors>
			<first>Sen</first>
		</editors>
		<editors>
			<last>Devdan</last>
		</editors>
		<title>Tabla - technique</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ffa3b8d529ede65b8a4cde66e246810e/siggi</id>
		<tags>datamining</tags>
		<description></description>
		<date>2007-09-16 02:41:34</date>
		<count>7</count>
		<booktitle>Proceedings of the Seventh International Conference on Music Information Retrieval (ISMIR'06)</booktitle>
		<address>Victoria, Canada</address>
		<year>2006</year>
		<url>http://www.cp.jku.at/research/papers/Schedl_etal_ISMIR_2006.pdf</url>
		<author>Markus Schedl</author>
		<author>Tim Pohle</author>
		<author>Peter Knees</author>
		<author>Gerhard Widmer</author>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<authors>
			<first>Tim</first>
		</authors>
		<authors>
			<last>Pohle</last>
		</authors>
		<authors>
			<first>Peter</first>
		</authors>
		<authors>
			<last>Knees</last>
		</authors>
		<authors>
			<first>Gerhard</first>
		</authors>
		<authors>
			<last>Widmer</last>
		</authors>
		<title>Assigning and Visualizing Music Genres by Web-based Co-Occurrence Analysis</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b01de186bb17417ebeff479c10cb7fe6/ocelma</id>
		<tags>PhD</tags>
		<description>PhD</description>
		<date>2009-01-08 17:06:55</date>
		<count>5</count>
		<booktitle>Proceedings of the 8th International Conference on Music Information Retrieval</booktitle>
		<address>Vienna, Austria</address>
		<year>2007</year>
		<url></url>
		<author>Gijs Geleijnse</author>
		<author>Markus Schedl</author>
		<author>Peter Knees</author>
		<authors>
			<first>Gijs</first>
		</authors>
		<authors>
			<last>Geleijnse</last>
		</authors>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<authors>
			<first>Peter</first>
		</authors>
		<authors>
			<last>Knees</last>
		</authors>
		<title>The Quest for Ground Truth in Musical Artist Tagging in the Social Web Era</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2de209574cb7dbd2e9923b768d04b9bbf/andre@ismll</id>
		<tags>music</tags>
		<tags>symbolic</tags>
		<description>dblp</description>
		<date>2009-12-28</date>
		<count>2</count>
		<booktitle>ISMIR</booktitle>
		<year>2008</year>
		<url>http://dblp.uni-trier.de/db/conf/ismir/ismir2008.html#Knopke08</url>
		<author>Ian Knopke</author>
		<authors>
			<first>Ian</first>
		</authors>
		<authors>
			<last>Knopke</last>
		</authors>
		<editor>Juan Pablo Bello</editor>
		<editor>Elaine Chew</editor>
		<editor>Douglas Turnbull</editor>
		<editors>
			<first>Ian</first>
		</editors>
		<editors>
			<last>Knopke</last>
		</editors>
		<editors>
			<first>Ian</first>
		</editors>
		<editors>
			<last>Knopke</last>
		</editors>
		<editors>
			<first>Ian</first>
		</editors>
		<editors>
			<last>Knopke</last>
		</editors>
		<pages>147-152</pages>
		<isbn>978-0-615-24849-3</isbn>
		<title>The PerlHumdrum and PerlLilypond Toolkits for Symbolic Music Information Retrieval.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20decff80be6541ee85a94c9bc9945d00/andre@ismll</id>
		<tags>music_genre</tags>
		<tags>temporal_features</tags>
		<tags>pitch</tags>
		<tags>genre</tags>
		<tags>beat</tags>
		<tags>sigir</tags>
		<tags>feature_selection</tags>
		<tags>classification</tags>
		<description>dblp</description>
		<date>2009-12-28</date>
		<count>2</count>
		<booktitle>ISMIR</booktitle>
		<year>2008</year>
		<url>http://dblp.uni-trier.de/db/conf/ismir/ismir2008.html#DoraisamyGNSU08</url>
		<author>Shyamala Doraisamy</author>
		<author>Shahram Golzari</author>
		<author>Noris Mohd. Norowi</author>
		<author>Md Nasir Sulaiman</author>
		<author>Nur Izura Udzir</author>
		<authors>
			<first>Shyamala</first>
		</authors>
		<authors>
			<last>Doraisamy</last>
		</authors>
		<authors>
			<first>Shahram</first>
		</authors>
		<authors>
			<last>Golzari</last>
		</authors>
		<authors>
			<first>Noris Mohd.</first>
		</authors>
		<authors>
			<last>Norowi</last>
		</authors>
		<authors>
			<first>Md Nasir</first>
		</authors>
		<authors>
			<last>Sulaiman</last>
		</authors>
		<authors>
			<first>Nur Izura</first>
		</authors>
		<authors>
			<last>Udzir</last>
		</authors>
		<editor>Juan Pablo Bello</editor>
		<editor>Elaine Chew</editor>
		<editor>Douglas Turnbull</editor>
		<editors>
			<first>Nur Izura</first>
		</editors>
		<editors>
			<last>Udzir</last>
		</editors>
		<editors>
			<first>Nur Izura</first>
		</editors>
		<editors>
			<last>Udzir</last>
		</editors>
		<editors>
			<first>Nur Izura</first>
		</editors>
		<editors>
			<last>Udzir</last>
		</editors>
		<pages>331-336</pages>
		<isbn>978-0-615-24849-3</isbn>
		<title>A Study on Feature Selection and Classification Techniques for Automatic Genre Classification of Traditional Malay Music.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a8df92b6a386679962801f8355837928/andre@ismll</id>
		<tags>Mel</tags>
		<tags>MFCC</tags>
		<description>initial imports</description>
		<date>2010-03-08 14:44:49</date>
		<count>6</count>
		<booktitle>Int. Symposium on Music Information Retrieval</booktitle>
		<year>2000</year>
		<url></url>
		<author>B. Logan</author>
		<authors>
			<first>B.</first>
		</authors>
		<authors>
			<last>Logan</last>
		</authors>
		<title>Mel Frequency Cepstral Coefficients for Music Modeling</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22566e28d0303b959108882283ecb15fa/andre@ismll</id>
		<tags>content-based</tags>
		<tags>music_genre</tags>
		<tags>similarity</tags>
		<tags>recommendation</tags>
		<tags>sigir</tags>
		<tags>mirex2007</tags>
		<tags>taxonomie</tags>
		<tags>classification</tags>
		<tags>last.fm</tags>
		<description>dblp</description>
		<date>2009-12-28</date>
		<count>2</count>
		<booktitle>ISMIR</booktitle>
		<year>2008</year>
		<url>http://dblp.uni-trier.de/db/conf/ismir/ismir2008.html#SordoCBG08</url>
		<author>Mohamed Sordo</author>
		<author>Òscar Celma</author>
		<author>Martin Blech</author>
		<author>Enric Guaus</author>
		<authors>
			<first>Mohamed</first>
		</authors>
		<authors>
			<last>Sordo</last>
		</authors>
		<authors>
			<first>Òscar</first>
		</authors>
		<authors>
			<last>Celma</last>
		</authors>
		<authors>
			<first>Martin</first>
		</authors>
		<authors>
			<last>Blech</last>
		</authors>
		<authors>
			<first>Enric</first>
		</authors>
		<authors>
			<last>Guaus</last>
		</authors>
		<editor>Juan Pablo Bello</editor>
		<editor>Elaine Chew</editor>
		<editor>Douglas Turnbull</editor>
		<editors>
			<first>Enric</first>
		</editors>
		<editors>
			<last>Guaus</last>
		</editors>
		<editors>
			<first>Enric</first>
		</editors>
		<editors>
			<last>Guaus</last>
		</editors>
		<editors>
			<first>Enric</first>
		</editors>
		<editors>
			<last>Guaus</last>
		</editors>
		<pages>255-260</pages>
		<isbn>978-0-615-24849-3</isbn>
		<title>The Quest for Musical Genres: Do the Experts and the Wisdom of Crowds Agree?</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2764a7081eebdb7ec9a5bd727bdc44783/lran022</id>
		<tags>imported</tags>
		<description></description>
		<date>2009-10-10 00:08:43</date>
		<count>2</count>
		<booktitle>Proceedings International Symposium on Musical Acoustics ISMA 1998,Leavenworth, Washington</booktitle>
		<year>1998</year>
		<url></url>
		<author>Judith C. Brown</author>
		<authors>
			<first>Judith C.</first>
		</authors>
		<authors>
			<last>Brown</last>
		</authors>
		<pages>291-295</pages>
		<title>Musical Instrument Identification using Autocorrelation Coefficients</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/223567b5c5ec2e81367d9ffa35975a3c4/aucelum</id>
		<tags>music</tags>
		<tags>early_music</tags>
		<tags>monteverdi</tags>
		<tags>l'orfeo</tags>
		<description></description>
		<date>2010-02-02 17:03:01</date>
		<count>1</count>
		<journal>Early Music</journal>
		<publisher>Oxford University Press</publisher>
		<year>1984</year>
		<url>http://www.jstor.org/stable/3137731</url>
		<author>Iain Fenlon</author>
		<authors>
			<first>Iain</first>
		</authors>
		<authors>
			<last>Fenlon</last>
		</authors>
		<volume>12</volume>
		<number>2</number>
		<pages>163--172</pages>
		<issn>03061078</issn>
		<copyright>Copyright © 1984 Oxford University Press</copyright>
		<title>Monteverdi's Mantuan Örfeo": Some New Documentation</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/229bc70e22253883f530602c8f699b293/einsa</id>
		<tags>imported</tags>
		<tags>music</tags>
		<tags>wifi</tags>
		<tags>recommender</tags>
		<description></description>
		<date>2010-02-03 14:19:50</date>
		<count>1</count>
		<journal>Mathematics and Computer Science, Delft University of technology, no. ICT-2003-01</journal>
		<publisher>Citeseer</publisher>
		<year>2003</year>
		<url>http://scholar.google.de/scholar.bib?q=info:6YjAsbiJd5oJ:scholar.google.com/&output=citation&hl=de&as_sdt=2000&ct=citation&cd=0</url>
		<author>J. Wang</author>
		<author>M.J.T. Reinders</author>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Wang</last>
		</authors>
		<authors>
			<first>M.J.T.</first>
		</authors>
		<authors>
			<last>Reinders</last>
		</authors>
		<title>Music Recommender System for Wi-Fi Walkman</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ddc1a9c4a7686802b578f97630a25c2d/tb2332</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>1</count>
		<booktitle>Proceedings of the International Computer Music Conference (ICMC)</booktitle>
		<year>2005</year>
		<url></url>
		<author>N. Casagrande</author>
		<author>D. Eck</author>
		<author>B. Kegl</author>
		<authors>
			<first>N.</first>
		</authors>
		<authors>
			<last>Casagrande</last>
		</authors>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Eck</last>
		</authors>
		<authors>
			<first>B.</first>
		</authors>
		<authors>
			<last>Kegl</last>
		</authors>
		<pages>207--210</pages>
		<source>OwnPublication</source>
		<title>Geometry in Sound: A Speech/Music Audio Classifier Inspired by an
	Image Classifier</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e168313cf5c806d2746d8d03a7d35e8d/tb2332</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>1</count>
		<journal>Music Perception</journal>
		<year>1996</year>
		<url></url>
		<author>Caroline Palmer</author>
		<authors>
			<first>Caroline</first>
		</authors>
		<authors>
			<last>Palmer</last>
		</authors>
		<volume>13</volume>
		<pages>433-453</pages>
		<title>Anatomy of a performance: Sources of musical expression</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/211e954c04470f832e2cfaea6b3788910/tb2332</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>1</count>
		<booktitle>Action and Perception in Rhythm and Music</booktitle>
		<publisher>Royal Swedish Academy of Music</publisher>
		<address>Stockholm</address>
		<year>1987</year>
		<url></url>
		<author>Eric F. Clarke</author>
		<authors>
			<first>Eric F.</first>
		</authors>
		<authors>
			<last>Clarke</last>
		</authors>
		<editor>A. Gabrielsson</editor>
		<editors>
			<first>Eric F.</first>
		</editors>
		<editors>
			<last>Clarke</last>
		</editors>
		<title>Categorical rhythm perception: An ecological perspective</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ce73c9a04486194dc20a3609de9033a2/tb2332</id>
		<tags>music</tags>
		<tags>describe,</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>1</count>
		<year>1990</year>
		<url></url>
		<author>Manfred Clynes</author>
		<authors>
			<first>Manfred</first>
		</authors>
		<authors>
			<last>Clynes</last>
		</authors>
		<volume>7</volume>
		<pages>403--422</pages>
		<origin>Kidd</origin>
		<own>Need to Get</own>
		<title>Some guidelines for the synthesis and testing of pulse microstructure
	in relation to musical meaning</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/218b2b58fef63bd36fef874e0908a91f2/tb2332</id>
		<tags>music</tags>
		<tags>describe,</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>1</count>
		<publisher>The Univ. of Chicago Press</publisher>
		<address>Chicago</address>
		<year>1960</year>
		<url></url>
		<author>Grosvenor Cooper</author>
		<author>Leonard B. Meyer</author>
		<authors>
			<first>Grosvenor</first>
		</authors>
		<authors>
			<last>Cooper</last>
		</authors>
		<authors>
			<first>Leonard B.</first>
		</authors>
		<authors>
			<last>Meyer</last>
		</authors>
		<origin>Kielian-Gilbert</origin>
		<own>IU Library</own>
		<title>The Rhythmic Structure of Music</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28006c947933e4c903ff7985b4dd42020/tb2332</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>7</count>
		<booktitle>Proceedings of the 8th International Conference on Music Information
	Retrieval (ISMIR 2007)</booktitle>
		<year>2007</year>
		<url></url>
		<author>Y. Raimond</author>
		<author>S. A. Abdallah</author>
		<author>M. Sandler</author>
		<author>F. Giasson</author>
		<authors>
			<first>Y.</first>
		</authors>
		<authors>
			<last>Raimond</last>
		</authors>
		<authors>
			<first>S. A.</first>
		</authors>
		<authors>
			<last>Abdallah</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Sandler</last>
		</authors>
		<authors>
			<first>F.</first>
		</authors>
		<authors>
			<last>Giasson</last>
		</authors>
		<title>The music ontology</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25b52e8692d9c25ea79c037559e87776c/tb2332</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>2</count>
		<booktitle>Proceedings of the 8th International Conference on Music Information
	Retrieval (ISMIR 2007)</booktitle>
		<year>2007</year>
		<url></url>
		<author>J. Skowronek</author>
		<author>M. McKinney</author>
		<author>S. van de Par</author>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Skowronek</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>McKinney</last>
		</authors>
		<authors>
			<first>S.</first>
		</authors>
		<authors>
			<last>van de Par</last>
		</authors>
		<title>A demonstrator for automatic music mood estimation</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/270a71e20de848afc1276f35ac87db376/tb2332</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>1</count>
		<address>www.idsia.ch/\-techrep.html</address>
		<year>2002</year>
		<url></url>
		<author>D. Eck</author>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Eck</last>
		</authors>
		<number>IDSIA-22-02</number>
		<abstract>Beat induction is best described by analogy to the activities of hand
	clapping or foot tapping, and involves finding important metrical
	components in an auditory signal, usually music. Though beat induction
	is intuitively easy to understand it is difficult to define and still
	more difficult to perform automatically. We will present a model
	of beat induction that uses a spiking neural network as the underlying
	synchronization mechanism. This approach has some advantages over
	existing methods; it runs online, responds at many levels in the
	metrical hierarchy, and produces good results on performed music
	(Beatles piano performances encoded as MIDI). In this paper the model
	is described in some detail and simulation results are discussed.</abstract>
		<source>OwnPublication</source>
		<title>Real-Time Musical Beat Induction with Spiking Neural Networks</title>
		<pubtype>techreport</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25692a10ffab0c7a9ec1cf06dfec90240/tb2332</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>2</count>
		<booktitle>Proceedings of the 6th International Conference on Music Information
	Retrieval (ISMIR 2005)</booktitle>
		<address>London: University of London</address>
		<year>2005</year>
		<url></url>
		<author>D. Eck</author>
		<author>N. Casagrande</author>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Eck</last>
		</authors>
		<authors>
			<first>N.</first>
		</authors>
		<authors>
			<last>Casagrande</last>
		</authors>
		<pages>504--509</pages>
		<source>OwnPublication</source>
		<title>Finding Meter in Music Using an Autocorrelation Phase Matrix and
	Shannon Entropy</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e572b0e86b16d70f9fb3f4c12f0c178a/tb2332</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>1</count>
		<address>www.idsia.ch/\-techrep.html</address>
		<year>2002</year>
		<url></url>
		<author>D. Eck</author>
		<author>J. Schmidhuber</author>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Eck</last>
		</authors>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Schmidhuber</last>
		</authors>
		<number>IDSIA-07-02</number>
		<abstract>In general music composed by recurrent neural networks (RNNs) suffers
	from a lack of global structure. Though networks can learn note-by-note
	transition probabilities and even reproduce phrases, attempts at
	learning an entire musical form and using that knowledge to guide
	composition have been unsuccessful. The reason for this failure seems
	to be that RNNs cannot keep track of temporally distant events that
	indicate global music structure. Long Short-Term Memory (LSTM) has
	succeeded in similar domains where other RNNs have failed, such as
	timing & counting and CSL learning. In the current study I show
	that LSTM is also a good mechanism for learning to compose music.
	I compare this approach to previous attempts, with particular focus
	on issues of data representation. I present experimental results
	showing that LSTM successfully learns a form of blues music and is
	able to compose novel (and I believe pleasing) melodies in that style.
	Remarkably, once the network has found the relevant structure it
	does not drift from it: LSTM is able to play the blues with good
	timing and proper structure as long as one is willing to listen.
	
	 Note: This is a more complete version of the 2002 ICANN submission
	Learning the Long-Term Structure of the Blues.</abstract>
		<source>OwnPublication</source>
		<title>A First Look at Music Composition using LSTM Recurrent Neural Networks</title>
		<pubtype>techreport</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/235b2310211f6805bfe45d541c73e8ed2/tb2332</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>2</count>
		<booktitle>Proceedings of the 5th International Conference on Music Information
	Retrieval (ISMIR 2004)</booktitle>
		<address>Barcelona</address>
		<year>2004</year>
		<url></url>
		<author>M. Alonso</author>
		<author>B. David</author>
		<author>G. Richard</author>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Alonso</last>
		</authors>
		<authors>
			<first>B.</first>
		</authors>
		<authors>
			<last>David</last>
		</authors>
		<authors>
			<first>G.</first>
		</authors>
		<authors>
			<last>Richard</last>
		</authors>
		<pages>158-163</pages>
		<title>Tempo and Beat Estimation of Musical Signals</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/250671053dc9eb8b47bc3fdbf6c2223b9/tb2332</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>2</count>
		<booktitle>Proceedings of the 8th International Conference on Music Information
	Retrieval (ISMIR 2007)</booktitle>
		<year>2007</year>
		<url></url>
		<author>M. Sordo</author>
		<author>C. Laurier</author>
		<author>O. Celma</author>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Sordo</last>
		</authors>
		<authors>
			<first>C.</first>
		</authors>
		<authors>
			<last>Laurier</last>
		</authors>
		<authors>
			<first>O.</first>
		</authors>
		<authors>
			<last>Celma</last>
		</authors>
		<title>Annotating music collections: how content-based similarity helps
	to propagate labels</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2cda5faa617fb49a2f3b782addbe56480/tb2332</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>1</count>
		<journal>Contemporary Music Review</journal>
		<year>1989</year>
		<url></url>
		<author>J. Sundberg</author>
		<author>A. Friberg</author>
		<author>L. Fryden</author>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Sundberg</last>
		</authors>
		<authors>
			<first>A.</first>
		</authors>
		<authors>
			<last>Friberg</last>
		</authors>
		<authors>
			<first>L.</first>
		</authors>
		<authors>
			<last>Fryden</last>
		</authors>
		<volume>8</volume>
		<pages>89--109</pages>
		<title>Rules for automated performance of ensemble music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2590d12e327aaa6253e40cc6b813137e6/tb2332</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>6</count>
		<booktitle>Proceedings of the 9th International Conference on Music Information
	Retrieval (ISMIR 2008)</booktitle>
		<year>2008</year>
		<url></url>
		<author>D. Turnbull</author>
		<author>L. Barrington</author>
		<author>G. Lanckriet</author>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Turnbull</last>
		</authors>
		<authors>
			<first>L.</first>
		</authors>
		<authors>
			<last>Barrington</last>
		</authors>
		<authors>
			<first>G.</first>
		</authors>
		<authors>
			<last>Lanckriet</last>
		</authors>
		<title>Five Approaches to Collecting Tags for Music</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e26deb14d18a71629ee44795b65337c5/tb2332</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-02-27 01:05:18</date>
		<count>3</count>
		<journal>Journal of New Music Research, special issue: "From genres to tags:
	Music Information Retrieval in the era of folksonomies."</journal>
		<year>2008</year>
		<url></url>
		<author>T. Bertin-Mahieux</author>
		<author>D. Eck</author>
		<author>F. Maillet</author>
		<author>P. Lamere</author>
		<authors>
			<first>T.</first>
		</authors>
		<authors>
			<last>Bertin-Mahieux</last>
		</authors>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Eck</last>
		</authors>
		<authors>
			<first>F.</first>
		</authors>
		<authors>
			<last>Maillet</last>
		</authors>
		<authors>
			<first>P.</first>
		</authors>
		<authors>
			<last>Lamere</last>
		</authors>
		<volume>37</volume>
		<number>2</number>
		<title>Autotagger: a model for Predicting social tags from acoustic features
	on large music databases</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bfd8dca126a40ca8ea2f2ad3e0589225/ichun</id>
		<tags>music</tags>
		<tags>computer</tags>
		<description></description>
		<date>2007-01-13 16:45:41</date>
		<count>3</count>
		<publisher>A-R Editions, Inc.</publisher>
		<year>1996</year>
		<url></url>
		<author>David Cope</author>
		<authors>
			<first>David</first>
		</authors>
		<authors>
			<last>Cope</last>
		</authors>
		<title>Experiments in musical intelligence</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2eea386ed89367dc9e8d317fac24ebe03/zazi</id>
		<tags>imported</tags>
		<description>The Bibliography of my Belegarbeit</description>
		<date>2010-01-28 22:00:35</date>
		<count>2</count>
		<year>2006</year>
		<url></url>
		<author>Micheline Lesaffre</author>
		<authors>
			<first>Micheline</first>
		</authors>
		<authors>
			<last>Lesaffre</last>
		</authors>
		<title>Music Information Retrieval - Conceptual Framework, Annotation and
	User Behaviour</title>
		<pubtype>phdthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22c8d183af1969e19d42750f0161e51b3/zazi</id>
		<tags>imported</tags>
		<description>The Bibliography of my Belegarbeit</description>
		<date>2010-01-28 22:00:35</date>
		<count>1</count>
		<booktitle>Proceedings of the 1st International Conference on Music Information
	Retrival</booktitle>
		<address>Plymouth, Massachusetts, USA</address>
		<year>2000</year>
		<url></url>
		<author>Beth Logan</author>
		<authors>
			<first>Beth</first>
		</authors>
		<authors>
			<last>Logan</last>
		</authors>
		<title>Mel Frequency Cepstral Coefficients for Music Modelling</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20f119c9fb1689955859bfbac2cd0e6f2/zazi</id>
		<tags>imported</tags>
		<description>The Bibliography of my Belegarbeit</description>
		<date>2010-01-28 22:00:35</date>
		<count>3</count>
		<booktitle>Proceedings of the 5th International Conference on Music Information
	Retrieval</booktitle>
		<address>Barcelona, Spanien</address>
		<year>2004</year>
		<url></url>
		<author>Sally Jo Cunningham</author>
		<author>Matt Jones</author>
		<author>Steve Jones</author>
		<authors>
			<first>Sally Jo</first>
		</authors>
		<authors>
			<last>Cunningham</last>
		</authors>
		<authors>
			<first>Matt</first>
		</authors>
		<authors>
			<last>Jones</last>
		</authors>
		<authors>
			<first>Steve</first>
		</authors>
		<authors>
			<last>Jones</last>
		</authors>
		<title>Organizing Digital Music for Use: An Examination of Personal Music
	Collections</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b370e53d104122cf9c0e29cabe179960/zazi</id>
		<tags>imported</tags>
		<description>The Bibliography of my Belegarbeit</description>
		<date>2010-01-28 22:00:35</date>
		<count>6</count>
		<booktitle>Proceedings of the 9th International Conference on Music Information
	Retrieval</booktitle>
		<address>Philadelphia, USA</address>
		<year>2008</year>
		<url></url>
		<author>Douglas Turnbull</author>
		<author>Luke Barrington</author>
		<author>Gert Lanckriet</author>
		<authors>
			<first>Douglas</first>
		</authors>
		<authors>
			<last>Turnbull</last>
		</authors>
		<authors>
			<first>Luke</first>
		</authors>
		<authors>
			<last>Barrington</last>
		</authors>
		<authors>
			<first>Gert</first>
		</authors>
		<authors>
			<last>Lanckriet</last>
		</authors>
		<title>Five Approaches to Collecting Tags for Music</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2be37177a2cfc396aa17c0331e540287b/zazi</id>
		<tags>imported</tags>
		<description>The Bibliography of my Belegarbeit</description>
		<date>2010-01-28 22:00:35</date>
		<count>2</count>
		<booktitle>Proceedings of the 1st International Conference on Music Information
	Retrieval</booktitle>
		<address>Plymouth, Massachusetts, USA</address>
		<year>2000</year>
		<url></url>
		<author>Eleanor Selfridge-Field</author>
		<authors>
			<first>Eleanor</first>
		</authors>
		<authors>
			<last>Selfridge-Field</last>
		</authors>
		<title>What Motivates a Musical Query?</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/218ab6ed70899f4f4cf3e51386033aa68/zazi</id>
		<tags>imported</tags>
		<description>The Bibliography of my Belegarbeit</description>
		<date>2010-01-28 22:00:35</date>
		<count>1</count>
		<year>2008</year>
		<url>http://musicbrainz.org/doc/MusicBrainzXMLMetaData</url>
		<author> Several</author>
		<authors>
			<first></first>
		</authors>
		<authors>
			<last>Several</last>
		</authors>
		<title>The MusicBrainz XML Metadata Format</title>
		<pubtype>webpage</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/277e2bacdaf6684369521753393d7531c/zazi</id>
		<tags>imported</tags>
		<description>The Bibliography of my Belegarbeit</description>
		<date>2010-01-28 22:16:30</date>
		<count>7</count>
		<booktitle>Proceedings of the 8th International Conference on Music Information 	Retrieval</booktitle>
		<address>Wien, Österreich</address>
		<year>2007</year>
		<url></url>
		<author>Yves Raimond</author>
		<author>Samer Abdallah</author>
		<author>Mark Sandler</author>
		<author>Frederick Giasson</author>
		<authors>
			<first>Yves</first>
		</authors>
		<authors>
			<last>Raimond</last>
		</authors>
		<authors>
			<first>Samer</first>
		</authors>
		<authors>
			<last>Abdallah</last>
		</authors>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Sandler</last>
		</authors>
		<authors>
			<first>Frederick</first>
		</authors>
		<authors>
			<last>Giasson</last>
		</authors>
		<pages>417--422</pages>
		<title>The Music Ontology</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21f1a293b03c150caf38e396a47f23b9c/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<publisher>Amistad</publisher>
		<address>New York</address>
		<year>2003</year>
		<url></url>
		<author>Peter Guralnick</author>
		<author>Martin Scorsese</author>
		<authors>
			<first>Peter</first>
		</authors>
		<authors>
			<last>Guralnick</last>
		</authors>
		<authors>
			<first>Martin</first>
		</authors>
		<authors>
			<last>Scorsese</last>
		</authors>
		<pages>287</pages>
		<isbn>0060525444 (alk. paper)</isbn>
		<title>Martin Scorsese presents the blues : a musical journey</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23c529ec9de661e0805a40949af05c8c4/bliek</id>
		<tags>Biography,</tags>
		<tags>Musicology</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<publisher>Gordon and Breach</publisher>
		<address>New York</address>
		<year>1988</year>
		<url></url>
		<author>Hans Lenneberg</author>
		<authors>
			<first>Hans</first>
		</authors>
		<authors>
			<last>Lenneberg</last>
		</authors>
		<pages>223</pages>
		<isbn>2881242103</isbn>
		<shorttitle>Witnesses and Scholars</shorttitle>
		<lccn>ML3797 .L46 1988</lccn>
		<title>Witnesses and Scholars: Studies in Musical Biography</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/239690b697cc241c5ed6fd5f03435a57a/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>Popular music and society. Vol. 28</journal>
		<year>2005</year>
		<url></url>
		<author>Don author Cusic</author>
		<authors>
			<first>Don</first>
		</authors>
		<authors>
			<last>author Cusic</last>
		</authors>
		<volume>28</volume>
		<number>2</number>
		<pages>171--177</pages>
		<abstract>Unedited The music industry of the late 20th and early 21st c. has been enamoured with the singer-songwriter; this has caused critics and fans to dismiss the singer who "covers" songs as a less legitimate artist. It is argued that the singer who sings songs written by others is also a legitimate artist and that cover songs represent a form of artistic interpretation that goes beyond mere "copying".</abstract>
		<issn>0300-7766</issn>
		<title>In defense of cover songs</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24659f0bc1dc5bff37255caab89129fc4/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>Popular music. Vol. 16</journal>
		<year>1997</year>
		<url></url>
		<author>Mike author Daley</author>
		<authors>
			<first>Mike</first>
		</authors>
		<authors>
			<last>author Daley</last>
		</authors>
		<volume>16</volume>
		<number>3</number>
		<pages>235--253</pages>
		<abstract>Smith's 1975 recording of the garage-band classic Gloria is less a cover version of the 1965 original--written by Van Morrison and performed by Them--than a complete reconfiguration. Smith simultaneously parodies and pays tribute to the oft-covered song by radically altering the lyrics and vocal delivery. Her recording serves as both a critique and a celebration of the male-dominated rock aesthetic that preceded her own work. A full transcription of Smith's version is included, highlighting the central communicative role of the voice in popular music. A newly devised notational scheme that indicates vocal timbre and microtonal inflections is used to track Smith's vocal delivery. A phonetic transcription of the text as delivered by Smith replicates her variant pronunciations and interpretations of Morrison's lyrics. Julia Kristeva's concept of intertextuality is used as a tool for unpacking the complexities of the performance. The multilayered nature of Smith's Gloria mixes contrasting styles of linguistic and musical discourse, transforming Morrison's song from a stand-alone text to a manipulated signifier.</abstract>
		<issn>0261-1430</issn>
		<shorttitle>Patti Smith's Gloria</shorttitle>
		<title>Patti Smith's Gloria: Intertextual play in a rock vocal performance</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2dcfff7a93b54afff94ae2823b0b0163f/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>Gamut: The online journal of the Music Theory Society of the Mid-Atlantic. Vol. 2</journal>
		<year>2009</year>
		<url></url>
		<author>Mark author Spicer</author>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>author Spicer</last>
		</authors>
		<volume>2</volume>
		<number>1</number>
		<pages>347--375</pages>
		<abstract>In a seminal 1985 article, Robert Hatten outlines a theory of musical intertextuality with the potential for a broad range of application. He suggests that intertextuality in music operates on two essential levels: stylistic and strategic. Stylistic intertextuality occurs when a composer adopts distinctive features of an earlier style without reference to any specific work in that style (such as the appropriation of high-Baroque devices in the string-octet accompaniment to McCartney's Eleanor Rigby). Strategic intertextuality is more pointed, occurring only when a composer makes deliberate reference to a particular earlier work or works, and this can involve a variety of techniques such as quotation, structural modeling, variation, or paraphrase. At the risk of oversimplification, we might say then that the goal of an intertextual analysis is to unravel the many ways in which the stylistic and strategic references contribute to the meaning of the new piece. Analyses of three of Lennon's late Beatles songs--All you need is love, Glass onion, and Because--demonstrate how strategic intertextuality can work to enrich a pop-rock song's overall message.</abstract>
		<issn>1938-6690</issn>
		<title>Strategic intertextuality in three of John Lennon's late Beatles songs</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d3ef10f2ba63f359afc879319c8707f4/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>Popular Music</journal>
		<year>1985</year>
		<url>http://www.jstor.org/stable/853282</url>
		<author>Richard Middleton</author>
		<authors>
			<first>Richard</first>
		</authors>
		<authors>
			<last>Middleton</last>
		</authors>
		<volume>5</volume>
		<pages>5--43</pages>
		<issn>02611430</issn>
		<doi>10.2307/853282</doi>
		<title>Articulating Musical Meaning/Re-Constructing Musical History/Locating the 'Popular'</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a8df92b6a386679962801f8355837928/bfields</id>
		<tags>feature_extraction</tags>
		<tags>MFCC</tags>
		<description>initial imports</description>
		<date>2010-01-28 11:42:45</date>
		<count>6</count>
		<booktitle>Int. Symposium on Music Information Retrieval</booktitle>
		<year>2000</year>
		<url></url>
		<author>B. Logan</author>
		<authors>
			<first>B.</first>
		</authors>
		<authors>
			<last>Logan</last>
		</authors>
		<title>Mel Frequency Cepstral Coefficients for Music Modeling</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/264357b8f71bfd96c19a5f92b00c2fc52/bfields</id>
		<tags>Mood_classification</tags>
		<tags>music_classification</tags>
		<tags>rockanango</tags>
		<description>initial imports</description>
		<date>2010-01-28 11:42:45</date>
		<count>1</count>
		<booktitle>Proc. of Int. Symposium on Music Information Retrieval</booktitle>
		<year>2007</year>
		<url></url>
		<author>Sten Govaerts</author>
		<author>Nik Corthhaut</author>
		<author>Erik Duval</author>
		<authors>
			<first>Sten</first>
		</authors>
		<authors>
			<last>Govaerts</last>
		</authors>
		<authors>
			<first>Nik</first>
		</authors>
		<authors>
			<last>Corthhaut</last>
		</authors>
		<authors>
			<first>Erik</first>
		</authors>
		<authors>
			<last>Duval</last>
		</authors>
		<title>MOOD-EX-MACHINA: Toward Automation of Moody Tunes</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/255339bf892db97fb7dd213847831659a/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>American Music</journal>
		<year>1997</year>
		<url>http://www.jstor.org/stable/3052732</url>
		<author>Charles Gower Price</author>
		<authors>
			<first>Charles Gower</first>
		</authors>
		<authors>
			<last>Price</last>
		</authors>
		<volume>15</volume>
		<number>2</number>
		<pages>208--232</pages>
		<issn>07344392</issn>
		<title>Sources of American Styles in the Music of the Beatles</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d3a111d5888bf35f6dbbcf6fbeadb2f1/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<publisher>Black Rose Books</publisher>
		<address>Montrâeal</address>
		<year>2003</year>
		<url></url>
		<author>Ajay Heble</author>
		<author>Daniel Fischlin</author>
		<authors>
			<first>Ajay</first>
		</authors>
		<authors>
			<last>Heble</last>
		</authors>
		<authors>
			<first>Daniel</first>
		</authors>
		<authors>
			<last>Fischlin</last>
		</authors>
		<pages>254</pages>
		<isbn>1551642301</isbn>
		<shorttitle>Rebel Musics</shorttitle>
		<lccn>306.4/842\textbar222</lccn>
		<title>Rebel Musics: Human Rights, Resistant Sounds, and the Politics of Making</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c9c21b77afa74c0efea59080adfc80ff/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>Acta Sociologica</journal>
		<year>1974</year>
		<url>http://www.jstor.org.ezproxy.library.yorku.ca/stable/4193994</url>
		<author>D. J. Hatch</author>
		<author>D. R. Watson</author>
		<authors>
			<first>D. J.</first>
		</authors>
		<authors>
			<last>Hatch</last>
		</authors>
		<authors>
			<first>D. R.</first>
		</authors>
		<authors>
			<last>Watson</last>
		</authors>
		<volume>17</volume>
		<number>2</number>
		<pages>162--178</pages>
		<issn>00016993</issn>
		<shorttitle>Hearing the Blues</shorttitle>
		<title>Hearing the Blues: An Essay in the Sociology of Music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d1693ebf164e220edc6e39e466a81e87/bliek</id>
		<tags>personal</tags>
		<tags>copy</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<publisher>Gollancz</publisher>
		<address>London</address>
		<year>1972</year>
		<url></url>
		<author>Richard Middleton</author>
		<authors>
			<first>Richard</first>
		</authors>
		<authors>
			<last>Middleton</last>
		</authors>
		<pages>271</pages>
		<isbn>0575014423</isbn>
		<shorttitle>Pop Music and the Blues</shorttitle>
		<lccn>ML3545 .M48</lccn>
		<title>Pop Music and the Blues: A Study of the Relationship and Its Siginificance</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/262e9fc79554d8ae4cccb813cab5cf692/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>Black Music Research Journal</journal>
		<year>1988</year>
		<url>http://www.jstor.org.ezproxy.library.yorku.ca/stable/779351</url>
		<author>Joseph Witek</author>
		<authors>
			<first>Joseph</first>
		</authors>
		<authors>
			<last>Witek</last>
		</authors>
		<volume>8</volume>
		<number>2</number>
		<pages>177--193</pages>
		<issn>02763605</issn>
		<title>Blindness as a Rhetorical Trope in Blues Discourse</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2efd0aaa37340029a8729d8049a5b62dd/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>THE WIRE, Adventures in Modern Music</journal>
		<series>The singer not the song (60 significant cover versions).</series>
		<year>2005</year>
		<url>http://ezproxy.library.yorku.ca/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=mah&AN=MAH0001482096&site=ehost-live</url>
		<author>B. Kopf</author>
		<author>B. Kopf</author>
		<authors>
			<first>B.</first>
		</authors>
		<authors>
			<last>Kopf</last>
		</authors>
		<authors>
			<first>B.</first>
		</authors>
		<authors>
			<last>Kopf</last>
		</authors>
		<number>261</number>
		<pages>38--49</pages>
		<issn>0952-0686</issn>
		<doi>Article</doi>
		<title>The singer not the song (60 significant cover versions).</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b593585a3fe673152de43b4b8b1d3aba/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>Canadian University Music Review</journal>
		<year>1996</year>
		<url>http://gateway.proquest.com/openurl?url_ver=Z39.88-2004&res_dat=xri:iimp:&rft_dat=xri:iimp:article:citation:iimp00024078</url>
		<author>Serge Lacasse</author>
		<authors>
			<first>Serge</first>
		</authors>
		<authors>
			<last>Lacasse</last>
		</authors>
		<volume>16</volume>
		<number>2</number>
		<pages>134--39</pages>
		<issn>0710-0353</issn>
		<title>Comptes rendus: Allan F. Moore, "Rock, the Primary Text: Developing a Musicology of Rock"</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29db425753eaf9af03436d4c3200ea0ed/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>Black Music Research Journal</journal>
		<year>2000</year>
		<url>http://www.jstor.org/stable/779314</url>
		<author>Alan Govenar</author>
		<authors>
			<first>Alan</first>
		</authors>
		<authors>
			<last>Govenar</last>
		</authors>
		<volume>20</volume>
		<number>1</number>
		<pages>7--21</pages>
		<issn>02763605</issn>
		<shorttitle>Blind Lemon Jefferson</shorttitle>
		<title>Blind Lemon Jefferson: The Myth and the Man</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/288c09bf53838ccc8ac6b7769278e5775/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>Music Library Association. Notes</journal>
		<year>2009</year>
		<url></url>
		<author>E. Komara</author>
		<authors>
			<first>E.</first>
		</authors>
		<authors>
			<last>Komara</last>
		</authors>
		<volume>66</volume>
		<number>1</number>
		<pages>77</pages>
		<abstract>Illustrations, bibliography, index. Because the blues in the Delta region in the northwest corner of Mississippi is so much a part of the region's oral culture, little about it has the appearance to historians of being known for certain.</abstract>
		<issn>00274380</issn>
		<shorttitle>Delta Blues</shorttitle>
		<title>Delta Blues: The Life and Times of the Mississippi Masters Who Revolutionized American Music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e06a31128f40f5ef06f299577f6504ab/bliek</id>
		<tags>Social</tags>
		<tags>20th</tags>
		<tags>aspects</tags>
		<tags>century,</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<publisher>University of California Press</publisher>
		<address>Berkeley</address>
		<year>2000</year>
		<url></url>
		<author>Georgina Born</author>
		<author>David Hesmondhalgh</author>
		<authors>
			<first>Georgina</first>
		</authors>
		<authors>
			<last>Born</last>
		</authors>
		<authors>
			<first>David</first>
		</authors>
		<authors>
			<last>Hesmondhalgh</last>
		</authors>
		<pages>360</pages>
		<isbn>0520220838</isbn>
		<shorttitle>Western Music and Its Others</shorttitle>
		<lccn>ML3795 .W45 2000</lccn>
		<title>Western Music and Its Others: Difference, Representation, and Riation in Music</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bdd49e9b2b345fd2b2884c2fc75b321b/bliek</id>
		<tags>imported</tags>
		<description></description>
		<date>2010-01-27 22:12:50</date>
		<count>1</count>
		<journal>British Journal of Music Education</journal>
		<year>2001</year>
		<url>http://gateway.proquest.com/openurl?url_ver=Z39.88-2004&res_dat=xri:iimp:&rft_dat=xri:iimp:article:citation:iimp00197649</url>
		<author>Allan Moore</author>
		<authors>
			<first>Allan</first>
		</authors>
		<authors>
			<last>Moore</last>
		</authors>
		<volume>18</volume>
		<number>1</number>
		<pages>83--86</pages>
		<issn>0265-0517</issn>
		<title>"Reading Pop: Approaches to Textual Analysis in Popular Music," Edited by Richard Middleton</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2abae0a38f92b267c4f1c60f7050801fc/jwbowers</id>
		<tags>hiphop,</tags>
		<tags>lfm</tags>
		<description></description>
		<date>2009-10-28 04:42:52</date>
		<count>1</count>
		<publisher>Routledge</publisher>
		<year>1994</year>
		<url>http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0415909082</url>
		<author>Tricia Rose</author>
		<authors>
			<first>Tricia</first>
		</authors>
		<authors>
			<last>Rose</last>
		</authors>
		<abstract>Youth music is the most creative and contested location on the cultural landscape.  It is a vehicle for generational moods and aspirations, a public refuge for fantasies outlawed in daily life, a testing ground for technical ingenuity, an enormously profitable commercial channel for mainstream narratives of thought and behavior, and one of the corporate state's main theatres for national moral panic.  Today's sounds, and the debates about their various forms, are inseparable from the social conditions of the last two decades:  class polarization, racial marginalization, and economic violence enacted to a degree that has left youth, as a whole, with drastically reduced opportunities in life.  Youth culture is still responding to these uneven developments with a passion that has been romanticized by some critics as a significant form of resistance, and denigrated by others as an avoidance of direct and political protest.<br><br><b></b><b><i>Microphone Fiends</i></b><b></b>, a collection of original essays and interviews, brings together some of the best known scholars, critics, journalists and performers to focus on the contemporary scene.  It includes theoretical discussions of musical history along with social commentaries about genres like disco, metal and rap music, and case histories of specific movements like the Riot Grrls, funk clubbing in Rio de Janeiro, and the British rave scene.  The contents of the volume engage with the broad tradition of cultural studies and sociology of youth music and culture, but they are also designed to address audiences reached by mainstream music journalism and fans of any musical taste.</abstract>
		<isbn>0415909082</isbn>
		<title>Microphone Fiends: Youth Music and Youth Culture</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/240cfaf8bd9781e8417e3b38077262854/knaevelboerrar</id>
		<tags>music</tags>
		<tags>composition</tags>
		<description></description>
		<date>2013-03-27 14:39:04</date>
		<count>2</count>
		<publisher>Logistikzentrum</publisher>
		<year>1999</year>
		<url>http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/3802403495</url>
		<author>Axel Kemper-Moll</author>
		<authors>
			<first>Axel</first>
		</authors>
		<authors>
			<last>Kemper-Moll</last>
		</authors>
		<isbn>3802403495</isbn>
		<title>Jazz und Pop Harmonielehre. Inkl. CD. Viele bekannte Beispiele aus verschiedenen Stilrichtungen.</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29b0d92b6922fdcd7ffe46ea4580be795/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-05-28 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj37.html#Feller13</url>
		<author>Ross Feller</author>
		<authors>
			<first>Ross</first>
		</authors>
		<authors>
			<last>Feller</last>
		</authors>
		<volume>37</volume>
		<number>1</number>
		<pages>70-73</pages>
		<title>Sonic Circuits 2012: A Festival of Experimental Music.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2274721859a5c53d217a05249ddb9d73d/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-05-28 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj37.html#Valle13</url>
		<author>Andrea Valle</author>
		<authors>
			<first>Andrea</first>
		</authors>
		<authors>
			<last>Valle</last>
		</authors>
		<volume>37</volume>
		<number>1</number>
		<pages>75-79</pages>
		<title>Laura Zattra: Studiare La Computer Music: Definizioni, Analisi, Fonti.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25edaba534a0eff78f3d3d41a43b6bf23/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-05-28 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj37.html#Kaliakatsos-PapakostasFV13</url>
		<author>Maximos A. Kaliakatsos-Papakostas</author>
		<author>Andreas Floros</author>
		<author>Michael N. Vrahatis</author>
		<authors>
			<first>Maximos A.</first>
		</authors>
		<authors>
			<last>Kaliakatsos-Papakostas</last>
		</authors>
		<authors>
			<first>Andreas</first>
		</authors>
		<authors>
			<last>Floros</last>
		</authors>
		<authors>
			<first>Michael N.</first>
		</authors>
		<authors>
			<last>Vrahatis</last>
		</authors>
		<volume>37</volume>
		<number>1</number>
		<pages>52-69</pages>
		<title>A Clustering Strategy for the Key Segmentation of Musical Audio.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22097e498b2660ee2c52345187bbaff49/sauli</id>
		<tags>domain</tags>
		<tags>design</tags>
		<tags>incollection</tags>
		<tags>book</tags>
		<tags>gradu</tags>
		<description></description>
		<date>2014-11-24 20:30:55</date>
		<count>1</count>
		<booktitle>From Pac-Man to pop music: interactive audio in games and new media</booktitle>
		<year>2008</year>
		<url></url>
		<author>Tim van Geelen</author>
		<authors>
			<first>Tim</first>
		</authors>
		<authors>
			<last>van Geelen</last>
		</authors>
		<pages>93-102</pages>
		<title>Realizing groundbreaking adaptive music</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d49539f4aa0c888825fe896eadad4750/joaakive</id>
		<tags>microtonality</tags>
		<tags>algorithms</tags>
		<tags>composition</tags>
		<tags>musical</tags>
		<description></description>
		<date>2014-12-01 14:39:06</date>
		<count>1</count>
		<year>2007</year>
		<url></url>
		<author>Warren Burt</author>
		<authors>
			<first>Warren</first>
		</authors>
		<authors>
			<last>Burt</last>
		</authors>
		<title>Algorithms, microtonality, performance: eleven musical compositions</title>
		<pubtype>phdthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27114416de80ba32d0bc7e7175d4ac57b/joaakive</id>
		<tags>mathematical</tags>
		<tags>music</tags>
		<tags>system</tags>
		<tags>tuning</tags>
		<description></description>
		<date>2015-01-27 17:48:31</date>
		<count>1</count>
		<journal>Perspectives of New Music</journal>
		<year>2009</year>
		<url></url>
		<author>L. Polansky</author>
		<author>D. Rockmore</author>
		<author>M.K. Johnson</author>
		<author>D. Repetto</author>
		<author>W. Pan</author>
		<authors>
			<first>L.</first>
		</authors>
		<authors>
			<last>Polansky</last>
		</authors>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Rockmore</last>
		</authors>
		<authors>
			<first>M.K.</first>
		</authors>
		<authors>
			<last>Johnson</last>
		</authors>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Repetto</last>
		</authors>
		<authors>
			<first>W.</first>
		</authors>
		<authors>
			<last>Pan</last>
		</authors>
		<title>A mathematical model for optimal tuning systems</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a91dc7cd922adde5eb2cedb28c60bca6/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-07-21 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2005</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj29.html#ShenoyW05</url>
		<author>Arun Shenoy</author>
		<author>Ye Wang</author>
		<authors>
			<first>Arun</first>
		</authors>
		<authors>
			<last>Shenoy</last>
		</authors>
		<authors>
			<first>Ye</first>
		</authors>
		<authors>
			<last>Wang</last>
		</authors>
		<volume>29</volume>
		<number>3</number>
		<pages>75-86</pages>
		<title>Key, Chord, and Rhythm Tracking of Popular Music Recordings.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f82d413e0f19fda9e9a741d98c98ba9d/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-07-21 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2005</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj29.html#Whalley05b</url>
		<author>Ian Whalley</author>
		<authors>
			<first>Ian</first>
		</authors>
		<authors>
			<last>Whalley</last>
		</authors>
		<volume>29</volume>
		<number>3</number>
		<pages>96-98</pages>
		<title>Christoph Cox and Daniel Warner, Editors: Audio Culture: Readings in Modern Music.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2118e4b146aca010ab293d02c959e7966/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2015-02-03 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2014</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj38.html#LyonKO14</url>
		<author>Eric D. Lyon</author>
		<author>R. Benjamin Knapp</author>
		<author>Gascia Ouzounian</author>
		<authors>
			<first>Eric D.</first>
		</authors>
		<authors>
			<last>Lyon</last>
		</authors>
		<authors>
			<first>R. Benjamin</first>
		</authors>
		<authors>
			<last>Knapp</last>
		</authors>
		<authors>
			<first>Gascia</first>
		</authors>
		<authors>
			<last>Ouzounian</last>
		</authors>
		<volume>38</volume>
		<number>3</number>
		<pages>64-75</pages>
		<title>Compositional and Performance Mapping in Computer Chamber Music: A Case Study.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25ca4ac1789c1eb21ef77f0a43885116e/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-07-21 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2005</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj29.html#Rudi05</url>
		<author>Jøran Rudi</author>
		<authors>
			<first>Jøran</first>
		</authors>
		<authors>
			<last>Rudi</last>
		</authors>
		<volume>29</volume>
		<number>4</number>
		<pages>36-44</pages>
		<title>Computer Music Video: A Composer's Perspective.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22e0e793f90d2c2ded921cd61167635da/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-07-21 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2005</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj29.html#Strother05a</url>
		<author>Eric S. Strother</author>
		<authors>
			<first>Eric S.</first>
		</authors>
		<authors>
			<last>Strother</last>
		</authors>
		<volume>29</volume>
		<number>3</number>
		<pages>106-108</pages>
		<title>Sony Media Acid 5.0 Music Studio and Acid 5.0 Pro.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f4fab77bf7b7bbf22e224c1561aae779/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-07-21 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2005</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj29.html#Marshall05</url>
		<author>Andrew Marshall</author>
		<authors>
			<first>Andrew</first>
		</authors>
		<authors>
			<last>Marshall</last>
		</authors>
		<volume>29</volume>
		<number>2</number>
		<pages>77-78</pages>
		<title>Workshop in Algorithmic Computer Music 2004 Music Center, University of California, Santa Cruz, USA; 25 June-9 July 2004.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2054ee208adb384524e6998564b48cbec/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-07-21 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2005</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj29.html#Cahill05</url>
		<author>Margaret Cahill</author>
		<authors>
			<first>Margaret</first>
		</authors>
		<authors>
			<last>Cahill</last>
		</authors>
		<volume>29</volume>
		<number>2</number>
		<pages>93-94</pages>
		<title>Bill Purse: The PrintMusic! Primer - Mastering the Art of Music Notation with Finale PrintMusic!</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f93135e35c97075f1c0da315abe87d46/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-07-21 00:00:00</date>
		<count>3</count>
		<journal>Computer Music Journal</journal>
		<year>2005</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj29.html#HausL05</url>
		<author>Goffredo Haus</author>
		<author>Maurizio Longari</author>
		<authors>
			<first>Goffredo</first>
		</authors>
		<authors>
			<last>Haus</last>
		</authors>
		<authors>
			<first>Maurizio</first>
		</authors>
		<authors>
			<last>Longari</last>
		</authors>
		<volume>29</volume>
		<number>1</number>
		<pages>70-85</pages>
		<title>A Multi-Layered, Time-Based Music Description Approach Based on XML.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e2f9becaef3d0f9500ee0bb9b5d1d606/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-07-21 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2005</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj29.html#Whalley05</url>
		<author>Ian Whalley</author>
		<authors>
			<first>Ian</first>
		</authors>
		<authors>
			<last>Whalley</last>
		</authors>
		<volume>29</volume>
		<number>2</number>
		<pages>83-87</pages>
		<title>International Computer Music Conference 2004: Papers.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e6106d0223eeab98eb4e892f42dfa903/flint63</id>
		<tags>semantic</tags>
		<tags>web</tags>
		<tags>entertain</tags>
		<tags>multimedia</tags>
		<tags>paper</tags>
		<tags>management</tags>
		<tags>ai</tags>
		<tags>ieee</tags>
		<tags>v1500</tags>
		<tags>information</tags>
		<description></description>
		<date>2012-05-30 10:52:39</date>
		<count>2</count>
		<journal>IEEE Multimedia</journal>
		<year>2009</year>
		<url></url>
		<author>Yves Raimond</author>
		<author>Christopher Sutton</author>
		<author>Mark Sandler</author>
		<authors>
			<first>Yves</first>
		</authors>
		<authors>
			<last>Raimond</last>
		</authors>
		<authors>
			<first>Christopher</first>
		</authors>
		<authors>
			<last>Sutton</last>
		</authors>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Sandler</last>
		</authors>
		<volume>16</volume>
		<number>2</number>
		<pages>52-63</pages>
		<abstract>This article describes how Semantic Web technologies can be used to interlink musical data sources that have traditionally been isolated and difficult to integrate. The results of automatic content analysis can be added to the Semantic Web as dynamic computation resources, allowing researchers to combine and reuse algorithm implementations.</abstract>
		<issn>1070-986X</issn>
		<doi>10.1109/MMUL.2009.29</doi>
		<title>Interlinking Music-Related Data on the Web</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d21f5e41452668827e2d9e342b071057/tlow</id>
		<tags>myown</tags>
		<description></description>
		<date>2015-03-02 16:13:27</date>
		<count>3</count>
		<booktitle>Proceedings of 14th International Society for Music Information Retrieval Conference</booktitle>
		<year>2013</year>
		<url>http://www.ppgia.pucpr.br/ismir2013/wp-content/uploads/2013/09/40_Paper.pdf</url>
		<author>Sebastian Stober</author>
		<author>Thomas Low</author>
		<author>Tatiana Gossen</author>
		<author>Andreas Nürnberger</author>
		<authors>
			<first>Sebastian</first>
		</authors>
		<authors>
			<last>Stober</last>
		</authors>
		<authors>
			<first>Thomas</first>
		</authors>
		<authors>
			<last>Low</last>
		</authors>
		<authors>
			<first>Tatiana</first>
		</authors>
		<authors>
			<last>Gossen</last>
		</authors>
		<authors>
			<first>Andreas</first>
		</authors>
		<authors>
			<last>Nürnberger</last>
		</authors>
		<title>Incremental Visualization of Growing Music Collections</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d40a7b7ac37f4bfd8f838fd41774c081/joaakive</id>
		<tags>music</tags>
		<tags>computer</tags>
		<tags>technology</tags>
		<description></description>
		<date>2015-03-06 13:02:51</date>
		<count>2</count>
		<booktitle>ISMIR</booktitle>
		<publisher>Austrian Computer Society</publisher>
		<year>2007</year>
		<url>http://dblp.uni-trier.de/db/conf/ismir/ismir2007.html#LartillotT07</url>
		<author>Olivier Lartillot</author>
		<author>Petri Toiviainen</author>
		<authors>
			<first>Olivier</first>
		</authors>
		<authors>
			<last>Lartillot</last>
		</authors>
		<authors>
			<first>Petri</first>
		</authors>
		<authors>
			<last>Toiviainen</last>
		</authors>
		<editor>Simon Dixon</editor>
		<editor>David Bainbridge</editor>
		<editor>Rainer Typke</editor>
		<editors>
			<first>Petri</first>
		</editors>
		<editors>
			<last>Toiviainen</last>
		</editors>
		<editors>
			<first>Petri</first>
		</editors>
		<editors>
			<last>Toiviainen</last>
		</editors>
		<editors>
			<first>Petri</first>
		</editors>
		<editors>
			<last>Toiviainen</last>
		</editors>
		<pages>127-130</pages>
		<isbn>978-3-85403-218-2</isbn>
		<title>MIR in Matlab (II): A Toolbox for Musical Feature Extraction from Audio.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29ac29c248d364ce6f9f5fa6d5e50a346/sauli</id>
		<tags>domain</tags>
		<tags>music</tags>
		<tags>rejected</tags>
		<tags>book</tags>
		<description></description>
		<date>2014-05-21 17:18:31</date>
		<count>1</count>
		<publisher>Berklee Press</publisher>
		<year>2010</year>
		<url></url>
		<author>Richard Davis</author>
		<authors>
			<first>Richard</first>
		</authors>
		<authors>
			<last>Davis</last>
		</authors>
		<abstract>Essential for anyone interested in the business, process and procedures of writing music for film or television, this book teaches the Berklee approach to the art, covering topics such as: preparing and recording a score, contracts and fees, publishing, royalties, copyrights and much more. Features interviews with 21 top film-scoring professionals, including Michael Kamen, Alf Clausen, Alan Silvestri, Marc Shaiman, Mark Snow, Harry Gregson-Williams and Elmer Bernstein. Now updated with info on today's latest technology, and invaluable insights into finding work in the industry.</abstract>
		<isbn>0876391099, 9780876391099</isbn>
		<title>Complete Guide to Film Scoring</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/258fa4b5de25f3adb4f2da87e55e7fe3a/joaakive</id>
		<tags>memory</tags>
		<tags>music</tags>
		<description></description>
		<date>2015-01-26 12:46:48</date>
		<count>1</count>
		<publisher>MIT Press</publisher>
		<year>2000</year>
		<url></url>
		<author>Bob Snyder</author>
		<authors>
			<first>Bob</first>
		</authors>
		<authors>
			<last>Snyder</last>
		</authors>
		<title>Music and Memory: An Introduction</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/236e957675f73215f1d2405a0a27248fe/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-11-28 00:00:00</date>
		<count>1</count>
		<series>Computational Music Science</series>
		<publisher>Springer</publisher>
		<year>2014</year>
		<url></url>
		<author>Soubhik Chakraborty</author>
		<author>Guerino Mazzola</author>
		<author>Swarima Tewari</author>
		<author>Moujhuri Patra</author>
		<authors>
			<first>Soubhik</first>
		</authors>
		<authors>
			<last>Chakraborty</last>
		</authors>
		<authors>
			<first>Guerino</first>
		</authors>
		<authors>
			<last>Mazzola</last>
		</authors>
		<authors>
			<first>Swarima</first>
		</authors>
		<authors>
			<last>Tewari</last>
		</authors>
		<authors>
			<first>Moujhuri</first>
		</authors>
		<authors>
			<last>Patra</last>
		</authors>
		<pages>1-107</pages>
		<isbn>978-3-319-11472-9</isbn>
		<title>Computational Musicology in Hindustani Music</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2cfe095041de016d4ac3f4ecc51922d70/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-02-07 00:00:00</date>
		<count>1</count>
		<series>Computational Music Science</series>
		<publisher>Springer</publisher>
		<year>2009</year>
		<url>http://dx.doi.org/10.1007/978-3-642-00148-2</url>
		<author>Gérard Milmeister</author>
		<authors>
			<first>Gérard</first>
		</authors>
		<authors>
			<last>Milmeister</last>
		</authors>
		<pages>1-293</pages>
		<isbn>978-3-642-00148-2</isbn>
		<title>The Rubato Composer Music Software - Component-Based Implementation of a Functorial Concept Architecture</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23dc080d57b89c207aba5a2398324a6d0/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-07-18 00:00:00</date>
		<count>1</count>
		<booktitle>Multimodal Music Processing</booktitle>
		<series>Dagstuhl Follow-Ups</series>
		<publisher>Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, Germany</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/dagstuhl/dfu3.html#ThomasFMC12</url>
		<author>Verena Thomas</author>
		<author>Christian Fremerey</author>
		<author>Meinard Müller</author>
		<author>Michael Clausen</author>
		<authors>
			<first>Verena</first>
		</authors>
		<authors>
			<last>Thomas</last>
		</authors>
		<authors>
			<first>Christian</first>
		</authors>
		<authors>
			<last>Fremerey</last>
		</authors>
		<authors>
			<first>Meinard</first>
		</authors>
		<authors>
			<last>Müller</last>
		</authors>
		<authors>
			<first>Michael</first>
		</authors>
		<authors>
			<last>Clausen</last>
		</authors>
		<editor>Meinard Müller</editor>
		<editor>Masataka Goto</editor>
		<editor>Markus Schedl</editor>
		<editors>
			<first>Michael</first>
		</editors>
		<editors>
			<last>Clausen</last>
		</editors>
		<editors>
			<first>Michael</first>
		</editors>
		<editors>
			<last>Clausen</last>
		</editors>
		<editors>
			<first>Michael</first>
		</editors>
		<editors>
			<last>Clausen</last>
		</editors>
		<volume>3</volume>
		<pages>1-22</pages>
		<isbn>978-3-939897-37-8</isbn>
		<title>Linking Sheet Music and Audio - Challenges and New Approaches.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c631a7f0d3e2266c87717b8a82e2a01b/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2014-07-18 00:00:00</date>
		<count>1</count>
		<booktitle>Multimodal Music Processing</booktitle>
		<series>Dagstuhl Follow-Ups</series>
		<publisher>Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, Germany</publisher>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/conf/dagstuhl/dfu3.html#Goto12</url>
		<author>Masataka Goto</author>
		<authors>
			<first>Masataka</first>
		</authors>
		<authors>
			<last>Goto</last>
		</authors>
		<editor>Meinard Müller</editor>
		<editor>Masataka Goto</editor>
		<editor>Markus Schedl</editor>
		<editors>
			<first>Masataka</first>
		</editors>
		<editors>
			<last>Goto</last>
		</editors>
		<editors>
			<first>Masataka</first>
		</editors>
		<editors>
			<last>Goto</last>
		</editors>
		<editors>
			<first>Masataka</first>
		</editors>
		<editors>
			<last>Goto</last>
		</editors>
		<volume>3</volume>
		<pages>217-226</pages>
		<isbn>978-3-939897-37-8</isbn>
		<title>Grand Challenges in Music Information Research.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28869408e5e9040545ed185ea16b21a39/simonha94</id>
		<tags>cover</tags>
		<tags>shingles</tags>
		<tags>embeddings</tags>
		<tags>audio</tags>
		<tags>plk</tags>
		<description>Applied Sciences | Free Full-Text | Learning Low-Dimensional Embeddings of Audio Shingles for Cross-Version Retrieval of Classical Music</description>
		<date>2021-09-28 15:19:28</date>
		<count>1</count>
		<journal>Applied Sciences</journal>
		<publisher>MDPI AG</publisher>
		<year>2019</year>
		<url>https://doi.org/10.3390%2Fapp10010019</url>
		<author>Frank Zalkow</author>
		<author>Meinard Müller</author>
		<authors>
			<first>Frank</first>
		</authors>
		<authors>
			<last>Zalkow</last>
		</authors>
		<authors>
			<first>Meinard</first>
		</authors>
		<authors>
			<last>Müller</last>
		</authors>
		<volume>10</volume>
		<number>1</number>
		<pages>19</pages>
		<doi>10.3390/app10010019</doi>
		<title>Learning Low-Dimensional Embeddings of Audio Shingles for Cross-Version Retrieval of Classical Music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20e84e818863822c4852226f8654fa92d/ijtsrd</id>
		<tags>a</tags>
		<tags>role</tags>
		<tags>rhythms</tags>
		<tags>in</tags>
		<tags>costumes</tags>
		<tags>performance</tags>
		<tags>type</tags>
		<tags>characters</tags>
		<tags>cognitive</tags>
		<tags>and</tags>
		<tags>interpretation</tags>
		<tags>heroes</tags>
		<tags>Kathakali</tags>
		<tags>anti-heroes</tags>
		<tags>contradiction</tags>
		<description></description>
		<date>2021-07-14 08:18:12</date>
		<count>1</count>
		<journal>International Journal of Trend in Scientific Research and Development</journal>
		<year>2021</year>
		<url>https://www.ijtsrd.comhumanities-and-the-arts/music/41239/the-chuvannathadi-in-kathakali-an-evaluation-of-the-character-type-vesham/dr-mohan-gopinath</url>
		<author>Dr. Mohan Gopinath</author>
		<authors>
			<first>Dr. Mohan</first>
		</authors>
		<authors>
			<last>Gopinath</last>
		</authors>
		<volume>5</volume>
		<number>4</number>
		<pages>251-255</pages>
		<abstract>The paper brings out the importance of the chuvannathadi in Kathakali, a character type which for many years earlier did not receive the importance it inherently possesses. It analyses the costume and the characters from the MahaBharatham and the Ramayanam who are linked to a chuvannathadi and then goes on to analyse how GuruNanu Nair who rightfully considered as the master of this role in modern times, gave the chuvannathadi the importance it deserved. The paper also looks at the concept of cognitive contradiction a term I have coined and makes references to Shakespeare’s Shylock from the play The Merchant of Venice. It goes on to discuss the when discussing the concept of the anti hero, a role in which Guru Nanu Nair specialized. Dr. Mohan Gopinath "The Chuvannathadi in Kathakali: An Evaluation of the Character Type (Vesham)" Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-5 | Issue-4 , June 2021, URL: https://www.ijtsrd.compapers/ijtsrd41239.pdf Paper URL: https://www.ijtsrd.comhumanities-and-the-arts/music/41239/the-chuvannathadi-in-kathakali-an-evaluation-of-the-character-type-vesham/dr-mohan-gopinath</abstract>
		<language>english</language>
		<issn>2456-6470</issn>
		<title>The Chuvannathadi in Kathakali An Evaluation of the Character Type Vesham</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2440ccb7ee441785d0ed5af748f4d506c/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2020-06-15 00:00:00</date>
		<count>1</count>
		<booktitle>Guide to Computing for Expressive Music Performance</booktitle>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/books/collections/KM2013.html#KirkeM13a</url>
		<author>Alexis Kirke</author>
		<author>Eduardo R. Miranda</author>
		<authors>
			<first>Alexis</first>
		</authors>
		<authors>
			<last>Kirke</last>
		</authors>
		<authors>
			<first>Eduardo R.</first>
		</authors>
		<authors>
			<last>Miranda</last>
		</authors>
		<editor>Alexis Kirke</editor>
		<editor>Eduardo R. Miranda</editor>
		<editors>
			<first>Eduardo R.</first>
		</editors>
		<editors>
			<last>Miranda</last>
		</editors>
		<editors>
			<first>Eduardo R.</first>
		</editors>
		<editors>
			<last>Miranda</last>
		</editors>
		<pages>1-47</pages>
		<isbn>978-1-4471-4122-8</isbn>
		<title>An Overview of Computer Systems for Expressive Music Performance.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ab34b3f3ded52f5f8c8995a07ea82388/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2019-09-16 00:00:00</date>
		<count>1</count>
		<booktitle>Guide to Computing for Expressive Music Performance</booktitle>
		<publisher>Springer</publisher>
		<year>2013</year>
		<url>http://dblp.uni-trier.de/db/books/collections/KM2013.html#0001MP13</url>
		<author>Rafael Ramírez</author>
		<author>Esteban Maestre</author>
		<author>Alfonso Pérez</author>
		<authors>
			<first>Rafael</first>
		</authors>
		<authors>
			<last>Ramírez</last>
		</authors>
		<authors>
			<first>Esteban</first>
		</authors>
		<authors>
			<last>Maestre</last>
		</authors>
		<authors>
			<first>Alfonso</first>
		</authors>
		<authors>
			<last>Pérez</last>
		</authors>
		<editor>Alexis Kirke</editor>
		<editor>Eduardo R. Miranda</editor>
		<editors>
			<first>Alfonso</first>
		</editors>
		<editors>
			<last>Pérez</last>
		</editors>
		<editors>
			<first>Alfonso</first>
		</editors>
		<editors>
			<last>Pérez</last>
		</editors>
		<pages>123-144</pages>
		<isbn>978-1-4471-4122-8</isbn>
		<title>Modeling, Analyzing, Identifying, and Synthesizing Expressive Popular Music Performances.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2905fab2f7ca92e279b0e6f642651fe8c/bauerc</id>
		<tags>imported</tags>
		<tags>myown</tags>
		<tags>music</tags>
		<tags>informatics</tags>
		<tags>business</tags>
		<tags>ecosystem</tags>
		<description></description>
		<date>2019-04-27 18:20:03</date>
		<count>1</count>
		<booktitle>Musikwirtschaftsforschung: Die Grundlagen einer neuen Disziplin</booktitle>
		<series>Musikwirtschafts- und Musikkulturforschung book series (MUSIK)</series>
		<publisher>Springer</publisher>
		<address>Wiesbaden, Germany</address>
		<year>2018</year>
		<url></url>
		<author>Christine Bauer</author>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<editor>Peter Tschmuck</editor>
		<editor>Beate Flath</editor>
		<editor>Martin Lücke</editor>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<editors>
			<first>Christine</first>
		</editors>
		<editors>
			<last>Bauer</last>
		</editors>
		<pages>97-116</pages>
		<abstract>This article takes the computer science perspective on music business research. It outlines the object of knowledge at the core this perspective and discusses the set of available methodological instruments. Thereby, this work substantiates that the computer science perspective on music business research has both a descriptive as well as a normative objective, including also the design and evaluation of artefacts in the real world setting of music business.
In a deep discussion, this work exemplifies problems and research questions that the computer science perspective on music business research is confronted with. Concrete examples for research fields are (i) music recommender systems, (ii) improvement in skills to use technology, and (iii) monitoring and reporting of digital music consumption.</abstract>
		<isbn>978-3-658-19399-7, 978-3-658-19398-0</isbn>
		<language>German</language>
		<doi>10.1007/978-3-658-19399-7_6</doi>
		<title>Der Beitrag der Informatik zur Musikwirtschaftsforschung</title>
		<pubtype>inbook</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/232a586cbcb1a3d89c142899db1a2366a/bauerc</id>
		<tags>imported</tags>
		<tags>myown</tags>
		<tags>music</tags>
		<tags>bands</tags>
		<tags>events</tags>
		<description></description>
		<date>2019-04-27 18:20:03</date>
		<count>1</count>
		<series>Electronic Business</series>
		<publisher>Peter Lang</publisher>
		<address>Frankfurt am Main, Berlin, Bern, Bruxelles, New York, Oxford, Wien</address>
		<year>2012</year>
		<url></url>
		<author>Christine Bauer</author>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<volume>8</volume>
		<abstract>Managing a big band is a challenge, similar to managing a small or medium- sized enterprise. A type of band particularly difficult to manage is a telephone band that does not have a fixed line-up of musicians. Together, the musicians form a virtual organisation with the bandleader as a focal company. Every parti- cipant in the organisation brings in a certain set of skills, has specific business goals, and has to bear some risks. The focal company has to assume full contrac- tual liability to the event organiser. However, bandleaders managing these orga- nisational constructs typically have an artistic background rather than a mana- gerial one.
To date, the processes involved in managing bands have not been analysed. It is nearly impossible to improve these processes because processes are not clear. In a competitive environment, members do not seek to share knowledge on processes with their competitors because knowledge on processes is a busi- ness asset. However, as virtual organisations, musicians and bandleaders are mutually dependent. Accordingly, knowledge sharing forms the basis for pro- cess improvements, which can only be achieved by joint efforts.
Accordingly, this work delivers results in the following areas. First, this in- vestigation targets the activities involved in managing a medium-sized telephone band, made transparent by modelling the processes. Second, this work analyses the resulting models and suggests points for improvement with particular em- phasis on the adoption of information and communication technologies.
Due to the case study’s explorative nature, using qualitative research me- thods appears to be the most appropriate alternative in this context. Data is col- lected through a semi-structured interview and direct participant-observation. Findings are modelled adhering to the UML (Unified Modeling Language) nota- tion for activity diagrams. For deriving implications and suggestions for process improvement, a SWOT (strengths, weaknesses, opportunities, threats) analysis is performed.
This study’s major findings include a thorough documentation of processes, making tacit knowledge explicit. Emphasising the use of ICT (information and communication technologies), the findings provide a chronological sequence of activities that may be generalised to band and event management.</abstract>
		<isbn>978-3-631-63057-0</isbn>
		<language>English</language>
		<doi>10.3726/978-3-653-01223-1</doi>
		<title>Bands as Virtual Organisations: Improving the Processes of Band and Event Management with Information and Communication Technologies</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2667c24dfbe81c8e24a5feceae3fa7ebd/bauerc</id>
		<tags>imported</tags>
		<tags>myown</tags>
		<tags>music</tags>
		<tags>ecosystem</tags>
		<description></description>
		<date>2019-04-27 18:20:03</date>
		<count>1</count>
		<journal>OCG Journal</journal>
		<year>2015</year>
		<url>http://www.ocg.at/sites/ocg.at/files/medien/pdfs/OCG-Journal1502.pdf</url>
		<author>Christine Bauer</author>
		<authors>
			<first>Christine</first>
		</authors>
		<authors>
			<last>Bauer</last>
		</authors>
		<volume>40</volume>
		<number>2</number>
		<pages>12-14</pages>
		<issn>1728-743X</issn>
		<language>German</language>
		<title>Der digitale Musikmarkt im Wandel: Marktteilnehmer, Machtverschiebungen und die Hoffnung auf Transparenz (The changing digital music market: market participants, power shifts, and the hope for transparency)</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a425c7b31a32cf2de5d92e145dfa7360/researchparks</id>
		<tags>culture</tags>
		<tags>nationalmusic</tags>
		<tags>younggeneration</tags>
		<tags>Contest</tags>
		<description></description>
		<date>2021-03-01 10:19:01</date>
		<count>1</count>
		<journal>International Journal on Integrated Education</journal>
		<year>2020</year>
		<url>https://journals.researchparks.org/index.php/IJIE/article/view/337</url>
		<author>Toshtemirov Kobuljon Qubbijonovich</author>
		<authors>
			<first>Toshtemirov Kobuljon</first>
		</authors>
		<authors>
			<last>Qubbijonovich</last>
		</authors>
		<volume>3</volume>
		<number>3</number>
		<pages>15-18</pages>
		<abstract>It is not an exaggeration to say that this article will help to show the prestige of festivals. It should be noted that the festivals organized by F.Abdurakhimova show that all participants are working on their own. It is not wrong to say that this article aims to highlight the accomplishments and future goals of the participants Toshtemirov Kobuljon Qubbijonovich 2020. The role of music festivals in determining the gift of youth. International Journal on Integrated Education. 3, 4 (Apr. 2020), 15-18. DOI:https://doi.org/10.31149/ijie.v3i4.337 Pdf Url : https://journals.researchparks.org/index.php/IJIE/article/view/337/330 Paper Url : https://journals.researchparks.org/index.php/IJIE/article/view/337</abstract>
		<language>English</language>
		<issn>2620-3502</issn>
		<title>The role of music festivals in determining the gift of youth</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21095eae0076dc42598c1205a14f2775c/researchparks</id>
		<tags>performanceonmusicinstrument</tags>
		<tags>shapingskillsteachersofthemusic</tags>
		<tags>listeningthemusic</tags>
		<tags>choralchant</tags>
		<description></description>
		<date>2021-02-27 08:46:15</date>
		<count>1</count>
		<journal>International Journal on Integrated Education</journal>
		<year>2020</year>
		<url>https://journals.researchparks.org/index.php/IJIE/article/view/222</url>
		<author>Muradov Mukhitdin Kadirovich | Khalilov F.N.</author>
		<authors>
			<first>Muradov Mukhitdin Kadirovich | Khalilov</first>
		</authors>
		<authors>
			<last>F.N.</last>
		</authors>
		<volume>2</volume>
		<number>4</number>
		<pages>16-18</pages>
		<abstract>Much profile is a discriminating particularity to activity of the teacher of the music. As is well known, lesson of the music comprises of itself choral chant, performance on music instrument, listening the music and analysis heard, questions historian-theoretical cycle, solfeggio and t d. Teacher of the music must possess deep "three-dimensional" knowledge’s, broad range of the music professions. Muradov Mukhitdin Kadirovich and Khalilov F.N. 2020. Improvement of professional quality of skills of music teachers. International Journal on Integrated Education. 2, 4 (Mar. 2020), 16-18. DOI:https://doi.org/10.31149/ijie.v2i4.222. Pdf Url : https://journals.researchparks.org/index.php/IJIE/article/view/222/215 Paper Url : https://journals.researchparks.org/index.php/IJIE/article/view/222</abstract>
		<language>English</language>
		<issn>2620-3502</issn>
		<title>Improvement of professional quality of skills of music teachers</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b3254363923fd6c3f4fef8252e0a7824/sapo</id>
		<tags>symbolic_music_analysis</tags>
		<description>The Use of Large Corpora to Train a New Type of Key-Finding Algorithm | Music Perception | University of California Press</description>
		<date>2020-12-10 10:56:05</date>
		<count>1</count>
		<journal>Music Perception</journal>
		<publisher>University of California Press</publisher>
		<year>2013</year>
		<url>https://doi.org/10.1525%2Fmp.2013.31.1.59</url>
		<author>Joshua Albrecht</author>
		<author>Daniel Shanahan</author>
		<authors>
			<first>Joshua</first>
		</authors>
		<authors>
			<last>Albrecht</last>
		</authors>
		<authors>
			<first>Daniel</first>
		</authors>
		<authors>
			<last>Shanahan</last>
		</authors>
		<volume>31</volume>
		<number>1</number>
		<pages>59--67</pages>
		<doi>10.1525/mp.2013.31.1.59</doi>
		<title>The Use of Large Corpora to Train a New Type of Key-Finding Algorithm</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26a3a60d54be8927ea7a9bae0bd26cc39/sapo</id>
		<tags>imported</tags>
		<tags>phdthesis</tags>
		<description></description>
		<date>2021-11-22 11:44:52</date>
		<count>2</count>
		<booktitle>Proceedings of the 12th International Society for Music Information Retrieval Conference (ISMIR 2011)</booktitle>
		<publisher>University of Miami</publisher>
		<year>2011</year>
		<url></url>
		<author>Rudolf Mayer</author>
		<author>Andreas Rauber</author>
		<authors>
			<first>Rudolf</first>
		</authors>
		<authors>
			<last>Mayer</last>
		</authors>
		<authors>
			<first>Andreas</first>
		</authors>
		<authors>
			<last>Rauber</last>
		</authors>
		<pages>675--680</pages>
		<isbn>978-0-615-54865-4</isbn>
		<title>Musical Genre Classification by Ensembles of Audio and Lyrics Features</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2eb01e943a056779e20fbd46bd5098c70/sapo</id>
		<tags>imported</tags>
		<tags>phdthesis</tags>
		<description></description>
		<date>2021-11-22 11:44:52</date>
		<count>3</count>
		<journal>Prooceedings of the International Society for Music Information Retrieval Conference</journal>
		<address>Delft, The Netherlands</address>
		<year>2019</year>
		<url></url>
		<author>Alexander Lerch</author>
		<author>Claire Arthur</author>
		<author>Ashis Pati</author>
		<author>Siddharth Gururani</author>
		<authors>
			<first>Alexander</first>
		</authors>
		<authors>
			<last>Lerch</last>
		</authors>
		<authors>
			<first>Claire</first>
		</authors>
		<authors>
			<last>Arthur</last>
		</authors>
		<authors>
			<first>Ashis</first>
		</authors>
		<authors>
			<last>Pati</last>
		</authors>
		<authors>
			<first>Siddharth</first>
		</authors>
		<authors>
			<last>Gururani</last>
		</authors>
		<title>Music Performance Analysis: A Survey</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25526ca29109a416755fd46d35319da2d/sapo</id>
		<tags>imported</tags>
		<tags>phdthesis</tags>
		<description></description>
		<date>2019-06-27 20:25:08</date>
		<count>2</count>
		<booktitle>Advances in Music Information Retrieval</booktitle>
		<publisher>Springer Berlin Heidelberg</publisher>
		<year>2010</year>
		<url></url>
		<author>Tetsuro Kitahara</author>
		<authors>
			<first>Tetsuro</first>
		</authors>
		<authors>
			<last>Kitahara</last>
		</authors>
		<pages>65--91</pages>
		<title>Mid-level Representations of Musical Audio Signals for Music Information Retrieval</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2cb0e243e977a0575c2c14e1738b4b2ad/sapo</id>
		<tags>imported</tags>
		<tags>phdthesis</tags>
		<description></description>
		<date>2021-11-22 11:44:52</date>
		<count>1</count>
		<booktitle>Proceedings of the Fourth Conference on Interdisciplinary Musicology (CIM08)</booktitle>
		<address>Thessaloniki, Greece</address>
		<year>2008</year>
		<url></url>
		<author>Cihan Isikhan</author>
		<author>Giyasettin Ozcan</author>
		<authors>
			<first>Cihan</first>
		</authors>
		<authors>
			<last>Isikhan</last>
		</authors>
		<authors>
			<first>Giyasettin</first>
		</authors>
		<authors>
			<last>Ozcan</last>
		</authors>
		<title>A Survey of Melody Extraction Techniques for Music Information Retrieval</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ba0d3a4233551ffa39ad1fe757014a43/sapo</id>
		<tags>imported</tags>
		<tags>phdthesis</tags>
		<description></description>
		<date>2021-11-22 11:44:52</date>
		<count>1</count>
		<journal>Journal of New Music Research</journal>
		<publisher>Taylor & Francis</publisher>
		<year>1999</year>
		<url></url>
		<author>Adam Lindsay</author>
		<author>Werner Kriechbaum</author>
		<authors>
			<first>Adam</first>
		</authors>
		<authors>
			<last>Lindsay</last>
		</authors>
		<authors>
			<first>Werner</first>
		</authors>
		<authors>
			<last>Kriechbaum</last>
		</authors>
		<volume>28</volume>
		<number>4</number>
		<pages>364--372</pages>
		<title>There's more than one way to hear it: multiple representations of music in MPEG-7</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f793adb5d49e92c53d83be5304edc6e5/zazi</id>
		<tags>imported</tags>
		<description></description>
		<date>2011-08-19 12:45:25</date>
		<count>1</count>
		<year>2011</year>
		<url>http://www.amplifindmusicservices.com/what/amplifind.php</url>
		<author> AmpliFIND Music Services</author>
		<authors>
			<first></first>
		</authors>
		<authors>
			<last>AmpliFIND Music Services</last>
		</authors>
		<title>AmpliFIND (formerly MusicDNS)</title>
		<pubtype>webpage</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22a46169080c7e748f3e4af230185f993/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2011-04-01 00:00:00</date>
		<count>1</count>
		<booktitle>EUROSPEECH</booktitle>
		<publisher>ISCA</publisher>
		<year>1993</year>
		<url>http://dblp.uni-trier.de/db/conf/interspeech/eurospeech1993.html#MusicP93</url>
		<author>Bradley Music</author>
		<author>Claus Povlsen</author>
		<authors>
			<first>Bradley</first>
		</authors>
		<authors>
			<last>Music</last>
		</authors>
		<authors>
			<first>Claus</first>
		</authors>
		<authors>
			<last>Povlsen</last>
		</authors>
		<title>The NLP module of a spoken language dialogue system for Danish flight reservations.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/257270eb52f719e65da9d0fb6db81e94a/mediadigits</id>
		<tags>semantic</tags>
		<tags>long-tail</tags>
		<tags>recommendation</tags>
		<tags>networks</tags>
		<description>Very interesting PhD Thesis with a very interesting comparison of different techniques.</description>
		<date>2009-04-14 19:08:06</date>
		<count>2</count>
		<address>Barcelona, Spain</address>
		<year>2008</year>
		<url>http://mtg.upf.edu/~ocelma/PhD/doc/ocelma-thesis.pdf</url>
		<author>Oscar Celma</author>
		<authors>
			<first>Oscar</first>
		</authors>
		<authors>
			<last>Celma</last>
		</authors>
		<abstract>Music consumption is biased towards a few popular artists. For instance, in 2007 only 1% of all digital tracks accounted for 80% of all sales. Similarly, 1,000 albums accounted for 50% of all album sales, and 80% of all albums sold were purchased less than 100 times. There is a need to assist people to filter, discover, personalise and recommend from the huge amount of music content available along the Long Tail. Current music recommendation algorithms try to accurately predict what people demand to listen to. However, quite often these algorithms tend to recommend popular -or well-known to the user- music, decreasing the effectiveness of the recommendations. These approaches focus on improving the accuracy of the recommendations. That is, try to make accurate predictions about what a user could listen to, or buy next, independently of how useful to the user could be the provided recommendations. In this Thesis we stress the importance of the user's perceived quality of the recommendations. We model the Long Tail curve of artist popularity to predict -potentially-interesting and unknown music, hidden in the tail of the popularity curve. Effective recommendation systems should promote novel and relevant material (non-obvious recommendations), taken primarily from the tail of a popularity distribution. The main contributions of this Thesis are: <i>(i)</i> a novel network-based approach for recommender systems, based on the analysis of the item (or user) similarity graph, and the popularity of the items, <i>(ii)</i> a user-centric evaluation that measures the user's relevance and novelty of the recommendations, and <i>(iii)</i> two prototype systems that implement the ideas derived from the theoretical work. Our findings have significant implications for recommender systems that assist users to explore the Long Tail, digging for content they might like.</abstract>
		<title>Music Recommendation and Discovery in the Long Tail</title>
		<pubtype>phdthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/212c7b77048aae021c60a5e0ae1a3a797/kurtjx</id>
		<tags>music_ontology</tags>
		<description></description>
		<date>2008-04-11 18:39:30</date>
		<count>7</count>
		<year>2007</year>
		<url></url>
		<author>Y Raimond</author>
		<author>S Abdallah</author>
		<author>M Sandler</author>
		<author>F Giasson</author>
		<authors>
			<first>Y</first>
		</authors>
		<authors>
			<last>Raimond</last>
		</authors>
		<authors>
			<first>S</first>
		</authors>
		<authors>
			<last>Abdallah</last>
		</authors>
		<authors>
			<first>M</first>
		</authors>
		<authors>
			<last>Sandler</last>
		</authors>
		<authors>
			<first>F</first>
		</authors>
		<authors>
			<last>Giasson</last>
		</authors>
		<title>The Music Ontology</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ab15003bb9e463fb66d5b8120bd030b4/kurtjx</id>
		<tags>music</tags>
		<tags>similarity</tags>
		<description>main bib file</description>
		<date>2008-03-17 17:08:40</date>
		<count>2</count>
		<year>2006</year>
		<url></url>
		<author>E. Pampalk</author>
		<authors>
			<first>E.</first>
		</authors>
		<authors>
			<last>Pampalk</last>
		</authors>
		<title>Computational Models of Music Similarity and their Application in Music Information Retrival</title>
		<pubtype>phdthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b9e76d5fd4f6927815d9845d20470ef0/kurtjx</id>
		<tags>imported</tags>
		<description>main bib file</description>
		<date>2008-03-17 17:08:40</date>
		<count>5</count>
		<journal>Computer Music J.</journal>
		<publisher>MIT Press</publisher>
		<address>Cambridge, MA, USA</address>
		<year>2004</year>
		<url></url>
		<author>Adam Berenzweig</author>
		<author>Beth Logan</author>
		<author>Daniel P. W. Ellis</author>
		<author>Brian P. W. Whitman</author>
		<authors>
			<first>Adam</first>
		</authors>
		<authors>
			<last>Berenzweig</last>
		</authors>
		<authors>
			<first>Beth</first>
		</authors>
		<authors>
			<last>Logan</last>
		</authors>
		<authors>
			<first>Daniel P. W.</first>
		</authors>
		<authors>
			<last>Ellis</last>
		</authors>
		<authors>
			<first>Brian P. W.</first>
		</authors>
		<authors>
			<last>Whitman</last>
		</authors>
		<volume>28</volume>
		<number>2</number>
		<pages>63-76</pages>
		<issn>0148-9267</issn>
		<doi>http://dx.doi.org/10.1162/014892604323112257</doi>
		<title>A Large-Scale Evaluation of Acoustic and Subjective Music-Similarity Measures</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/279c3ee6ac2e4d5e41fb5f2c10828e36c/kurtjx</id>
		<tags>similarity,</tags>
		<tags>music</tags>
		<tags>collaborative</tags>
		<tags>ratings,</tags>
		<tags>user</tags>
		<tags>filtering</tags>
		<description>main bib file</description>
		<date>2008-03-17 17:08:40</date>
		<count>3</count>
		<booktitle>Proc. of Int. Symposium on Music Information Retrieval</booktitle>
		<year>2007</year>
		<url></url>
		<author>Malcom Slaney</author>
		<author>William White</author>
		<authors>
			<first>Malcom</first>
		</authors>
		<authors>
			<last>Slaney</last>
		</authors>
		<authors>
			<first>William</first>
		</authors>
		<authors>
			<last>White</last>
		</authors>
		<title>Similarity Based On Rating Data</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b170ab92de4041cf2d84133d7e70bd90/yaxu</id>
		<tags>toplap</tags>
		<tags>livecoding</tags>
		<description>The Programming Language as a Musical Instrument (ResearchIndex)</description>
		<date>2007-03-12 23:09:33</date>
		<count>4</count>
		<journal>Psychology of Programming Interest Group</journal>
		<year>2005</year>
		<url>http://citeseer.ist.psu.edu/blackwell05programming.html</url>
		<author>Alan Blackwell</author>
		<author>Nick Collins</author>
		<authors>
			<first>Alan</first>
		</authors>
		<authors>
			<last>Blackwell</last>
		</authors>
		<authors>
			<first>Nick</first>
		</authors>
		<authors>
			<last>Collins</last>
		</authors>
		<title>The Programming Language as a Musical Instrument</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/238610fd3b4ec3711782eb6d35bff733c/yaxu</id>
		<tags>toplap</tags>
		<tags>livecoding</tags>
		<description>On-the-fly programming</description>
		<date>2007-03-12 23:17:29</date>
		<count>3</count>
		<booktitle>NIME '04: Proceedings of the 2004 conference on New interfaces for musical expression</booktitle>
		<publisher>National University of Singapore</publisher>
		<address>Singapore, Singapore</address>
		<year>2004</year>
		<url></url>
		<author>Ge Wang</author>
		<author>Perry R. Cook</author>
		<authors>
			<first>Ge</first>
		</authors>
		<authors>
			<last>Wang</last>
		</authors>
		<authors>
			<first>Perry R.</first>
		</authors>
		<authors>
			<last>Cook</last>
		</authors>
		<pages>138--143</pages>
		<title>On-the-fly programming: using code as an expressive musical instrument</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e0c04c1582630877beb82e0006bd3579/yijunyu</id>
		<tags>music</tags>
		<tags>composition</tags>
		<tags>aspects</tags>
		<description></description>
		<date>2007-03-17 15:34:03</date>
		<count>2</count>
		<booktitle>AOSD '06: Proceedings of the 5th international conference on Aspect-oriented software development</booktitle>
		<publisher>ACM Press</publisher>
		<address>New York, NY, USA</address>
		<year>2006</year>
		<url>http://portal.acm.org/citation.cfm?id=1119685</url>
		<author>Patrick Hill</author>
		<author>Simon Holland</author>
		<author>Robin Laney</author>
		<authors>
			<first>Patrick</first>
		</authors>
		<authors>
			<last>Hill</last>
		</authors>
		<authors>
			<first>Simon</first>
		</authors>
		<authors>
			<last>Holland</last>
		</authors>
		<authors>
			<first>Robin</first>
		</authors>
		<authors>
			<last>Laney</last>
		</authors>
		<pages>226--236</pages>
		<doi>10.1145/1119655.1119685</doi>
		<isbn>159593300X</isbn>
		<title>Symmetric composition of musical concerns</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25cf90adfd15f58cd134f91ca89741ac5/iblis</id>
		<tags>imported</tags>
		<description></description>
		<date>2007-03-19 17:08:38</date>
		<count>1</count>
		<journal>The Journal of the Royal Anthropological Institute</journal>
		<publisher>Royal Anthropological Institute of Great Britain and Ireland</publisher>
		<year>1997</year>
		<url>http://links.jstor.org/sici?sici=1359-0987%28199712%293%3A4%3C673%3AVAPHRA%3E2.0.CO%3B2-X</url>
		<author>Martin Stokes</author>
		<authors>
			<first>Martin</first>
		</authors>
		<authors>
			<last>Stokes</last>
		</authors>
		<volume>3</volume>
		<number>4</number>
		<pages>673--691</pages>
		<abstract>This article examines the ways in which musicians participate in historical knowledge, through an analysis of the spatial metaphors that surround musical experience in two distinct urban locales in Algeria and Turkey. The article proposes an alternative to two approaches which have dominated the sociological and anthropological discussion of music: one which suggests that music simply reproduces social conditions; and another which suggests that it opposes them. Here I argue that the temporalities created through music constitute a form of engagement with experiences of time and history generated elsewhere.</abstract>
		<issn>1359-0987</issn>
		<jstor_date>199712</jstor_date>
		<copyright>Copyright 1997 Royal Anthropological Institute of Great Britain and Ireland</copyright>
		<title>Voices and Places: History, Repetition and the Musical Imagination</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e8376a48c036e9753c35c62c837e1575/stefano</id>
		<tags>genetic</tags>
		<tags>music</tags>
		<tags>creativity</tags>
		<tags>algorithm</tags>
		<description>dblp</description>
		<date>2007-05-16</date>
		<count>3</count>
		<journal>Computers and Artificial Intelligence</journal>
		<year>2005</year>
		<url>http://dblp.uni-trier.de/db/journals/cai/cai24.html#Dostal05</url>
		<author>Martin Dostál</author>
		<authors>
			<first>Martin</first>
		</authors>
		<authors>
			<last>Dostál</last>
		</authors>
		<volume>24</volume>
		<number>3</number>
		<title>Genetic Algorithms as a Model of Musical Creativity - on Generating of a Human-Like Rhythmic Accompaniment.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28b9137056a41ef7280ce55cd009fb594/ocelma</id>
		<tags>imported</tags>
		<description></description>
		<date>2008-02-17 16:51:33</date>
		<count>2</count>
		<booktitle>Proceedings of 4th International Symposium on Music Information Retrieval</booktitle>
		<address>Baltimore, Maryland</address>
		<year>2003</year>
		<url></url>
		<author>A. Berenzweig</author>
		<author>B. Logan</author>
		<author>D. Ellis</author>
		<author>B. Whitman</author>
		<authors>
			<first>A.</first>
		</authors>
		<authors>
			<last>Berenzweig</last>
		</authors>
		<authors>
			<first>B.</first>
		</authors>
		<authors>
			<last>Logan</last>
		</authors>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Ellis</last>
		</authors>
		<authors>
			<first>B.</first>
		</authors>
		<authors>
			<last>Whitman</last>
		</authors>
		<title>A large-scale evalutation of acoustic and subjective music similarity measures</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20ccd13b9e120468ac90e1512a1ab9c78/ocelma</id>
		<tags>imported</tags>
		<description></description>
		<date>2008-02-17 16:51:33</date>
		<count>2</count>
		<booktitle>Proceedings of 5th International Conference on Music Information Retrieval</booktitle>
		<address>Barcelona</address>
		<year>2004</year>
		<url></url>
		<author>M. Zadel</author>
		<author>I. Fujinaga</author>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Zadel</last>
		</authors>
		<authors>
			<first>I.</first>
		</authors>
		<authors>
			<last>Fujinaga</last>
		</authors>
		<title>Web Services for Music Information Retrieval</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29c3080f719e0f82411a32c024d0aa557/ocelma</id>
		<tags>imported</tags>
		<description></description>
		<date>2008-02-17 16:51:33</date>
		<count>2</count>
		<journal>Journal of New Music Research</journal>
		<year>2005</year>
		<url></url>
		<author>S. Baumann</author>
		<author>O. Hummel</author>
		<authors>
			<first>S.</first>
		</authors>
		<authors>
			<last>Baumann</last>
		</authors>
		<authors>
			<first>O.</first>
		</authors>
		<authors>
			<last>Hummel</last>
		</authors>
		<volume>34</volume>
		<number>2</number>
		<title>Enhancing Music Recommendation Algorithms Using Cultural Metadata</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/212b80875eacc7c391c37b42b46d73902/ocelma</id>
		<tags>imported</tags>
		<description></description>
		<date>2008-02-17 16:51:33</date>
		<count>2</count>
		<booktitle>Proceedings of 7th International Conference on Music Information Retrieval (to be published)</booktitle>
		<address>Victoria, Canada</address>
		<year>2006</year>
		<url></url>
		<author>V. Sandvold</author>
		<author>T. Aussenac</author>
		<author>O. Celma</author>
		<author>P. Herrera</author>
		<authors>
			<first>V.</first>
		</authors>
		<authors>
			<last>Sandvold</last>
		</authors>
		<authors>
			<first>T.</first>
		</authors>
		<authors>
			<last>Aussenac</last>
		</authors>
		<authors>
			<first>O.</first>
		</authors>
		<authors>
			<last>Celma</last>
		</authors>
		<authors>
			<first>P.</first>
		</authors>
		<authors>
			<last>Herrera</last>
		</authors>
		<title>Good Vibrations: Music Discovery through Personal Musical Concepts</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2feece0bd99d91771dcf814a5a99dee64/ocelma</id>
		<tags>imported</tags>
		<description></description>
		<date>2008-02-17 16:51:33</date>
		<count>2</count>
		<journal>INFORMS Journal on Computing, Special Cluster on Computation in Music</journal>
		<year>2006</year>
		<url></url>
		<author>E. Gomez</author>
		<authors>
			<first>E.</first>
		</authors>
		<authors>
			<last>Gomez</last>
		</authors>
		<volume>18</volume>
		<number>3</number>
		<title>Tonal description of polyphonic audio for music content processing</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2307278ae553bb8e9ff8b1822dadab578/ocelma</id>
		<tags>music,</tags>
		<tags>recommendation,</tags>
		<tags>ontology,</tags>
		<tags>foaf,</tags>
		<tags>rss,</tags>
		<tags>semanticweb</tags>
		<description>Imported BibTeX</description>
		<date>2008-05-30 07:04:55</date>
		<count>1</count>
		<booktitle>Proceedings of 5th International Semantic Web Conference</booktitle>
		<address>Athens, GA, USA</address>
		<year>2006</year>
		<url>http://dx.doi.org/10.1007/11926078\_67</url>
		<author>Òscar Celma</author>
		<authors>
			<first>Òscar</first>
		</authors>
		<authors>
			<last>Celma</last>
		</authors>
		<pages>927-934</pages>
		<doi>10.1007/11926078\_67</doi>
		<title>Foafing the Music: Bridging the Semantic Gap in Music Recommendation</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/263071527da9971a21d698f7be6dadd64/ocelma</id>
		<tags>imported</tags>
		<description>Imported BibTeX</description>
		<date>2008-05-30 07:04:55</date>
		<count>2</count>
		<booktitle>Proceedings of 8th International Conference on Music Information
	Retrieval</booktitle>
		<address>Vienna, Austria</address>
		<year>2007</year>
		<url>http://dblp.uni-trier.de/db/conf/icde/icdew2007.html#Donaldson07</url>
		<author>Justin Donaldson</author>
		<authors>
			<first>Justin</first>
		</authors>
		<authors>
			<last>Donaldson</last>
		</authors>
		<pages>811-817</pages>
		<title>Music Recommendation Mapping and Interface Based on Structural Network
	Entropy.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2cea7d699b87e468c1bb812baccdf3b04/ocelma</id>
		<tags>imported</tags>
		<description>Imported BibTeX</description>
		<date>2008-05-30 07:04:55</date>
		<count>1</count>
		<year>2001</year>
		<url></url>
		<author>Wei Chai</author>
		<author>Barry Vercoe</author>
		<authors>
			<first>Wei</first>
		</authors>
		<authors>
			<last>Chai</last>
		</authors>
		<authors>
			<first>Barry</first>
		</authors>
		<authors>
			<last>Vercoe</last>
		</authors>
		<title>Using User Models in Music Information Retrieval Systems</title>
		<pubtype>techreport</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20f1c679aac57b2a9109c218043653ee5/ocelma</id>
		<tags>imported</tags>
		<description>Imported BibTeX</description>
		<date>2008-05-30 07:04:55</date>
		<count>2</count>
		<booktitle>Proceedings of 8th International Conference on Music Information
	Retrieval</booktitle>
		<address>Vienna, Austria</address>
		<year>2007</year>
		<url></url>
		<author>Elias Pampalk</author>
		<author>Masataka Goto</author>
		<authors>
			<first>Elias</first>
		</authors>
		<authors>
			<last>Pampalk</last>
		</authors>
		<authors>
			<first>Masataka</first>
		</authors>
		<authors>
			<last>Goto</last>
		</authors>
		<title>MusicSun: A New Approach to Artist Recommendation</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/228bead1d29e91ecc7c6e3d8053555b68/ocelma</id>
		<tags>semantic</tags>
		<tags>music</tags>
		<tags>playlist</tags>
		<description>Imported BibTeX</description>
		<date>2008-05-30 07:04:55</date>
		<count>5</count>
		<booktitle>Proceedings of 6th International Conference on Music Information
	Retrieval</booktitle>
		<address>London, UK</address>
		<year>2005</year>
		<url>http://ismir2005.ismir.net/proceedings/2072.pdf</url>
		<author>Elias Pampalk</author>
		<author>Tim Pohle</author>
		<author>Gerhard Widmer</author>
		<authors>
			<first>Elias</first>
		</authors>
		<authors>
			<last>Pampalk</last>
		</authors>
		<authors>
			<first>Tim</first>
		</authors>
		<authors>
			<last>Pohle</last>
		</authors>
		<authors>
			<first>Gerhard</first>
		</authors>
		<authors>
			<last>Widmer</last>
		</authors>
		<editor>University of London Queen Mary</editor>
		<editors>
			<first>Gerhard</first>
		</editors>
		<editors>
			<last>Widmer</last>
		</editors>
		<abstract>Common approaches to creating playlists are to randomly shuffle a
	collection (e.g. iPod shuffle) or manually select songs. In this
	paper we present and evaluate heuristics to adapt playlists automatically
	given a song to start with (seed song) and immediate user feedback.
	Instead of rich metadata we use audio-based similarity. The user
	gives feedback by pressing a skip button if the user dislikes the
	current song. Songs similar to skipped songs are removed, while songs
	similar to accepted ones are added to the playlist. We evaluate the
	heuristics with hypothetical use cases. For each use case we assume
	a specific user behavior (e.g. the user always skips songs by a particular
	artist). Our results show that using audio similarity and simple
	heuristics it is possible to drastically reduce the number of necessary
	skips.</abstract>
		<title>Dynamic playlist generation based on skipping behavior</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/236a2185c5f7217fb7ed674a7f6eb0beb/ocelma</id>
		<tags>imported</tags>
		<description>Imported BibTeX</description>
		<date>2008-05-30 07:04:55</date>
		<count>3</count>
		<booktitle>Proceedings of 8th International Conference on Music Information
	Retrieval</booktitle>
		<address>Vienna, Austria</address>
		<year>2007</year>
		<url></url>
		<author>Daniel McEnnis</author>
		<author>Sally Jo Cunningham</author>
		<authors>
			<first>Daniel</first>
		</authors>
		<authors>
			<last>McEnnis</last>
		</authors>
		<authors>
			<first>Sally Jo</first>
		</authors>
		<authors>
			<last>Cunningham</last>
		</authors>
		<title>Sociology and Music Recommendation Systems</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/281358e75b4ee55b1be5146332373ab4f/matschmidt</id>
		<tags>Narrativität</tags>
		<description></description>
		<date>2011-11-28 09:17:54</date>
		<count>1</count>
		<journal>Journal of the Royal Musical Association</journal>
		<year>1990</year>
		<url></url>
		<author>Jean-Jacques Nattiez</author>
		<authors>
			<first>Jean-Jacques</first>
		</authors>
		<authors>
			<last>Nattiez</last>
		</authors>
		<volume>115</volume>
		<pages>240-257</pages>
		<title>Can one Speak of Narrativity in Music?</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20395a21b5e9ae1636403e9b6710d5e6e/cinzia</id>
		<tags>Performance</tags>
		<tags>of</tags>
		<tags>Music</tags>
		<tags>Psychology</tags>
		<tags>Anxiety</tags>
		<description></description>
		<date>2012-01-23 18:06:22</date>
		<count>1</count>
		<year>2011</year>
		<url></url>
		<author>Dianna T. Kenny</author>
		<authors>
			<first>Dianna T.</first>
		</authors>
		<authors>
			<last>Kenny</last>
		</authors>
		<editor>OXFORD University Press</editor>
		<editors>
			<first>Dianna T.</first>
		</editors>
		<editors>
			<last>Kenny</last>
		</editors>
		<title>The psychology of music performance anxiety</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/291c3f49033752dabc8edc87cdad85d36/lran022</id>
		<tags>imported</tags>
		<description></description>
		<date>2011-11-02 03:50:50</date>
		<count>2</count>
		<booktitle>ISMIR 2003, 4th Symposium Conference on Music
                   Information Retrieval</booktitle>
		<year>2003</year>
		<url></url>
		<author>A. L. Wang</author>
		<authors>
			<first>A. L.</first>
		</authors>
		<authors>
			<last>Wang</last>
		</authors>
		<pages>7--13</pages>
		<title>An industrial-strength audio search algorithm</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22381506d3367963f25d5cb4bfce13332/andymilne</id>
		<tags>imported</tags>
		<description></description>
		<date>2011-12-08 13:36:28</date>
		<count>1</count>
		<address>The Open University, Milton Keynes, UK</address>
		<year>2010</year>
		<url></url>
		<author>Andrew J. Milne</author>
		<authors>
			<first>Andrew J.</first>
		</authors>
		<authors>
			<last>Milne</last>
		</authors>
		<abstract>Microtonality is a huge and diverse area: I will be focussing on the use of microtonal well-formed scales that embed numerous major and minor triads. Such scales cannot be played in any conventional Western tuning (so they really are novel and different), but they also generalise many of the most important properties of the standard Western diatonic (major) scale (so they may provide a fertile resource for musical experimentation).

I'll also demonstrate a Thummer---a button-lattice MIDI controller that makes the playing of microtonal well-formed scales as straightforward as playing standard Western scales.</abstract>
		<title>Microtonal music theory</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2868395b083a7c0100c38a3b9d0f591df/andymilne</id>
		<tags>imported</tags>
		<description></description>
		<date>2011-12-08 13:36:28</date>
		<count>1</count>
		<booktitle>Proceedings of the 2011 International Conference on New Interfaces for Musical Expression (NIME11), Oslo, Norway</booktitle>
		<year>2011</year>
		<url></url>
		<author>Andrew J. Milne</author>
		<author>Anna Xambó</author>
		<author>Robin Laney</author>
		<author>David B. Sharp</author>
		<author>Anthony Prechtl</author>
		<author>Simon Holland</author>
		<authors>
			<first>Andrew J.</first>
		</authors>
		<authors>
			<last>Milne</last>
		</authors>
		<authors>
			<first>Anna</first>
		</authors>
		<authors>
			<last>Xambó</last>
		</authors>
		<authors>
			<first>Robin</first>
		</authors>
		<authors>
			<last>Laney</last>
		</authors>
		<authors>
			<first>David B.</first>
		</authors>
		<authors>
			<last>Sharp</last>
		</authors>
		<authors>
			<first>Anthony</first>
		</authors>
		<authors>
			<last>Prechtl</last>
		</authors>
		<authors>
			<first>Simon</first>
		</authors>
		<authors>
			<last>Holland</last>
		</authors>
		<editor>Alexander Refsum Jensenius</editor>
		<editor>Rolf Inge Godøy</editor>
		<editors>
			<first>Simon</first>
		</editors>
		<editors>
			<last>Holland</last>
		</editors>
		<editors>
			<first>Simon</first>
		</editors>
		<editors>
			<last>Holland</last>
		</editors>
		<title>Hex Player---a virtual musical controller</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/253ac83f204bd3d7e8daa5622a7b2588e/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2012-01-23 00:00:00</date>
		<count>2</count>
		<journal>Computer Music Journal</journal>
		<year>2011</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj35.html#KapurDDMHVB11</url>
		<author>Ajay Kapur</author>
		<author>Michael Darling</author>
		<author>Dimitri Diakopoulos</author>
		<author>Jim W. Murphy</author>
		<author>Jordan Hochenbaum</author>
		<author>Owen Vallis</author>
		<author>Curtis Bahn</author>
		<authors>
			<first>Ajay</first>
		</authors>
		<authors>
			<last>Kapur</last>
		</authors>
		<authors>
			<first>Michael</first>
		</authors>
		<authors>
			<last>Darling</last>
		</authors>
		<authors>
			<first>Dimitri</first>
		</authors>
		<authors>
			<last>Diakopoulos</last>
		</authors>
		<authors>
			<first>Jim W.</first>
		</authors>
		<authors>
			<last>Murphy</last>
		</authors>
		<authors>
			<first>Jordan</first>
		</authors>
		<authors>
			<last>Hochenbaum</last>
		</authors>
		<authors>
			<first>Owen</first>
		</authors>
		<authors>
			<last>Vallis</last>
		</authors>
		<authors>
			<first>Curtis</first>
		</authors>
		<authors>
			<last>Bahn</last>
		</authors>
		<volume>35</volume>
		<number>4</number>
		<pages>49-63</pages>
		<title>The Machine Orchestra: An Ensemble of Human Laptop Performers and Robotic Musical Instruments.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21dabc5996ca0128f77a96f954266c851/edaehn</id>
		<tags>imported</tags>
		<description></description>
		<date>2012-01-26 14:37:39</date>
		<count>3</count>
		<journal>User Modeling, Adaption and Personalization</journal>
		<publisher>Springer</publisher>
		<year>2011</year>
		<url></url>
		<author>M. Kaminskas</author>
		<author>F. Ricci</author>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Kaminskas</last>
		</authors>
		<authors>
			<first>F.</first>
		</authors>
		<authors>
			<last>Ricci</last>
		</authors>
		<pages>183--194</pages>
		<comment>UMAP, recommender, similarity, music, POIs</comment>
		<title>Location-adapted music recommendation using tags</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/201c008aff529e8b68a7d0798a2c1301d/tfalk</id>
		<tags>imported</tags>
		<description>My bibtex file</description>
		<date>2009-02-28 21:01:39</date>
		<count>9</count>
		<journal>Journal of New Music Research</journal>
		<year>2003</year>
		<url></url>
		<author>Jean-Julien Aucouturier</author>
		<author>Fran?ois Pachet</author>
		<authors>
			<first>Jean-Julien</first>
		</authors>
		<authors>
			<last>Aucouturier</last>
		</authors>
		<authors>
			<first>Fran?ois</first>
		</authors>
		<authors>
			<last>Pachet</last>
		</authors>
		<volume>32</volume>
		<pages>83-93</pages>
		<pdf>unsorted/pachet-02c.pdf</pdf>
		<title>Representing Musical Genre: A State of the Art</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ffa3b8d529ede65b8a4cde66e246810e/snarc</id>
		<tags>datamining</tags>
		<description></description>
		<date>2009-02-23 16:43:09</date>
		<count>7</count>
		<booktitle>Proceedings of the Seventh International Conference on Music Information Retrieval (ISMIR'06)</booktitle>
		<address>Victoria, Canada</address>
		<year>2006</year>
		<url>http://www.cp.jku.at/research/papers/Schedl_etal_ISMIR_2006.pdf</url>
		<author>Markus Schedl</author>
		<author>Tim Pohle</author>
		<author>Peter Knees</author>
		<author>Gerhard Widmer</author>
		<authors>
			<first>Markus</first>
		</authors>
		<authors>
			<last>Schedl</last>
		</authors>
		<authors>
			<first>Tim</first>
		</authors>
		<authors>
			<last>Pohle</last>
		</authors>
		<authors>
			<first>Peter</first>
		</authors>
		<authors>
			<last>Knees</last>
		</authors>
		<authors>
			<first>Gerhard</first>
		</authors>
		<authors>
			<last>Widmer</last>
		</authors>
		<title>Assigning and Visualizing Music Genres by Web-based Co-Occurrence Analysis</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/263e0d648489b10a9bb05453800ce9f05/mediadigits</id>
		<tags>semantic</tags>
		<tags>music</tags>
		<tags>genre</tags>
		<tags>taxonomy</tags>
		<tags>ontology</tags>
		<description>My bibtex file</description>
		<date>2009-04-07 16:40:14</date>
		<count>6</count>
		<booktitle>Content-Based Multimedia Information Access Conference (RIAO) proceedings</booktitle>
		<year>2000</year>
		<url></url>
		<author>François Pachet</author>
		<author>Daniel Cazaly</author>
		<authors>
			<first>François</first>
		</authors>
		<authors>
			<last>Pachet</last>
		</authors>
		<authors>
			<first>Daniel</first>
		</authors>
		<authors>
			<last>Cazaly</last>
		</authors>
		<pdf>unsorted/pachet-riao2000.pdf</pdf>
		<title>A Taxonomy of Musical Genres</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22362f2483383b51dc4ee142d801c0145/butz</id>
		<tags>imported</tags>
		<description>diverse cognitive systems bib</description>
		<date>2009-06-26 15:25:19</date>
		<count>1</count>
		<booktitle>Anticipatory Behavior in Adaptive Learning Systems: From Brains
	to Individual and Social Behavior</booktitle>
		<publisher>Springer-Verlag</publisher>
		<year>2007</year>
		<url></url>
		<author>Gérard Assayag Arshia Cont</author>
		<authors>
			<first>Gérard Assayag Arshia</first>
		</authors>
		<authors>
			<last>Cont</last>
		</authors>
		<editor>Martin V. Butz</editor>
		<editor>Olivier Sigaud</editor>
		<editor>Giovanni Pezzulo</editor>
		<editor>Gianluca Baldassarre</editor>
		<editors>
			<first>Gérard Assayag Arshia</first>
		</editors>
		<editors>
			<last>Cont</last>
		</editors>
		<editors>
			<first>Gérard Assayag Arshia</first>
		</editors>
		<editors>
			<last>Cont</last>
		</editors>
		<editors>
			<first>Gérard Assayag Arshia</first>
		</editors>
		<editors>
			<last>Cont</last>
		</editors>
		<editors>
			<first>Gérard Assayag Arshia</first>
		</editors>
		<editors>
			<last>Cont</last>
		</editors>
		<abstract>The role of expectation in listening and composing music has drawn
	much attention in music cognition since about half a century ago.
	In this paper, we provide a first attempt to model some aspects of
	musical expectation specifically pertained to short-time and working
	memories, in an anticipatory framework. In our proposition Anticipation
	is the mental realization of possible predicted actions and their
	effect on the perception of the world at an instant in time. We demonstrate
	the model in applications to automatic improvisation and style imitation.
	The proposed model, based on cognitive foundations of musical expectation,
	is an active model using reinforcement learning techniques with multiple
	agents that learn competitively and in collaboration. We show that
	compared to similar models, this anticipatory framework needs little
	training data and demonstrate complex musical behavior such as long-term
	planning and formal shapes as a result of the anticipatory architecture.
	We provide sample results and discuss further research.</abstract>
		<title>Anticipatory Model of Musical Style Imitation using Collaborative
	and Competitive Reinforcement Learning</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fad725ef0f03c8e7815189a9feb26290/jjoao</id>
		<tags>imported</tags>
		<description></description>
		<date>2008-06-04 18:08:26</date>
		<count>1</count>
		<booktitle>New Trends in Artificial Intelligence</booktitle>
		<year>2007</year>
		<url></url>
		<author>Alberto Simões</author>
		<author>Anália Lourenço</author>
		<author>José João Almeida</author>
		<authors>
			<first>Alberto</first>
		</authors>
		<authors>
			<last>Simões</last>
		</authors>
		<authors>
			<first>Anália</first>
		</authors>
		<authors>
			<last>Lourenço</last>
		</authors>
		<authors>
			<first>José João</first>
		</authors>
		<authors>
			<last>Almeida</last>
		</authors>
		<editor>José Neves</editor>
		<editor>Manuel Filipe Santos</editor>
		<editor>José Manuel Machado</editor>
		<editors>
			<first>José João</first>
		</editors>
		<editors>
			<last>Almeida</last>
		</editors>
		<editors>
			<first>José João</first>
		</editors>
		<editors>
			<last>Almeida</last>
		</editors>
		<editors>
			<first>José João</first>
		</editors>
		<editors>
			<last>Almeida</last>
		</editors>
		<pages>791--799</pages>
		<abstract>Abstract.Music Classification is a particular area
                  of Computational Musicology that provides valuable
                  insights about the evolving of composition patterns
                  and assists in catalogue generation. The proposed work
                  detaches from former works by classifying music based
                  on music score information. Text Mining techniques
                  support music score processing while Classification
                  techniques are used in the construction of decision
                  models. Although research is still at its earliest
                  beginnings, the work already provides valuable
                  contributes to symbolic music representation processing 
                  and subsequent analysis. Score processing involved
                  the counting of ascending and descending chromatic
                  intervals, note duration and meta-information
                  tagging. Analysis involved feature selection and
                  the evaluation of several data mining algorithms,
                  ensuring extensibility towards larger repositories or
                  more complex problems. Experiments report the analysis
                  of composition epochs on a subset of the Mutopia project 
                  open archive of classical LilyPond-annotated
                  music scores.</abstract>
		<shortin>Epia, TEMA</shortin>
		<title>Using Text Mining Techniques for Classical Music Scores
Analysis</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/278da8c8e78f522ec01c1e354bec82e13/kregulski</id>
		<tags>year-2008</tags>
		<tags>music</tags>
		<tags>Magnus</tags>
		<tags>recording</tags>
		<tags>performance</tags>
		<tags>imperfection</tags>
		<tags>article</tags>
		<tags>improvisation</tags>
		<tags>kor-2008</tags>
		<description>Volltext über die Nationallizenzen von Oxford Journals erhältlich</description>
		<date>2008-09-03 16:12:06</date>
		<count>1</count>
		<journal>British Journal of Aesthetics</journal>
		<year>2008</year>
		<url></url>
		<author>P. D. Magnus</author>
		<authors>
			<first>P. D.</first>
		</authors>
		<authors>
			<last>Magnus</last>
		</authors>
		<volume>48</volume>
		<number>3</number>
		<pages>338 - 345</pages>
		<abstract>Christy Mag Uidhir has recently argued (a) that there is no in principle aesthetic
difference between a live performance and a recording of that performance, and
(b) that the proper aesthetic object is a type which is instantiated by the performance
and potentially repeatable when recordings are played back. This paper
considers several objections to (a) and fi nds them lacking. I then consider improvised
music, a subject that Mag Uidhir explicitly brackets in his discussion. Improvisation
reveals problems with (b), because the performance-event and the
performance-type are distinct but equally proper aesthetic objects.</abstract>
		<title>Mag Uidhir on Performance</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f9a0f8473f98766c8929ed3bba921236/hunab</id>
		<tags>imported</tags>
		<description></description>
		<date>2008-10-11 08:30:35</date>
		<count>1</count>
		<journal>Empirical Musicology Review</journal>
		<year>2008</year>
		<url></url>
		<author>O. Lartillot</author>
		<author>P. Toiviainen</author>
		<author>T. Eerola</author>
		<authors>
			<first>O.</first>
		</authors>
		<authors>
			<last>Lartillot</last>
		</authors>
		<authors>
			<first>P.</first>
		</authors>
		<authors>
			<last>Toiviainen</last>
		</authors>
		<authors>
			<first>T.</first>
		</authors>
		<authors>
			<last>Eerola</last>
		</authors>
		<volume>3</volume>
		<number>3</number>
		<title>Commentary on ``Comparative Analysis of Music Recordings from Western
	and Non-Western traditions by Automatic Tonal Feature Extraction''
	by Emilia Gómez, and Perfecto Herrera</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24859d3b44b7f6ed28a7b174d477691a7/brazovayeye</id>
		<tags>genetic</tags>
		<tags>algorithms,</tags>
		<tags>programming</tags>
		<description></description>
		<date>2008-06-19 17:35:00</date>
		<count>2</count>
		<booktitle>2007 IEEE Congress on Evolutionary Computation</booktitle>
		<publisher>IEEE Press</publisher>
		<address>Singapore</address>
		<year>2007</year>
		<url></url>
		<author>Daichi Ando</author>
		<author>Hitoshi Iba</author>
		<authors>
			<first>Daichi</first>
		</authors>
		<authors>
			<last>Ando</last>
		</authors>
		<authors>
			<first>Hitoshi</first>
		</authors>
		<authors>
			<last>Iba</last>
		</authors>
		<editor>Dipti Srinivasan</editor>
		<editor>Lipo Wang</editor>
		<editors>
			<first>Hitoshi</first>
		</editors>
		<editors>
			<last>Iba</last>
		</editors>
		<editors>
			<first>Hitoshi</first>
		</editors>
		<editors>
			<last>Iba</last>
		</editors>
		<pages>4258--4265</pages>
		<abstract>Research on the application of Interactive
                 Evolutionary Computation (IEC) to the field of musical
                 computation has been improved in recent years, marking
                 an interesting parallel to the current trend of
                 applying human characteristics or sensitivities to
                 computer systems. However, past techniques developed
                 for IEC-based composition have not necessarily proven
                 very effective for professional use. This is due to the
                 large difference between data representation used by
                 IEC and authored classical music composition. To solve
                 this difficulties, the authors purpose a new IEC
                 approach to music composition based on classical music
                 theory. In this paper, the authors describe an
                 established system according to the above idea, and
                 detail of making success of composition a piece.</abstract>
		<isbn>1-4244-1340-0</isbn>
		<notes>CEC 2007 - A joint meeting of the IEEE, the EPS, and
                 the IET.

                 IEEE Catalog Number: 07TH8963C</notes>
		<title>Interactive Composition Aid System by Means of Tree
                 Representation of Musical Phrase</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a28f86d32b82a9ebb399315301521cf5/ocelma</id>
		<tags>PhD</tags>
		<description>PhD</description>
		<date>2009-01-08 17:06:55</date>
		<count>3</count>
		<booktitle>Proceedings of the 6th International Symposium on Computer Music Modeling and Retrieval</booktitle>
		<year>2008</year>
		<url></url>
		<author>Kurt Jacobson</author>
		<author>Mark Sandler</author>
		<authors>
			<first>Kurt</first>
		</authors>
		<authors>
			<last>Jacobson</last>
		</authors>
		<authors>
			<first>Mark</first>
		</authors>
		<authors>
			<last>Sandler</last>
		</authors>
		<title>Musically Meaningful or Just Noise? An Analysis of On-line Artist Networks</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b709e61e0bd0eb05143088535520cfca/ocelma</id>
		<tags>PhD</tags>
		<description>PhD</description>
		<date>2009-01-08 17:06:55</date>
		<count>4</count>
		<booktitle>Proceedings of eleventh ACM international conference
	on Multimedia</booktitle>
		<publisher>ACM Press</publisher>
		<address>New York, NY, USA</address>
		<year>2003</year>
		<url></url>
		<author>K. Hoashi</author>
		<author>K. Matsumoto</author>
		<author>N. Inoue</author>
		<authors>
			<first>K.</first>
		</authors>
		<authors>
			<last>Hoashi</last>
		</authors>
		<authors>
			<first>K.</first>
		</authors>
		<authors>
			<last>Matsumoto</last>
		</authors>
		<authors>
			<first>N.</first>
		</authors>
		<authors>
			<last>Inoue</last>
		</authors>
		<pages>110-119</pages>
		<isbn>1-58113-722-2</isbn>
		<doi>http://doi.acm.org/10.1145/957013.957040</doi>
		<title>Personalization of user profiles for content-based music retrieval
	based on relevance feedback</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/28231db35cfa71b27a367ae5df3128905/mediadigits</id>
		<tags>music</tags>
		<tags>logic</tags>
		<tags>fuzzy</tags>
		<description>dblp</description>
		<date>2010-10-15 14:43:25</date>
		<count>2</count>
		<booktitle>IFSA/EUSFLAT Conf.</booktitle>
		<year>2009</year>
		<url>http://dblp.uni-trier.de/db/conf/eusflat/eusflat2009.html#BosteelsPK09</url>
		<author>Klaas Bosteels</author>
		<author>Elias Pampalk</author>
		<author>Etienne E. Kerre</author>
		<authors>
			<first>Klaas</first>
		</authors>
		<authors>
			<last>Bosteels</last>
		</authors>
		<authors>
			<first>Elias</first>
		</authors>
		<authors>
			<last>Pampalk</last>
		</authors>
		<authors>
			<first>Etienne E.</first>
		</authors>
		<authors>
			<last>Kerre</last>
		</authors>
		<editor>João Paulo Carvalho</editor>
		<editor>Didier Dubois</editor>
		<editor>Uzay Kaymak</editor>
		<editor>João Miguel da Costa Sousa</editor>
		<editors>
			<first>Etienne E.</first>
		</editors>
		<editors>
			<last>Kerre</last>
		</editors>
		<editors>
			<first>Etienne E.</first>
		</editors>
		<editors>
			<last>Kerre</last>
		</editors>
		<editors>
			<first>Etienne E.</first>
		</editors>
		<editors>
			<last>Kerre</last>
		</editors>
		<editors>
			<first>Etienne E.</first>
		</editors>
		<editors>
			<last>Kerre</last>
		</editors>
		<pages>25-29</pages>
		<isbn>978-989-95079-6-8</isbn>
		<title>On the Benefits of Representing Music Objects as Fuzzy Sets.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/288f7e7d06a75455aa8ad0f8dc449e92a/mediadigits</id>
		<tags>music</tags>
		<tags>spotify</tags>
		<tags>delivery</tags>
		<tags>media</tags>
		<tags>p2p</tags>
		<tags>architecture</tags>
		<description></description>
		<date>2010-11-09 18:13:47</date>
		<count>1</count>
		<booktitle>Peer-to-Peer Computing (P2P), 2010 IEEE Tenth International Conference on</booktitle>
		<year>2010</year>
		<url></url>
		<author>G. Kreitz</author>
		<author>F. Niemela</author>
		<authors>
			<first>G.</first>
		</authors>
		<authors>
			<last>Kreitz</last>
		</authors>
		<authors>
			<first>F.</first>
		</authors>
		<authors>
			<last>Niemela</last>
		</authors>
		<pages>1--10</pages>
		<title>Spotify--Large Scale, Low Latency, P2P Music-on-Demand Streaming</title>
		<pubtype>conference</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e48d858ceeb65c4e7daaaa8969df6a79/mediadigits</id>
		<tags>music</tags>
		<tags>recommendation</tags>
		<tags>context</tags>
		<tags>sport</tags>
		<tags>mobile</tags>
		<description></description>
		<date>2010-10-15 16:10:42</date>
		<count>1</count>
		<journal>USCCS'08</journal>
		<year>2008</year>
		<url></url>
		<author>Vivian Esquivias</author>
		<authors>
			<first>Vivian</first>
		</authors>
		<authors>
			<last>Esquivias</last>
		</authors>
		<pages>111</pages>
		<title>Mobile Music Fitness Coach: Interaction techniques and guidelines</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2541ae3a9615b6f7af6421541ef5b0612/mediadigits</id>
		<tags>music</tags>
		<tags>service</tags>
		<description></description>
		<date>2010-10-01 19:18:39</date>
		<count>2</count>
		<journal>AMCIS 2010 Proceedings</journal>
		<year>2010</year>
		<url></url>
		<author>Christiaan Katsma</author>
		<author>Ton Spil</author>
		<authors>
			<first>Christiaan</first>
		</authors>
		<authors>
			<last>Katsma</last>
		</authors>
		<authors>
			<first>Ton</first>
		</authors>
		<authors>
			<last>Spil</last>
		</authors>
		<pages>559</pages>
		<title>A taxonomy of digital music services</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/251e44f5165264fd1eada16457af10f8c/mediadigits</id>
		<tags>3d</tags>
		<tags>discover</tags>
		<tags>semantic</tags>
		<tags>music</tags>
		<tags>visualization</tags>
		<description>Paper about the 3D visualiwation of online libraries.</description>
		<date>2009-09-29 10:59:04</date>
		<count>1</count>
		<year>2007</year>
		<url></url>
		<author>Paul Lamere</author>
		<author>Douglas Eck</author>
		<authors>
			<first>Paul</first>
		</authors>
		<authors>
			<last>Lamere</last>
		</authors>
		<authors>
			<first>Douglas</first>
		</authors>
		<authors>
			<last>Eck</last>
		</authors>
		<editor> ISMIR</editor>
		<editors>
			<first>Douglas</first>
		</editors>
		<editors>
			<last>Eck</last>
		</editors>
		<title>Using 3D Visualization to explore and discover Music</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d08fae2886445b4921523e91ebfe7fc9/jamie.bullock</id>
		<tags>myown</tags>
		<tags>music</tags>
		<tags>dx7</tags>
		<tags>harvey</tags>
		<description></description>
		<date>2011-03-30 15:00:08</date>
		<count>2</count>
		<booktitle>Proceedings of the 2005 International Computer Music Conference</booktitle>
		<publisher>ICMA</publisher>
		<address>Barcelona, Spain</address>
		<year>2005</year>
		<url></url>
		<author>Jamie Bullock</author>
		<author>Lamberto Coccioli</author>
		<authors>
			<first>Jamie</first>
		</authors>
		<authors>
			<last>Bullock</last>
		</authors>
		<authors>
			<first>Lamberto</first>
		</authors>
		<authors>
			<last>Coccioli</last>
		</authors>
		<pages>551-554</pages>
		<title>Modernising live electronics technology in the works of Jonathan Harvey</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/284f5ce7b30c8f5755d0c485855cdcfe1/yevb0</id>
		<tags>Factors</tags>
		<tags>Stimulation,Humans,Music,Music:</tags>
		<tags>Acoustic</tags>
		<tags>psychology,Time</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Nature</journal>
		<year>2008</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/18580933</url>
		<author>Nicholas Cook</author>
		<authors>
			<first>Nicholas</first>
		</authors>
		<authors>
			<last>Cook</last>
		</authors>
		<volume>453</volume>
		<number>7199</number>
		<pages>1186--1187</pages>
		<issn>1476-4687</issn>
		<doi>10.1038/4531186a</doi>
		<title>Science & music: Beyond the notes</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a5c90b6c5ab78a75f0e07e448b979330/yevb0</id>
		<tags>music,scale</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Ethnomusicology</journal>
		<year>1965</year>
		<url>http://www.jstor.org/stable/850317</url>
		<author>Catherine J Ellis</author>
		<authors>
			<first>Catherine J</first>
		</authors>
		<authors>
			<last>Ellis</last>
		</authors>
		<volume>9</volume>
		<number>2</number>
		<pages>126--137</pages>
		<title>Pre-instrumental scales</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c24b03484c67513f63d2e6ce335b669e/yevb0</id>
		<tags>L2,Mandarin,language,music,musicality,perception,tone</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>2</count>
		<booktitle>Proceedings of the 9th European Conference on Speech Communication
	and Technology</booktitle>
		<address>Lisbon</address>
		<year>2005</year>
		<url></url>
		<author>Jennifer A Alexander</author>
		<author>Patrick C.M. Wong</author>
		<author>Ann R Bradlow</author>
		<authors>
			<first>Jennifer A</first>
		</authors>
		<authors>
			<last>Alexander</last>
		</authors>
		<authors>
			<first>Patrick C.M.</first>
		</authors>
		<authors>
			<last>Wong</last>
		</authors>
		<authors>
			<first>Ann R</first>
		</authors>
		<authors>
			<last>Bradlow</last>
		</authors>
		<title>Lexical tone perception in musicians and non-musicians</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2003ad129b4ba4824aca3ecf17246ef16/yevb0</id>
		<tags>evolution,music,typology</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>The World of Music</journal>
		<year>2006</year>
		<url></url>
		<author>Victor A Grauer</author>
		<authors>
			<first>Victor A</first>
		</authors>
		<authors>
			<last>Grauer</last>
		</authors>
		<volume>48</volume>
		<number>2</number>
		<pages>5--59</pages>
		<title>Echoes of our forgotten ancestors</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2647f04264a4f4ca968ba3f75358683f6/yevb0</id>
		<tags>abr,audition,development,eeg,erp,learning,music\_cognition</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Brain</journal>
		<publisher>Oxford University Press</publisher>
		<year>2006</year>
		<url></url>
		<author>Takako Fujioka</author>
		<author>Bernhard Ross</author>
		<author>Ryusuke Kakigi</author>
		<author>Christo Pantev</author>
		<author>Laurel J. Trainor</author>
		<authors>
			<first>Takako</first>
		</authors>
		<authors>
			<last>Fujioka</last>
		</authors>
		<authors>
			<first>Bernhard</first>
		</authors>
		<authors>
			<last>Ross</last>
		</authors>
		<authors>
			<first>Ryusuke</first>
		</authors>
		<authors>
			<last>Kakigi</last>
		</authors>
		<authors>
			<first>Christo</first>
		</authors>
		<authors>
			<last>Pantev</last>
		</authors>
		<authors>
			<first>Laurel J.</first>
		</authors>
		<authors>
			<last>Trainor</last>
		</authors>
		<volume>129</volume>
		<number>10</number>
		<pages>2593--2608</pages>
		<doi>http://dx.doi.org/10.1093/brain/awl247</doi>
		<title>One Year of Musical Training Affects Development of Auditory Cortical-Evoked
	Fields in Young Children</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20c30e83fdb9454558646c228f9b9fe7d/yevb0</id>
		<tags>imported</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>The performing arts: music and dance</booktitle>
		<publisher>Walter de Gruyter</publisher>
		<year>1979</year>
		<url></url>
		<author>J Blacking</author>
		<authors>
			<first>J</first>
		</authors>
		<authors>
			<last>Blacking</last>
		</authors>
		<editor>John Blacking</editor>
		<editor>Joann W. Kealiinohomoku</editor>
		<editors>
			<first>J</first>
		</editors>
		<editors>
			<last>Blacking</last>
		</editors>
		<editors>
			<first>J</first>
		</editors>
		<editors>
			<last>Blacking</last>
		</editors>
		<title>The study of man as music-maker</title>
		<pubtype>inbook</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2266eaac92ba3c0f31b412ecb69fdf82c/jamie.bullock</id>
		<tags>myown</tags>
		<tags>music</tags>
		<tags>XMP</tags>
		<tags>model</tags>
		<tags>temporal</tags>
		<tags>OO</tags>
		<description></description>
		<date>2011-03-27 00:08:10</date>
		<count>2</count>
		<booktitle>Proceedings of the International Computer Music Conference</booktitle>
		<address>Montreal, Quebec, Canada</address>
		<year>2009</year>
		<url></url>
		<author>Jamie Bullock</author>
		<author>Henrik Frisk</author>
		<authors>
			<first>Jamie</first>
		</authors>
		<authors>
			<last>Bullock</last>
		</authors>
		<authors>
			<first>Henrik</first>
		</authors>
		<authors>
			<last>Frisk</last>
		</authors>
		<title>An Object Oriented Model for the Representation of Temporal Data in the Integra Framework</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2949edd1b6a880168e2e7b561c3d9b5cc/yevb0</id>
		<tags>L2,language,musicality,neuro,perception,production</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>Publications of the Department of English, University of Turku</booktitle>
		<year>2009</year>
		<url></url>
		<author>Riia Milovanov</author>
		<authors>
			<first>Riia</first>
		</authors>
		<authors>
			<last>Milovanov</last>
		</authors>
		<volume>27</volume>
		<title>The connectivity of musical aptitude and foreign language learning
	skills: Neural and behavioural evidence</title>
		<pubtype>phdthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24f35fcc6fe692bf37e2c2b1f6d67204a/yevb0</id>
		<tags>imported</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Annals of the New York Academy of Sciences</journal>
		<year>2003</year>
		<url>http://blackwell-synergy.com/doi/abs/10.1196/annals.1284.045</url>
		<author>Stéphanie Khalfa</author>
		<author>Simone Dalla Bella</author>
		<author>Mathieu Roy</author>
		<author>Isabelle Peretz</author>
		<author>Sonia J. Lupien</author>
		<authors>
			<first>Stéphanie</first>
		</authors>
		<authors>
			<last>Khalfa</last>
		</authors>
		<authors>
			<first>Simone</first>
		</authors>
		<authors>
			<last>Dalla Bella</last>
		</authors>
		<authors>
			<first>Mathieu</first>
		</authors>
		<authors>
			<last>Roy</last>
		</authors>
		<authors>
			<first>Isabelle</first>
		</authors>
		<authors>
			<last>Peretz</last>
		</authors>
		<authors>
			<first>Sonia J.</first>
		</authors>
		<authors>
			<last>Lupien</last>
		</authors>
		<volume>999</volume>
		<number>1</number>
		<pages>374--376</pages>
		<issn>00778923</issn>
		<doi>10.1196/annals.1284.045</doi>
		<title>Effects of relaxing music on salivary cortisol level after psychological
	stress</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29eafe44918a938ad03db51effea658a8/yevb0</id>
		<tags>period,structure,tempo</tags>
		<tags>metrical,related</tags>
		<tags>period</tags>
		<tags>an,lengths</tags>
		<tags>theory,it</tags>
		<tags>is</tags>
		<tags>estimation,the</tags>
		<tags>the</tags>
		<tags>2,according</tags>
		<tags>approximately</tags>
		<tags>hierarchical</tags>
		<tags>of</tags>
		<tags>to</tags>
		<tags>beat</tags>
		<tags>metrical,levels,mirex,of</tags>
		<tags>other</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<booktitle>Proceedings of the 6th International Conference on Music Information
	Retrieval</booktitle>
		<address>London</address>
		<year>2005</year>
		<url>http://www.music-ir.org/evaluation/mirex-results/articles/all/uhle.pdf</url>
		<author>Christian Uhle</author>
		<authors>
			<first>Christian</first>
		</authors>
		<authors>
			<last>Uhle</last>
		</authors>
		<pages>1--3</pages>
		<title>Tempo induction by investigating the metrical structure of music
	using a periodicity signal that relates to the tatum period</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/212cf3e31cf327d683dedc69c3f86bcd2/yevb0</id>
		<tags>Africa,Uganda,music</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Ethnomusicology</journal>
		<year>1962</year>
		<url>http://www.jstor.org/stable/924242</url>
		<author>J H Kwabena Nketia</author>
		<authors>
			<first>J H Kwabena</first>
		</authors>
		<authors>
			<last>Nketia</last>
		</authors>
		<volume>6</volume>
		<number>1</number>
		<pages>1--7</pages>
		<title>The problem of meaning in African music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2742e1e5fda3787ac2e8bb449e2fcce7a/yevb0</id>
		<tags>imported</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Nature Reviews Neuroscience</journal>
		<publisher>Nature Publishing Group, a division of Macmillan Publishers Limited.
	All Rights Reserved.</publisher>
		<year>2010</year>
		<url>http://dx.doi.org/10.1038/nrn2882</url>
		<author>Nina Kraus</author>
		<author>Bharath Chandrasekaran</author>
		<authors>
			<first>Nina</first>
		</authors>
		<authors>
			<last>Kraus</last>
		</authors>
		<authors>
			<first>Bharath</first>
		</authors>
		<authors>
			<last>Chandrasekaran</last>
		</authors>
		<volume>11</volume>
		<number>8</number>
		<pages>599--605</pages>
		<abstract>The effects of music training in relation to brain plasticity have
	caused excitement, evident from the popularity of books on this topic
	among scientists and the general public. Neuroscience research has
	shown that music training leads to changes throughout the auditory
	system that prime musicians for listening challenges beyond music
	processing. This effect of music training suggests that, akin to
	physical exercise and its impact on body fitness, music is a resource
	that tones the brain for auditory fitness. Therefore, the role of
	music in shaping individual development deserves consideration.</abstract>
		<issn>1471-003X</issn>
		<shorttitle>Nat Rev Neurosci</shorttitle>
		<doi>10.1038/nrn2882</doi>
		<title>Music training for the development of auditory skills</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d17a4cd5f535c57fe3df6343366a1808/yevb0</id>
		<tags>acquisition,music,perception,syntax</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Psychological Science</journal>
		<year>1990</year>
		<url></url>
		<author>Carol L Krumhansl</author>
		<author>Peter W. Jusczyk</author>
		<authors>
			<first>Carol L</first>
		</authors>
		<authors>
			<last>Krumhansl</last>
		</authors>
		<authors>
			<first>Peter W.</first>
		</authors>
		<authors>
			<last>Jusczyk</last>
		</authors>
		<volume>1</volume>
		<number>1</number>
		<pages>70--73</pages>
		<title>Infants' perception of phrase structure in music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21e6af8053d04bc6da457f80cfad8807a/yevb0</id>
		<tags>language,music,musical</tags>
		<tags>rhythm,stress-timing,syllable-timing</tags>
		<tags>rhythm,musicality,rhythm,speech,speech</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Neurosciences</journal>
		<year>2003</year>
		<url></url>
		<author>Aniruddh D. Patel</author>
		<author>Joseph R Daniele</author>
		<authors>
			<first>Aniruddh D.</first>
		</authors>
		<authors>
			<last>Patel</last>
		</authors>
		<authors>
			<first>Joseph R</first>
		</authors>
		<authors>
			<last>Daniele</last>
		</authors>
		<volume>87</volume>
		<doi>10.1016/S0</doi>
		<title>An empirical comparison of rhythm in language and music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/215878f190fdea5db5a87695826f06e98/yevb0</id>
		<tags>Sequence,Linguistics,Models,Music,Statistical</tags>
		<tags>Base</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Nature</journal>
		<year>2008</year>
		<url>http://www.ncbi.nlm.nih.gov/pubmed/18563138</url>
		<author>Damián Zanette</author>
		<authors>
			<first>Damián</first>
		</authors>
		<authors>
			<last>Zanette</last>
		</authors>
		<volume>453</volume>
		<number>7198</number>
		<pages>988--989</pages>
		<issn>1476-4687</issn>
		<doi>10.1038/453988a</doi>
		<title>Science & music: Playing by numbers.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fc664478b793e01bfc3898482f75d9e1/yevb0</id>
		<tags>imported</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<publisher>WNET.ORG</publisher>
		<year>2009</year>
		<url>http://www.pbs.org/wnet/musicinstinct/</url>
		<author>Elena Mannes</author>
		<authors>
			<first>Elena</first>
		</authors>
		<authors>
			<last>Mannes</last>
		</authors>
		<title>The music instinct: Science and song</title>
		<pubtype>misc</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/250a6ceddc2211561b76d8f8cde4f04df/yevb0</id>
		<tags>imported</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Music Perception</journal>
		<year>2004</year>
		<url>http://caliber.ucpress.net/doi/abs/10.1525/mp.2004.22.1.15</url>
		<author>Alexander Rozin</author>
		<authors>
			<first>Alexander</first>
		</authors>
		<authors>
			<last>Rozin</last>
		</authors>
		<volume>22</volume>
		<number>1</number>
		<pages>15--39</pages>
		<issn>0730-7829</issn>
		<doi>10.1525/mp.2004.22.1.15</doi>
		<title>The Feeling of Music Past: How Listeners Remember Musical Affect</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22e927b7c632af708b70a6d3933ec406f/yevb0</id>
		<tags>skills,music,musicality,neuro,neuronal</tags>
		<tags>imaging,motor</tags>
		<tags>cortex,motor</tags>
		<tags>corpus</tags>
		<tags>plasticity</tags>
		<tags>callosum,laterality,magnetic</tags>
		<tags>resonance</tags>
		<description></description>
		<date>2011-03-27 17:20:41</date>
		<count>1</count>
		<journal>Neuropsychologia</journal>
		<publisher>Elsevier</publisher>
		<year>1995</year>
		<url>http://linkinghub.elsevier.com/retrieve/pii/0028393295000455</url>
		<author>Gottfried Schlaug</author>
		<author>Lutz Jäncke</author>
		<author>Yanxiong Huang</author>
		<author>J.F. Staiger</author>
		<author>Helmuth Steinmetz</author>
		<authors>
			<first>Gottfried</first>
		</authors>
		<authors>
			<last>Schlaug</last>
		</authors>
		<authors>
			<first>Lutz</first>
		</authors>
		<authors>
			<last>Jäncke</last>
		</authors>
		<authors>
			<first>Yanxiong</first>
		</authors>
		<authors>
			<last>Huang</last>
		</authors>
		<authors>
			<first>J.F.</first>
		</authors>
		<authors>
			<last>Staiger</last>
		</authors>
		<authors>
			<first>Helmuth</first>
		</authors>
		<authors>
			<last>Steinmetz</last>
		</authors>
		<volume>33</volume>
		<number>8</number>
		<pages>1047--1055</pages>
		<title>Increased corpus callosum size in musicians</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a5fce06ff6af8729491fbfed2ab7b3f8/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2011-04-18 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2011</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj35.html#DriessenDP11</url>
		<author>Peter F. Driessen</author>
		<author>Thomas E. Darcie</author>
		<author>Bipin Pillay</author>
		<authors>
			<first>Peter F.</first>
		</authors>
		<authors>
			<last>Driessen</last>
		</authors>
		<authors>
			<first>Thomas E.</first>
		</authors>
		<authors>
			<last>Darcie</last>
		</authors>
		<authors>
			<first>Bipin</first>
		</authors>
		<authors>
			<last>Pillay</last>
		</authors>
		<volume>35</volume>
		<number>1</number>
		<pages>76-89</pages>
		<title>The Effects of Network Delay on Tempo in Musical Performance.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/235c307126ad0e98da6694713963442b3/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2011-04-18 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2011</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj35.html#Harley11a</url>
		<author>James Harley</author>
		<authors>
			<first>James</first>
		</authors>
		<authors>
			<last>Harley</last>
		</authors>
		<volume>35</volume>
		<number>1</number>
		<pages>111-112</pages>
		<title>Henk Badings: More Electronic Music.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2bcfa7f155de7f213d19e6cd55cad1241/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2011-06-20 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2011</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj35.html#Wolf11</url>
		<author>KatieAnna Wolf</author>
		<authors>
			<first>KatieAnna</first>
		</authors>
		<authors>
			<last>Wolf</last>
		</authors>
		<volume>35</volume>
		<number>2</number>
		<pages>84-86</pages>
		<title>The Eleventh International Society for Music Information Retrieval Conference (ISMIR 2010).</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/260491a3cf23e4bc91e6f67beae9b0130/keinstein</id>
		<tags>Musiktheorie</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2002 %@ 1468-2249 %+ Yale University</date>
		<count>2</count>
		<journal>Music Analysis</journal>
		<year>2002</year>
		<url>http://dx.doi.org/10.1111/j.0262-5245.2002.00162.x</url>
		<author>Allen Forte</author>
		<authors>
			<first>Allen</first>
		</authors>
		<authors>
			<last>Forte</last>
		</authors>
		<volume>21</volume>
		<number>s1</number>
		<pages>13-15</pages>
		<refer1>10.1111/j.0262-5245.2002.00162.x</refer1>
		<title>Thoughts on Music Analysis</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27735ccfc21d8a2aa79680b24ff3b50ea/muhe</id>
		<tags>imported</tags>
		<description></description>
		<date>2012-01-27 14:10:42</date>
		<count>1</count>
		<journal>The Journal of Alternative and Complementary Medicine</journal>
		<year>2004</year>
		<url></url>
		<author>Creath K</author>
		<authors>
			<first>Creath</first>
		</authors>
		<authors>
			<last>K</last>
		</authors>
		<volume>10</volume>
		<pages>113-122</pages>
		<abstract>OBJECTIVE:To measure biologic effects of music, noise, and healing
	energy without human preferences or placebo effects using seed germination
	as an objective biomarker.METHODS:A series of five experiments were
	performed utilizing okra and zucchini seeds germinated in acoustically
	shielded, thermally insulated, dark, humid growth chambers. Conditions
	compared were an untreated control, musical sound, pink noise, and
	healing energy. Healing energy was administered for 15-20 minutes
	every 12 hours with the intention that the treated seeds would germinate
	faster than the untreated seeds. The objective marker was the number
	of seeds sprouted out of groups of 25 seeds counted at 12-hour intervals
	over a 72-hour growing period. Temperature and relative humidity
	were monitored every 15 minutes inside the seed germination containers.
	A total of 14 trials were run testing a total of 4600 seeds.RESULTS:Musical
	sound had a highly statistically significant effect on the number
	of seeds sprouted compared to the untreated control over all five
	experiments for the main condition (p < 0.002) and over time (p <
	0.000002). This effect was independent of temperature, seed type,
	position in room, specific petri dish, and person doing the scoring.
	Musical sound had a significant effect compared to noise and an untreated
	control as a function of time (p < 0.03) while there was no significant
	difference between seeds exposed to noise and an untreated control.
	Healing energy also had a significant effect compared to an untreated
	control (main condition, p < 0.0006) and over time (p < 0.0001) with
	a magnitude of effect comparable to that of musical sound.CONCLUSION:This
	study suggests that sound vibrations (music and noise) as well as
	biofields (bioelectromagnetic and healing intention) both directly
	affect living biologic systems, and that a seed germination bioassay
	has the sensitivity to enable detection of effects caused by various
	applied energetic conditions.</abstract>
		<title>Measuring Effects of Music, Noise, and Healing Energy Using a Seed
	Germination Bioassay</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e77086806b77bd3b6a5fc1a6b4ecde38/lysander07</id>
		<tags>music</tags>
		<tags>twitter</tags>
		<tags>msm2012</tags>
		<tags>www2012</tags>
		<tags>Recommendation</tags>
		<description></description>
		<date>2012-04-16 11:48:57</date>
		<count>1</count>
		<booktitle>Proceedings, 2nd Workshop on Making Sense of Microposts (\#MSM2012): Big things come in small packages, Lyon, France, 16 April 2012</booktitle>
		<year>2012</year>
		<url>http://ceur-ws.org/Vol-838/paper_09.pdf</url>
		<author>Exploiting Twitter's Collective Knowledge for Music Recommendations</author>
		<authors>
			<first>Exploiting Twitter's Collective Knowledge</first>
		</authors>
		<authors>
			<last>for Music Recommendations</last>
		</authors>
		<editor>Matthew Rowe</editor>
		<editor>Milan Stankovic</editor>
		<editor>Aba-Sah Dadzie</editor>
		<editors>
			<first>Exploiting Twitter's Collective Knowledge</first>
		</editors>
		<editors>
			<last>for Music Recommendations</last>
		</editors>
		<editors>
			<first>Exploiting Twitter's Collective Knowledge</first>
		</editors>
		<editors>
			<last>for Music Recommendations</last>
		</editors>
		<editors>
			<first>Exploiting Twitter's Collective Knowledge</first>
		</editors>
		<editors>
			<last>for Music Recommendations</last>
		</editors>
		<pages>14--17</pages>
		<title>Eva Zangerle and Wolfgang Gassler and Günther Specht</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26fd67961cdaa9d7e010cb87b58724a4b/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2012-05-15 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj36.html#Kowalkowski12</url>
		<author>Jeff Kowalkowski</author>
		<authors>
			<first>Jeff</first>
		</authors>
		<authors>
			<last>Kowalkowski</last>
		</authors>
		<volume>36</volume>
		<number>2</number>
		<pages>89-90</pages>
		<title>Bob Ostertag: Creative Life: Music, Politics, People, and Machines.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20b60e4c864eb403d7b873509396cd07b/jabreftest</id>
		<description></description>
		<date>2012-02-20 13:13:22</date>
		<count>3</count>
		<booktitle>Proceedings of the International Computer Music Conference</booktitle>
		<address>Montreal, Canada</address>
		<year>2009</year>
		<url>http://ccrma.stanford.edu/~slegroux/pubs/2009/ICMC09.pdf</url>
		<author>Sylvain Le Groux</author>
		<author>Paul F. M. J. Verschure</author>
		<authors>
			<first>Sylvain</first>
		</authors>
		<authors>
			<last>Le Groux</last>
		</authors>
		<authors>
			<first>Paul F. M. J.</first>
		</authors>
		<authors>
			<last>Verschure</last>
		</authors>
		<title>Situated Interactive Music System: Connecting Mind and Body Through Musical Interaction</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/208e91020d428553f56fee10dc9cf2243/sauli</id>
		<tags>psychology</tags>
		<tags>rejected</tags>
		<tags>rationale</tags>
		<description></description>
		<date>2014-05-21 13:28:32</date>
		<count>1</count>
		<journal>Nordic Journal of Music Therapy</journal>
		<year>2004</year>
		<url>/brokenurl#         http://dx.doi.org/10.1080/08098130409478111    </url>
		<author>Susan Hadley</author>
		<authors>
			<first>Susan</first>
		</authors>
		<authors>
			<last>Hadley</last>
		</authors>
		<volume>13</volume>
		<number>2</number>
		<pages>151-153</pages>
		<doi>10.1080/08098130409478111</doi>
		<eprint>http://dx.doi.org/10.1080/08098130409478111</eprint>
		<title>The Polyphonic Potential of Music and the Human Psyche</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2abcb11e0991aa886f9f0d7326c102b8a/joaakive</id>
		<tags>interfaces</tags>
		<tags>music</tags>
		<tags>computer</tags>
		<description></description>
		<date>2014-04-27 09:45:05</date>
		<count>3</count>
		<journal>ACM Comput. Surv.</journal>
		<year>1985</year>
		<url>http://dblp.uni-trier.de/db/journals/csur/csur17.html#Pennycook85</url>
		<author>Bruce W. Pennycook</author>
		<authors>
			<first>Bruce W.</first>
		</authors>
		<authors>
			<last>Pennycook</last>
		</authors>
		<volume>17</volume>
		<number>2</number>
		<pages>267-289</pages>
		<title>Computer-Music Interfaces: A Survey.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c6104788852dc983a3e71709cce89b61/joaakive</id>
		<tags>interacting</tags>
		<tags>music</tags>
		<tags>system</tags>
		<description></description>
		<date>2014-04-27 09:43:56</date>
		<count>2</count>
		<booktitle>ACM Multimedia</booktitle>
		<publisher>ACM</publisher>
		<year>2005</year>
		<url>http://dblp.uni-trier.de/db/conf/mm/mm2005.html#Obrenovic05</url>
		<author>Zeljko Obrenovic</author>
		<authors>
			<first>Zeljko</first>
		</authors>
		<authors>
			<last>Obrenovic</last>
		</authors>
		<pages>996-1004</pages>
		<isbn>1-59593-044-2</isbn>
		<title>A flexible system for creating music while interacting with the computer</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/25b993a82ffd00c72bce17f8b6d1e31b7/joaakive</id>
		<tags>interfaces</tags>
		<tags>musical</tags>
		<description></description>
		<date>2014-04-27 09:51:22</date>
		<count>3</count>
		<year>2001</year>
		<url></url>
		<author>"Ivan Poupyrev"</author>
		<author>"Michael J. Lyons"</author>
		<author>"Sidney Fels"</author>
		<author>"Tina Blaine"</author>
		<authors>
			<first>"Ivan</first>
		</authors>
		<authors>
			<last>Poupyrev"</last>
		</authors>
		<authors>
			<first>"Michael J.</first>
		</authors>
		<authors>
			<last>Lyons"</last>
		</authors>
		<authors>
			<first>"Sidney</first>
		</authors>
		<authors>
			<last>Fels"</last>
		</authors>
		<authors>
			<first>"Tina</first>
		</authors>
		<authors>
			<last>Blaine"</last>
		</authors>
		<title>New Interfaces for Musical Expression</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2889c3b52c70e493cb9d9b7d6e4b44ce5/srhthw</id>
		<tags>srhthw</tags>
		<description></description>
		<date>2014-05-23 10:13:32</date>
		<count>1</count>
		<booktitle>Paper presented at the World Congress of Music Therapy</booktitle>
		<address>Krems, Austria</address>
		<year>2014, July</year>
		<url></url>
		<author>J. Geipel</author>
		<author>A. Ranger</author>
		<author>B.M. Menke</author>
		<author>J. Vagedes</author>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Geipel</last>
		</authors>
		<authors>
			<first>A.</first>
		</authors>
		<authors>
			<last>Ranger</last>
		</authors>
		<authors>
			<first>B.M.</first>
		</authors>
		<authors>
			<last>Menke</last>
		</authors>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Vagedes</last>
		</authors>
		<title>Music therapy with neonates with hyperbilirubinemia - a pilot study</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2656f5db47100713782a745abdfb7f339/srhthw</id>
		<tags>srhthw</tags>
		<description></description>
		<date>2014-08-21 14:32:10</date>
		<count>1</count>
		<journal>Music and Medicine</journal>
		<year>in press</year>
		<url></url>
		<author>R. Bhatt</author>
		<author>M. Kessler</author>
		<author>T.K. Hillecke</author>
		<author>J.F. Thayer</author>
		<author>Koenig J.</author>
		<authors>
			<first>R.</first>
		</authors>
		<authors>
			<last>Bhatt</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Kessler</last>
		</authors>
		<authors>
			<first>T.K.</first>
		</authors>
		<authors>
			<last>Hillecke</last>
		</authors>
		<authors>
			<first>J.F.</first>
		</authors>
		<authors>
			<last>Thayer</last>
		</authors>
		<authors>
			<first>Koenig</first>
		</authors>
		<authors>
			<last>J.</last>
		</authors>
		<volume>in press</volume>
		<title>The Dark Side of the Moon: Music may Reduce Pain but White Noise may Increase it!</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22e6f390cebd370c7ebade8f9f480a761/ar0berts</id>
		<tags>Disorders;</tags>
		<tags>Child;</tags>
		<tags>Humans;</tags>
		<tags>Filtration;</tags>
		<tags>Cerebral</tags>
		<tags>Methods;</tags>
		<tags>Preschool;</tags>
		<tags>Child,</tags>
		<tags>Infant;</tags>
		<tags>Palsy;</tags>
		<tags>Ear</tags>
		<tags>Hearing</tags>
		<tags>Music</tags>
		<tags>Diseases;</tags>
		<tags>Audiometry;</tags>
		<description></description>
		<date>2014-07-19 21:53:51</date>
		<count>1</count>
		<journal>S Afr Med J</journal>
		<year>1974</year>
		<url></url>
		<author>H. L. Waldman</author>
		<author>C. L. Cockcroft</author>
		<author>B. Ludi</author>
		<authors>
			<first>H. L.</first>
		</authors>
		<authors>
			<last>Waldman</last>
		</authors>
		<authors>
			<first>C. L.</first>
		</authors>
		<authors>
			<last>Cockcroft</last>
		</authors>
		<authors>
			<first>B.</first>
		</authors>
		<authors>
			<last>Ludi</last>
		</authors>
		<volume>48</volume>
		<number>41</number>
		<pages>1772--1774</pages>
		<title>Filtered music: a hearing test for young children.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2916f953fe2407c0efd71a2c3e71b12e3/florian.pf</id>
		<tags>thema:playlistpredictionviametricembedding</tags>
		<description>nips.dvi - fastsparsemusic.pdf</description>
		<date>2013-11-08 17:34:23</date>
		<count>1</count>
		<year>2004</year>
		<url>http://research.microsoft.com/pubs/68916/fastsparsemusic.pdf</url>
		<author>John C. Platt</author>
		<authors>
			<first>John C.</first>
		</authors>
		<authors>
			<last>Platt</last>
		</authors>
		<pages>8</pages>
		<abstract>This paper applies fast sparse multidimensional scaling (MDS) to a large
graph of music similarity, with 267K vertices that represent artists, al-
bums, and tracks; and 3.22M edges that represent similarity between
those entities. Once vertices are assigned locations in a Euclidean space,
the locations can be used to browse music and to generate playlists.
MDS on very large sparse graphs can be effectively performed by a
family of algorithms called Rectangular Dijsktra (RD) MDS algorithms.
These RD algorithms operate on a dense rectangular slice of the distance
matrix, created by calling Dijsktra a constant number of times. Two RD
algorithms are compared: Landmark MDS, which uses the Nyström ap-
proximation to perform MDS; and a new algorithm called Fast Sparse
Embedding, which uses FastMap. These algorithms compare favorably
to Laplacian Eigenmaps, both in terms of speed and embedding quality.</abstract>
		<title>Fast Embedding of Sparse Music Similarity Graphs</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/210f861d665f7442e706f0ae9b43c9639/sandholm</id>
		<tags>geo</tags>
		<description></description>
		<date>2014-01-29 08:44:22</date>
		<count>2</count>
		<booktitle>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</booktitle>
		<series>CHI '13</series>
		<publisher>ACM</publisher>
		<address>New York, NY, USA</address>
		<year>2013</year>
		<url>http://doi.acm.org/10.1145/2470654.2481411</url>
		<author>Anupriya Ankolekar</author>
		<author>Thomas Sandholm</author>
		<author>Louis Yu</author>
		<authors>
			<first>Anupriya</first>
		</authors>
		<authors>
			<last>Ankolekar</last>
		</authors>
		<authors>
			<first>Thomas</first>
		</authors>
		<authors>
			<last>Sandholm</last>
		</authors>
		<authors>
			<first>Louis</first>
		</authors>
		<authors>
			<last>Yu</last>
		</authors>
		<pages>2959--2968</pages>
		<abstract>Current location-based services (LBS) typically allow users to locate points of interest (POI) in their vicinity but can detract from the user's emotional experience of exploring a new location. In this paper, we examine how cues in the form of popular music (musicons) can emotionally engage users and enhance their experience of discovering nearby POIs serendipitously in unfamiliar places. The primary contribution of this paper is a field study, in which we evaluate the performance and emotional engagement of different types of audio-based cues for directing users' attention to specific POIs. Musicons and mixed-modality cues performed close to visual and speech cues, and significantly better than auditory icons, for POI identification while creating a much more pleasant and engaging user experience. We conclude that cues for POI discovery need not always be as explicit as the baseline visual cues. Indeed, the most challenging cues, auditory icons, led to a heightened sense of autonomy.</abstract>
		<isbn>978-1-4503-1899-0</isbn>
		<doi>10.1145/2470654.2481411</doi>
		<title>Play It by Ear: A Case for Serendipitous Discovery of Places with Musicons</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2329bc31ea84dc04b9fba4c8445a3f57f/joaakive</id>
		<tags>arabic</tags>
		<tags>music</tags>
		<tags>ethnomusicolog</tags>
		<tags>theory</tags>
		<description></description>
		<date>2014-11-08 11:28:06</date>
		<count>1</count>
		<publisher>Amadeus Press</publisher>
		<year>2003</year>
		<url></url>
		<author>Habib Hassan Touma</author>
		<authors>
			<first>Habib Hassan</first>
		</authors>
		<authors>
			<last>Touma</last>
		</authors>
		<title>The Music of the Arabs</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2d04b787532781c63325a979ab8e2961f/sauli</id>
		<tags>psychology</tags>
		<tags>gradu</tags>
		<tags>rationale</tags>
		<description></description>
		<date>2014-05-21 11:38:33</date>
		<count>1</count>
		<journal>Music Perception</journal>
		<publisher>University of California Press</publisher>
		<address>Berkeley</address>
		<year>2006</year>
		<url>http://search.proquest.com/docview/222278395?accountid=11774</url>
		<author>Zohar Eitan</author>
		<author>Roni Y Granot</author>
		<authors>
			<first>Zohar</first>
		</authors>
		<authors>
			<last>Eitan</last>
		</authors>
		<authors>
			<first>Roni Y</first>
		</authors>
		<authors>
			<last>Granot</last>
		</authors>
		<volume>23</volume>
		<number>3</number>
		<pages>221--247</pages>
		<title>HOW MUSIC MOVES: Musical Parameters and Listeners' Images of Motion</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2467fb55c0c7dc9a8d00c1774fe4967ef/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2011-01-17 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2008</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj32.html#McCabe08</url>
		<author>Matthew McCabe</author>
		<authors>
			<first>Matthew</first>
		</authors>
		<authors>
			<last>McCabe</last>
		</authors>
		<volume>32</volume>
		<number>3</number>
		<pages>105-108</pages>
		<title>Andrew R. Brown: Computers in Music Education: Amplifying Musicality.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fcc2c4ee0087f83716ffc005a05e5d25/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2011-01-17 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2009</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj33.html#Budon09</url>
		<author>Osvaldo Budón</author>
		<authors>
			<first>Osvaldo</first>
		</authors>
		<authors>
			<last>Budón</last>
		</authors>
		<volume>33</volume>
		<number>4</number>
		<pages>83-87</pages>
		<title>Eleanor Stubley, Editor: Compositional Crossroads: Music, McGill, Montreal.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20287fe11b35f418978cda6055e688589/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2011-01-17 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2009</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj33.html#Feller09</url>
		<author>Ross Feller</author>
		<authors>
			<first>Ross</first>
		</authors>
		<authors>
			<last>Feller</last>
		</authors>
		<volume>33</volume>
		<number>1</number>
		<pages>78-80</pages>
		<title>Various: Music from SEAMUS, Volume 17.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27f4619da948d961980a5342ce98a96f1/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2011-01-17 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2010</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj34.html#Harley10a</url>
		<author>James Harley</author>
		<authors>
			<first>James</first>
		</authors>
		<authors>
			<last>Harley</last>
		</authors>
		<volume>34</volume>
		<number>4</number>
		<pages>77-79</pages>
		<title>Various: Recovery/Discovery - 40 Years of Surround Electronic Music in the UK.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a58edfe9fe74d9f502aaa9bd0b309ddd/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2011-01-17 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2010</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj34.html#Whalley10</url>
		<author>Ian Whalley</author>
		<authors>
			<first>Ian</first>
		</authors>
		<authors>
			<last>Whalley</last>
		</authors>
		<volume>34</volume>
		<number>1</number>
		<pages>102-104</pages>
		<title>David Temperley: Music and Probability.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/297caf4762a1cdb9ad14038c4fe8c013f/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2011-01-17 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2010</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj34.html#Whalley10a</url>
		<author>Ian Whalley</author>
		<authors>
			<first>Ian</first>
		</authors>
		<authors>
			<last>Whalley</last>
		</authors>
		<volume>34</volume>
		<number>2</number>
		<pages>93-96</pages>
		<title>Robert Reigle and Paul Whitehead (Editors): Spectral World Musics: Proceedings of the Istanbul Spectral Music Conference.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24f69ced2efa888049dff42ce19f79326/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2011-01-17 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2010</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj34.html#ShintaniK10a</url>
		<author>Joyce Shintani</author>
		<author>Metin Kara</author>
		<authors>
			<first>Joyce</first>
		</authors>
		<authors>
			<last>Shintani</last>
		</authors>
		<authors>
			<first>Metin</first>
		</authors>
		<authors>
			<last>Kara</last>
		</authors>
		<volume>34</volume>
		<number>3</number>
		<pages>67-69</pages>
		<title>Sónar in 2009: 16th Barcelona International Festival of Advanced Music and Multimedia Art.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/213b6b6f810ef9044095644e66380bc8b/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2011-01-17 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2008</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj32.html#Eigenfeldt08</url>
		<author>Arne Eigenfeldt</author>
		<authors>
			<first>Arne</first>
		</authors>
		<authors>
			<last>Eigenfeldt</last>
		</authors>
		<volume>32</volume>
		<number>1</number>
		<pages>91-94</pages>
		<title>International Computer Music Conference 2007: Live Electronics.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20a36a811e981b2c35846be9fe3a73173/abromwell</id>
		<description></description>
		<date>2010-12-22 20:04:12</date>
		<count>1</count>
		<journal>Proceedings of the Musical Association</journal>
		<year>1929</year>
		<url>http://www.jstor.org/stable/765761</url>
		<author>Marion Scott</author>
		<authors>
			<first>Marion</first>
		</authors>
		<authors>
			<last>Scott</last>
		</authors>
		<volume>56</volume>
		<pages>91-108</pages>
		<isbn>09588442</isbn>
		<language>English</language>
		<title>Paul Hindemith: His Music and Its Characteristics</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2cb2f6caa310affb30466eaf9beb800c6/abromwell</id>
		<description></description>
		<date>2010-12-22 20:04:04</date>
		<count>1</count>
		<journal>Music & Letters</journal>
		<year>1960</year>
		<url>http://www.jstor.org/stable/729523</url>
		<author>Colin Mason</author>
		<authors>
			<first>Colin</first>
		</authors>
		<authors>
			<last>Mason</last>
		</authors>
		<volume>41</volume>
		<number>2</number>
		<pages>150-155</pages>
		<isbn>00274224</isbn>
		<language>English</language>
		<title>Some Aspects of Hindemith's Chamber Music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fd834903fc0925624fc543b9333df431/abromwell</id>
		<description></description>
		<date>2010-12-22 20:04:02</date>
		<count>1</count>
		<journal>Musical Times</journal>
		<year>Summer 2002</year>
		<url></url>
		<author>Nicholas Jones</author>
		<authors>
			<first>Nicholas</first>
		</authors>
		<authors>
			<last>Jones</last>
		</authors>
		<volume>143</volume>
		<title>All Set: "Serial Music, Serial Aesthetics: Compositional Theory in Post-War Europe," by M. J. Grant</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b4423545f909bfd6faaf420306e3afac/abromwell</id>
		<description></description>
		<date>2010-12-22 20:03:57</date>
		<count>1</count>
		<journal>Journal of Music Theory</journal>
		<year>Spring 1998</year>
		<url></url>
		<author>Allen Forte</author>
		<author>Paul Hindemith</author>
		<authors>
			<first>Allen</first>
		</authors>
		<authors>
			<last>Forte</last>
		</authors>
		<authors>
			<first>Paul</first>
		</authors>
		<authors>
			<last>Hindemith</last>
		</authors>
		<volume>42</volume>
		<pages>1-14</pages>
		<title>Paul Hindemith's Contribution to Music Theory in the United States</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b9e76d5fd4f6927815d9845d20470ef0/andre@ismll</id>
		<tags>evaluation</tags>
		<tags>acoustic</tags>
		<description>initial imports</description>
		<date>2010-05-06 08:43:06</date>
		<count>5</count>
		<journal>Computer Music J.</journal>
		<publisher>MIT Press</publisher>
		<address>Cambridge, MA, USA</address>
		<year>2004</year>
		<url></url>
		<author>Adam Berenzweig</author>
		<author>Beth Logan</author>
		<author>Daniel P. W. Ellis</author>
		<author>Brian P. W. Whitman</author>
		<authors>
			<first>Adam</first>
		</authors>
		<authors>
			<last>Berenzweig</last>
		</authors>
		<authors>
			<first>Beth</first>
		</authors>
		<authors>
			<last>Logan</last>
		</authors>
		<authors>
			<first>Daniel P. W.</first>
		</authors>
		<authors>
			<last>Ellis</last>
		</authors>
		<authors>
			<first>Brian P. W.</first>
		</authors>
		<authors>
			<last>Whitman</last>
		</authors>
		<volume>28</volume>
		<number>2</number>
		<pages>63-76</pages>
		<issn>0148-9267</issn>
		<doi>http://dx.doi.org/10.1162/014892604323112257</doi>
		<title>A Large-Scale Evaluation of Acoustic and Subjective Music-Similarity Measures</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b4207248180bbef0afb0d187b6648012/svrist</id>
		<tags>imported</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>1</count>
		<year>1966</year>
		<url></url>
		<author>D. Pruslin</author>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Pruslin</last>
		</authors>
		<title>Automatic recognition of sheet music</title>
		<pubtype>phdthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/292a464440e5bb138bdfc9ecb0ea0f673/svrist</id>
		<tags>imported</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>2</count>
		<journal>Computers and the Humanities</journal>
		<year>2001</year>
		<url>http://www.springerlink.com/content/x1nv36548113k51u</url>
		<author>David Bainbridge</author>
		<author>Tim Bell</author>
		<authors>
			<first>David</first>
		</authors>
		<authors>
			<last>Bainbridge</last>
		</authors>
		<authors>
			<first>Tim</first>
		</authors>
		<authors>
			<last>Bell</last>
		</authors>
		<volume>35</volume>
		<number>2</number>
		<pages>95 -- 121</pages>
		<doi>10.1023/A:1002485918032</doi>
		<title>The Challenge of Optical Music Recognition</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a3dea1b4bf0dbee6f6b9505f74027032/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2010-04-19</date>
		<count>2</count>
		<booktitle>Advances in Music Information Retrieval</booktitle>
		<series>Studies in Computational Intelligence</series>
		<publisher>Springer</publisher>
		<year>2010</year>
		<url>http://dblp.uni-trier.de/db/series/sci/sci274.html#Kitahara10</url>
		<author>Tetsuro Kitahara</author>
		<authors>
			<first>Tetsuro</first>
		</authors>
		<authors>
			<last>Kitahara</last>
		</authors>
		<editor>Zbigniew W. Ras</editor>
		<editor>Alicja Wieczorkowska</editor>
		<editors>
			<first>Tetsuro</first>
		</editors>
		<editors>
			<last>Kitahara</last>
		</editors>
		<editors>
			<first>Tetsuro</first>
		</editors>
		<editors>
			<last>Kitahara</last>
		</editors>
		<volume>274</volume>
		<pages>65-91</pages>
		<isbn>978-3-642-11673-5</isbn>
		<title>Mid-level Representations of Musical Audio Signals for Music Information Retrieval.</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/293aae0b2bd45fb395e668bf53654dee9/smatthiesen</id>
		<tags>music</tags>
		<description></description>
		<date>2010-11-30 22:39:03</date>
		<count>1</count>
		<journal>BMC Neuroscience</journal>
		<year>2009</year>
		<url>http://dx.doi.org/10.1186/1471-2202-10-143</url>
		<author>Rossitza Draganova</author>
		<author>Andreas Wollbrink</author>
		<author>Matthias Schulz</author>
		<author>Hidehiko Okamoto</author>
		<author>Christo Pantev</author>
		<authors>
			<first>Rossitza</first>
		</authors>
		<authors>
			<last>Draganova</last>
		</authors>
		<authors>
			<first>Andreas</first>
		</authors>
		<authors>
			<last>Wollbrink</last>
		</authors>
		<authors>
			<first>Matthias</first>
		</authors>
		<authors>
			<last>Schulz</last>
		</authors>
		<authors>
			<first>Hidehiko</first>
		</authors>
		<authors>
			<last>Okamoto</last>
		</authors>
		<authors>
			<first>Christo</first>
		</authors>
		<authors>
			<last>Pantev</last>
		</authors>
		<volume>10</volume>
		<number>1</number>
		<pages>143+</pages>
		<abstract>BACKGROUND:Due to auditory experience, musicians have better auditory expertise than non-musicians. An increased neocortical activity during auditory oddball stimulation was observed in different studies for musicians and for non-musicians after discrimination training. This suggests a modification of synaptic strength among simultaneously active neurons due to the training. We used amplitude-modulated tones (AM) presented in an oddball sequence and manipulated their carrier or modulation frequencies. We investigated non-musicians in order to see if behavioral discrimination training could modify the neocortical activity generated by change detection of AM tone attributes (carrier or modulation frequency). Cortical evoked responses like N1 and mismatch negativity (MMN) triggered by sound changes were recorded by a whole head magnetoencephalographic system (MEG). We investigated (i) how the auditory cortex reacts to pitch difference (in carrier frequency) and changes in temporal features (modulation frequency) of AM tones and (ii) how discrimination training modulates the neuronal activity reflecting the transient auditory responses generated in the auditory cortex.RESULTS:The results showed that, additionally to an improvement of the behavioral discrimination performance, discrimination training of carrier frequency changes significantly modulates the MMN and N1 response amplitudes after the training. This process was accompanied by an attention switch to the deviant stimulus after the training procedure identified by the occurrence of a P3a component. In contrast, the training in discrimination of modulation frequency was not sufficient to improve the behavioral discrimination performance and to alternate the cortical response (MMN) to the modulation frequency change. The N1 amplitude, however, showed significant increase after and one week after the training. Similar to the training in carrier frequency discrimination, a long lasting involuntary attention to the deviant stimulus was observed.CONCLUSION:We found that discrimination training differentially modulates the cortical responses to pitch changes and to envelope fluctuation changes of AM tones. This suggests that discrimination between AM tones requires additional neuronal mechanisms compared to discrimination process between pure tones. After the training, the subjects demonstrated an involuntary attention switch to the deviant stimulus (represented by the P3a-component in the MEG) even though attention was not prerequisite.</abstract>
		<issn>1471-2202</issn>
		<doi>10.1186/1471-2202-10-143</doi>
		<title>Modulation of auditory evoked responses to spectral and temporal changes by behavioral discrimination training</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b66168ccf50048c322559c4a4cf38711/svrist</id>
		<tags>imported</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>1</count>
		<journal>Proceedings of International Workshop on Structural and Syntactic Pattern Recognition</journal>
		<year>1992</year>
		<url></url>
		<author>S. Baumann</author>
		<author>A. Dengel</author>
		<authors>
			<first>S.</first>
		</authors>
		<authors>
			<last>Baumann</last>
		</authors>
		<authors>
			<first>A.</first>
		</authors>
		<authors>
			<last>Dengel</last>
		</authors>
		<pages>363-72</pages>
		<title>Transforming printed piano music into MIDI</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e23305c82f6cf2eb542c975217a0412c/svrist</id>
		<tags>imported</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>1</count>
		<year>1982</year>
		<url></url>
		<author>J. V. Mahoney</author>
		<authors>
			<first>J. V.</first>
		</authors>
		<authors>
			<last>Mahoney</last>
		</authors>
		<title>Automatic analysis of musical score images</title>
		<pubtype>phdthesis</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a92b61094bd9670414cc778b3fe63e44/svrist</id>
		<tags>imported</tags>
		<description>Optical Music Recognition</description>
		<date>2010-04-12 16:18:45</date>
		<count>2</count>
		<booktitle>ISMIR</booktitle>
		<year>2006</year>
		<url></url>
		<author>Laurent Pugin</author>
		<authors>
			<first>Laurent</first>
		</authors>
		<authors>
			<last>Pugin</last>
		</authors>
		<pages>53-56</pages>
		<bibsource>DBLP, http://dblp.uni-trier.de</bibsource>
		<title>Optical Music Recognitoin of Early Typographic Prints using
               Hidden Markov Models</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/211b2abb768be5f84001ec7061a86f886/keinstein</id>
		<tags>Zentralblatt_Math</tags>
		<tags>Musiktheorie</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2009-10-23 11:46:41</date>
		<count>2</count>
		<journal>J. Math. Music</journal>
		<year>2007</year>
		<url></url>
		<author>Thomas Noll</author>
		<authors>
			<first>Thomas</first>
		</authors>
		<authors>
			<last>Noll</last>
		</authors>
		<volume>1</volume>
		<number>2</number>
		<pages>121-137</pages>
		<doi>10.1080/17459730701375026</doi>
		<title>Musical intervals and special linear transformations.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/21a402049fccea9105093c40081b0f74f/keinstein</id>
		<tags>MaMu</tags>
		<description>JSTOR: Studia Musicologica Academiae Scientiarum Hungaricae, T. 9, Fasc. 1/2 (1967), pp. 163-186</description>
		<date>2012-11-22 21:10:12</date>
		<count>2</count>
		<journal>Studia Musicologica Academiae Scientiarum Hungaricae</journal>
		<year>1967</year>
		<url></url>
		<author>W. Reckziegel</author>
		<authors>
			<first>W.</first>
		</authors>
		<authors>
			<last>Reckziegel</last>
		</authors>
		<volume>9</volume>
		<number>1/2</number>
		<pages>163–186</pages>
		<title>Musikanalyse und Wissenschaft</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2112bec1599a096fb4423d7b8b29b51c8/keinstein</id>
		<tags>Musiktheorie</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2009-10-23 11:46:41</date>
		<count>2</count>
		<journal>MUSIC THEORY SPECTRUM</journal>
		<publisher>UNIV CALIF PRESS</publisher>
		<address>JOURNALS DEPT 2120 BERKELEY WAY, BERKELEY, CA 94720 USA</address>
		<year>{1999}</year>
		<url></url>
		<author>O Vaisala</author>
		<authors>
			<first>O</first>
		</authors>
		<authors>
			<last>Vaisala</last>
		</authors>
		<volume>21</volume>
		<number>2</number>
		<pages>230-259</pages>
		<abstract>The study discusses whether prolongational organization occurs in Schoenberg's op. 19/2. The four conditions of prolongation postulated by Joseph Strauss are adopted as the basis of the discussion, but the method of identifying harmonies with pitch-class sets is rejected. This is justified partly on perceptual grounds. Using a conception of intervals and harmonies with restricted octave equivalence, an analysis is presented showing a structure in which the four condition are fulfilled to a significant degree. Additional examples showing comparable organization in Schoenberg's op. 11 are presented.</abstract>
		<issn>0195-6167</issn>
		<language>English</language>
		<title>Concepts of harmony and prolongation in Schoenberg's Op. 19/2</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2fd53972a3553938d1b72b0f8d63ca32e/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2013-01-17 00:00:00</date>
		<count>1</count>
		<journal>Computer Music Journal</journal>
		<year>2012</year>
		<url>http://dblp.uni-trier.de/db/journals/comj/comj36.html#BenetosD12</url>
		<author>Emmanouil Benetos</author>
		<author>Simon Dixon</author>
		<authors>
			<first>Emmanouil</first>
		</authors>
		<authors>
			<last>Benetos</last>
		</authors>
		<authors>
			<first>Simon</first>
		</authors>
		<authors>
			<last>Dixon</last>
		</authors>
		<volume>36</volume>
		<number>4</number>
		<pages>81-94</pages>
		<title>A Shift-Invariant Latent Variable Model for Automatic Music Transcription.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26c9399f9319794142c183a617ea3be54/keinstein</id>
		<tags>Zentralblatt_Math</tags>
		<tags>Musiktheorie</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2009-10-23 11:46:41</date>
		<count>2</count>
		<publisher>Cambridge University Press</publisher>
		<address>Cambridge</address>
		<year>2007</year>
		<url></url>
		<author>Dave Benson</author>
		<authors>
			<first>Dave</first>
		</authors>
		<authors>
			<last>Benson</last>
		</authors>
		<pages>xiii, 411</pages>
		<title>Music: a mathematical offering.</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/200895030472d8a4f6ef07a3ab305f57a/keinstein</id>
		<tags>Notensatz</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2009-10-23 11:46:41</date>
		<count>3</count>
		<booktitle>Proc. International Computer Music Conference 2000 (ICMC-2000)</booktitle>
		<publisher>International Computer Music Association (ICMA), San Francisco, CA</publisher>
		<year>2000</year>
		<url></url>
		<author>Jürgen Kilian</author>
		<author>Holger H. Hoos</author>
		<authors>
			<first>Jürgen</first>
		</authors>
		<authors>
			<last>Kilian</last>
		</authors>
		<authors>
			<first>Holger H.</first>
		</authors>
		<authors>
			<last>Hoos</last>
		</authors>
		<pages>454--457</pages>
		<title>VISCO -- Visual SALIERI Components</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24f336f11e6bd1cee644ac7cb8681d4d1/keinstein</id>
		<tags>Software</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2012-09-21 10:45:03</date>
		<count>5</count>
		<year>2002</year>
		<url>http://elib.tu-darmstadt.de/diss/000265/kair̲enzd̲iss.pdf</url>
		<author>Kai Renz</author>
		<authors>
			<first>Kai</first>
		</authors>
		<authors>
			<last>Renz</last>
		</authors>
		<genre>Notenschrift</genre>
		<rawdata>LDR 01332nmm 2200397 i 4500 001 377787485 003 GBV 008 040129s2002 gw 000 eng d 015 $aDDB967375975 040 $aGBV$bger 245 00 $aAlgorithms and data structures for a music notation system based on GUIDO music notation /$hcomputer file.$cvorgelegt von Kai Renz 260 $c2002. 300 $aOnline-Ressource. 502 $aTechn. Univ., Diss--Darmstadt, 2002. 650 $aNotenschrift$xComputerunterstütztes Verfahren$vOnline-Publikation 700 1 $aRenz, Kai 801 0 $aDE$bGyFmDB$gRAK-WB 856 7 $uhttp://elib.tu-darmstadt.de/diss/000265/kair̲enzd̲iss.pdf 856 7 $uhttp://elib.tu-darmstadt.de/diss/000265 856 7 $uhttp://deposit.ddb.de/cgi-bin/dokserv?idn=967375975 900 $aGBV$bUB Braunschweig <84> 900 $aGBV$bSUB Bremen <46> 900 $aGBV$bSUB+Uni Hamburg <18> 900 $aGBV$bTUB Hamburg <830> 900 $aGBV$bUB Clausthal <104> 900 $aGBV$bSUB Goettingen <7> 900 $aGBV$bHSU Hamburg <705> 900 $aGBV$bUB Lueneburg <Luen 4> 990 $a65327663X$z01 990 $a65970143X$z01 990 $a65337772X$z01 990 $a737811528$z01 990 $a719885930$z01 990 $a653473834$z01 990 $a653572670$z01 990 $a719990920$z01 $</rawdata>
		<title>Algorithms and data structures for a music notation system based on GUIDO music notation</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2eca0a7435bc0d10f3d63d245ec00e81f/keinstein</id>
		<tags>Musik</tags>
		<description></description>
		<date>2012-09-20 18:23:09</date>
		<count>2</count>
		<year>2005</year>
		<url>http://deposit.ddb.de/cgi-bin/dokserv?idn=974040584</url>
		<author>Jürgen Kilian</author>
		<authors>
			<first>Jürgen</first>
		</authors>
		<authors>
			<last>Kilian</last>
		</authors>
		<genre>Tonsignal</genre>
		<rawdata>LDR 01232nmm 2200385 i 4500 001 488089026 003 GBV 008 050602s2005 gw 000 eng d 015 $aDDB974040584 040 $aGBV$bger 245 00 $aInferring score level musical information from low-level musical data /$hcomputer file.$cvorgelegt von Jürgen Kilian 260 $c2005. 300 $aOnline-Ressource. 500 $aDateien im PDF-Format.$5GBV 502 $aTechn. Univ., Diss--Darmstadt, 2004. 650 $aTonsignal$xNotensatz$xMustervergleich$vOnline-Publikation 700 1 $aKilian, Jürgen 801 0 $aDE$bGyFmDB$gRAK-WB 856 4 $uhttp://deposit.ddb.de/cgi-bin/dokserv?idn=974040584 900 $aGBV$bUB Braunschweig <84> 900 $aGBV$bSUB Bremen <46> 900 $aGBV$bSUB+Uni Hamburg <18> 900 $aGBV$bTUB Hamburg <830> 900 $aGBV$bUB Clausthal <104> 900 $aGBV$bSUB Goettingen <7> 900 $aGBV$bHSU Hamburg <705> 900 $aGBV$bUB Lueneburg <Luen 4> 990 $a722714742$z01 990 $a722747470$z01 990 $a722722869$z01 990 $a737856211$z01 990 $a722762879$z01 990 $a722731051$z01 990 $a722739168$z01 990 $a722755481$z01</rawdata>
		<title>Inferring score level musical information from low-level musical data</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e50a065bce4890f1c962758de95fb612/avs</id>
		<tags>Orpheus</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2013-04-28 16:48:09</date>
		<count>1</count>
		<series>Musicae sacrae melethmata</series>
		<publisher>Coppenrath</publisher>
		<address>Altötting</address>
		<year>1976</year>
		<url></url>
		<author>Robert A. Skeris</author>
		<authors>
			<first>Robert A.</first>
		</authors>
		<authors>
			<last>Skeris</last>
		</authors>
		<number>1</number>
		<title>ΧΡΩΜΑ ΘΕΟΥ. On the origins and theological interpretation of the musical imagery used by the ecclesiastical writers of the first three centuries, with special reference to the image of Orpheus</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2ad136736ec625ddcd2919d25ba0cef66/fbw_hannover</id>
		<tags>Multimedia</tags>
		<tags>Musik:_Sonstiges</tags>
		<tags>Rechnerkommunikation</tags>
		<tags>Electronic_Commerce</tags>
		<tags>Marketing</tags>
		<tags>Musikmarkt</tags>
		<description></description>
		<date>2009-08-21 12:18:06</date>
		<count>1</count>
		<publisher>IEEE Computer Society</publisher>
		<address>Los Alamitos, Calif. u.a.</address>
		<year>2004</year>
		<url>http://gso.gbv.de/DB=2.1/CMD?ACT=SRCHA&SRT=YOP&IKT=1016&TRM=ppn+473225727&sourceid=fbw_bibsonomy</url>
		<editor>Jaime Delgado</editor>
		<editors>
			<first>Robert A.</first>
		</editors>
		<editors>
			<last>Skeris</last>
		</editors>
		<isbn>0769521576</isbn>
		<subtitle>International Conference on WEB Delivering of Music</subtitle>
		<pagetotal>IX, 181</pagetotal>
		<ppn_gvk>473225727</ppn_gvk>
		<title>Proceedings of the Fourth International Conference on WEB Delivering of Music, WEDELMUSIC 2004, 13 - 14 September 2004, Barcelona, Spain</title>
		<pubtype>proceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2f2ac025ac416ea47f9ec37d8a0c6d0c0/dblp</id>
		<tags>dblp</tags>
		<description></description>
		<date>2015-01-10 00:00:00</date>
		<count>1</count>
		<booktitle>EUROSIM</booktitle>
		<publisher>Elsevier</publisher>
		<year>1995</year>
		<url>http://dblp.uni-trier.de/db/conf/eurosim/eurosim1995.html#MusicM95</url>
		<author>Gasper Music</author>
		<author>Drago Matko</author>
		<authors>
			<first>Gasper</first>
		</authors>
		<authors>
			<last>Music</last>
		</authors>
		<authors>
			<first>Drago</first>
		</authors>
		<authors>
			<last>Matko</last>
		</authors>
		<editor>Felix Breitenecker</editor>
		<editor>Irmgard Husinsky</editor>
		<editors>
			<first>Drago</first>
		</editors>
		<editors>
			<last>Matko</last>
		</editors>
		<editors>
			<first>Drago</first>
		</editors>
		<editors>
			<last>Matko</last>
		</editors>
		<pages>1299-1304</pages>
		<isbn>0-444-82241-0</isbn>
		<title>Teachware for Modelling: An Air Conditioning Pilot Plant Case Study.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/228ef5b228cff7aa941a6ab06020c7ef8/ijritcc</id>
		<tags>MUSIC</tags>
		<tags>antenna</tags>
		<tags>GSU</tags>
		<tags>Section</tags>
		<tags>Smart</tags>
		<tags>Classification)</tags>
		<tags>(Gold</tags>
		<tags>DOA(Directional</tags>
		<tags>(Multiple</tags>
		<tags>Arrival)</tags>
		<tags>minimization</tags>
		<tags>of</tags>
		<tags>Univariate)</tags>
		<tags>Signal</tags>
		<description></description>
		<date>2015-08-27 08:38:58</date>
		<count>1</count>
		<journal>International Journal on Recent and Innovation Trends in Computing and Communication</journal>
		<publisher>Auricle Technologies, Pvt., Ltd.</publisher>
		<year>2015</year>
		<url>http://dx.doi.org/10.17762/ijritcc2321-8169.1504125</url>
		<author>Ganesh V. Karbhari</author>
		<author> Prof.A.S.Deshpande</author>
		<authors>
			<first>Ganesh V.</first>
		</authors>
		<authors>
			<last>Karbhari</last>
		</authors>
		<authors>
			<first></first>
		</authors>
		<authors>
			<last>Prof.A.S.Deshpande</last>
		</authors>
		<volume>3</volume>
		<number>4</number>
		<pages>2353--2355</pages>
		<abstract>A smart antenna is a digital wireless communications antenna system that takes advantage of diversity effect at the source (transmitter), the destination (receiver) or both. Diversity effect involves the transmission and/or reception of multiple radio frequency (RF) waves to increase data speed and reduce the error rate. A smart antenna enables a higher capacity in wireless networks by effectively reducing multipath and co-channel interference. This is achieved by focusing the radiation only in the desired direction and adjusting itself to changing traffic conditions or signal environments. Smart antennas employ a set of radiating elements arranged in the form of an array. The GSU-MUSIC algorithm for DOA estimation of smart antenna is similar to MUSIC and it uses iterative approach based on GSU minimization to find accurate values of the peaks. The GSU-MUSIC Algorithm overcomes the problems associated with previous techniques used for DOA estimation of smart antenna.</abstract>
		<doi>10.17762/ijritcc2321-8169.1504125</doi>
		<title>Direction of Arrival Algorithm using GSUminimization</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/20e5015c33e33a45aa573e9d251507a38/kerstinbischoff</id>
		<tags>myown</tags>
		<tags>pharos</tags>
		<description></description>
		<date>2012-12-07 16:52:51</date>
		<count>3</count>
		<booktitle>ISMIR</booktitle>
		<publisher>International Society for Music Information Retrieval</publisher>
		<year>2009</year>
		<url>http://dblp.uni-trier.de/db/conf/ismir/ismir2009.html#BischoffFPNLS09</url>
		<author>Kerstin Bischoff</author>
		<author>Claudiu S. Firan</author>
		<author>Raluca Paiu</author>
		<author>Wolfgang Nejdl</author>
		<author>Cyril Laurier</author>
		<author>Mohamed Sordo</author>
		<authors>
			<first>Kerstin</first>
		</authors>
		<authors>
			<last>Bischoff</last>
		</authors>
		<authors>
			<first>Claudiu S.</first>
		</authors>
		<authors>
			<last>Firan</last>
		</authors>
		<authors>
			<first>Raluca</first>
		</authors>
		<authors>
			<last>Paiu</last>
		</authors>
		<authors>
			<first>Wolfgang</first>
		</authors>
		<authors>
			<last>Nejdl</last>
		</authors>
		<authors>
			<first>Cyril</first>
		</authors>
		<authors>
			<last>Laurier</last>
		</authors>
		<authors>
			<first>Mohamed</first>
		</authors>
		<authors>
			<last>Sordo</last>
		</authors>
		<editor>Keiji Hirata</editor>
		<editor>George Tzanetakis</editor>
		<editor>Kazuyoshi Yoshii</editor>
		<editors>
			<first>Mohamed</first>
		</editors>
		<editors>
			<last>Sordo</last>
		</editors>
		<editors>
			<first>Mohamed</first>
		</editors>
		<editors>
			<last>Sordo</last>
		</editors>
		<editors>
			<first>Mohamed</first>
		</editors>
		<editors>
			<last>Sordo</last>
		</editors>
		<pages>657-662</pages>
		<isbn>978-0-9813537-0-8</isbn>
		<title>Music Mood and Theme Classification - a Hybrid Approach.</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c4a5f2a9867cf09825fbcb3d0817eeae/udphonlab</id>
		<tags>music</tags>
		<tags>cognition</tags>
		<tags>perception</tags>
		<description></description>
		<date>2012-03-15 18:27:21</date>
		<count>1</count>
		<year>1982</year>
		<url></url>
		<author>Carl L. Krumhansl</author>
		<author>Edward J. Kessler</author>
		<authors>
			<first>Carl L.</first>
		</authors>
		<authors>
			<last>Krumhansl</last>
		</authors>
		<authors>
			<first>Edward J.</first>
		</authors>
		<authors>
			<last>Kessler</last>
		</authors>
		<title>Tracing the Dynamic Changes in Perceived Tonal Organization in a Spatial Representation of Musical Keys</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23d8b80948679fdc6d96b7ea01a3d6532/udphonlab</id>
		<tags>music</tags>
		<tags>pitch</tags>
		<tags>cognition</tags>
		<tags>rhythm</tags>
		<description></description>
		<date>2012-03-15 18:18:07</date>
		<count>1</count>
		<year>2000</year>
		<url></url>
		<author>Carl L. Krumhansl</author>
		<authors>
			<first>Carl L.</first>
		</authors>
		<authors>
			<last>Krumhansl</last>
		</authors>
		<title>Rhythm and Pitch in Music Cognition</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/29e90f8739460ef9fdca20dfa0ff4cb7d/ks-plugin-devel</id>
		<tags>MIDI</tags>
		<tags>Instrument</tags>
		<tags>elektronische_Musik</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2013-02-02 14:40:36</date>
		<count>3</count>
		<booktitle>Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010), Sydney, Australia</booktitle>
		<year>2010</year>
		<url>http://www.educ.dab.uts.edu.au/nime/PROCEEDINGS/papers/Paper%20D1-D3/P76_Essl.pdf</url>
		<author>Georg Essl</author>
		<author>Alexander Müller</author>
		<authors>
			<first>Georg</first>
		</authors>
		<authors>
			<last>Essl</last>
		</authors>
		<authors>
			<first>Alexander</first>
		</authors>
		<authors>
			<last>Müller</last>
		</authors>
		<title>Designing Mobile Musical Instruments and Environments with urMus</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2065b6f3c6527844516e76a452ac49911/ks-plugin-devel</id>
		<tags>Willezitat</tags>
		<description></description>
		<date>2013-02-02 14:42:28</date>
		<count>2</count>
		<journal>Novi Commentarii academiae scientiarum Petropolitanae</journal>
		<year>1774</year>
		<url>http://www.math.dartmouth.edu/~euler/pages/E457.html</url>
		<author>L. Euler</author>
		<authors>
			<first>L.</first>
		</authors>
		<authors>
			<last>Euler</last>
		</authors>
		<volume>18</volume>
		<pages>330--353</pages>
		<title>De harmoniae veris principiis per speculum musicum repraesentatis</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/26964a37554af3bd48bad1819e105dea7/ks-plugin-devel</id>
		<tags>Willezitat</tags>
		<description></description>
		<date>2013-02-02 14:42:27</date>
		<count>2</count>
		<booktitle>De musica libri VI</booktitle>
		<series>Sammlung musikwissenschaftlicher Abhandlungen</series>
		<publisher>Heitz</publisher>
		<year>1937</year>
		<url>http://www.hs-augsburg.de/~harsch/aug_mu00.html</url>
		<author>Aurelius Augustinus</author>
		<author>Carl Johann (Übers.) Perl</author>
		<authors>
			<first>Aurelius</first>
		</authors>
		<authors>
			<last>Augustinus</last>
		</authors>
		<authors>
			<first>Carl Johann (Übers.)</first>
		</authors>
		<authors>
			<last>Perl</last>
		</authors>
		<volume>Teil 23</volume>
		<title>De musica libri VI</title>
		<pubtype>book</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a63d6e54231b75575e5be72b84e6c71a/ks-plugin-devel</id>
		<tags>imported</tags>
		<description></description>
		<date>2013-02-02 14:43:17</date>
		<count>3</count>
		<booktitle>Proc. International Computer Music Conference 1999 (ICMC-99)</booktitle>
		<publisher>International Computer Music Association (ICMA), San Francisco, CA</publisher>
		<year>1999</year>
		<url></url>
		<author>Holger H. Hoos</author>
		<author>Keith A. Hamel</author>
		<author>Kai Renz</author>
		<authors>
			<first>Holger H.</first>
		</authors>
		<authors>
			<last>Hoos</last>
		</authors>
		<authors>
			<first>Keith A.</first>
		</authors>
		<authors>
			<last>Hamel</last>
		</authors>
		<authors>
			<first>Kai</first>
		</authors>
		<authors>
			<last>Renz</last>
		</authors>
		<pages>395--398</pages>
		<title>Using Advanced GUIDO as a Notation Interchange Format</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/200895030472d8a4f6ef07a3ab305f57a/ks-plugin-devel</id>
		<tags>Notensatz</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2013-02-02 14:43:17</date>
		<count>3</count>
		<booktitle>Proc. International Computer Music Conference 2000 (ICMC-2000)</booktitle>
		<publisher>International Computer Music Association (ICMA), San Francisco, CA</publisher>
		<year>2000</year>
		<url></url>
		<author>Jürgen Kilian</author>
		<author>Holger H. Hoos</author>
		<authors>
			<first>Jürgen</first>
		</authors>
		<authors>
			<last>Kilian</last>
		</authors>
		<authors>
			<first>Holger H.</first>
		</authors>
		<authors>
			<last>Hoos</last>
		</authors>
		<pages>454--457</pages>
		<title>VISCO -- Visual SALIERI Components</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/211b2abb768be5f84001ec7061a86f886/ks-plugin-devel</id>
		<tags>Zentralblatt_Math</tags>
		<tags>Musiktheorie</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2013-02-02 14:43:10</date>
		<count>2</count>
		<journal>J. Math. Music</journal>
		<year>2007</year>
		<url></url>
		<author>Thomas Noll</author>
		<authors>
			<first>Thomas</first>
		</authors>
		<authors>
			<last>Noll</last>
		</authors>
		<volume>1</volume>
		<number>2</number>
		<pages>121-137</pages>
		<doi>10.1080/17459730701375026</doi>
		<title>Musical intervals and special linear transformations.</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27c47d3e55f1413b82160f1b16e80cd15/ks-plugin-devel</id>
		<tags>Mathematik</tags>
		<tags>Musiktheorie</tags>
		<tags>Musik</tags>
		<description></description>
		<date>2013-02-02 14:41:01</date>
		<count>2</count>
		<journal>Music Theory Online</journal>
		<year>2009</year>
		<url>http://mto.societymusictheory.org/</url>
		<editor>Yonatan Malin</editor>
		<editors>
			<first>Thomas</first>
		</editors>
		<editors>
			<last>Noll</last>
		</editors>
		<issn>1067-3040</issn>
		<title>Music Theory Online</title>
		<pubtype>periodical</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2b4276dba69fc8ff10bbf3ac8843520d1/mandrean</id>
		<tags>timbre</tags>
		<description></description>
		<date>2015-12-19 09:35:07</date>
		<count>1</count>
		<journal>Contemporary Music Review</journal>
		<year>1994</year>
		<url>/brokenurl#         http://dx.doi.org/10.1080/07494469400640291    </url>
		<author>Leigh Landy</author>
		<authors>
			<first>Leigh</first>
		</authors>
		<authors>
			<last>Landy</last>
		</authors>
		<volume>10</volume>
		<number>2</number>
		<pages>49-60</pages>
		<abstract>A great deal of today's electroacoustic music has been found to be highly complex and difficult to penetrate for the listener, yet no one likes to get lost during a work. In my experience offering the first time listener a helping hand is one of the great ways of making contemporary works more accessible. In this article I attempt to describe a tool, illustrate it and to an extent categorise applications relevant to timbral electroacoustic composition, music in which there is often no melody (or Hauptstimme), no metre, no tonic, perhaps no audible structure. If none of these traditional devices is a composer's aural focus, what is, and can this device be perceived by the listener? The goal here is not to stimulate simplicity in timbral music, but instead to strive for greater music appreciation and consequently music evaluation in which composers, musicologists and all others involved in electroacoustic music can profit. An extensive CDiscography has been included.</abstract>
		<eprint>http://dx.doi.org/10.1080/07494469400640291</eprint>
		<doi>10.1080/07494469400640291</doi>
		<title>The “something to hold on to factor” in timbral composition</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2c73235b1485d26d96c3f493eb620a8d8/mandrean</id>
		<tags>schaeffer</tags>
		<tags>historical-analysis</tags>
		<description></description>
		<date>2015-12-19 09:32:33</date>
		<count>1</count>
		<journal>Contemporary Music Review</journal>
		<year>1994</year>
		<url>/brokenurl#         http://dx.doi.org/10.1080/07494469400640251    </url>
		<author>John Dack</author>
		<authors>
			<first>John</first>
		</authors>
		<authors>
			<last>Dack</last>
		</authors>
		<volume>10</volume>
		<number>2</number>
		<pages>3-11</pages>
		<abstract>Pierre Schaeffer's reaction to his perception of “concrete sounds” was of fundamental importance to his later musical investigations. However, it is often dismissed as merely the promotion of explicitly anecdotal sound vocabularies drawn from the “real-world”. This is a misconception which obscures the significance of Schaeffer's specifically musical researches; the term “concrete” becomes vague and its cultural and philosophical resonances remain concealed. Schaeffer realised that in the acousmatic environment of radio broadcasting “real”, “documentary” sounds could be powerfully evocative, thereby transcending their causal origins. Subsequently, these experiences were refined as he examined how musical aspects might be extracted from any object in the sound universe. Thus investigations of musical language were actively promoted by techniques of radio production. Furthermore, anecdotal sounds encouraged a humanist dimension in Schaeffer's thinking: Man's reaction to the acoustic world when modulated by the transforming power of technology.</abstract>
		<eprint>http://dx.doi.org/10.1080/07494469400640251</eprint>
		<doi>10.1080/07494469400640251</doi>
		<title>Pierre Schaeffer and the significance of radiophonic art</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/271e6ecbd93e57f34d83968528215a9d8/mandrean</id>
		<tags>real-world</tags>
		<tags>JCRISSET</tags>
		<description></description>
		<date>2015-12-19 09:16:47</date>
		<count>1</count>
		<journal>Contemporary Music Review</journal>
		<publisher>Informa UK Limited</publisher>
		<year>1996</year>
		<url>http://dx.doi.org/10.1080/07494469600640341</url>
		<author>Jean-Claude Risset</author>
		<authors>
			<first>Jean-Claude</first>
		</authors>
		<authors>
			<last>Risset</last>
		</authors>
		<volume>15</volume>
		<number>1</number>
		<pages>29--47</pages>
		<doi>10.1080/07494469600640341</doi>
		<title>Real-world sounds and simulacra in my computer music</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/22453a940a04c770c8e23bb62f786a1a3/alexarje</id>
		<tags>imported</tags>
		<description></description>
		<date>2016-02-26 20:58:13</date>
		<count>1</count>
		<booktitle>Proceedings of the International Conference on New Interfaces for Musical Expression</booktitle>
		<address>Oslo, Norway</address>
		<year>2011</year>
		<url></url>
		<author>N. Schnell</author>
		<author>F. Bevilacqua</author>
		<author>N. Rasamimana</author>
		<author>J. Bloit</author>
		<author>F. Guedy</author>
		<author>E. Flety</author>
		<authors>
			<first>N.</first>
		</authors>
		<authors>
			<last>Schnell</last>
		</authors>
		<authors>
			<first>F.</first>
		</authors>
		<authors>
			<last>Bevilacqua</last>
		</authors>
		<authors>
			<first>N.</first>
		</authors>
		<authors>
			<last>Rasamimana</last>
		</authors>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Bloit</last>
		</authors>
		<authors>
			<first>F.</first>
		</authors>
		<authors>
			<last>Guedy</last>
		</authors>
		<authors>
			<first>E.</first>
		</authors>
		<authors>
			<last>Flety</last>
		</authors>
		<pages>535--536</pages>
		<title>Playing the ''MO''---Gestural Control and Re---Embodiment of Recorded Sound and Music</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2a889d1045254d89649fb9cfb371e376d/alexarje</id>
		<tags>imported</tags>
		<description></description>
		<date>2016-02-26 20:58:13</date>
		<count>1</count>
		<booktitle>Music and Emotion</booktitle>
		<publisher>Oxford University Press</publisher>
		<year>2001</year>
		<url></url>
		<author>I. Peretz</author>
		<authors>
			<first>I.</first>
		</authors>
		<authors>
			<last>Peretz</last>
		</authors>
		<editor>P. N. Juslin</editor>
		<editor>J. A. Sloboda</editor>
		<editors>
			<first>I.</first>
		</editors>
		<editors>
			<last>Peretz</last>
		</editors>
		<editors>
			<first>I.</first>
		</editors>
		<editors>
			<last>Peretz</last>
		</editors>
		<title>Listen to the brain: a biological perspective on musical emotions</title>
		<pubtype>incollection</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27fd3cfce7c94aae467ec4d98d2d054bc/alexarje</id>
		<tags>imported</tags>
		<description></description>
		<date>2016-02-26 20:58:13</date>
		<count>1</count>
		<booktitle>Proceedings of the International Computer Music Conference</booktitle>
		<year>2005</year>
		<url></url>
		<author>Vincent Puig</author>
		<author>Fabrice Guedy</author>
		<author>Michel Fingerhut</author>
		<author>Fabienne Serriere</author>
		<author>Jean Bresson</author>
		<author>Olivier Zeller</author>
		<authors>
			<first>Vincent</first>
		</authors>
		<authors>
			<last>Puig</last>
		</authors>
		<authors>
			<first>Fabrice</first>
		</authors>
		<authors>
			<last>Guedy</last>
		</authors>
		<authors>
			<first>Michel</first>
		</authors>
		<authors>
			<last>Fingerhut</last>
		</authors>
		<authors>
			<first>Fabienne</first>
		</authors>
		<authors>
			<last>Serriere</last>
		</authors>
		<authors>
			<first>Jean</first>
		</authors>
		<authors>
			<last>Bresson</last>
		</authors>
		<authors>
			<first>Olivier</first>
		</authors>
		<authors>
			<last>Zeller</last>
		</authors>
		<pages>419--422</pages>
		<title>Musique Lab 2: A three level approach for music education at school</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2e0f1be037c2664a8400a704827dab28f/alexarje</id>
		<tags>imported</tags>
		<description></description>
		<date>2016-02-26 20:58:13</date>
		<count>1</count>
		<booktitle>2nd International Workshop on Mobile Music Technology</booktitle>
		<address>Vancouver, Canada</address>
		<year>2005</year>
		<url></url>
		<author>T. Yamauchi</author>
		<author>T. Iwatake</author>
		<authors>
			<first>T.</first>
		</authors>
		<authors>
			<last>Yamauchi</last>
		</authors>
		<authors>
			<first>T.</first>
		</authors>
		<authors>
			<last>Iwatake</last>
		</authors>
		<title>Mobile User Interface for Music</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/27759867edf2ff90da05eceedda3948c6/alexarje</id>
		<tags>imported</tags>
		<description></description>
		<date>2016-02-26 20:58:13</date>
		<count>2</count>
		<journal>Computer Music Journal</journal>
		<year>2011</year>
		<url></url>
		<author>A. Kapur</author>
		<author>M. Darling</author>
		<author>D. Diakopoulos</author>
		<author>J. Murphy</author>
		<author>J. Hochenbaum</author>
		<author>O. Vallis</author>
		<author>C. Bahn</author>
		<authors>
			<first>A.</first>
		</authors>
		<authors>
			<last>Kapur</last>
		</authors>
		<authors>
			<first>M.</first>
		</authors>
		<authors>
			<last>Darling</last>
		</authors>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Diakopoulos</last>
		</authors>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Murphy</last>
		</authors>
		<authors>
			<first>J.</first>
		</authors>
		<authors>
			<last>Hochenbaum</last>
		</authors>
		<authors>
			<first>O.</first>
		</authors>
		<authors>
			<last>Vallis</last>
		</authors>
		<authors>
			<first>C.</first>
		</authors>
		<authors>
			<last>Bahn</last>
		</authors>
		<volume>35</volume>
		<number>4</number>
		<title>The Machine Orchestra: An Ensemble of Human Laptop Performers and Robotic Musical Instruments</title>
		<pubtype>article</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/23102d3822882db1ba329e5bd873f02fb/alexarje</id>
		<tags>imported</tags>
		<description></description>
		<date>2016-02-26 20:58:13</date>
		<count>2</count>
		<booktitle>Proceedings of the 1989 International Computer Music Conference</booktitle>
		<address>San Francisco</address>
		<year>1989</year>
		<url></url>
		<author>D. Huron</author>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Huron</last>
		</authors>
		<pages>131--133</pages>
		<title>Characterizing Musical Textures</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/24fe8573b59a2a15b017a2acb60d459ab/alexarje</id>
		<tags>Learning,</tags>
		<tags>instruments</tags>
		<tags>performance,</tags>
		<tags>NIME,</tags>
		<tags>Digital</tags>
		<tags>Music</tags>
		<tags>musical</tags>
		<description></description>
		<date>2016-02-26 20:58:13</date>
		<count>1</count>
		<booktitle>Proceedings of the International Conference on New Interfaces for Musical Expression</booktitle>
		<year>2005</year>
		<url></url>
		<author>S. Oore</author>
		<authors>
			<first>S.</first>
		</authors>
		<authors>
			<last>Oore</last>
		</authors>
		<title>Learning Advanced Skills on New Instruments</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/2288d3fb44231c17a2081c0ee840de765/alexarje</id>
		<tags>mapping</tags>
		<tags>or</tags>
		<tags>performers</tags>
		<tags>within</tags>
		<tags>motion</tags>
		<tags>h</tags>
		<tags>however;</tags>
		<tags>enrol;</tags>
		<tags>instruments;</tags>
		<tags>cre-;</tags>
		<tags>fail</tags>
		<tags>e;</tags>
		<tags>engage</tags>
		<tags>to</tags>
		<tags>musical</tags>
		<tags>Cognition;</tags>
		<tags>field;</tags>
		<tags>constraint;</tags>
		<tags>Embodied</tags>
		<tags>beyond</tags>
		<tags>instrument;</tags>
		<tags>constraint.;</tags>
		<tags>performance;</tags>
		<tags>those</tags>
		<tags>electronic</tags>
		<tags>the;</tags>
		<tags>quantity</tags>
		<tags>violin</tags>
		<tags>many</tags>
		<tags>Music</tags>
		<tags>capture;</tags>
		<tags>audiences;</tags>
		<tags>motion;</tags>
		<tags>of;</tags>
		<tags>movement;</tags>
		<tags>interaction;</tags>
		<tags>t;</tags>
		<tags>periodic</tags>
		<tags>analysis;</tags>
		<tags>their</tags>
		<tags>body</tags>
		<tags>ators</tags>
		<tags>strategies;</tags>
		<tags>instruments</tags>
		<tags>gesture;</tags>
		<tags>composed</tags>
		<tags>and</tags>
		<tags>transparency;</tags>
		<tags>human-computer</tags>
		<description></description>
		<date>2016-02-26 20:58:13</date>
		<count>2</count>
		<booktitle>Proceedings of the International Conference on New Interfaces for Musical Expression</booktitle>
		<address>University of Oslo</address>
		<year>2011</year>
		<url></url>
		<author>D. Overholt</author>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Overholt</last>
		</authors>
		<pages>4--7</pages>
		<title>The Overtone Fiddle: an Actuated Acoustic Instrument</title>
		<pubtype>inproceedings</pubtype>
	</publication>
	<publication>
		<type>Publication</type>
		<id>https://www.bibsonomy.org/bibtex/250288188d18a60d30d10cf6e96f50358/alexarje</id>
		<tags>imported</tags>
		<description></description>
		<date>2016-02-26 20:58:13</date>
		<count>2</count>
		<booktitle>Proceedings of the International Conference on New Interfaces for Musical Expression</booktitle>
		<publisher>ACM SIGCHI</publisher>
		<address>Seattle, WA</address>
		<year>2001</year>
		<url></url>
		<author>D. Overholt</author>
		<authors>
			<first>D.</first>
		</authors>
		<authors>
			<last>Overholt</last>
		</authors>
		<title>The MATRIX: A Novel Controller for Musical Expression</title>
		<pubtype>inproceedings</pubtype>
	</publication>
</publications>
